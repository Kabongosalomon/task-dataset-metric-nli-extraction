<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CBNet: A Composite Backbone Network Architecture for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Chu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
						</author>
						<title level="a" type="main">CBNet: A Composite Backbone Network Architecture for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Learning</term>
					<term>Object Detection</term>
					<term>Backbone Networks</term>
					<term>Composite Architectures</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern top-performing object detectors depend heavily on backbone networks, whose advances bring consistent performance gains through exploring more effective network structures. In this paper, we propose a novel and flexible backbone framework, namely CBNet, to construct high-performance detectors using existing open-source pre-trained backbones under the pre-training fine-tuning paradigm. In particular, CBNet architecture groups multiple identical backbones, which are connected through composite connections. Specifically, it integrates the high-and low-level features of multiple identical backbone networks and gradually expands the receptive field to more effectively perform object detection. We also propose a better training strategy with auxiliary supervision for CBNetbased detectors. CBNet has strong generalization capabilities for different backbones and head designs of the detector architecture. Without additional pre-training of the composite backbone, CBNet can be adapted to various backbones (i.e., CNN-based vs. Transformer-based) and head designs of most mainstream detectors (i.e., one-stage vs. two-stage, anchor-based vs. anchorfree-based). Experiments provide strong evidence that, compared with simply increasing the depth and width of the network, CBNet introduces a more efficient, effective, and resourcefriendly way to build high-performance backbone networks. Particularly, our CB-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO test-dev under the single-model and single-scale testing protocol, which are significantly better than the state-of-the-art results (i.e., 57.7% box AP and 50.2% mask AP) achieved by Swin-L, while reducing the training time by 6?. With multi-scale testing, we push the current best single model result to a new record of 60.1% box AP and 52.3% mask AP without using extra training data. Code is available at https://github.com/VDIGPKU/CBNetV2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O BJECT detection aims to locate each object instance from a predefined set of classes in an arbitrary image. It serves a wide range of applications such as autonomous driving, intelligent video surveillance, remote sensing, etc. In recent years, great progresses have been made for object detection thanks to the booming development of deep convolutional networks <ref type="bibr" target="#b1">[2]</ref>, and excellent detectors have been proposed, e.g., SSD <ref type="bibr" target="#b2">[3]</ref>, YOLO <ref type="bibr" target="#b3">[4]</ref>, Faster R-CNN <ref type="bibr" target="#b4">[5]</ref>, RetinaNet <ref type="bibr" target="#b5">[6]</ref>, ATSS <ref type="bibr" target="#b6">[7]</ref>, Mask R-CNN <ref type="bibr" target="#b7">[8]</ref>, Cascade R-CNN <ref type="bibr" target="#b8">[9]</ref>, etc.</p><p>Typically, in a Neural Network (NN)-based detector, a backbone network is used to extract basic features for detecting objects, and in most cases designed originally for image classification and pre-trained on ImageNet <ref type="bibr" target="#b9">[10]</ref>. Intuitively, the more representative features extracted by the backbone, the better the performance of its host detector. To obtain higher accuracy, deeper and wider backbones have been exploited by mainstream detectors (i.e., from mobile-size models <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and ResNet <ref type="bibr" target="#b12">[13]</ref>, to ResNeXt <ref type="bibr" target="#b13">[14]</ref> and Res2Net <ref type="bibr" target="#b14">[15]</ref>). Recently, Transformer <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> based backbones have shown very promising performance. Overall, advances in large backbone pre-training demonstrate a trend towards more effective multi-scale representations in object detection.</p><p>Encouraged by the results achieved by pre-trained large backbone-based detectors, we seek further improvement to construct high-performance detectors by exploiting existing well-designed backbone architectures and their pre-trained weights. Though one may design a new improved backbone, the expertise and computing resources overhead can be expensive. On the one hand, designing a new architecture of backbone requires expert experience and a lot of trials and errors. On the other hand, pre-training a new backbone (especially for large models) on ImageNet requires a large number of computational resources, which makes it costly to obtain better detection performance following the pre-training and fine-tuning paradigm. Alternatively, training detectors from scratch saves the cost of pre-training but requires even more computing resources and training skills <ref type="bibr" target="#b17">[18]</ref>.</p><p>In this paper, we present a simple and novel composition approach to use existing pre-trained backbones under the pretraining fine-tuning paradigm. Unlike most previous methods that focus on modular crafting and require pre-training on ImageNet to strengthen the representation, we improve the existing backbone representation ability without additional pre-training. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, our solution, named Composite Backbone Network (CBNet), groups multiple identical backbones together. Specifically, parallel backbones (named assisting backbones and lead backbone) are connected via composite connections. From left to right in <ref type="figure" target="#fig_0">Fig. 1</ref>, the output of each stage in an assisting backbone flows to the parallel and lower-level stages of its succeeding sibling. Finally, the features of the lead backbone are fed to the neck and detection head for bounding box regression and classification. Contrary to simple network deepening or widening, CBNet integrates the high-and low-level features of multiple backbone networks and progressively expands the receptive field for more effective arXiv:2107.00420v7 [cs.CV] 18 Oct 2022 object detection. Notably, each composed backbone of CBNet is initialized by the weights of an existing open-source pretrained individual backbone (e.g., CB-ResNet50 is initialized by the weights of ResNet50 <ref type="bibr" target="#b12">[13]</ref>, which are available in the open-source community). In addition, to further exploit the potential of CBNet, we propose an effective training strategy with supervision for assisting backbones, achieving higher detection accuracy while sacrificing no inference speed. In particular, we propose a pruning strategy to reduce the model complexity while not sacrificing accuracy. We present two versions of CBNet. The first, named CB-NetV1 <ref type="bibr" target="#b0">[1]</ref>, connects only the adjacent stages of parallel backbones, providing a simple implementation of our composite backbone that is easy to follow. The other one, CBNetV2, combines the dense higher-level composition strategy, the auxiliary supervision, and a special pruning strategy, to fully explore the potential of CBNet for object detection. We empirically demonstrate the superiority of CBNetV2 over CBNetV1.</p><p>We demonstrate the effectiveness of our framework by conducting experiments on the challenging MS COCO benchmark <ref type="bibr" target="#b18">[19]</ref>. Experiments show that CBNet has strong generalization capabilities for different backbones and head designs of the detector architecture, which enables us to train detectors that significantly outperform detectors based on larger backbones. Specifically, CBNet can be applied to various backbones, from convolution-based <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> to Transformerbased <ref type="bibr" target="#b19">[20]</ref>. Compared to the original backbones, CBNet boosts their performances by 3.4%?3.5% AP, demonstrating the effectiveness of the proposed CBNet. At comparable model complexity, our CBNet still improves by 1.1% ? 2.1% AP, indicating that the composed backbone is more efficient than the pre-trained wider and deeper networks. Moreover, CBNet can be flexibly plugged into mainstream detectors (e.g., Reti-naNet <ref type="bibr" target="#b5">[6]</ref>, ATSS <ref type="bibr" target="#b6">[7]</ref>, Faster R-CNN <ref type="bibr" target="#b4">[5]</ref>, Mask R-CNN <ref type="bibr" target="#b7">[8]</ref>, Cascade R-CNN and Cascade Mask R-CNN <ref type="bibr" target="#b8">[9]</ref>), and consistently improve the performances of these detectors by 3%?3.8% AP, demonstrating its strong adaptability to various head designs of detectors. Besides, CBNet is compatible with feature enhancing networks <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> and model ensemble method <ref type="bibr" target="#b22">[23]</ref>. Remarkably, it presents a general and resourcefriendly framework to drive the accuracy ceiling of high- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Object Detection</head><p>Object detection aims to locate each object instance from a predefined set of classes in an input image. With the rapid development of convolutional neural networks (CNNs), there is a popular paradigm for deep learning-based object detectors: the backbone network (typically designed for classification and pre-trained on ImageNet) extracts basic features from the input image, and then the neck (e.g., feature pyramid network <ref type="bibr" target="#b24">[25]</ref>) enhances the multi-scale features from the backbone, after which the detection head predicts the object bounding boxes with position and classification information. Based on detection heads, the cutting-edge methods for generic object detection can be briefly categorized into two major branches. The first branch contains one-stage detectors such as YOLO <ref type="bibr" target="#b3">[4]</ref>, SSD <ref type="bibr" target="#b2">[3]</ref>, RetinaNet <ref type="bibr" target="#b5">[6]</ref>, NAS-FPN <ref type="bibr" target="#b25">[26]</ref>, Ef-ficientDet <ref type="bibr" target="#b26">[27]</ref>, and <ref type="bibr" target="#b27">[28]</ref>. The other branch contains twostage methods such as Faster R-CNN <ref type="bibr" target="#b4">[5]</ref>, FPN <ref type="bibr" target="#b24">[25]</ref>, Mask  R-CNN <ref type="bibr" target="#b7">[8]</ref>, Cascade R-CNN <ref type="bibr" target="#b8">[9]</ref>, and Libra R-CNN <ref type="bibr" target="#b28">[29]</ref>.</p><p>Recently, academic attention has been geared toward anchorfree detectors due partly to the emergence of FPN <ref type="bibr" target="#b24">[25]</ref> and focal Loss <ref type="bibr" target="#b5">[6]</ref>, where more elegant end-to-end detectors have been proposed. On the one hand, FSAF <ref type="bibr" target="#b29">[30]</ref>, FCOS <ref type="bibr" target="#b30">[31]</ref>, ATSS <ref type="bibr" target="#b6">[7]</ref> and GFL <ref type="bibr" target="#b31">[32]</ref> improve RetinaNet with center-based anchor-free methods. On the other hand, CornerNet <ref type="bibr" target="#b32">[33]</ref>, CenterNet <ref type="bibr" target="#b33">[34]</ref>, and FoveaBox <ref type="bibr" target="#b34">[35]</ref> detect object bounding boxes with a keypoint-based method. In addition to the above CNN-based detectors, Transformer <ref type="bibr" target="#b15">[16]</ref> has also been utilized for detection. DETR <ref type="bibr" target="#b35">[36]</ref> proposes a fully end-to-end detector by combining CNN and Transformer encoder-decoders. More recently, Neural Architecture Search (NAS) is applied to automatically search the architecture for a specific detector. NAS-FPN <ref type="bibr" target="#b25">[26]</ref>, NAS-FCOS <ref type="bibr" target="#b36">[37]</ref> and SpineNet <ref type="bibr" target="#b37">[38]</ref> use reinforcement learning to control the architecture sampling and obtain promising results. SM-NAS <ref type="bibr" target="#b38">[39]</ref> uses the evolutionary algorithm and partial order pruning method to search the optimal combination of different parts of the detector. Auto-FPN <ref type="bibr" target="#b39">[40]</ref> uses the gradient-based method to search for the best detector. OPANAS <ref type="bibr" target="#b40">[41]</ref> uses the one-shot method to search for an efficient neck for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Backbones for Object Detection</head><p>Starting from AlexNet <ref type="bibr" target="#b1">[2]</ref>, deeper and wider backbones have been exploited by mainstream detectors, such as VGG <ref type="bibr" target="#b41">[42]</ref>, ResNet <ref type="bibr" target="#b12">[13]</ref>, DenseNet <ref type="bibr" target="#b42">[43]</ref>, ResNeXt <ref type="bibr" target="#b13">[14]</ref>, and Res2Net <ref type="bibr" target="#b14">[15]</ref>. Since the backbone network is usually designed for classification, whether it is pre-trained on ImageNet and finetuned on a given detection dataset or trained from scratch on the detection dataset, it requires many computational resources and is difficult to optimize. Recently, two non-trivially designed backbones, i.e., DetNet <ref type="bibr" target="#b43">[44]</ref> and FishNet <ref type="bibr" target="#b44">[45]</ref>, are specifically designed for the detection task. However, they still require pre-training for the classification task before finetuning for the detection task. Res2Net <ref type="bibr" target="#b14">[15]</ref> achieves impressive results in object detection by representing multi-scale features at the granular level. HRNet <ref type="bibr" target="#b20">[21]</ref> maintains high-resolution representations and achieves promising results in human pose estimation, semantic segmentation, and object detection. In addition to manually designing the backbone architecture, DetNAS <ref type="bibr" target="#b45">[46]</ref> and Joint-DetNAS <ref type="bibr" target="#b46">[47]</ref> use NAS to search for a better backbone for object detection, thereby reducing the cost of manual design. Swin Transformer <ref type="bibr" target="#b19">[20]</ref> and PVT <ref type="bibr" target="#b16">[17]</ref> utilize Transformer modular to build the backbone and achieve impressive results, despite the need for expensive pre-training.</p><p>It is well known that designing and pre-training a new and robust backbone requires significant computational costs. Alternatively, we propose a more economical and efficient solution to build a more powerful object detection backbone, by grouping multiple identical existing backbones (e.g., ResNet <ref type="bibr" target="#b12">[13]</ref>, ResNeXt <ref type="bibr" target="#b13">[14]</ref>, Res2Net <ref type="bibr" target="#b14">[15]</ref>, HRNet <ref type="bibr" target="#b20">[21]</ref>, and Swin Transformer <ref type="bibr" target="#b19">[20]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Recurrent Convolution Neural Network</head><p>Different from the feed-forward architecture of CNN, Recurrent CNN (RCNN) <ref type="bibr" target="#b23">[24]</ref> incorporates recurrent connections into each convolution layer to enhance the contextual information integration ability of the model. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, our proposed Composite Backbone Network shares some similarities with the unfolded RCNN <ref type="bibr" target="#b23">[24]</ref>, but they are very different. First, the connections between the parallel stages in CBNet are unidirectional, while they are bidirectional in RCNN. Second, in RCNN, the parallel stages at different time steps share parameter weights, while in the proposed CBNet, the parallel stages of backbones are independent of each other. Moreover, we need to pre-train RCNN on ImageNet if we use it as the backbone of the detector. By contrast, CBNet does not require additional pre-training because it directly uses existing pre-trained weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Ensemble</head><p>It is well known that a combination of many different predictors can lead to more accurate predictions, e.g., ensemble methods are considered as the state-of-the-art solution for many machine learning challenges. The model ensemble improves the prediction performance of a single model by training multiple different models and combining their prediction results through post processing <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>.</p><p>There are two key characteristics for model ensemble: model diversity and voting. Model diversity means that the models with different architectures or training techniques are trained separately, and its importance for the model ensemble is well established <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Most ensemble methods need voting strategies to compare the outputs of different models and refine the final predictions <ref type="bibr" target="#b22">[23]</ref>. In terms of the above two characteristics, our CBNet is very different from the model ensemble. In fact, CBNet benefits from the identical backbones grouping, the recurrent style feature enhancing by jointly training. Furthermore, the output of the lead backbone is used directly for the final prediction without the need to be assembled with other backbones. More practical analysis can be found in Sec. IV-E2.</p><p>In practice, leading approaches to the challenge object detection benchmarks like MS COCO <ref type="bibr" target="#b18">[19]</ref> or OpenImage <ref type="bibr" target="#b53">[54]</ref> are based on the usage of model ensemble <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>. For example, <ref type="bibr" target="#b59">[60]</ref> separately trains 28 models of different architectures, heads, data splits, class sampling strategies, augmentation strategies and supervisions and aggregate these detector's outputs by ensembling method. <ref type="bibr" target="#b22">[23]</ref> proposes the Probabilistic Ranking Aware Ensemble (PRAE) that refines the confidence of bounding boxes from different detectors. Our CBNet is compatible with such model ensemble methods, as are other conventional backbones. More details can be found in Sec. IV-F6. E. Our Approach.</p><p>Our network groups multiple identical backbones in parallel. It integrates the high-and low-level features of multiple identical backbones and gradually expands the receptive field to more efficiently perform object detection. This paper represents a very substantial extension of our previous conference paper <ref type="bibr" target="#b0">[1]</ref> with results under recently developed start-of-theart object detection frameworks. The main technical novelties compared with <ref type="bibr" target="#b0">[1]</ref> lie in three aspects. (1) We extend the network (named as CBNetV1) proposed in <ref type="bibr" target="#b0">[1]</ref>, with three modifications: a specialized training method, a better composite strategy and a pruning strategy, which respectively optimizes the training process, more efficiently enhances feature representation and reduces the model complexity of CBNetV2. <ref type="bibr" target="#b1">(2)</ref> We show the strong generalization capabilities of CBNetV2 for various backbones and head designs of detector architecture.</p><p>(3) We show the superiority of CBNetV2 over CBNetV1 and present the state-of-art result of CBNetV2 in object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>This section elaborates the proposed CBNet in details. In Sec. III-A and Sec. III-B, we describe its basic architecture and variants, respectively. In Sec. III-C, we propose a training strategy for CBNet-based detectors. In Sec. III-D, we briefly introduce the pruning strategy. In Sec. III-E, we summarize the detection framework of CBNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture of CBNet</head><p>The proposed CBNet consists of K identical backbones (K ? 2). In particular, we call the case K = n as CB-Backbone-Kn, where '-Kn' is omitted when K = 2.</p><p>As in <ref type="figure" target="#fig_0">Fig. 1</ref>, the CBNet architecture includes two types of backbones: lead backbone B K and assisting backbones B 1 , B 2 , ..., B K?1 . Each backbone comprises L stages (usually L = 5), and each stage consists of several convolutional layers with feature maps of the same size. The l-th stage of the backbone implements the non-linear transformation F l (?)(l = 1, 2, ..., L).</p><p>Most conventional convolutional networks follow the design of encoding the input images into intermediate features with monotonically decreased resolution. In particular, the l-th stage takes the output (denoted as x l?1 ) of the previous (l ? 1)-th stage as input, which can be expressed as follows:</p><formula xml:id="formula_0">x l = F l (x l?1 ), l ? 2.<label>(1)</label></formula><p>Differently, we adopt assisting backbones B 1 , B 2 , ..., B K?1 to improve the representative ability of lead backbone B K . We iterate the features of a backbone to its successor in a stageby-stage fashion. Thus, Equation <ref type="formula" target="#formula_0">(1)</ref> can be rewritten as:</p><formula xml:id="formula_1">x l k = F l k (x l?1 k + g l?1 (x k?1 )), l ? 2, k = 2, 3, . . . , K,<label>(2)</label></formula><p>where g l?1 (?) represents the composite connection, which takes features (denoted as x k?1 = {x i k?1 |i = 1, 2, . . . , L}) from assisting backbone B k?1 as input and takes the features of the same size as x l?1 k as output. Therefore, the output features of B k?1 are transformed and contribute to the input of each stage in B k . Note that x 1 1 , x 1 2 , . . . , x 1 K are weight sharing. For the object detection task, only the output features of the lead backbone {x i K , i = 2, 3, . . . , L} are fed into the neck and then the RPN/detection head, while the outputs of the assisting backbone are forwarded to its succeeding siblings. It is worth noting that B 1 , B 2 , ..., B K?1 can be used for various backbone architectures (e.g., ResNet <ref type="bibr" target="#b12">[13]</ref>, ResNeXt <ref type="bibr" target="#b13">[14]</ref>, Res2Net <ref type="bibr" target="#b14">[15]</ref>, and Swin Transformer <ref type="bibr" target="#b19">[20]</ref>) and initialized directly from the pre-trained weights of a single backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Possible Composite Strategies</head><p>For composite connection g l (x) which takes x = {x i |i = 1, 2, . . . , L} from an assisting backbone as input and outputs a feature of the same size of x l (omitting k for simplicity), we propose the following five different composite strategies.</p><p>1) Same Level Composition (SLC): An intuitive and simple way of compositing is to fuse the output features from the same stage of backbones. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.a, the operation of SLC can be formulated as:</p><formula xml:id="formula_2">g l (x) = w(x l ), l ? 2,<label>(3)</label></formula><p>where w represents a 1 ? 1 convolution layer and a batch normalization layer.  2) Adjacent Higher-Level Composition (AHLC): Motivated by Feature Pyramid Networks <ref type="bibr" target="#b24">[25]</ref>, the top-down pathway introduces the spatially coarser, but semantically stronger, higher-level features to enhance the lower-level features of the bottom-up pathway, we introduce AHLC to feed the output of the adjacent higher-level stage of the previous backbone to the subsequent one (from left to right in <ref type="figure" target="#fig_1">Fig. 2</ref></p><formula xml:id="formula_3">.b): g l (x) = U(w(x l+1 )), l ? 1,<label>(4)</label></formula><p>where U(?) indicates the up-sampling operation.</p><p>3) Adjacent Lower-Level Composition (ALLC): Contrary to AHLC, we introduce a bottom-up pathway to feed the output of the adjacent lower-level stage of the previous backbone to the succeeding one. We show ALLC in <ref type="figure" target="#fig_1">Fig. 2</ref>.c, which is formulated as:</p><formula xml:id="formula_4">g l (x) = D(w(x l?1 )), l ? 2,<label>(5)</label></formula><p>where D(?) denotes the down-sample operation. 4) Dense Higher-Level Composition (DHLC): In DenseNet <ref type="bibr" target="#b42">[43]</ref>, each layer is connected to all subsequent layers to build comprehensive features. Inspired by this, we utilize dense composite connections in our CBNet architecture. The operation of DHLC is expressed as follows:</p><formula xml:id="formula_5">g l (x) = L i=l+1 U(w i (x i )), l ? 1.<label>(6)</label></formula><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.d, when K = 2, we compose the features from all the higher-level stages in the previous backbone and add them to the lower-level stages in the latter one. 5) Full-connected Composition (FCC): As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.e, we compose features from all the stages in the previous backbones and feed them to each stage in the following one. Compared to DHLC, we add connections in low-highlevel case. The operation of FCC can be expressed as:</p><formula xml:id="formula_6">g l (x) = L i=2 I(w i (x i )), l ? 1,<label>(7)</label></formula><p>where I(?) denotes scale-resizing, I(?) = D(?) when i &gt; l, and I(?) = U(?) when i &lt; l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Auxiliary Supervision</head><p>Although increasing the depth usually leads to performance improvement <ref type="bibr" target="#b12">[13]</ref>, it may introduce additional optimization difficulties, as in the case of image classification <ref type="bibr" target="#b60">[61]</ref>. The studies in <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref> introduce the auxiliary classifiers of intermediate layers to improve the convergence of very deep networks. In original CBNet, although the composite backbones are parallel, the latter backbone (e.g., lead backbone in <ref type="figure" target="#fig_4">Fig. 4</ref>.a) deepen the network through adjacent connections between the previous backbone (e.g., assisting backbone in <ref type="figure" target="#fig_4">Fig. 4</ref>.a). To better train the CBNet-based detector, We propose to generate initial results of assisting backbones by supervision with the auxiliary neck and detection head to provide additional regularization.</p><p>An example of our supervised CBNet when K=2 is illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>.b. Apart from the original loss that uses the lead backbone feature to train the detection head 1, another detection head 2 takes assisting backbone features as input, producing auxiliary supervision. Note that detection head 1 and detection head 2 are weight sharing, as are the two necks. The auxiliary supervision helps to optimize the learning process, while the original loss for the lead backbone takes the greatest responsibility. We add weights to balance the auxiliary supervision, where the total loss is defined as:</p><formula xml:id="formula_7">L = L Lead + K?1 i=1 (? i ? L i Assist ).<label>(8)</label></formula><p>where L Lead is the loss of lead backbone, L Assist is the loss of assisting backbones, and ? i is the loss weight for the i-th assisting backbone. During the inference phase, we abandon the auxiliary supervision branch and only utilize the output features of the lead backbone in CBNet ( <ref type="figure" target="#fig_4">Fig. 4.b)</ref>. Consequently, auxiliary supervision does not affect the inference speed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Pruning Strategy for CBNet</head><p>To reduce the model complexity of CBNet, we explore the possibility of pruning the different number of stages in 2, 3, ..., K-th backbones instead of composing the backbones in a holistic manner. For simplicity, we show five pruning methods when K = 2 in <ref type="figure" target="#fig_5">Fig. 5</ref>. s i indicates there are i stages {x j |j ? 6 ? i and j ? 5, i = 0, 1, 2, 3, 4} in the 2, 3, ..., K-th backbone and the pruned stages are filled by the features of the same stages in the first backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Architecture of Detection Network with CBNet</head><p>CBNet can be applied to various off-the-shelf detectors without additional modifications to network architectures. In practice, we attach the lead backbone with functional networks, e.g., FPN <ref type="bibr" target="#b24">[25]</ref> and detection head. The inference phase of CBNet is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Note that we present two versions of CBNet. The first one, named CBNetV1 <ref type="bibr" target="#b0">[1]</ref>, uses only AHLC composition strategy, providing a simple implementation of the composite backbone that is easy to follow. The other one, CBNetV2, combines DHLC composition strategy, the auxiliary supervision, and a special pruning strategy, to fully explore the potential of CBNet for object detection. We empirically demonstrate the superior of CBNetV2 over CB-NetV1 in the following Sec. IV. In this paper, CBNet denotes CBNetV2 in the following experiments if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate our CBNet through extensive experiments. In Sec. IV-A, we detail the experimental setup. In Sec. IV-B, we compare CBNet with state-of-the-art detection methods. In Sec. IV-C, we demonstrate the generality of our method over different backbones and detectors. In Sec. IV-E, we show the compatibility of CBNet with DCN and model ensemble. In Sec. IV-F, we conduct an extensive ablation study to investigate individual components of our framework.</p><p>A. Implementation details 1) Datasets and Evaluation Criteria: We conduct experiments on the COCO <ref type="bibr" target="#b18">[19]</ref> benchmark. The training is conducted on the 118k training images, and ablation studies on the 5k minival images. We also report the results on the 20k images in test-dev for comparison with the state-of-the-art (SOTA) methods. For evaluation, we adopt the metrics from the COCO detection evaluation criteria, including the mean Average Precision (AP) across IoU thresholds ranging from 0.5 to 0.95 at different scales.</p><p>2) Training and Inference Details: Our experiments are based on the open-source detection toolbox MMDetection <ref type="bibr" target="#b70">[71]</ref>. For ablation studies and simple comparisons, we resize the input size to 800 ? 500 during training and inference if not specified. We choose Faster R-CNN (ResNet50 <ref type="bibr" target="#b12">[13]</ref>) with FPN <ref type="bibr" target="#b24">[25]</ref> as the baseline. We use the SGD optimizer with an initial learning rate of 0.02, the momentum of 0.9, and 10 ?4 as weight decay. We train detectors for 12 epochs with a learning rate decreased by 10? at epoch 8 and 11. We use only random flip for data augmentation and set the batch size to 16. Note that experiments related to special backbones, e.g., Swin Transformer <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b71">[72]</ref>, HRNet <ref type="bibr" target="#b20">[21]</ref>, PVT <ref type="bibr" target="#b16">[17]</ref>, PVTv2 <ref type="bibr" target="#b72">[73]</ref>, and DetectoRS <ref type="bibr" target="#b73">[74]</ref>, are not highlighted specifically following the hyper-parameters of the original papers. The inference speed FPS (frames per second) for the detector is measured on a machine with 1 V100 GPU.</p><p>To compare with state-of-the-art detectors, we utilize multiscale training <ref type="bibr" target="#b74">[75]</ref> (the short side resized to 400 ? 1400 and the long side is at most 1600) and a longer training schedule (details can be found in Sec. IV-B). During the inference phase, we use Soft-NMS <ref type="bibr" target="#b75">[76]</ref> with a threshold of 0.001, and the input size is set to 1600?1400. All other hyper-parameters in this paper follow MMDetection if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with State-of-the-Art</head><p>We compare our methods with cutting-edge detectors. We divide the results into object detection <ref type="table" target="#tab_1">(Table I</ref>) and instance segmentation <ref type="table" target="#tab_1">(Table II)</ref> according to whether or not the instance segmentation annotations are used during training. Following <ref type="bibr" target="#b19">[20]</ref>, we improve the detector heads of Cascade R-CNN, Cascade Mask R-CNN, and HTC in the above two tables by adding four convolution layers <ref type="bibr" target="#b76">[77]</ref> in each bounding box head and using GIoU loss <ref type="bibr" target="#b77">[78]</ref> instead of Smooth L1 <ref type="bibr" target="#b78">[79]</ref>.</p><p>1) Object Detection: For detectors trained with only bounding box annotations, we summarize them into two categories: anchor-based, and anchor-free-based in <ref type="table" target="#tab_1">Table I</ref>. We select ATSS <ref type="bibr" target="#b6">[7]</ref> as the anchor-free representative, and Cascade R-CNN as the anchor-based representative. Anchor-free. CB-Res2Net101-DCN equipped with ATSS is trained for 20 epochs, where the learning rate is decayed  by 10? in the 16th and 19th epochs. Notably, our CB-Res2Net101-DCN achieves 52.8% AP, outperforming previous anchor-free methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b63">[64]</ref> under single-scale testing protocol.</p><p>Anchor-based. Our CB-Res2Net101-DCN achieves 55.6% AP, surpassing other anchor-based detectors <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b79">[80]</ref>. It is worth noting that our CBNet trains only for 32 epochs (the first 20 epochs are regular training and the remaining 12 epochs are trained with Stochastic Weights Averaging <ref type="bibr" target="#b80">[81]</ref>), being 16? and 12? shorter than EfficientDet and YOLOv4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Instance Segmentation:</head><p>We further compare our method with state-of-the-art results <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref> using both bounding box and instance segmentation annotations in Table II. Following <ref type="bibr" target="#b19">[20]</ref>, we provide results with the backbone pre-trained on regular ImageNet-1K and ImageNet-22K to show the high capacity of CBNet.</p><p>Results with regular ImageNet-1K pre-train. Following <ref type="bibr" target="#b19">[20]</ref>, 3x schedule (36 epochs with the learning rate decayed by 10? at epochs 27 and 33) is used for CB-Swin-S. Using Cascade Mask R-CNN, our CB-Swin-S achieves 56.3% box AP and 48.6% mask AP on COCO minival in terms of the bounding box and instance segmentation, showing significant gains of +4.4% box AP and +3.6% mask AP to Swin-B with similar model size and the same training protocol. In addition, CB-Swin-S achieves 56.9% box AP and 49.1% mask AP on COCO dev, outperforming other ImageNet-1K pre-trained backbone-based detectors.</p><p>Results with ImageNet-22K pre-train. Our CB-Swin-B achieves single-scale result of 58.4% box AP and 50.7% mask AP on COCO minival, which is 1.3% box AP and 1.2% mask AP higher than that of Swin-L (HTC++) <ref type="bibr" target="#b19">[20]</ref> while the number of parameters is decreased by 17% and the training schedule is reduced by 3.6?. Especially, with only 12 epochs  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalization Capability of CBNet</head><p>CBNet expands the receptive field by combining the backbones in parallel rather than simply increasing the depth of the network. To demonstrate the generality of our design strategy, we perform experiments on various backbones and different head designs of the detector architecture.</p><p>1) Generality for Main-stream Backbone Architectures: Effectiveness. To demonstrate the effectiveness of CBNet, we conduct experiments on Faster R-CNN with different backbone architectures. As shown in <ref type="table" target="#tab_1">Table III</ref>, for CNN-based backbones (e.g., ResNet, ResNeXt-32x4d, and Res2Net), our method can boost baseline by over 3.4% AP. Efficiency. Note that the number of parameters in CBNet has increased compared to the baseline. To better demonstrate the efficiency of the composite architecture, we compare CBNet with deeper and wider backbone networks. As shown in <ref type="table" target="#tab_1">Table IV</ref>, with comparable number of number and inference speed, CBNet improves ResNet101, ResNeXt101-32x4d, Res2Net101 by 1.7%, 2.1%, and 1.1% AP, respectively. Additionally, CB-ResNeXt50-32x4d is 1.1% AP higher than that of ResNeXt101-64x4d, while the number of parameters is only 70%. The results demonstrate that our composite  backbone architecture is more efficient and effective than simply increasing the depth and width of the network.</p><p>2) Generality for Swin Transformer: Transformer is notable for the use of attention to model long-range dependencies in data, and Swin Transformer <ref type="bibr" target="#b19">[20]</ref> is one of the most representative recent arts. We conduct experiments on Swin Transformer to show the model generality of CBNet. For a fair comparison, we follow the same training strategy as <ref type="bibr" target="#b19">[20]</ref> with multi-scale training (the short side resized to 480 ? 800 and the long side at most 1333), AdamW optimizer (initial learning rate of 0.0001, weight decay of 0.05, and batch size of 16), and 3x schedule (36 epochs). As shown in <ref type="table" target="#tab_5">Table V</ref>, the accuracy of the model slowly increases as the Swin Transformer is deepened and widened, and saturates at Swin-S. Swin-B is only 0.1% AP higher than that of Swin-S, but the amount of parameters increases by 38M. When using CB-Swin-T, we achieve 53.6% box AP and 46.2% mask AP by improving Swin-T 3.1% box AP and 2.5% mask AP. Surprisingly, our CB-Swin-T is 1.7% box AP and 1.2% mask AP higher than that of the deeper and wider Swin-B while the model complexity is lower (e.g., FLOPs 836G vs. 975G, Params 113.8M vs. 145.0M, FPS 6.5 vs. 5.9). These results prove that CBNet can also improve non-pure convolutional architectures. They also demonstrate that CBNet pushes the upper limit of accuracy for highperformance detectors more effectively than simply increasing the depth and width of the network.</p><p>3) Generality for special backbones: To further show the generality of CBNet for various backbones, we conduct experiments on CBNet equipped with different backbones including MobileNetV2 <ref type="bibr" target="#b71">[72]</ref>, HRNet <ref type="bibr" target="#b20">[21]</ref>, PVT <ref type="bibr" target="#b16">[17]</ref>, and PVTv2 <ref type="bibr" target="#b72">[73]</ref>.   <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b72">[73]</ref>. Our CB-PVT-Small improves PVT-Small by 3% AP and is 0.8% AP higher than PVT-Large with only 83% number of parameters. Furthermore, our CB-PVTv2-B2 improves PVTv2-B2 by 3.1% AP and is 1.6% AP higher than PVTv2-B5 with only 66% number of parameters. The results show that our CBNet improves a wide variety of backbones and achieves better accuracy under comparable or less parameters and FLOPs, which verify the effectiveness and efficiency of CBNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Model Adaptability for Mainstream Detectors:</head><p>We evaluate the adaptability of CBNet by plugging it into mainstream detectors such as RetinaNet, ATSS, Faster R-CNN, Mask R-CNN, and Cascade R-CNN. These methods present a variety of detector head designs (e.g., two-stage vs. one-stage, anchorbased vs. anchor-free). As shown in <ref type="table" target="#tab_1">Table VII</ref>, our CBNet significantly boosts all popular object detectors by over 3% AP. The instance segmentation accuracy of Mask R-CNN is also improved by 2.9% AP. These results demonstrate the robust adaptability of CBNet to various head designs of detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with Relevant Works.</head><p>There are several relevant detectors, such as DetectoRS <ref type="bibr" target="#b73">[74]</ref> that composites both backbone and FPN and Joint-DetNAS <ref type="bibr" target="#b46">[47]</ref> searches for the model scaling strategy. We conduct comparisons between CBNet and these two methods. Joint-DetNAS <ref type="bibr" target="#b46">[47]</ref> integrates neural architecture search (NAS), pruning and knowledge distillation for optimizing detectors. Similarly, our CBNet also uses pruning strategy but focus more on scaling backbones using composite strategy. Thanks to the strong generalization ability, our CBNet can boosts the performance of advanced high-performance detectors (e.g., YOLOX <ref type="bibr" target="#b83">[84]</ref>). As shown in <ref type="table" target="#tab_1">Table VIII</ref>, our CB-CSPNet-L improves CSPNet-L [84] by 2.6% AP and is 1.1% AP higher than CSPNet-X with only 85% number of parameters. We further compare our CBNet using an existing handdesigned detector (i.e., YOLOX) with Joint-DetNAS which uses an advanced knowledge distillation training strategy. Our CBNet achieves 52% AP with 118 GFLOPs, superior to that of Joint-DetNAS (X101-FPN based) at 45.7% AP with 266 GFLOPs. Note that it is hard to have a fair comparison because our CBNet focuses on the architecture design of the backbone while Joint-DetNAS focuses on the joint optimization of the architecture and training for the entire detector.</p><p>DetectoRS <ref type="bibr" target="#b73">[74]</ref> conducts a similar design as CBNet while DetectoRS composites both backbone and FPN. We compare CBNetV2 and DetectoRS with different backbones on Faster R-CNN in <ref type="table" target="#tab_1">Table IX</ref>. Under the same training strategy of DetectoRS with 1333 ? 800 as input size, CBNet achieves comparable or higher AP with fewer FLOPs. Specifically, with advanced backbone Swin-Tiny, our CBNet outperforms DetectoRS by 0.8% AP with only 84% FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Compatibility of CBNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Compatibility with Deformable Convolution:</head><p>Deformable convolution <ref type="bibr" target="#b21">[22]</ref> enhances the transformation modeling capability of CNNs and is widely used for accurate object detectors (e.g., simply adding DCN improves Faster R-CNN ResNet50 from 34.6% to 37.4% AP). To show the compatibility of CBNet architecture with deformable convolution, we perform experiments on ResNet and ResNeXt equipped with Faster R-CNN. As shown in <ref type="table" target="#tab_10">Table X</ref>, DCN is still effective on CBNet with 2.3% AP?2.7% AP improvement. This improvement is greater than the 2.0% AP and 1.3% AP increments on ResNet152 and ResNeXt101-64x4d. On the other hand, CB-ResNet50-DCN increases the AP of ResNet50-DCN and the deeper ResNet152-DCN by 3.0% and 0.6%, respectively. In addition, CB-ResNet50-32x4d-DCN increases the AP of ResNet50-32x4d-DCN and the deeper and wider ResNeXt101-64x4d-DCN by 3.7% and 1.3%, respectively.  The results show that the effects of CBNet and deformable convolution can be superimposed without conflicting with each other.</p><p>2) Compatibility with Model Ensemble: The model ensemble improves the prediction performance of a single model by training multiple different models and combining their prediction results through post-processing <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Probabilistic Ranking Aware Ensemble (PRAE) <ref type="bibr" target="#b22">[23]</ref> refines the confidence of bounding boxes from different detectors and outperforms other ensemble learning methods for object detection by significant margins (e.g., assembling Faster R-CNN ResNet50 and Faster R-CNN ReNeXt50 improves the single model best AP from 36.3% to 37.3%), Note that assembling two same detectors (i.e., two Faster R-CNN ResNeXt50) does not improve the performance (same as the single detector 36.3% AP). To show the compatibility of our CBNet architecture with the model ensemble method PRAE, we perform experiments on traditional backbones (i.e., ResNet, ResNeXt, Res2Net) and their Composite Backbones equipped with Faster R-CNN. As shown in <ref type="table" target="#tab_1">Table XI</ref>, PRAE is still effective for assembling detectors with CBNet, with 0.8% ? 1.7% AP improvement, which is consistent with the case of assembling detectors with traditional backbones. In addition, CBNet is more effective than the model ensembling method PRAE, e.g., Faster R-CNN CB-R50 achieves 38.0% AP, superior to the 37.3% AP of assembling Faster R-CNN ResNet50 and Faster R-CNN ReNeXt50. The results show that the effects of CBNet and  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablation Studies</head><p>We ablate various design choices for our proposed CBNet. For simplicity, all accuracy results here are on the COCO validation set with 800 ? 500 input size if not specified.</p><p>1) Effectiveness of Different Composite Strategies: We conduct experiments to compare the proposed composite strategies in <ref type="figure" target="#fig_1">Fig. 2</ref>, including SLC, AHLC, ALLC, DHLC and FCC. All these experiments are conducted based on the Faster R-CNN CB-ResNet50 architecture. Results are shown in <ref type="table" target="#tab_1">Table XII.</ref> SLC gets a slightly improves accuracy of the singlebackbone baseline (35% vs. 34.6% AP). The features extracted by the same stage of both backbones are similar, and thus SLC can only learn slightly more semantic information than a single backbone does.</p><p>AHLC raises the baseline by 1.4% AP, which verifies our motivation in Sec. III-B2, i.e., the semantic information higher-level features of the former backbone enhances the representation ability of the latter backbone.</p><p>ALLC degrades the performance of the baseline by 2.2% AP. We infer that directly adding the lower-level features of the assisting backbone to the higher-level ones of the lead backbone impair the representation ability of the latter.</p><p>DHLC improves the performance of the baseline by a large margin (from 34.6% AP to 37.3% AP by 2.7% AP). More composite connections of the high-low cases enrich the representation ability of features to some extent.</p><p>FCC achieves the best performance of 37.4% AP while being 7% slower than DHLC (19.9 vs. 21.4 FPS).</p><p>In summary, FCC and DHLC achieve the two best results. Considering the computational simplicity, we recommend using DHLC for CBNet. All the above composite strategies have a similar amount of parameters, but the accuracy varies greatly. The results prove that simply increasing the number of We conduct a grid search by proxy task to search for better composite strategies. To reduce the search cost, we simplify the search space by only searching the connections including x 3 , x 4 , x 5 stages in composite backbones, and design a proxy task with 1/5 of the COCO training set with input size set to 800 ? 500. In this way, we only need to train (2 3 ) 3 = 512 detectors for 205 GPU days. The best-searched strategy is a simplified DHLC(s 3 ) without the connection between x 4 of the former backbone to the input of x 3 of the latter one. The searched strategy achieves 37.3% AP with 69.1 M, 126 GFLOPs, and performs un-par with our designed DHLC (37.3% AP with 69.7 M, 127 GFLOPs), further validating the necessity of high-to-low connections in our handcraft design.</p><p>2) Weights for Auxiliary Supervision: Experimental results related to weighting the auxiliary supervision are presented in <ref type="table" target="#tab_1">Table XIV</ref>. For simplicity, we perform DHLC composite strategy on CBNet. The first setting is the Faster R-CNN CB-ResNet50 baseline and the second is the CB-ResNet50-K3 (K = 3 in CBNet) baseline, where the ? for assisting backbone in Equation <ref type="formula" target="#formula_7">(8)</ref> is set to zero. For the case K = 2, the baseline can be improved by 0.8% AP by setting ? 1 to 0.5. For the case K = 3, the baseline can be improved by 1.8% AP by setting {? 1 , ? 2 } to {0.5, 1.0}. The experimental results verify that the auxiliary supervision forms an effective training strategy that improves the performance of CBNet.</p><p>3) Efficiency of Pruning Strategy: As shown in <ref type="figure" target="#fig_6">Fig. 6a</ref>, with the pruning strategy, our CB-ResNet50 family and CB-ResNet50-K3 family achieve better FLOPs-accuracy trade-offs than ResNet family. This also illustrates the efficiency of our pruning strategy. In particular, the number of FLOPs in s 3 is reduced by 10% compared to s 4 , but the accuracy is decreased by only 0.1%. This is because the weights of the pruned stage are fixed during the detector training <ref type="bibr" target="#b70">[71]</ref> so pruning this stage does not sacrifice detection accuracy. Hence, when speed and memory cost need to be prioritized, we suggest pruning the fixed stages in 2, 3, ..., K-th backbones in CBNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Number of Backbones in CBNet:</head><p>To further explore the ability to construct high-performance detectors of CBNet, we evaluate the efficiency of our CBNet by controlling the number of backbones. As shown in <ref type="figure" target="#fig_6">Fig. 6b</ref>, we vary the number of backbones (e.g., K = 1,2,3,4,5) and compare their accuracy and efficiency (GFLOPs) with the ResNet family. Note that the accuracy continues to increase as the complexity of the model increases. Compared with ResNet152, our method obtains higher accuracy at K=2 while computation cost is lower. Meanwhile, the accuracy can be further improved for K=3,4,5. CBNet provides an effective and efficient alternative to improve the model performance rather than simply increasing the depth or width of the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Comparison of CBNetV1 and CBNetV2:</head><p>To fairly compare CBNetV1 <ref type="bibr" target="#b0">[1]</ref> and CBNetV2, we progressively apply the DHLC composite strategy, auxiliary supervision, and pruning strategy to CBNetV1, where AHLC is the default composite strategy in <ref type="table" target="#tab_5">Table XV</ref>. As in the 1st and 2nd rows of <ref type="table" target="#tab_5">Table XV</ref>, the composite backbone structure CBNetV1 <ref type="bibr" target="#b0">[1]</ref> improves the Faster R-CNN ResNet50 baseline by 1.4% AP. As in the 2nd and 3rd row, the accelerated version of CBNetV1 (s 2 pruning version in <ref type="figure" target="#fig_5">Fig. 5</ref>) improves the inference speed from 22.4 FPS to 26.6 FPS while decreasing the accuracy by 0.4% AP. As in the 2nd and 4th rows, the auxiliary supervision brings a 0.9% AP increment to CBNetV1, thanks to the better training strategy that improves the representative ability of the lead backbone. Note that the auxiliary supervision does not introduce extra parameters during the inference phase. As in the 2nd and 5th rows, DHLC composite strategy improves the detection performance of CBNetV1 by 1.3% AP with higher model complexity. The results confirm that DHLC enables a larger receptive field, with features at each level obtaining rich semantic information from all higher-level features. As in the 1st and 6th rows, when combining the DHLC and the auxiliary supervision, there is a significant improvement of 2.1% AP over the baseline. As in the 2nd and last row, when we perform our default pruning strategy (s 3 version in <ref type="figure" target="#fig_5">Fig. 5</ref>), CBNetV2 is faster (23.3 vs. 22.4 FPS) and much more accurate (38.0% vs. 36.0% AP) than CBNetV1 <ref type="bibr" target="#b0">[1]</ref>. DHLC slows down the detector, while the pruning strategy effectively speeds up the inference speed of CBNetV2. 6) Importance of Identical Backbones for CBNet: To verify the necessity of identical backbones in CBNet, we explore the diversity backbones by compositing ResNet50, ResNet101, Res2Net50, and Res2Net101. Note that no pruning is conducted for compositing diverse backbones and backbones from different families do not share the stem layer (Conv1 in <ref type="figure" target="#fig_2">Fig. 3</ref>). As shown in <ref type="table" target="#tab_1">Table XVI</ref>, for backbones belonging to the same family, compositing identical backbones outperforms compositing diverse ones. For example, CB-ResNet50 achieves higher AP with fewer parameters than both ResNet50-C-ResNet101 and ResNet101-C-ResNet50. Similarly, CB-Res2Net50 gains higher or comparable AP with fewer parameters than both Res2Net50-C-Res2Net101 and Res2Net101-C-Res2Net50. For backbones from different families, the observation still holds. For example, CB-Res2Net50 achieves better performance than ResNet50-C-Res2Net101, Res2Net101-C-ResNet50, ResNet101-C-Res2Net50, and Res2Net50-C-ResNet101. These experimental results indicate that increasing the diversity of composite models is not the most efficient way for CBNet. We believe the reason is that using different backbones needs different optimization strategies, which usually output very different learned features and are difficult for joint training. CBNet intends to learn similar features for each grouped backbone, and the stronger the former backbones are, the more representative features the lead backbone outputs. Our experiments show that such a jointtraining strategy works best for identical backbone grouping.</p><p>This validates the necessity of the identical backbones in CBNet and further distinguishes our approach from ensemble methods where diversity is a key character.</p><p>V. CONCLUSION In this paper, we propose a novel and flexible backbone framework, called Composite Backbone Network (CBNet), to improve the performance of cutting-edge object detectors. CBNet consists of a series of backbones with the same network architecture in parallel, the Dense Higher-Level composition strategy, and the auxiliary supervision. Together they construct a robust representative backbone network that uses existing pre-trained backbones under the pre-training finetuning paradigm. CBNet has strong generalization capabilities </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of the proposed Composite Backbone Network (CBNet) architecture for object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Five kinds of composite strategies for Composite Backbone architecture when K = 2. (a) Same Level Composition (SLC). (b) Adjacent Higher-Level Composition (AHLC). (c) Adjacent Lower-Level Composition (ALLC). (d) Dense Higher-Level Composition (DHLC). (e) Full-connected Composition (FCC). The composite connections are colored lines representing some operations such as element-wise operation, scaling, 1?1 Conv layer, and BN layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison between our proposed CBNet architecture (K = 2) and the unrolled architecture of RCNN [24] (T = 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>(a) CBNetV1<ref type="bibr" target="#b0">[1]</ref> (K = 2). (b) CBNetV2 (K = 2) with auxiliary supervision. The two FPNs and detection heads share the same weights. It is worth noting that the main differences between CBNetV2 and CBNetV1 are in composite strategies and training strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Illustration of different pruning strategies for composite backbone when K = 2. s i indicates there are i stages {x j |j ? 6 ? i and j ? 5, i = 0, 1, 2, 3, 4} in the 2, 3, ..., K-th backbone and the pruned stages are filled by the features of the same stages in the first backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Performance comparison of CBNet with different numbers of composite backbones (K) and pruning strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>performance detectors. Without bells and whistles, our CB-Swin-L achieves unparalleled single-model single-scale result of 59.4% box AP and 51.6% mask AP on COCO test-dev, surpassing the state-of-the-art result (i.e., 57.7% box AP and 50.2% mask AP obtained by Swin-L), while reducing the training schedule by 6?. With multi-scale testing, we push the current best single-model result to a new record of 60.1% box AP and 52.3% mask AP.</figDesc><table /><note>The main contributions of this paper are listed as follows:? We propose a general, efficient and effective framework, CBNet (Composite Backbone Network), to construct high-performance backbone networks for object detection without additional pre-training.? We propose a Dense Higher-Level Composition (DHLC) strategy, auxiliary supervision, and a pruning strategy to efficiently use existing pre-trained weights for object detection under the pre-training fine-tuning paradigm.? Our CB-Swin-L achieves a new record of single-model single-scale result on COCO at a shorter (by 6?) train- ing schedule than Swin-L. With multi-scale testing, our method achieves the best-known result without extra training data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Comparison with the state-of-the-art results on COCO test-dev. Our CB-Res2Net101-DCN achieves higher bbox AP over previous anchor-free and anchor-based detectors while using comparable or fewer training epochs.</figDesc><table><row><cell></cell><cell>Method</cell><cell>AP box</cell><cell>AP box 50</cell><cell>AP box 75</cell><cell>AP box S</cell><cell>AP box M</cell><cell>AP box L</cell><cell>Params</cell><cell>Epochs</cell></row><row><cell></cell><cell>FSAF [30]</cell><cell>42.9</cell><cell>63.8</cell><cell>46.3</cell><cell>26.6</cell><cell>46.2</cell><cell>52.7</cell><cell>94 M</cell><cell>24</cell></row><row><cell>anchor-free-based</cell><cell>FCOS [31] DETR-DC5 [36] NAS-FCOS [37] ATSS [7] GFL [32] Deformable DETR [64] PAA [65]</cell><cell>44.7 44.9 46.1 47.7 48.2 50.1 50.8</cell><cell>64.1 64.7 -66.5 67.4 69.7 69.7</cell><cell>48.4 47.7 -51.9 52.6 54.6 55.1</cell><cell>27.6 23.7 -29.7 29.2 30.6 31.4</cell><cell>47.5 49.5 -50.8 51.7 52.8 54.7</cell><cell>55.6 62.3 -59.4 60.2 64.7 65.2</cell><cell>90 M 60 M 89 M 95 M 53 M -129 M</cell><cell>24 500 24 24 24 50 24</cell></row><row><cell></cell><cell>CB-Res2Net101-DCN (ATSS)</cell><cell>52.7</cell><cell>71.1</cell><cell>57.7</cell><cell>35.7</cell><cell>56.6</cell><cell>62.7</cell><cell>107 M</cell><cell>20</cell></row><row><cell></cell><cell>Auto-FPN [40]</cell><cell>44.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90 M</cell><cell>24</cell></row><row><cell></cell><cell>SM-NAS:E5 [39]</cell><cell>45.9</cell><cell>64.6</cell><cell>49.6</cell><cell>27.1</cell><cell>49.0</cell><cell>58.0</cell><cell>-</cell><cell>24</cell></row><row><cell>anchor-based</cell><cell>NAS-FPN [26] SP-NAS [66] Joint-DetNAS [47] SpineNet-190 [38] OPANAS [41]</cell><cell>48.3 49.1 50.7 52.1 52.2</cell><cell>-67.1 69.6 71.8 71.3</cell><cell>-53.5 55.4 56.5 57.3</cell><cell>-31 31.3 35.4 33.3</cell><cell>-52.6 53.8 55 55.6</cell><cell>-63.7 64.0 63.6 65.4</cell><cell>---164 M 83 M</cell><cell>150 50 16 250 24</cell></row><row><cell></cell><cell>EfficientDet-D7x [27]</cell><cell>55.1</cell><cell>74.3</cell><cell>59.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>77 M</cell><cell>600</cell></row><row><cell></cell><cell>YOLOv4-P7 [67]</cell><cell>55.5</cell><cell>73.4</cell><cell>60.8</cell><cell>38.4</cell><cell>59.4</cell><cell>67.7</cell><cell>288 M</cell><cell>450</cell></row><row><cell></cell><cell>CB-Res2Net101-DCN (Cascade R-CNN)</cell><cell>55.6</cell><cell>73.7</cell><cell>60.8</cell><cell>37.4</cell><cell>59.0</cell><cell>67.6</cell><cell>146 M</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell>Pre-trained on</cell><cell>Method</cell><cell cols="2">mini-val AP box AP mask</cell><cell cols="2">test-dev AP box AP mask</cell><cell>Params</cell><cell>Epochs</cell></row><row><cell></cell><cell>GCNet  *  [68]</cell><cell>51.8</cell><cell>44.7</cell><cell>52.3</cell><cell>45.4</cell><cell>-</cell><cell>36</cell></row><row><cell></cell><cell>Swin-B [20]</cell><cell>51.9</cell><cell>45.0</cell><cell>-</cell><cell>-</cell><cell>145 M</cell><cell>36</cell></row><row><cell>ImageNet-1K</cell><cell>ResNeSt-200  *  [69]</cell><cell>52.5</cell><cell>-</cell><cell>53.3</cell><cell>47.1</cell><cell>-</cell><cell>36</cell></row><row><cell></cell><cell>CopyPaste [70]  ?</cell><cell>55.9</cell><cell>47.2</cell><cell>56.0</cell><cell>47.4</cell><cell>185 M</cell><cell>96</cell></row><row><cell></cell><cell>CB-Swin-S (Cascade Mask R-CNN)</cell><cell>56.3</cell><cell>48.6</cell><cell>56.9</cell><cell>49.1</cell><cell>156 M</cell><cell>36</cell></row><row><cell></cell><cell>Swin-L (HTC++) [20]</cell><cell>57.1</cell><cell>49.5</cell><cell>57.7</cell><cell>50.2</cell><cell>284 M</cell><cell>72</cell></row><row><cell></cell><cell>Swin-L (HTC++) [20]  *</cell><cell>58.0</cell><cell>50.4</cell><cell>58.7</cell><cell>51.1</cell><cell>284 M</cell><cell>72</cell></row><row><cell>ImageNet-22K</cell><cell>CB-Swin-B (HTC) CB-Swin-B (HTC)  *</cell><cell>58.4 58.9</cell><cell>50.7 51.3</cell><cell>58.7 59.3</cell><cell>51.1 51.8</cell><cell>235 M 235 M</cell><cell>20 20</cell></row><row><cell></cell><cell>CB-Swin-L (HTC)</cell><cell>59.1</cell><cell>51.0</cell><cell>59.4</cell><cell>51.6</cell><cell>453 M</cell><cell>12</cell></row><row><cell></cell><cell>CB-Swin-L (HTC)  *</cell><cell>59.6</cell><cell>51.8</cell><cell>60.1</cell><cell>52.3</cell><cell>453 M</cell><cell>12</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>: Comparison with the state-of-the-art object detection and instance segmentation results on COCO. In collaboration with Swin Transformer, our CBNetV2 achieves the state-of-the-art box AP and mask AP while using fewer training epochs.* indicates method with multi-scale testing.2 ? indicates additional unlabeled data are used to pretrain backbone.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Comparison between CBNet and conventional backbones in terms of different architectures. Our CBNet boosts the performance of CNN-based backbones by over 3.4% AP.</figDesc><table><row><cell>Backbone</cell><cell cols="2">AP box Origin CB-</cell><cell>?AP</cell></row><row><cell>ResNet50</cell><cell>34.6</cell><cell>38.0</cell><cell>+3.4</cell></row><row><cell>ResNeXt50-32x4d</cell><cell>36.3</cell><cell>39.8</cell><cell>+3.5</cell></row><row><cell>Res2Net50</cell><cell>37.7</cell><cell>41.2</cell><cell>+3.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="5">: Comparison between CB-Backbone and deeper</cell></row><row><cell cols="5">and wider single backbones in terms of different architec-</cell></row><row><cell cols="5">tures. The backbones in each group are sorted by FLOPs for</cell></row><row><cell cols="5">efficiency comparison. Backbones armed with our proposed</cell></row><row><cell cols="5">method are more efficient than their own wider and deeper</cell></row><row><cell>version.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell>AP box</cell><cell>Params</cell><cell>FLOPs</cell><cell>FPS</cell></row><row><cell>ResNet101</cell><cell>36.3</cell><cell>60.5 M</cell><cell>121 G</cell><cell>25.8</cell></row><row><cell>CB-ResNet50</cell><cell>38.0</cell><cell>69.4 M</cell><cell>121 G</cell><cell>23.3</cell></row><row><cell>ResNet152</cell><cell>37.8</cell><cell>76.2 M</cell><cell>151 G</cell><cell>21.3</cell></row><row><cell>ResNeXt101-32x4d</cell><cell>37.7</cell><cell>60.2 M</cell><cell>122 G</cell><cell>20.8</cell></row><row><cell>CB-ResNeXt50-32x4d</cell><cell>39.8</cell><cell>68.4 M</cell><cell>123 G</cell><cell>19.3</cell></row><row><cell>ResNeXt101-64x4d</cell><cell>38.7</cell><cell>99.3 M</cell><cell>183 G</cell><cell>15.8</cell></row><row><cell>Res2Net101</cell><cell>40.1</cell><cell>61.2 M</cell><cell>125 G</cell><cell>20.7</cell></row><row><cell>CB-Res2Net50</cell><cell>41.2</cell><cell>69.7 M</cell><cell>125 G</cell><cell>20.2</cell></row><row><cell>Res2Net200</cell><cell>41.7</cell><cell>92.2 M</cell><cell>185 G</cell><cell>14.8</cell></row></table><note>training (which is 6? shorter than Swin-L), our CB-Swin- L achieves 59.4% box AP and 51.6% mask AP on COCO test-dev, outperforming prior arts. We can push the current best result to a new record of 60.1% box AP and 52.3% mask AP through multi-scale testing. The results demonstrate that CBNet proposes an efficient, effective, and resource-friendly framework to build high-performance detectors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Comparison between CB-Swin-T and the Swin equipped with Cascade Mask R-CNN. CBNet is more effective and efficient than the wider and deeper version of Swin-Transformer.</figDesc><table><row><cell>Backbone</cell><cell>AP box</cell><cell>AP mask</cell><cell>Params</cell><cell cols="2">FLOPs FPS</cell></row><row><cell>Swin-T</cell><cell>50.5</cell><cell>43.7</cell><cell>85.6 M</cell><cell>742 G</cell><cell>7.8</cell></row><row><cell>Swin-S</cell><cell>51.8 +1.3</cell><cell>44.7 +1.0</cell><cell>107.0 M</cell><cell>832 G</cell><cell>7.0</cell></row><row><cell>Swin-B</cell><cell>51.9 +1.4</cell><cell>45.0 +1.3</cell><cell>145.0 M</cell><cell>975 G</cell><cell>5.9</cell></row><row><cell>CB-Swin-T</cell><cell>53.6 +3.1</cell><cell>46.2 +2.5</cell><cell>113.8 M</cell><cell>836 G</cell><cell>6.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Generalization capability of CBNet over various special backbones.</figDesc><table><row><cell>Detector</cell><cell>Backbone</cell><cell cols="3">mAP Params FLOPs</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell>22.2</cell><cell>3.74 M</cell><cell>1.69 G</cell></row><row><cell>YOLOV3</cell><cell>CB-MobileNetV2</cell><cell>25.4</cell><cell>5.20 M</cell><cell>2.27 G</cell></row><row><cell></cell><cell>MobileNetV2 (1.4x)</cell><cell>24.4</cell><cell>5.04 M</cell><cell>2.21 G</cell></row><row><cell></cell><cell>HRNetv2p w32</cell><cell>40.2</cell><cell>47.3 M</cell><cell>285 G</cell></row><row><cell>Faster-RCNN</cell><cell>CB-HRNetv2p w32</cell><cell>42.6</cell><cell>76.8 M</cell><cell>449 G</cell></row><row><cell></cell><cell>HRNetv2p w48</cell><cell>42.0</cell><cell>83.4 M</cell><cell>460 G</cell></row><row><cell></cell><cell>PVT-Small</cell><cell>40.4</cell><cell>34.2 M</cell><cell>214 G</cell></row><row><cell></cell><cell>CB-PVT-Small</cell><cell>43.4</cell><cell>58.9 M</cell><cell>278 G</cell></row><row><cell>RetinaNet</cell><cell>PVT-Large</cell><cell>42.6</cell><cell>71.1 M</cell><cell>309 G</cell></row><row><cell></cell><cell>PVTv2-B2</cell><cell>44.6</cell><cell>35.1 M</cell><cell>218 G</cell></row><row><cell></cell><cell>CB-PVTv2-B2</cell><cell>47.7</cell><cell>60.7 M</cell><cell>287 G</cell></row><row><cell></cell><cell>PVTv2-B5</cell><cell>46.1</cell><cell>91.7 M</cell><cell>335 G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Comparison of ResNet50 and our CB-ResNet50. CB-ResNet50 significantly boosts all popular object detectors by 3.0% ? 3.8% bbox AP and 2.9% mask AP. 'R50' is short for 'ResNet50'.</figDesc><table><row><cell>Detector</cell><cell cols="2">AP box R50 CB-R50</cell><cell>?AP</cell><cell cols="2">AP mask R50 CB-R50</cell><cell>?AP</cell></row><row><cell>RetinaNet</cell><cell>33.2</cell><cell>36.2</cell><cell>+3.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ATSS</cell><cell>36.9</cell><cell>39.9</cell><cell>+3.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Faster R-CNN 34.6</cell><cell>38.0</cell><cell>+3.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mask R-CNN</cell><cell>35.2</cell><cell>39.0</cell><cell cols="2">+3.8 31.8</cell><cell>34.7</cell><cell>+2.9</cell></row><row><cell cols="2">Cascade R-CNN 38.2</cell><cell>41.2</cell><cell>+3.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII :</head><label>VIII</label><figDesc>Comparison of CBNet with YOLOX and Joint-DetNAS.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell cols="3">mAP Params FLOPs</cell></row><row><cell>CSPNet-L</cell><cell></cell><cell>49.4</cell><cell>54.2 M</cell><cell>78 G</cell></row><row><cell>CSPNet-X</cell><cell>YOLOX</cell><cell>50.9</cell><cell>99.1 M</cell><cell>141 G</cell></row><row><cell>CB-CSPNet-L</cell><cell></cell><cell>52.0</cell><cell>83.8 M</cell><cell>118 G</cell></row><row><cell cols="2">Searched X101-FPN Joint-DetNAS</cell><cell>45.7</cell><cell>-</cell><cell>266 G</cell></row><row><cell cols="5">For a fair comparison, we choose the publicly available pre-</cell></row><row><cell cols="5">trained backbones and all experiment settings (e.g., choice of</cell></row><row><cell cols="5">detectors, training, and inference details) are following their</cell></row><row><cell cols="5">settings in MMDetection [82]. Results are shown in Table VI.</cell></row><row><cell cols="5">For Mobile settings with YOLOV3 [83], our CB-MobileNetV2</cell></row><row><cell cols="5">improves MobileNetV2 by 3.1% AP and is 1% AP higher</cell></row><row><cell cols="5">than MobileNetV2(1.4x) with comparable model complexity.</cell></row><row><cell cols="5">For backbones with high-resolution representations, our CB-</cell></row><row><cell cols="5">HRNetv2p w32 improves HRNetv2p w32 by 2.4% AP and</cell></row><row><cell cols="5">is 0.6% AP higher than HRNetv2p w48 with less model</cell></row><row><cell cols="5">complexity. For global transformer backbones, we choose</cell></row><row><cell cols="4">RetinaNet as detector follow the original paper</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX :</head><label>IX</label><figDesc>Comparison between CBNet and DetecoRS based on Faster-RCNN with 1? training. CBNet achieves on par or better accuracy-efficiency trade-offs.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell>mAP</cell><cell>Params</cell><cell>FLOPs</cell></row><row><cell>ResNet101</cell><cell>CBNet DetectoRS</cell><cell>42.7 42.8</cell><cell>107.4 M 104.0 M</cell><cell>436 G 512 G</cell></row><row><cell>ResNeXt101-32x4d</cell><cell>CBNet DetectoRS</cell><cell>44.2 44.0</cell><cell>106.7 M 103.5 M</cell><cell>444 G 519 G</cell></row><row><cell>Res2Net101</cell><cell>CBNet DetectoRS</cell><cell>45.8 45.7</cell><cell>108.7 M 105.4 M</cell><cell>452 G 533 G</cell></row><row><cell>Swin-Tiny</cell><cell>CBNet DetectoRS</cell><cell>46.7 45.9</cell><cell>74.1 M 73.4 M</cell><cell>307 G 366 G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE X :</head><label>X</label><figDesc>Experimental results on the compatibility of CBNet and deformable convolution. CBNet and deformable convolution can be superimposed on each other without conflict.</figDesc><table><row><cell>Backbone</cell><cell cols="2">AP box w/o DCN w/ DCN</cell><cell>?AP</cell></row><row><cell>ResNet50</cell><cell>34.6</cell><cell>37.4</cell><cell>+2.8</cell></row><row><cell>ResNet152</cell><cell>37.8</cell><cell>39.8</cell><cell>+2.0</cell></row><row><cell>CB-ResNet50</cell><cell>38.0</cell><cell>40.4</cell><cell>+2.4</cell></row><row><cell>ResNeXt50-32x4d</cell><cell>36.3</cell><cell>38.6</cell><cell>+2.3</cell></row><row><cell>ResNeXt101-64x4</cell><cell>38.7</cell><cell>41.0</cell><cell>+2.3</cell></row><row><cell>CB-ResNeXt50-32x4</cell><cell>39.8</cell><cell>42.5</cell><cell>+2.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XI</head><label>XI</label><figDesc></figDesc><table><row><cell cols="4">: Compatibility of CBNet and Model Ensemble</cell></row><row><cell cols="4">Method. They can be superimposed on each other with-</cell></row><row><cell cols="4">out conflict. 'R50', 'X50', 'R2-50', 'R101', 'X101', 'R2-</cell></row><row><cell cols="4">101' are short for Faster R-CNN equipped with ResNet50,</cell></row><row><cell cols="4">ResNeXt50-32x4d, Res2Net50, ResNet101, ResNeXt101-</cell></row><row><cell cols="4">64x4, Res2Net101 respectively. 'Single' denotes single model</cell></row><row><cell>best AP.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ensemble Models</cell><cell cols="2">AP box Single Ensem.</cell><cell>?AP</cell></row><row><cell>R50(34.6) &amp; X50(36.3)</cell><cell>36.3</cell><cell>37.3</cell><cell>1.0</cell></row><row><cell>R101(36.3) &amp; X101(38.7)</cell><cell>38.7</cell><cell>39.7</cell><cell>1.0</cell></row><row><cell>CB-R50(38.0) &amp; CB-X50(39.8)</cell><cell>39.8</cell><cell>40.8</cell><cell>1.0</cell></row><row><cell>R50(34.6) &amp; R2-50(37.7)</cell><cell>37.7</cell><cell>38.8</cell><cell>1.1</cell></row><row><cell>R101(36.3) &amp; R2-101(40.1)</cell><cell>40.1</cell><cell>40.9</cell><cell>0.8</cell></row><row><cell>CB-R50(38.0) &amp; CB-R2-50(41.2)</cell><cell>41.2</cell><cell>42.2</cell><cell>1.0</cell></row><row><cell>X50(36.3) &amp; R2-50(37.7)</cell><cell>37.7</cell><cell>39.4</cell><cell>1.7</cell></row><row><cell>X101(38.7) &amp; R2-101(40.1)</cell><cell>40.1</cell><cell>41.9</cell><cell>1.8</cell></row><row><cell>CB-X50(39.8) &amp; CB-R2-50(41.2)</cell><cell>41.2</cell><cell>42.9</cell><cell>1.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XII :</head><label>XII</label><figDesc>Comparison between different composite strategies. Obviously, DHLC acheives the best FLOPs-accuracy and Params-accuracy trade-offs.</figDesc><table><row><cell cols="5">Composite Strategy AP box Params FLOPs FPS</cell></row><row><cell>-</cell><cell>34.6</cell><cell>41.5 M</cell><cell>90 G</cell><cell>30.0</cell></row><row><cell>SLC</cell><cell>35.0</cell><cell>64.8 M</cell><cell>123 G</cell><cell>22.6</cell></row><row><cell>AHLC</cell><cell>36.0</cell><cell>67.6 M</cell><cell>126 G</cell><cell>22.4</cell></row><row><cell>ALLC</cell><cell>32.4</cell><cell>67.6 M</cell><cell>133 G</cell><cell>22.5</cell></row><row><cell>DHLC</cell><cell>37.3</cell><cell>69.7 M</cell><cell>127 G</cell><cell>21.4</cell></row><row><cell>FCC</cell><cell>37.4</cell><cell>72.0 M</cell><cell>145 G</cell><cell>19.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XIII :</head><label>XIII</label><figDesc>Comparison between AHLC and DHLC on different backbones.</figDesc><table><row><cell>Backbone</cell><cell cols="3">Composite AP box Params FLOPs</cell></row><row><cell>CB-ResNet101</cell><cell>AHLC DHLC</cell><cell>37.9 38.7</cell><cell>105.6 M 186 G 107.6 M 188 G</cell></row><row><cell>CB-ResNeXt101</cell><cell>AHLC DHLC</cell><cell>39.9 41.0</cell><cell>183.0 M 312 G 185.1 M 313 G</cell></row><row><cell cols="4">model ensemble can be superimposed without conflicting with</cell></row><row><cell cols="4">each other, suggesting that the detector equipped with CBNet</cell></row><row><cell cols="4">should be considered as a single detector/model despite having</cell></row><row><cell cols="4">multiple identical backbones compositions.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XIV :</head><label>XIV</label><figDesc>Ablation study of loss weights for assistant supervision. ] and this paper for DHLC and AHLC. The reason is that CBNetV1 and this paper are performed under different deep learning platforms (CAFFE vs. PyTorch), which use different model initialization strategies and result in different model performances. We compare DHLC and AHLC on different backbones inTable XIII. DHLC outperforms AHLC by 0.8% AP and 1.1% AP on ResNet101 and ResNeXt101-64x4d, respectively, showing the generality of DHLC for different backbones.</figDesc><table><row><cell>Backbone</cell><cell>? 1</cell><cell>? 2</cell><cell>AP bbox</cell></row><row><cell></cell><cell>0</cell><cell>-</cell><cell>37.3</cell></row><row><cell></cell><cell>0.125</cell><cell>-</cell><cell>37.6</cell></row><row><cell>CB-ResNet50</cell><cell>0.25</cell><cell>-</cell><cell>37.8</cell></row><row><cell></cell><cell>0.5</cell><cell>-</cell><cell>38.1</cell></row><row><cell></cell><cell>1.0</cell><cell>-</cell><cell>37.9</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>37.4</cell></row><row><cell></cell><cell>0.25</cell><cell>0.25</cell><cell>37.9</cell></row><row><cell>CB-ResNet50-K3</cell><cell>0.25</cell><cell>0.5</cell><cell>38.9</cell></row><row><cell></cell><cell>0.5</cell><cell>0.5</cell><cell>38.6</cell></row><row><cell></cell><cell>0.5</cell><cell>1.0</cell><cell>39.2</cell></row><row><cell cols="4">parameters or adding a backbone network does not guarantee</cell></row><row><cell cols="4">a better result while the composite connection plays a crucial</cell></row><row><cell cols="4">part. These results show that the suggested DHLC composite</cell></row><row><cell cols="2">strategy is effective and nontrivial.</cell><cell></cell><cell></cell></row><row><cell cols="4">Note that there is a minor performance gap between the</cell></row><row><cell>original CBNetV1 in [1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE XV :</head><label>XV</label><figDesc>Comparison between CBNetV1 [1] and CB-NetV2. CBNetV2 is superior to CBNetV1 in terms of accuracy and complexity. DHLC Sup. Prun. AP box Params FLOPs FPS</figDesc><table><row><cell>Backbone ResNet50</cell><cell>34.6</cell><cell>41.5 M 90 G 30.0</cell></row><row><cell>CBNetV1</cell><cell>36.0</cell><cell>67.6 M 126 G 22.4</cell></row><row><cell>CBNetV1</cell><cell>35.6</cell><cell>66.2 M 111 G 26.6</cell></row><row><cell></cell><cell>36.9</cell><cell>67.6 M 126 G 22.4</cell></row><row><cell></cell><cell>37.3</cell><cell>69.7 M 127 G 21.4</cell></row><row><cell></cell><cell>38.1</cell><cell>69.7 M 127 G 21.4</cell></row><row><cell>CBNetV2</cell><cell>38.0</cell><cell>69.4 M 121 G 23.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE XVI :</head><label>XVI</label><figDesc>Importance of identical backbones for CBNet. Compositing two identical ResNet50 achieves better performance than compositing ResNet50 and ResNet101. for different backbones and head designs of the detector architecture. Extensive experimental results demonstrate that the proposed CBNet is compatible with various backbones networks, including CNN-based (ResNet, ResNeXt, Res2Net) and Transformer-based (Swin-Transformer) ones. At the same time, CBNet is more effective and efficient than simply increasing the depth and width of the network. Furthermore, CB-Net can be flexibly plugged into most mainstream detectors, including one-stage (e.g., RetinaNet) and two-stage (Faster R-CNN, Mask R-CNN, Cascade R-CNN, and Cascade Mask R-CNN) detectors, as well as anchor-based (e.g., Faster R-CNN) and anchor-free-based (ATSS) ones. CBNet is compatible with feature enhancing networks (DCN and HRNet) and model ensemble methods. Specifically, the performances of the above detectors are increased by over 3% AP. In particular, our CB-Swin-L achieves a new record of 59.4% box AP and 51.6% mask AP on COCO test-dev, outperforming prior singlemodel single-scale results. With multi-scale testing, we achieve a new state-of-the-art result of 60.1% box AP and 52.3% mask AP without extra training data.</figDesc><table><row><cell>Backbone</cell><cell>AP box</cell><cell>Params</cell><cell>FLOPs</cell></row><row><cell>ResNet50-C-ResNet101</cell><cell>37.8</cell><cell>88.7 M</cell><cell>157 G</cell></row><row><cell>ResNet101-C-ResNet50</cell><cell>37.8</cell><cell>88.7 M</cell><cell>157 G</cell></row><row><cell>CB-ResNet50</cell><cell>38.0</cell><cell>69.4 M</cell><cell>121 G</cell></row><row><cell>Res2Net50-C-Res2Net101</cell><cell>41.1</cell><cell>89.5 M</cell><cell>163 G</cell></row><row><cell>Res2Net101-C-Res2Net50</cell><cell>41.4</cell><cell>89.5 M</cell><cell>163 G</cell></row><row><cell>CB-Res2Net50</cell><cell>41.2</cell><cell>69.7 M</cell><cell>125 G</cell></row><row><cell>CB-Res2Net101</cell><cell>43.0</cell><cell>108.7 M</cell><cell>188 G</cell></row><row><cell>ResNet50-C-Res2Net101</cell><cell>40.0</cell><cell>89.3 M</cell><cell>162 G</cell></row><row><cell>Res2Net101-C-ResNet50</cell><cell>41.2</cell><cell>89.3 M</cell><cell>162 G</cell></row><row><cell>ResNet101-C-Res2Net50</cell><cell>39.5</cell><cell>88.8 M</cell><cell>162 G</cell></row><row><cell>Res2Net50-C-ResNet101</cell><cell>40.6</cell><cell>88.8 M</cell><cell>162 G</cell></row><row><cell>CB-Res2Net50</cell><cell>41.2</cell><cell>69.7 M</cell><cell>125 G</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by National Natural Science Foundation of China under Grant 62176007. This work was also a research achievement of Key Laboratory of Science, Technology, and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). We thank Dr. Han Hu, Prof. Ming-Ming Cheng and Shang-Hua Gao for the insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cbnet: A novel composite backbone network architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Res2net: A new multi-scale backbone architecture,&quot; TPAMI, 2021. 1, 2, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rethinking training from scratch for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Probabilistic ranking-aware ensembles for enhanced object detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03139.2</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">NAS-FPN: learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parallel residual bi-fusion feature pyramid network for accurate single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">FCOS: fully convolutional onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchor-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">NAS-FCOS: fast neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spinenet: Learning scale-permuted backbone for recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SM-NAS: structural-tomodular neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Auto-fpn: Automatic network architecture adaptation for object detection beyond classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Opanas: One-shot path aggregation network architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Detnet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fishnet: A versatile backbone for image, region, and pixel level prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Detnas: Backbone search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint-detnas: Upgrade your detector with nas, pruning and dynamic distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural network ensembles, cross validation, and active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vedelsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ensemble learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WIDM</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Ensemble machine learning: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Diversity in neural network ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Birmingham</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Diversity creation methods: a survey and categorisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Wyatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">One-shot neural ensemble architecture search by diversity-guided search space shrinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>2020. 4</idno>
		<title level="m">The open images dataset V4</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Object detection networks on convolutional feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>2017. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>2017. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02475.4</idno>
		<title level="m">Joint COCO and mapillary workshop at ICCV 2019: COCO instance segmentation challenge track</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">1st place solutions for openimage2019 -object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07557.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<idno>2020. 7</idno>
		<editor>ECCV, A. Vedaldi, H. Bischof, T. Brox, and J. Frahm</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">SP-NAS: serial-toparallel backbone search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scaled-yolov4: Scaling cross stage partial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno>2021. 7</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Global context networks,&quot; TPAMI, 2020. 7</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Splitattention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pvt v2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Soft-NMSimproving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">IJCV</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Dynamic head: Unifying object detection heads with attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Swa object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12645</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Yolox: Exceeding yolo series in 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
