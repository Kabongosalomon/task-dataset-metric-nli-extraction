<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MRL: Learning to Mix with Attention and Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlok</forename><surname>Mohta</surname></persName>
							<email>shlok.mohta@sony.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sony Group Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisahiro</forename><surname>Suganuma</surname></persName>
							<email>hisahiro.suganuma@sony.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sony Group Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiki</forename><surname>Tanaka</surname></persName>
							<email>yoshiki.tanaka@sony.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sony Group Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MRL: Learning to Mix with Attention and Convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a new neural architectural block for the vision domain, named Mixing Regionally and Locally (MRL), developed with the aim of effectively and efficiently mixing the provided input features. We bifurcate the input feature mixing task as mixing at a regional and local scale. To achieve an efficient mix, we exploit the domain-wide receptive field provided by self-attention for regionalscale mixing and convolutional kernels restricted to local scale for local-scale mixing. More specifically, our proposed method mixes regional features associated with local features within a defined region, followed by a local-scale features mix augmented by regionally features. Experiments show that this hybridization of selfattention and convolution brings improved capacity, generalization (right inductive bias), and efficiency. Under similar network settings, MRL outperforms or is at par with its counterparts in classification, object detection, and segmentation tasks. We also show that our MRL-based network architecture achieves state-of-the-art performance for H&amp;E histology datasets. We achieved DICE of 0.843, 0.855, and 0.892 for Kumar, CoNSep, and CPM-17 datasets, respectively, while highlighting the versatility offered by the MRL framework by incorporating layers like group convolutions to improve dataset-specific generalization. * Corresponding Author.</p><p>Preprint. Under review. arXiv:2208.13975v1 [cs.CV] 30 Aug 2022 design features interact with each other for an efficient network design. While [9] offers a systematic approach for hybridizing convolutions and attention from the aspect of generalization and model capacity, more first-principles analysis in terms of feature mixing, computational cost, robustness to transferring in low data regimes is required to create efficient layers and network architectures.</p><p>In this work, we fundamentally explore the idea of efficiently 'mixing' input features by a neural network block. To this end, we take inspiration from <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b28">29]</ref>, suggesting feature mixing is sufficient for good performance and assimilating ideas from established techniques in the vision domain for achieving an efficient mixing of contextual information across the input domain <ref type="bibr" target="#b3">[4]</ref>. Our experiments show that coarsely mixing features using self-attention on a global scale can supplement fine scales feature mixing using convolutions. Such an arrangement of layers provides essential global scale contextual information, with higher model capacity in a computationally efficient manner while allowing exploitation of the dataset's inherent symmetries, which conforms to better generalization and faster convergence speed attributed to strong priors of inductive bias. In this paper, we investigate the following two insights: Firstly, we observe that composing a block with self-attention and convolutions can be used as a drop-in replacement to the Multi-Head Attention block employed as part of various vision Transformer architectures, allowing for computationally efficient network architecture with no-to-minimal accuracy degradation; Secondly, based on the inherent symmetries presents in the datasets, the block augmented with layers utilizing such prior knowledge can facilitate better generalization, network convergence, and robustness to low data regimes. Based on these insights, we propose a simple yet effective novel architectural unit, termed as "Mixing Regionally and Locally" (MRL) block for building specialized hybrid vision neural architectures, with self-attention and convolutions at the core.</p><p>We set our priority to evaluate the proposed neural network block's ability to perform as a generalpurpose architectural unit rather than put forward a new neural network architecture design based around it. To that end, our proposed MRL block is computationally efficient and achieves accuracy similar to the variants of Transformer architectures for which we replace the Multi-Head Attention block, responsible for feature mixing, with the MRL block, highlighting the scalability offered by self-attention while at the same time enjoying the generalization and faster convergence properties offered by convolutions. Further to demonstrate that MRLs perform favorably on downstream tasks of classification, detection, and segmentation, requiring the ability for dense prediction, we compare MRL against current state-of-the-art methods under similar network constraints on COCO datasets. Also, for datasets with relatively small data sizes and strong symmetry priors, we evaluate MRL on multiple H&amp;E histology image datasets. We show that MRL-based backbone architectures achieve state-of-the-art performance on these datasets, a domain dominated by ConvNets of relatively small parameter sizes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the last decade, significant advancements in the neural network model architecture for computer vision through the use of convolutions <ref type="bibr" target="#b25">[26]</ref> and, more recently, through the use of self-attention based models like Transformers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">43]</ref>, popularized by the application in natural language processing domain <ref type="bibr" target="#b46">[46]</ref>, have helped realize unprecedented success in the domain. Within these advancements, recurring themes of exploiting datasets' inductive bias <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>, feature mixing <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11]</ref>, large datasets <ref type="bibr" target="#b40">[40]</ref>, and corresponding ever-larger model parameters <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">43]</ref> have been driving innovation.</p><p>While models based on self-attention <ref type="bibr" target="#b11">[12]</ref> or MLP <ref type="bibr" target="#b42">[42]</ref>, offer parameter scaling to up to billions of parameters, higher model capacities, and are best suited to exploit a large dataset, they often come at the cost of sub-par performance in the low data regime compared to ConvNets counterparts given the same data and computational resources. Such behavior indicates a lack of desirable inductive biases in such models, which are learned parametrically from raw data. ConvNets, on the other hand, with their desirable inductive biases, are efficient on datasets of various sizes, offer a better generalization, and faster convergence, but lack scaling, being explored in recent works <ref type="bibr" target="#b30">[31]</ref>. As noted in <ref type="bibr" target="#b8">[9]</ref>, various recent works explore architecture design, designed to exploit the best of both high modeling capacities of self-attention type modules and inductive biases of ConvNets through local receptive fields for computationally efficient attention <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b45">45]</ref>, or augmenting networks with explicit or implicit convolutional layers <ref type="bibr" target="#b50">[50]</ref>, however, such approaches lack a systematic understanding of how such</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we focus on how to optimally 'mix' the provided input feature to a neural architecture unit, which we pose as the following questions to further our analysis of image-based inputs:</p><p>1. How to efficiently and effectively capture global scale information for a given input feature map? 2. How to exploit datasets' inherent symmetries for achieving better generalization, augmented with access to global scale contextual information?</p><p>The above decomposition of the 'mixing' task guides our analysis by doing a two-part process described in the Section 2.1 and Section 2.2, respectively. Additionally, in A.2, we detail some practical considerations we experiment with while exploring MRL. As we further elucidate our design, our rationale for adopting the above decomposition of the task will become clear. <ref type="figure" target="#fig_0">Figure 1</ref> presents a schematic diagram of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mixing Regionally</head><p>Considering an image as a signal consisting of multiple frequencies, to efficiently capture global scale information, a large receptive field would be crucial to capture essential contextual information embedded in the frequency components spanning across the width of the input signal. However, receptive fields spanning the entire spatial domain might be redundant considering a trade-off between model performance and associated computational cost. Focusing on optimizing the computational aspect of capturing relevant global scale structure, optimally processing the low-frequency component of the input signal by an efficient sampling of the input signal followed by processing with a receptive field spanning the entire sampled spatial domain would be crucial. Such processing on the input signal would minimize computational redundancy while enriching the network with high-level concepts for enhanced performance. For any input of size n ? n (channel dimension omitted for clarity), we devise the following step for efficiently capturing global scale information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Group input features into regions</head><p>This step is akin to preparing the input signal for sampling the frequency components. Based on a sampling window size r &lt; n, a design hyperparameter, the input is partitioned into patches of r ? r. For the current study, the sampling windows that divide the input features are strictly non-overlapping. However, overlapping sampling windows can also be utilized but not studied as part of the current study. Also, as proposed in <ref type="bibr" target="#b29">[30]</ref>, a shifted window scheme can also be employed to allow for cross-window connection. We refer to the input partitioned by the sampling window as region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sample down each region.</head><p>This step is akin to sampling the region. Distilling the region to a singular feature vector, using a learned convolutional kernel with a size equivalent to sampling window size, r, is performed as part of the sampling. This step is crucial as this helps reduce the input space to a new, n r ? n r , sampled input. We use a convolution layer for this step, with kernel size and stride of r, due to highly optimized compute kernels, but down-sampling techniques like max/min pooling, and bilinear downsampling, may work in practice. 3. Process the sampled input with a large receptive field.</p><p>Finally, as the initial input signal has been down-sampled, processing the input with a large receptive field is computationally efficient. For this purpose, self-attention with its receptive field spanning the entire spatial location fits the bill, as it allows for capturing inputdependent complex relational interactions, a desirable property when processing high-level concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mixing Locally</head><p>Image contains strong local structures, like, spatially neighboring pixels are usually highly correlated, object symmetries, exploiting which is crucial for better generalization, faster convergence, and increased robustness for a given model. For this purpose, convolutions play the role perfectly as convolutional layers with their local receptive fields, shared weights, and spatial subsampling <ref type="bibr" target="#b27">[28]</ref> are well suited for infusing information regarding texture and local structure <ref type="bibr" target="#b13">[14]</ref>. Moreover, enhancing convolutional layers through group convolutions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref> to exploit symmetries other than translational symmetries, such as rotational symmetries, mirror symmetries, embedded in the local structure of the input can allow for networks that are less data hungry with minimal parameter overhead. While convolutional layers perform well at a local scale, as noted previously, large receptive fields are essential for capturing more contextual information for higher model capacity. To that end, by combining the feature map generated as part of the 2.1 to the feature map to input feature map, n ? n input, for extracting local features, we can bring in the advantages offered by both. Continuing from the steps devised in Section 2.1, we propose the following steps for Mixing Locally augmented with features containing global scale information, as a part of Mixing Regionally and Locally block: 4. Upscale-Sum feature map containing global scale information.</p><p>The feature map calculated as a part of the Mixing Regionally has a scale of n r ? n r , compared to the input n ? n. To augment the block input with global scale information, we broadcast and sum the individual feature vector obtained by Mixing Regionally to the features in their respective region of the block input n ? n. This step is akin to doing a nearest neighbor upscaling of Mixing Regional feature map by a factor of r and adding the result to the input to the block. <ref type="bibr" target="#b4">5</ref>. Process the augmented input feature with local receptive fields.</p><p>Using convolutional layers with kernel sizes restricted to local scales, we process the input feature set to the block to extract local scale structures by taking advantage of the built-in inductive biases of convolutional layers to produce the block output. Depending on the inherent structural symmetries present in the datasets, utilizing group convolutions can allow the model to achieve better generalization and faster convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Works</head><p>Mixing Features Our fundamental analysis is aimed to gain a better understanding of how we can efficiently 'mix' features. As presented by <ref type="bibr" target="#b42">[42]</ref>, they show that the 'mixing' of input features, which they achieved by applying MLPs along and across input feature tokens, is sufficient for attaining high accuracies for computer vision tasks. Similarly, from the natural language processing domain, <ref type="bibr" target="#b28">[29]</ref> shows that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sub-layers with simple linear transformations that "mix" input tokens. Our insights from these papers helped define the hypothesis that, at its base, models based on convolutions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b14">15]</ref> and self-attention (vision-transformer models) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b44">44]</ref> are mixing features. While at the same time, they augment their output representation with their specific functional advantages, for example, input-independent invariance properties of convolutions and parameter independent scaling of receptive field of self-attention. In contrast convolutional, self-attention or hybrid approaches, our proposed neural network architecture unit stands to achieve the 'mixing' of the input features as the unit's intended behavior, not as the consequences of its constituents.</p><p>Using Regions A common recurring theme in various literature <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4]</ref> is to divide the input feature set into regions/patches, specifically while using self-attention, to improve the model's speed and memory usage. The standard vision transformer proposed in <ref type="bibr" target="#b11">[12]</ref> uses patches to control the computation-accuracy trade-off of using self-attention. <ref type="bibr" target="#b33">[34]</ref> replaces standard convolutional kernels with self-attention, restricting the receptive field of self-attention while allowing for increased computational performance. <ref type="bibr" target="#b29">[30]</ref> purpose the approach of shifted windows which brings in greater computational efficiency when utilizing self-attention by limiting the computation to non-overlapping local windows while allowing for a cross-window connection. <ref type="bibr" target="#b10">[11]</ref> purpose a similar window-based self-attention approach, wherein, they propose computing self-attention in the horizontal and vertical stripes in parallel, forming a cross-shaped window. <ref type="bibr" target="#b6">[7]</ref> performs a subsampling of the input feature set for computing self-attention, interleaving sub-sampled feature set self-attention block and full-feature set self-attention to improve network efficiency. While <ref type="bibr" target="#b3">[4]</ref> adopts a strategy of utilizing self-attention to learn both regional and local scale features while augmenting the local scale self-attention through their respective processed regional tokens.</p><p>Convolutions &amp; Attention Various previous works have explored combining convolutions and attention for designing neural network models to reap the benefits provided by both. <ref type="bibr" target="#b51">[51]</ref> presents the use of self-attention and convolutions for allowing the network to gain different perspective of global and local information while reducing the computational cost for NLP tasks. <ref type="bibr" target="#b55">[55]</ref> presents a study where convolutions are used to augment self-attention with stronger local information for long sequences. For vision based tasks, one line of exploration is augmenting existing ConvNet backbones with self-attention modules <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b1">2]</ref> or replacing convolutional layers with self-attention <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Another line of research explores augmenting Transformer based backbones with convolutions <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b54">54]</ref>, trying to infuse desirable qualities of convolutions into successful self-attention-based architectures <ref type="bibr" target="#b11">[12]</ref>. While these approaches focus on augmenting networks with convolutions and attention, in an ad-hoc manner, <ref type="bibr" target="#b8">[9]</ref> explore the combining self-attention and convolutional block within one basic computational block.</p><p>While our work belongs to this category and shares certain design aspects with <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b3">4]</ref>, and comes closest to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b55">55]</ref> in terms of combining convolutions and self-attention as a part a one computational block, our proposed neural architectural unit is, by design, an elementary way of mixing global scale and local scale information. Like <ref type="bibr" target="#b8">[9]</ref>, MRL inherits the generalization properties inherited by ConvNets, and scalability offered by self-attention while at the same the has a lower computational footprint for self-attention, making it suitable for high-resolution inputs. We have provided a detailed list of differentiating factors between our proposed MRL and other techniques utilizing Convolutions and Attention in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we stand to evaluate the MRL block's ability to serve as an efficient neural network architectural block and establish its utility for practical tasks. For our evaluation, we replace the Multi-Head Self-Attention (MHSA or simply SA) module in existing Transformer architectures with an MRL block and evaluate if the network maintains similar network accuracy. Replacing MHSA with MRL helps verify the hypothesis that self-attention is one way of 'mixing' features, hence can be replaced by MRL, and showing that MRL can preserve the high modeling capacity offered by self-attention based neural network models. To evaluate MRL, we decompose our evaluation criteria into two categories, described as follows:</p><p>1. Section 4.1: Ability to serve the role of a general-purpose neural network architecture unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Section 4.2, &amp; 4.3: Ability to perform favorably on downstream tasks.</head><p>For any neural network, the ability to perform well on various downstream tasks is essential to validate its utility. To that end, we focus on tasks that require dense predictions while posing practical challenges in terms of dataset size and exploiting dataset objects' symmetries. We also evaluate how MRL's design choices impact the network's generalization and convergence. We also discuss potential limitations of MRL on such tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ImageNet Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Setup</head><p>Base Network We replace the MHSA module in CvT 2 , the network architecture proposed by <ref type="bibr" target="#b50">[50]</ref>, with the MRL module. We adopt CvT as our base network for the following reasons: 1) A fully self-attention based network architecture, effective in highlighting the effects of replacing the MHSA module with MRL; 2) Multiple model variants, varying by parameters and FLOPs, allowing for evaluating MRL's ability to scale with deeper networks; 3) Use of convolutional embeddings for a simplified adaption to various vision tasks, blending well with the input-outputs for the MRL module. The family of models used for this evaluation, replacing the MHSA with the MRL module, based on the CvT family of models, is described in <ref type="table">Table 6</ref> of Appendix A.1. Except for the MHSA module of CvT, the other network design parameters are the same. Also, from the MRL side, considering the network's intermediate window sizes, we use region window sizes with a multiple of 2 for ease of evaluation.</p><p>Training &amp; Finetuning We train the models specified in <ref type="table">Table 6</ref> on the ImageNet dataset with 1.3M images. For the individual network specified in <ref type="table">Table 6</ref>, we use training settings similar to <ref type="bibr" target="#b50">[50]</ref>, not limited to the number of training epochs, optimizer, and weight decay, allowing us to compare our proposed MRL one-on-one against purely SA blocks. We adopt data augmentation and regularization methods similar to ViT <ref type="bibr" target="#b11">[12]</ref>. Unless stated otherwise, an input of 224 ? 224 is utilized. Similarly, we employ the same fine-tuning strategies, along with fine-tuning hyperparameters, as in <ref type="bibr" target="#b50">[50]</ref>, where-in we train the models on an input size of 224 ? 224, from ImageNet-1K, and fine-tune the models on an input of 384 ? 384 of ImageNet dataset. <ref type="table" target="#tab_0">Table 1</ref> presents the summarizes and compares the results of training CvT models, replacing the SA block with the MRL block. MRL model variants achieve Top-1 accuracies similar to or higher than their CvT model counterparts, with up to 50% FLOPs , corresponding to replaced part of the network, and 12% training throughput improvement for the pre-training phase where an input of 224 ? 224 is utilized. MRL brings significant FLOP and training throughput improvements for larger inputs, as is the case for inputs of 384 ? 384, for finetuning. MRL achieves finetuning accuracies similar to its CvT counterparts with up to 30% improvement in network training throughput and 70% FLOPs improvement in replaced part of the network. Moreover, the reduced memory consumption of the MRL block compared to SA allows for using a larger mini-batch size per GPU (executed on A100 GPU with 40GB memory), allowing for further throughput improvement. Experimentation Limit. One limitation of the presented experiments is that we have not presented results strongly relating MRL-based networks modeling capacity, by performing experiments with larger models (number of parameters) and larger datasets like ImageNet-21k <ref type="bibr" target="#b36">[37]</ref>. However, we have highlighted MRL-based networks' higher modeling capacity in Section 4.3, by showing MRL's ability to exploit extra datasets, leading to better performance than its competition. Also, we have not presented the study evaluating the effect of larger/smaller region size and of increasing the number of filters per channel for the depthwise convolutions in the MRL block. We leave that as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COCO Object Detection</head><p>Next, we evaluate our MRL block on the COCO object detection task with the Mask R-CNN <ref type="bibr" target="#b18">[19]</ref> and the Cascade Mask R-CNN <ref type="bibr" target="#b2">[3]</ref> framework. Specifically, since we do not propose our own network design, that is the number of network stages or the number if MRL blocks per stage, we use the Swin-T and Swin-S [30] 3 as backbone and only replace the self-attention mechanism with MRL and compare our results against the same. We employ the same pre-training and finetuning strategy as used in Swin Transformer <ref type="bibr" target="#b29">[30]</ref> on the COCO training set.    <ref type="table" target="#tab_3">Table 3</ref> presents the results of the Cascade Mask R-CNN framework. We observe a similar trend to the results of Mask R-CNN in <ref type="table" target="#tab_1">Table 2</ref>, wherein the MRL variant surpasses or is at par with its Swin variants while maintaining equivalent throughput.</p><p>Additionally, we compare MRL block performance against existing variants of the self-attention mechanism used in the vision domain on the COCO dataset. We do this as MRL-based networks can be considered one of the variants of Vanilla Vision Transformer. Following the comparison setting in <ref type="bibr" target="#b29">[30]</ref>, we use Swin-T as the backbone and change the self-attention mechanism. The result of the comparison using the Mask R-CNN framework, trained on 1? schedule is reported in <ref type="table" target="#tab_4">Table 4</ref>. While our MRL network variant does not achieve the highest Top1 accuracy on the ImageNet-1K dataset used for pretraining the network, our MRL variant outperforms other self-attention variants on this task, highlighting better generalization offered by convolutions embedded in the MRL block. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Histopathology: Instance Segmentation</head><p>Task &amp; Motivation This current task pertains to performing nuclear level instance segmentation of Haematoxylin and Eosin (H&amp;E) stained histology slides. Whole-Slide Images (WSIs) obtained by digitizing glass histology slides using optical scanning devices contains thousands of nuclei of various types. Assessing such WSIs can help predict clinical outcomes, for example, survivability <ref type="bibr" target="#b0">[1]</ref> and diagnosing the grade and disease type <ref type="bibr" target="#b31">[32]</ref>. The following aspects of the current task motivated our analysis for the use of MRL based neural network architecture: 1) A dense prediction task, sufficient to highlight MRL's ability to perform as a neural network backbone architectural unit; 2) Relatively small dataset sizes for training/finetuning purposes, making model generalization a challengiing task; 3) Large neural network inputs and corresponding large intermediate neural network outputs, requiring large computational resources which can be alleviated by using MRL; 4) Strong symmetry bias in the dataset, exploiting which is necessary for better generalization and efficient models; 5) This task is dominated by ConvNet type architectures <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6]</ref>, with limited progress with self-attention based architectures for histopathology <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b49">49</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Setup</head><p>Base Network For our analysis, we use the network architecture, HoVerNet 4 , proposed in <ref type="bibr" target="#b16">[17]</ref>, as our base network architecture design for its state-of-the-art results on nuclear segmentation task. HoverNet, at its base, is a U-Net <ref type="bibr" target="#b35">[36]</ref> style neural network architecture with a base encoder network and decoder network. A detailed description of the HoVerNet architecture is presented in Appendix A.3.1. We utilize MRL-based encoder and decoder architecture in HoVerNet. The architecture described in <ref type="bibr" target="#b16">[17]</ref> used a Preact-ResNet50 <ref type="bibr" target="#b20">[21]</ref> as the encoder architecture and a series of up-sampling operations and densely connected units <ref type="bibr" target="#b22">[23]</ref> for decoder networks, as shown in <ref type="figure">Figure 2</ref> of the Appendix. We replace the encoder with MRL blocks-based architecture, with the same neural network units as in MRL networks described in <ref type="table">Table 6</ref>. Similarly, we replace the decoder networks with a series of up-sampling and MRL blocks. We refer to the this neural network architecture as MHVN. Additionally, to exploit the inherent rotational symmetries present in these datasets <ref type="bibr" target="#b14">[15]</ref> we augment our MRL blocks by appending group convolutions <ref type="bibr" target="#b7">[8]</ref> to the Mixing-Locally part, referring the resultant neural network architecture as GC-MHVN. The network, along with the network parameters, are presented in Appendix.</p><p>Dataset As part of our analysis, we use the following datasets to perform instance segmentation evaluation: CoNSep <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr">Kumar [27]</ref>, and CPM-17 <ref type="bibr" target="#b47">[47]</ref>. Additionally, we use Lizard dataset, <ref type="bibr" target="#b15">[16]</ref>, as an extra pre-training dataset, detail for which would be elucidated in the following sections. A full summary of these datasets can be found in Appendix A.3.2. For dataset processing, we utilize similar data-processing pipelines as proposed in <ref type="bibr" target="#b16">[17]</ref> for all the dataset, that is, splitting the large WSIs into patches, which are then used as inputs to the network after corresponding data-augmentation.</p><p>Training &amp; Finetuning Similar to <ref type="bibr" target="#b16">[17]</ref>, we pre-train the encoder part of the network with ImageNet-1K, utilizing all the same training and data-augmentation hyperparameters as described in <ref type="bibr" target="#b50">[50]</ref>, which were also used for training the networks in Section 4.1. However, to further exploit the model capacity offered by the self-attention module in the MRL block, we train both the encoder (pretrained on ImageNet-1K) and decoder simultaneously on the Lizard dataset. For this step, we use the same training hyperparameters as used by <ref type="bibr" target="#b16">[17]</ref> for training the encoder and decoder simultaneously, however, we found that using warm-up for five epochs helped stabilize the training. Also, as the Lizard dataset is a culmination of other smaller datasets, including the CoNSep dataset, we remove all data corresponding to the CoNSep dataset while pre-processing the Lizard dataset for this phase. We finetune the network pre-trained on Lizard(-CoNSep) dataset on CoNSep, Kumar and CPM17 dataset. We maintain the same training and test splits for all the datasets as used in <ref type="bibr" target="#b16">[17]</ref>. For this finetuning phase, we finetune the encoder and decoder parts of the networks simultaneously, using the same finetuning hyperparameters as utilized by <ref type="bibr" target="#b16">[17]</ref>, except we use a warm-up phase for five epochs for stabilizing the training. Also, to fully characterize and understand the performance of the proposed method, we use the evaluation metrics of DICE score, Aggregated Jaccard Index (AJI), &amp; Panoptic Quality (PQ), as utilized by <ref type="bibr" target="#b16">[17]</ref>. <ref type="table" target="#tab_5">Table 5</ref> compares our MRL-based models to other segmentation approaches used in computer vision <ref type="bibr" target="#b18">[19]</ref>, medical imaging <ref type="bibr" target="#b35">[36]</ref>, and other approaches specifically tuned for nuclear segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b14">15]</ref>. We present results of HoVerNet architecture pre-trained on Lizard dataset, <ref type="bibr" target="#b16">[17]</ref> with ImageNet-1K pretrained Pre-ResNet-50 <ref type="bibr" target="#b20">[21]</ref> and BiT101 <ref type="bibr" target="#b24">[25]</ref> as encoders architectures. This allows us to compare and highlight the high model capacity offered by self-attention in MRL blocks, for networks with equivalent parameter complexity. For final evaluation, we select the model corresponding to the epoch with the best DICE value on the validation set. The comparison table shows that the MRL-based model, GC-MHVN, outperforms previous state-of-the-art approaches with a significant margin on all three datasets. In particular, we observe the following two characteristics of MRL, considering its performance on this downstream task. Firstly, MRL based HoVerNet architecture successfully exploits the extra training data available through the Lizard dataset, performing better than its ConvNet-HoVerNet counterparts with access to similar training data. It highlights MRL's ability to harness the higher model capacity offered by self-attention. Secondly, the higher performance of GC-MHVN over MHVN points towards MRL's ability to exploit the inductive biases offered by convolutions, in this case, rotational inductive bias by group convolutions, for better generalization. It highlights the dataset-dependent design versatility offered by the MRL unit framework. Both of the above characteristics indicate MRL framework's ability to combine self-attention and convolutions for better capacity and generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>Limitation. In our testing, we observe that MRL-based networks (both MHVN and GC-MHVN) during finetuning phase converge to high validation accuracies much faster, that is, within the first few epochs, compared to its ConvNet counterparts. This faster convergence is followed by a continuous improvement in the overall training loss, with a gradual drop in validation accuracies. This points toward over-fitting, commonly faced in self-attention-based models on limited datasets. Currently, we do not explore training settings to allow MRL-based models to continue learning relevant features, making post-epoch validation evaluation crucial depending on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have introduced a new neural network computational block named Mixing Regionally and Locally block (MRL). The core design of the block is based on the idea of mixing features, which we decomposed as a task of mixing features at regional and local scales, to promote computational efficiency while maintaining the block's effectiveness. This mixing was achieved by exploiting self-attention and convolutions for mixing at the regional and local scales, respectively. The use of self-attention and convolutions provide models with better generalization while maintaining higher model capacity. Extensive experiments on several benchmarks highlight MRLs' ability to perform at par or better than competing techniques. Also, MRL's design versatility allows for incorporating components like Group Convolutions which can be used to exploit additional dataset symmetries for better generalization. <ref type="table">Table 6</ref>: L denotes the number of blocks, D denotes the length of the hidden dimension maintained throughout the block with the hidden dimension per head fixed to 64, and r denotes the window size of the regions of the MRL block. We use a depthwise convolution with one filter per channel for the Mixing Locally part. The expansion rate for the inverted bottleneck is always 4. CommonQKV. For generating QKV values for the (multi-head)self-attention (MHA) layer, generally, linear transformation (linear regression) is performed corresponding to each QKV pair for each head in the MHA layer. These linear layers account for O((hd h ) 2 ) parameter requirement, where h is the number of heads in the MHA layer and d h is the embedding vector length of each head input token. These linear layers also contribute to O(n 2 (hd h ) 3 ) time complexity, which increases quadratically with the input spatial dimension, n, and cubic in number of total channels, hd h . The effect of these parameters on computing and memory requirements becomes pronounced for networks with greater depth and large input sizes to the network. The core idea of this proposed method is to have QKV values share a common basis set (a function of the Transformer block input), using which calculating QKV is cheaper (fewer parameters) and efficient (low computational and memory cost). We transform the channel dimension of the input feature map for generating the QKV value set, using a linear transformation to create the basis set. Following the generation of a basis set, by processing the basis set by a depthwise convolution layer, individually for Q, K, and V, we generate the QKV value set for the given input feature set. By using CommonQKV, we can reduce the time complexity and parameter requirement for computing the QKV value set by one-third, as the computational complexity due to the linear layer is limited to generating the basis set only, with limited overhead due to depthwise convolutions. <ref type="table">Table 7</ref> further provides comparison results of utilizing the proposed CommonQKV A.2 together with MRL for computing the QKV value set. Using CommonQKV brings in additional FLOP and training throughput improvement, up to 68% and 20% respectively for the pre-training phase and 75% and 42% for the fine-tuning phase with a larger input size. However, using CommomQKV leads to a drop of up to 0.2% accuracy during the pretraining phase, highlighting the dis-advantage brought in by parameter-shallow representation, also studied by <ref type="bibr" target="#b50">[50]</ref>, by reducing the feature set of K and V value set to reduce computation. Additionally, to establish the reason for the accuracy drop, we do an ablation study by utilizing CommonQKV to create the QKV value set for performing Full SA in CvT models, presented in <ref type="table" target="#tab_7">Table 8</ref>. Comparing results in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_7">Table 8</ref>, we can see a similar drop in accuracy corresponding to models utilizing CommonQKV. This drop in the accuracy suggests that MRL, for these cases, perfectly models the role played by SA, and the accuracy drop noticed with CommonQKV is due to the under-representation of the QKV value set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stages Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 HoVerNet</head><p>HoVerNet is one of the state-of-the-art CNN models that perform nuclei instance segmentation.HoVerNet's architecture comprises Encoder and Decoder, typical for image segmentation tasks. The original architecture of HoVerNet adopts ResNet-50-based architecture as an encoder whose purpose is extracting features from input images. The decoder aims to detect nuclei regions while upsampling input features. the decoder network is a group of decoder networks branches, namely, <ref type="table">Table 7</ref>: Model performance (Top-1 classification accuracy) on ImageNet. 1K denotes training on ImageNet-1K; 1K 384 denotes pre-training on ImageNet-1K followed by finetuning on ImageNet-1K with an input size of 384; SA/MRL denotes the Parameter (in Millions) and FLOPs (in Giga-FLOPs) corresponding to SA or MRL blocks of the network; images/s correspond to training throughput on one DGX-A100 Node, with the subscript denoting the mini-batch size used per GPU (provided to highlight cases where a large-batch size could could not be processed for CvT models due to out-of-memory errors); CQ+MRL denotes models using the proposed CommonQKV A.2 and MRL, together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters (M)</head><p>FLOPs <ref type="formula">(</ref>   <ref type="figure">Figure 2</ref>); (ii) HoVer branch (middle one in figure 2), and (iii) nuclear classification (NC) (lower one in <ref type="figure">figure 2</ref>). The NP branch predicts whether a given pixel corresponds to nuclei or the background, the HoVer branch predicts the horizontal and vertical distances from the nuclei's center of mass to detect and separate overlapping nuclei instances, and the NC branch assigns class labels of nuclei for reach pixel. For the current task, only the NP branch and the HoVer branch are utilized as they jointly achieve instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Original HoVerNet architecture</head><p>In this paper, we utilize only the basic ideas of the original HoVerNet architecture that exploits the encoder's output for the several decoder branches to enhance inference quality. Thus we replaced the whole internal architecture with our MRL-based Transformers, as shown in <ref type="figure">Figure 3</ref>. We call this architecture "MRL-HoVerNet." <ref type="figure">Figure 3</ref>: MRL-HoVerNet architecture. L corresponds to number of layers; r corresponds to the region size; H corresponds to the number of heads, where the hidden dimension is fixed to 96.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Datasets</head><p>We utilized the following publicly available histopathology image datasets, CoNSeP <ref type="bibr" target="#b16">[17]</ref>, CPM17 <ref type="bibr" target="#b47">[47]</ref>, Kumar <ref type="bibr" target="#b26">[27]</ref>, and Lizard <ref type="bibr" target="#b15">[16]</ref>. We used Lizard only for pretraining HoVerNet based networks and we trained and evaluated the networks with the other four datasets respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Related Works (Extended)</head><p>Presented herewith are the some core distinctions between are presented MRL layer and some of the other works utilizing Convolutions and Self-Attention as part of their neural network design.</p><p>? ConVit <ref type="bibr" target="#b9">[10]</ref>:</p><p>-ConViT implements convolutions through the use of Gated Self-Attention.</p><p>-Though this allows for better capture of local dependencies however in contrast to MRL, there is no reduction in the computational requirement of the network.</p><p>-Additionally, MRL allows for a more dynamic design by incorporating components like Group-Convolutions. ? UTNet <ref type="bibr" target="#b12">[13]</ref>:</p><p>-UTNet at its base is an UNet architecture augmented with SA.</p><p>-The core difference between UTNet and MRL is evident in the way SA and convolution are used as part of the NN. In UTNet convolution and SA (MHSA layer) are stacked. However, in MRL, convolution, and SA are stacked as part of layers serving specific functions described in the manuscript. ? CvT <ref type="bibr" target="#b50">[50]</ref>:</p><p>-The core difference between CvT and MRL is very obvious by the fact that though CvT does use convolutions and SA, CvT uses convolutions for do-away with position embeddings, down-sampling, &amp; generating QKV values, however, uses full-SA in its MHSA layer. -Instead in MRL-based CvT like NN architecture, the network can use convolutions as CvT does, while using MRL layer design as the way to mix features replacing the MHSA layer. ? ViTAE <ref type="bibr" target="#b52">[52]</ref>:</p><p>-ViTAE utilizes the SA and convolutions for a similar purpose as we do, that is to capture long-scale dependencies using SA and local-scale dependencies using convolutions. -However, there is a subtle difference between MRL and VITAE in how those captured dependencies are processed which changes the design complexity of the two. In ViTAE local and long-scale dependencies are processed separately without any information exchange between the two, requiring the various levels of dilation as described in their Pyramid reduction module to capture dependencies interaction. -The core difference between Coatnet and MRL is in how a hybrid of SA and convolution is made. -In Coatnet, uses a 'pre-normalization relative attention' wherein Attention weights are jointly decided by input-adaptive component and translation equivariant convolutional kernel. This serves the same purpose as the MRL block however at an added cost of doing convolution for all the input features. -However, in the case of MRL, we do away with that additional cost by exploiting the fact that regional relations can be captured cheaply by doing down-sampling, reducing the cost of a typical MHSA layer. ? MUSE <ref type="bibr" target="#b55">[55]</ref>:</p><p>-Other than the fact MUSE is for NLP tasks, the core difference is how local and global information is used. -In MUSE global information does not influence local information, however, in MRL local mixing is influenced by the information mixed at the regional scale. This helps to add additional context to the local feature mix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematic diagram representing Mixing Regionally and Locally approach. Each rectangle corresponds to an individual feature vector. Numbers in the diagram correspond to the respective steps described in Section 2.1 and Section 2.2, as part of the MRL block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 , 4 S1 1 / 8 L 1 = 2 , 4 S2 1 / 16 L 2 = 10 , 2 A. 2</head><label>141812411621022</label><figDesc>D 0 = 64, r 0 = 4 L 0 = 1, D 0 = 64, r 0 = D 1 = 192, r 1 = 4 L 1 = 4, D 1 = 192, r 1 = D 2 = 384, r 2 = 2 L 2 = 16, D 2 = 384, r 2 = Practical MRL While Section 2.1 enables an efficient feature mixture of global scale features, however, in Step 3, for performing self-attention, we observe parameter redundancy and input-dependent high computational cost due to linear layers for calculating the QKV value set. To allow for a more parameter efficient and computationally efficient representation of the QKV value set, we propose CommonQKV, as explained in the following.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>-</head><label></label><figDesc>However, in the case of MRL, the local scale dependencies are captured/mixed informed by captured/mixed long-scale (Step 4 of Section 2.2). This reduced the design complexity of MRL in comparison with ViTAE. ? CeiT[53]: -The difference between CeiT and MRL is in where the convolutions are used. -In CeiT, convolutions are used in the Feed-Forward network of the encoder block leaving the MHSA module unchanged, whereas, in MRL-based networks, MHSA is replaced by the MRL layer while leaving the feed-forward network unchanged. -This changes the way local-long scale dependencies are captured and information is exchanged. MRL is more computationally efficient as MRL reduces the computational costs associated with MHSA, while CeiT brings in additional computational costs through their LeFF layer. ? CoAtNet[9]:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Model performance (Top-1 classification accuracy) on ImageNet. 1K denotes training on ImageNet-1K; 1K 384 denotes pre-training on ImageNet-1K followed by finetuning on ImageNet-1K with an input size of 384; SA/MRL denotes the Parameter and FLOPs corresponding to SA or MRL blocks of the network; images/s correspond to training throughput on one DGX-A100 Node, with the subscript denoting the mini-batch size used per GPU (provided to highlight cases where a large-batch size could could not be processed for CvT models due to out-of-memory errors).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">#Paras (M)</cell><cell cols="2">FLOPs (G)</cell><cell>Top-1</cell><cell></cell></row><row><cell>Dataset</cell><cell>Model</cell><cell>SA/MRL</cell><cell>Full</cell><cell>SA/MRL</cell><cell>Full</cell><cell cols="2">Accuracy images/s</cell></row><row><cell></cell><cell>CvT-13</cell><cell>6.40</cell><cell>19.98</cell><cell>1.42</cell><cell>4.50</cell><cell>81.6</cell><cell>496.4</cell></row><row><cell></cell><cell>MRL-13</cell><cell>6.40</cell><cell>19.98</cell><cell>0.63</cell><cell>3.71</cell><cell>81.63</cell><cell>555.9</cell></row><row><cell>1K</cell><cell>CvT-21</cell><cell>10.34</cell><cell>31.54</cell><cell>2.17</cell><cell>7.13</cell><cell>82.3</cell><cell>308.6</cell></row><row><cell></cell><cell>MRL-21</cell><cell>10.35</cell><cell>31.55</cell><cell>1.03</cell><cell>5.98</cell><cell>82.3</cell><cell>346.3</cell></row><row><cell></cell><cell>CvT-13</cell><cell>6.40</cell><cell>19.98</cell><cell>7.10</cell><cell>16.30</cell><cell>83</cell><cell>132.2</cell></row><row><cell></cell><cell>MRL-13</cell><cell>6.40</cell><cell>19.98</cell><cell>1.99</cell><cell>11.19</cell><cell>83.1</cell><cell>172.4</cell></row><row><cell>1K 384</cell><cell>CvT-21</cell><cell>10.34</cell><cell>31.54</cell><cell>10.20</cell><cell>24.90</cell><cell>83.3</cell><cell>81.7 32</cell></row><row><cell></cell><cell>MRL-21</cell><cell>10.35</cell><cell>31.55</cell><cell>3.23</cell><cell>17.93</cell><cell>83.3</cell><cell>107.8 64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>reports the results of the Mask R-CNN framework with "1?" (12 training epochs) and "3? + MS" (36 training epochs with multi-scale training). It shows that MRL variant of the network outperforms or performs at par with its Swin Transformer counterparts, with +1.5 box AP, +1.</figDesc><table /><note>3 mask AP with 1? schedule for Tiny (T) variant. This increase in accuracy in 1? schedule points towards better faster convergence and better generalization provided by convolutions, part of Mixing Locally in the MRL blocks. We also achieve similar accuracy statistics on the Small (S) 1? schedule configuration. While for 3? schedule, the MRL variant performs at par with its Swin counterpart.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Object detection and instance segmentation performance on the COCO val2017 with Mask R-CNN framework. The FLOPs and FPS (Frames per second) are calculated for an input size of 800 ? 1200, on an A100 GPU. MRL is the model corresponding to networks with Swin Transformer attention replaced with MRL blocks.</figDesc><table><row><cell></cell><cell cols="2">#Params FLOPs (M) (G)</cell><cell>FPS</cell><cell cols="4">Mask R-CNN 1? schedule AP b AP b 50 AP b 75 AP m AP m 50</cell><cell>AP m 75</cell><cell cols="4">Mask R-CNN 3? + MS schedule AP b AP b 50 AP b 75 AP m AP m 50</cell><cell>AP m 75</cell></row><row><cell>Swin-T</cell><cell>48</cell><cell>267</cell><cell cols="2">25.2 43.7 66.6</cell><cell>47.7</cell><cell>39.8</cell><cell>63.3</cell><cell cols="2">42.7 46.0 68.1</cell><cell>50.3</cell><cell>41.6</cell><cell>65.1</cell><cell>44.9</cell></row><row><cell>MRL-T</cell><cell>50</cell><cell>251</cell><cell cols="2">25.4 45.2 68.2</cell><cell>49.4</cell><cell>41.1</cell><cell>65.0</cell><cell cols="2">44.0 46.5 68.6</cell><cell>51.2</cell><cell>42.0</cell><cell>65.4</cell><cell>45.2</cell></row><row><cell>Swin-S</cell><cell>69</cell><cell>354</cell><cell cols="2">19.1 44.8 66.6</cell><cell>48.9</cell><cell>40.9</cell><cell>63.4</cell><cell cols="2">44.2 48.5 70.2</cell><cell>53.5</cell><cell>43.3</cell><cell>67.3</cell><cell>46.4</cell></row><row><cell>MRL-S</cell><cell>72</cell><cell>322</cell><cell cols="2">19.2 45.7 67.9</cell><cell>50.1</cell><cell>41.1</cell><cell>64.6</cell><cell cols="2">44.4 48.5 70.3</cell><cell>53.5</cell><cell>43.3</cell><cell>67.3</cell><cell>46.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Object detection and instance segmentation performance on the COCO val2017 with Cascade Mask R-CNN framework.</figDesc><table><row><cell></cell><cell cols="2">#Params FLOPs (M) (G)</cell><cell>FPS</cell><cell cols="6">Cascade Mask R-CNN 1? schedule AP b AP b 50 AP b 75 AP m AP m 50 AP m 75</cell><cell cols="5">Cascade Mask R-CNN 3? + MS schedule AP b AP b 50 AP b 75 AP m AP m 50 AP m 75</cell></row><row><cell>Swin-T</cell><cell>86</cell><cell>745</cell><cell cols="3">14.4 48.1 67.1</cell><cell>52.2</cell><cell>41.7</cell><cell>64.4</cell><cell cols="2">45.0 50.4 69.2</cell><cell>54.7</cell><cell>43.7</cell><cell>66.6</cell><cell>47.3</cell></row><row><cell>MRL-T</cell><cell>88</cell><cell>732</cell><cell cols="3">14.4 49.5 68.8</cell><cell>53.4</cell><cell>42.9</cell><cell>66.1</cell><cell cols="2">46.4 50.4 69.3</cell><cell>54.7</cell><cell>43.7</cell><cell>66.5</cell><cell>47.5</cell></row><row><cell>Swin-S</cell><cell>107</cell><cell>833</cell><cell>11.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51.9 70.7</cell><cell>56.3</cell><cell>45.0</cell><cell>68.2</cell><cell>48.8</cell></row><row><cell>MRL-S</cell><cell>110</cell><cell>800</cell><cell>12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51.6 70.4</cell><cell>56.1</cell><cell>44.9</cell><cell>68.0</cell><cell>48.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>ImageNet</cell><cell>COCO</cell></row><row><cell></cell><cell cols="2">Top1 (%) AP b AP m</cell></row><row><cell>Shifted window [30]</cell><cell>81.3</cell><cell>43.7 39.8</cell></row><row><cell>Spatially Sep [7]</cell><cell>81.5</cell><cell>42.7 39.5</cell></row><row><cell>Sequential Axial [22]</cell><cell>81.5</cell><cell>40.4 37.6</cell></row><row><cell>Criss-Cross [24]</cell><cell>81.7</cell><cell>42.9 39.7</cell></row><row><cell>Cross-shaped window [11]</cell><cell>82.2</cell><cell>43.4 40.2</cell></row><row><cell>MRL</cell><cell>81.4</cell><cell>45.2 41.1</cell></row></table><note>Comparison between different self-attention mechanism on COCO dataset, using Mask R-CNN framework. Results taken from [11].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparative experiments on Kumar<ref type="bibr" target="#b26">[27]</ref>, CoNSep<ref type="bibr" target="#b16">[17]</ref>, and CPM-17<ref type="bibr" target="#b47">[47]</ref> datasets. Liz denotes network pre-trained on Lizard<ref type="bibr" target="#b15">[16]</ref> dataset.</figDesc><table><row><cell></cell><cell></cell><cell>Kumar</cell><cell></cell><cell></cell><cell>CoNSep</cell><cell></cell><cell></cell><cell>CPM-17</cell><cell></cell></row><row><cell>Method</cell><cell>DICE</cell><cell>AJI</cell><cell>PQ</cell><cell>DICE</cell><cell>AJI</cell><cell>PQ</cell><cell>DICE</cell><cell>AJI</cell><cell>PQ</cell></row><row><cell>U-Net[36]</cell><cell cols="9">0.758 0.556 0.478 0.724 0.482 0.328 0.813 0.643 0.578</cell></row><row><cell cols="10">Mask-RCNN[19] 0.760 0.546 0.509 0.740 0.474 0.460 0.850 0.684 0.674</cell></row><row><cell>DIST[33]</cell><cell cols="9">0.789 0.559 0.443 0.804 0.502 0.398 0.826 0.616 0.504</cell></row><row><cell>Micro-Net[35]</cell><cell cols="9">0.797 0.560 0.519 0.794 0.527 0.449 0.857 0.668 0.661</cell></row><row><cell>HoVer-Net[17]</cell><cell cols="9">0.826 0.618 0.597 0.853 0.571 0.547 0.869 0.705 0.697</cell></row><row><cell>DSF-CNN[15]</cell><cell>0.826</cell><cell>-</cell><cell>0.597</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CDNet[18]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.853 0.571</cell><cell>-</cell><cell cols="2">0.880 0.733</cell><cell>-</cell></row><row><cell>BRP-Net[6]</cell><cell>-</cell><cell>0.642</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">0.877 0.731</cell><cell>-</cell></row><row><cell>CIA-Net[56]</cell><cell cols="3">0.818 0.620 0.577</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="10">Pre-ResNet50 Liz 0.798 0.584 0.563 0.844 0.543 0.519 0.880 0.724 0.706</cell></row><row><cell>BiT101 Liz</cell><cell cols="9">0.826 0.623 0.604 0.848 0.570 0.554 0.882 0.726 0.715</cell></row><row><cell>MHVN Liz</cell><cell cols="9">0.830 0.624 0.599 0.849 0.569 0.554 0.885 0.734 0.721</cell></row><row><cell>GC-MHVN Liz</cell><cell cols="9">0.843 0.652 0.625 0.855 0.576 0.559 0.892 0.743 0.733</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Model performance on ImageNet-1K, training with input size of 224 ? 224, utilizing CommonQKV for generating QKV value set for CvT models.</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>CQ+CvT-13</cell><cell>81.54</cell></row><row><cell>CQ+CvT-21</cell><cell>82.1</cell></row><row><cell>(i) nuclear pixel (NP) branch (upper one in</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>CoNSeP The CoNSeP dataset consists of 41 H&amp;E (Hematoxylin &amp; Eosin) stained images, each of size 1,000?1,000 pixels at 40? magnification, containing 24,319 exhaustively annotated nuclear regions with associated class labels. The images were extracted from 16 colorectal adenocarcinoma (CRA) WSIs, each belonging to an individual patient, and scanned in the department of pathology at University Hospitals Coventry and Warwickshire, UK.CPM17 The CPM17 dataset consists of 64 H&amp;E stained images, each of size 500x500 to 600x600 pixels at 20x or 40x magnifications, containing 7570 annotated nuclear regions. The images were extracted from four different organs' WSIs of TCGA database.Kumar The Kumar dataset contains 30 H&amp;E stained images, each of size 1000x1000 pixels at 40x magnification. Within each image, the boundary of each nuclear region is fully annotated. The images were extracted from seven organs' WSIs (6 breast, 6 liver, 6 kidney, 6 prostate, 2 bladder, 2 colon and 2 stomach) of TCGA database.Lizard The Lizard dataset consists of 291 H&amp;E stained images, each of average size 1016x917 pixels at 20x magnification, containing 495,179 annotated nuclear regions with associated class labels. The images were extracted CRA images of six other datasets. Lizard's data sources include the CoNSeP dataset, so that we carefully removed CoNSeP related images at pretraining phase.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/microsoft/CvT</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/SwinTransformer/Swin-Transformer-Object-Detection</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/vqdang/hover_net</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 ImageNet Classification </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A bottom-up approach for tumour differentiation in whole slide images of lung adenocarcinoma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korsuk</forename><surname>Najah Alsubaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><forename type="middle">E Ahmed</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2018: Digital Pathology</title>
		<editor>John E. Tomaszewski and Metin N. Gurcan</editor>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">10581</biblScope>
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Attention augmented convolutional networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cascade R-CNN: high quality object detection and instance segmentation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regionvit: Regional-to-local attention for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gashis-transformer: A multi-scale visual transformer approach for gastric histopathology image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Grzegorzek</surname></persName>
		</author>
		<idno>abs/2104.14528</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Boundary-assisted region proposal networks for nucleus segmentation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Twins: Revisiting spatial attention design in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno>abs/2104.13840</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1602.07576</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<idno>abs/2106.04803</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno>abs/2103.10697</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Utnet: A hybrid transformer architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno>abs/2107.00781</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<idno>abs/1811.12231</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dense steerable filter cnns for exploiting rotational symmetry in histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4124" to="4136" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lizard: A large-scale dataset for colonic nuclear instance segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Jahanifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayesha</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Nimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Wah</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Dodd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Hero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harvir</forename><surname>Sahota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atisha</forename><surname>Tank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ksenija</forename><surname>Benes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="684" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">Dang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Tae</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno>abs/1812.06499</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cdnet: Centripetal direction network for nuclear instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="4026" to="4035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1603.05027</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1811.11721</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Large scale learning of general visual representations for transfer. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A dataset and a technique for generalized nuclear segmentation for computational pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruchika</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanuj</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surabhi</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Object Recognition with Gradient-Based Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="319" to="345" />
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fnet: Mixing tokens with fourier transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Onta??n</surname></persName>
		</author>
		<idno>abs/2105.03824</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>A convnet for the 2020s. CoRR, abs/2201.03545, 2022</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nuclear shape and orientation features from H&amp;E images predict survival in early-stage estrogen receptor-positive breast cancers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Romo-Bucheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Janowczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shridar</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anant</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lab. Invest</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1438" to="1448" />
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Segmentation of nuclei in histopathology images by deep regression of the distance map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Naylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marick</forename><surname>La?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Reyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Micro-net: A unified model for segmentation of various objects in microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shan-E-Ahmed Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pelengaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajpoot</surname></persName>
		</author>
		<idno>abs/1804.08145</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<editor>Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">TransMIL: Transformer based correlated multiple instance learning for whole slide image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuchen</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno>abs/2101.11605</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno>abs/1707.02968</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Rethinking model scaling for convolutional neural networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dosovitskiy</surname></persName>
		</author>
		<idno>abs/2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>abs/2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>abs/2103.17239</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno>abs/2103.12731</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Methods for segmentation and classification of digital microscopy tissue images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc Dang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahsin</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Nguyen Nhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Talha</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename><surname>Navid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koohbanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayashree</forename><surname>Khurram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarsi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Tae</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyvan</forename><surname>Saltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Bioeng. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno>abs/2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Transpath: Transformer-based self-supervised learning for histopathological image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2021</title>
		<editor>Marleen de Bruijne, Philippe C. Cattin, St?phane Cotin, Nicolas Padoy, Stefanie Speidel, Yefeng Zheng, and Caroline Essert</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2103.15808</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Lite transformer with long-short range attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ViTAE: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Qiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/2103.11816</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/2101.11986</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">MUSE: parallel multi-scale attention for sequence to sequence learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Cia-net: Robust nuclei instance segmentation with contour-aware information aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><forename type="middle">Fahri</forename><surname>Onder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Tsougenis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<idno>abs/1903.05358</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
