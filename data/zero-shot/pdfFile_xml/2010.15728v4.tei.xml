<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-16">16 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre for Medical Informatics</orgName>
								<orgName type="department" key="dep2">Usher Institute of Population Health Sciences and Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Health Data Research UK</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?ctor</forename><surname>Su?rez-Paniagua</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre for Medical Informatics</orgName>
								<orgName type="department" key="dep2">Usher Institute of Population Health Sciences and Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Health Data Research UK</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Whiteley</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Clinical Brain Sciences</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh c Institute of Health Informatics</orgName>
								<orgName type="institution" key="instit2">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Health Data Research UK</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghan</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Health Data Research UK</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explainable Automated Coding of Clinical Notes using Hierarchical Label-wise Attention Networks and Label Embedding Initialisation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-16">16 Jul 2021</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Journal of Biomedical Informatics July 20, 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automated medical coding</term>
					<term>Deep learning</term>
					<term>Attention Mechanisms</term>
					<term>Explainability</term>
					<term>Natural Language Processing</term>
					<term>Multi-label classification</term>
					<term>Label correlation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background: Diagnostic or procedural coding of clinical notes aims to derive a coded summary of disease-related information about patients. Such coding is usually done manually in hospitals but could potentially be automated to improve the efficiency and accuracy of medical coding. Recent studies on deep learning for automated medical coding achieved promising performances.</p><p>However, the explainability of these models is usually poor, preventing them to be used confidently in supporting clinical practice.</p><p>Another limitation is that these models mostly assume independence among labels, ignoring the complex correlations among medical codes which can potentially be exploited to improve the performance.</p><p>Methods: To address the issues of model explainability and label correlations, we propose a Hierarchical Label-wise Attention Network (HLAN), which aimed to interpret the model by quantifying importance (as attention weights) of words and sentences related to each of the labels. Secondly, we propose to enhance the major deep learning models with a label embedding (LE) initialisation approach, which learns a dense, continuous vector representation and then injects the representation into the final layers and the label-wise attention layers in the models. We evaluated the methods using three settings on the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS (National Health Service) COVID-19 (Coronavirus disease 2019) shielding codes. Experiments were conducted to compare the HLAN model and label embedding initialisation to the state-of-the-art neural network based methods, including variants of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).</p><p>Results: HLAN achieved the best Micro-level AUC and F 1 on the top-50 code prediction, 91.9% and 64.1%, respectively; and comparable results on the NHS COVID-19 shielding code prediction to other models: around 97% Micro-level AUC. More importantly, in the analysis of model explanations, by highlighting the most salient words and sentences for each label, HLAN showed more meaningful and comprehensive model interpretation compared to the CNN-based models and its downgraded baselines, HAN and HA-GRU. Label embedding (LE) initialisation significantly boosted the previous state-of-the-art model, CNN with attention mechanisms, on the full code prediction to 52.5% Micro-level F 1 . The analysis of the layers initialised with label embeddings further explains the effect of this initialisation approach. The source code of the implementation and the results are openly available at https://github.com/acadTags/Explainable-Automated-Medical-Coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion:</head><p>We draw the conclusion from the evaluation results and analyses. First, with hierarchical label-wise attention mechanisms, HLAN can provide better or comparable results for automated coding to the state-of-the-art, CNN-based models. Second, HLAN can provide more comprehensive explanations for each label by highlighting key words and sentences in the discharge summaries, compared to the n-grams in the CNN-based models and the downgraded baselines, HAN and HA-GRU. Third, the performance of deep learning based multi-label classification for automated coding can be consistently boosted by initialising label embeddings that captures the correlations among labels. We further discuss the advantages and drawbacks of the overall method regarding its potential to be deployed to a hospital and suggest areas for future studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Diagnostic or procedural coding of medical free-text documents (e.g. discharge summaries) aims to derive a coded summary of disease-related information about patients, for clinical care, audit, and research. In hospitals, such coding is usually done manually, requiring much cognitive human effort, but could potentially be automated. An automated program could efficiently take a clinical note as input and then output medical codes from existing classification systems, e.g. ICD (International Classification of Diseases). This could facilitate coding professionals provide more accurate results.</p><p>This clinical task is technically challenging, due to (i) the explainability required to process long documents, in average about 2000 tokens in a discharge summary in MIMIC-III <ref type="bibr" target="#b0">[1]</ref>, and thus pose a "needle-in-a-haystack" issue to locate the key words and sentences relevant to each code; (ii) the complex label correlations in the multi-label setting, in average about 16 different ICD-9 (the Ninth Revision) codes per discharge summary in the MIMIC-III dataset <ref type="bibr" target="#b1">[2]</ref>, which inherently exhibit the complex relations among codes; and (iii) a large set of codes when using all the codes as candidates for prediction, e.g. around 13k unique codes in ICD-9 and many times further in ICD-10 <ref type="bibr" target="#b2">[3]</ref> and ICD-11 <ref type="bibr" target="#b3">[4]</ref>.</p><p>Automated medical coding has been studied for more than a decade. Early studies mostly use systems based on rules, grammar, and string matching, as reviewed in <ref type="bibr" target="#b4">[5]</ref>. Recent studies adapt deep learning based document classification methods, which commonly formalise the task as a multi-label classification problem <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1]</ref>. Typically, they use variations of Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) to derive a continuous representation of clinical notes matched to the high-dimensional coding space. However, few studies have tackled the above challenges above regarding explainability and label correlations.</p><p>Explainability (or interpretability, used interchangeably in this paper) is a key requirement for models applied to the clinical domain, particularly regarding the ethical aspect and to build medical professionals' trust in machine learning models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Also, to facilitate the work of coding professionals, a desired automated coding system should be able to highlight the most essential part of a long clinical note to support the assignment of medical codes. To address this, models based on CNNs can be adapted to highlight n-gram information to support the explanation, as in <ref type="bibr" target="#b6">[7]</ref>. Solely the n-grams, however, may not be enough to provide accurate interpretation reflecting the document structure.</p><p>In this work, we propose to highlight the most essential words and sentences in a document for automated medical coding, inspired and adapted from Hierarchical Attention Networks (HAN) <ref type="bibr" target="#b9">[10]</ref> and the recent model, Hierarchical Attention bidirectional Gated Re-current Units (HA-GRU) <ref type="bibr" target="#b0">[1]</ref>. With attention mechanisms, HAN can highlight the salient words and sentences related to the overall prediction. However, HAN could not generate a specific interpretation for each label. HA-GRU <ref type="bibr" target="#b0">[1]</ref> can provide a sentence-level explanation for each label, but still could not specify the most essential words leading to the decision of each code. We present a novel model, Hierarchical</p><p>Label-wise Attention Network (HLAN), which has label-wise word-level and sentence-level attention mechanisms, so as to provide a richer explainability of the model.</p><p>We formally evaluated HLAN along with HAN, HA-GRU, and CNN-based neural network approaches for automated medical coding. With better or comparative coding performance in various data settings, HLAN can further generate more comprehensive explanations through key sentences and words for each label, as indicated from the analysis on model explainability.</p><p>The analysis of the false positive predictions also shows that the explanation based on the hierarchical label-wise attention mechanisms in HLAN can serve as a reference for medical professionals and engineers to make reasonable coding decisions and system iterations even when the model seems to predict erroneously.</p><p>Apart from model interpretability, another issue not thoroughly studied in deep learning based multi-label classification is label correlation. Medical codes are related and can be predicted together, for example, the code 486 (ICD 9 for Pneumonia) commonly appeared for over 1.5k times (out of about 53k documents) with the code 518.81 (Acute respiratory failure) in the MIMIC-III dataset. Such co-occurrences are under-lied by the clinical, biomedical, and biological associations among different diseases. Deep learning for multi-label classification represents the label space with orthogonal vectors: each label as a one-hot vector and each label set as a multi-hot representation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7]</ref>. This, however, assumes independence among labels.</p><p>We propose an effective label embedding initialisation approach to tackle the label correlation problem. We encode the label correlation using pre-trained label embeddings from the label sets in the training data, derived from the coding practice.</p><p>Then the label embeddings are used to initialise the weights in the final hidden layer and label-wise attention layers. The idea is that the linear projection can automatically leverage the label similarity encoded in the continuous label embedding space. This approach shows consistent and significant improvement, while not requiring hyper-parameter tuning or further computational complexity.</p><p>We evaluate our approach with three specific datasets based on the openly available, MIMIC-III database <ref type="bibr" target="#b1">[2]</ref>, containing clinical notes in the critical care sector in the US. The first two datasets, full code and top-50 code predictions, are the same as in the work <ref type="bibr" target="#b6">[7]</ref> for comparison. The third dataset was created to simulate the task of identifying high-risk patients for shielding during the COVID-19 (Coronavirus disease 2019) pandemic by predicting the ICD-9 codes matched to the codes used in the UK NHS (National Health Service) patient shielding identification method <ref type="bibr" target="#b0">1</ref> .</p><p>Thus, the contribution of the paper includes:</p><p>? A novel, Hierarchical Label-wise Attention Network (HLAN) for automated medical coding. The proposed HLAN model provides an explanation in the form of attention weights on both the word level and the sentence level for the prediction of each medical code.</p><p>? An effective label embedding (LE) initialisation approach to enhance the performance of various deep learning models for multi-label classification. Analysis of the LE initialised layers shows the efficacy to leverage label correlations for medical coding.  analysis and comparison of model explainability, and analysis on the layers initialised with LE, are in Section 4. We finally discuss the advantages and drawbacks of the overall methods in Section 5 and summarise the work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We will first present the task of automated medical coding with the methods used especially in most recent studies, then introduce in detail the mainstream breakthrough on deep learning-based multi-label classification for the task, and finally review the label correlation issue, particularly relevant to the medical and clinical domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Automated Medical Coding with Explainability</head><p>Automated medical coding is the task of transforming medical records, especially the natural language in the clinical notes, into a set of structured, medical codes to facilitate clinical care, audit, and research <ref type="bibr" target="#b4">[5]</ref>. The applied alphanumerical codes in the clinical domain, such as ICD and SNOMED-CT, represent patients' diagnosis, procedures and other information with controlled clinical terminology.</p><p>One of the earliest reviews back in 2010 <ref type="bibr" target="#b4">[5]</ref> surveyed 113 studies on coding or classification of clinical notes. Most of the studies applied tools with rule-based, grammar-based, and string matching methods, and they in overall suffered the challenges of reasoning and the lack of method generalisability.</p><p>The field of automated medical coding has in more recent years been advanced with the open, benchmarking datasets like radiology reports in <ref type="bibr" target="#b11">[12]</ref> and MIMIC-III <ref type="bibr" target="#b1">[2]</ref> discharge summaries.</p><p>With the datasets, deep learning based approaches have been proposed and tested, which have generally demonstrated better performance than traditional machine learning methods. The work in <ref type="bibr" target="#b5">[6]</ref> compared the deep learning based method, CNN, with several traditional machine learning methods, support vector machine, random forests, and logistic regression, for ICD-9 code prediction (number of ICD-9 codes |Y|=38) from 978 radiology reports in <ref type="bibr" target="#b11">[12]</ref>. The result showed comparable or improved results of the deep learning approach to the traditional methods, even without parameter tuning in the CNN model. The work in <ref type="bibr" target="#b6">[7]</ref> adapted CNN with attention mechanisms and established a state-of-the-art performance in predict-3 ing the full set (|Y|=8,922) and the top-50 most frequent ICD-9 codes (|Y|=50) from MIMIC-III discharge summaries.</p><p>A key aspect of clinical applications is their requirement of the explainability of models. Users are entitled to a "right of explanation" when their data being used for AI algorithms, as potentially regulated by the General Data Protection Regulation (GDPR) <ref type="bibr" target="#b8">[9]</ref>. For clinical applications, e.g. radiology, the Joint European and North American Multisociety Statement raises great ethical concern on AI algorithms regarding explainability, i.e. "the ability to explain what happened when the model made a decision, in terms that a person understands" <ref type="bibr">[8, p. 438</ref>].</p><p>While deep learning achieves better results in general, the approach is inherently less transparent than traditional methods due to its extremely complex networks of non-linear activation.</p><p>Few studies explored the explainability of deep learning models for automated medical coding. A representative work is the study <ref type="bibr" target="#b6">[7]</ref>, which compared the ability of different models to highlight n-grams along with the models' ICD-9 code prediction. A manual evaluation showed that the CNN model with attention mechanisms can generate more meaningful n-grams relevant to the labels <ref type="bibr" target="#b6">[7]</ref>. The study <ref type="bibr" target="#b0">[1]</ref> proposed a Hierarchical Attention bi-directional Gated Recurrent Unit (HA-GRU) to produce a sentence-level explanation for each code, instead of n-gram-level explanation. In this work, we propose an approach with enhanced interpretability, from both the label-wise word-level and the sentence-level attention weights, to support automated coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Learning-based Multi-label Classification with Attention Mechanisms</head><p>Automated medical coding is mainly formulated as a multilabel classification problem <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1]</ref>, where each object (e.g. clinical note) is associated with a set of labels (e.g. diagnosis or procedure ICD codes) instead of a single label in binary or multi-class classification.</p><p>Deep learning has become the main approach for multi-label document classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref> in recent years. The advantage of multi-label deep learning models lies in their straightforward problem formulation and strong approximation power on large datasets, resulting in better performance over traditional machine learning approaches, as compared in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6]</ref>. For automated coding, some of the notable neural network models adapted for multi-label classification are variations of CNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and RNNs <ref type="bibr" target="#b0">[1]</ref> with attention mechanisms. Pre-trained models with multi-head self-attention blocks (e.g. BERT, Bidirectional Encoder Representations from Transformers) <ref type="bibr" target="#b16">[17]</ref>, while substantially improved many NLP tasks, so far still are under-performing for automated coding with the MIMIC-III discharge summaries <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>The idea of the above mentioned attention mechanism is a key, recent advancement in deep learning for NLP, originated from machine translation to align (or attend to) words in the source sentence in one language to predict each of the target words in another language <ref type="bibr" target="#b19">[20]</ref>. This inspires to jointly learn to represent the important words and sentences while classifying a document in HAN <ref type="bibr" target="#b9">[10]</ref>, thus also enables to explain the inner working of deep learning models. HAN was adapted to a multi-label classification setting to classify socially shared texts in <ref type="bibr" target="#b14">[15]</ref> and for automated medical coding <ref type="bibr" target="#b0">[1]</ref>. Founded on the studies above, our approach provides a richer label-wise attention mechanism at both the word and the sentence level for automated medical coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Label Correlation</head><p>In multi-label classification, labels are potentially correlated to each other. As the example in Section 1, the medical codes of "Pneumonia" and "Acute Respiratory Failure" tend to appear together in the MIMIC-III discharge summaries. In automated medical coding, the number of unique code |Y| is large (|Y| = 8, 922 in the MIMIC-III dataset) and further the possible label relations (e.g. the number of pairwise combinations is near to |Y| 2 ). Such correlations among the labels represent additional knowledge that could be exploited to improve performance <ref type="bibr" target="#b20">[21]</ref>. This issue of label correlation (or "label dependence") remains an ongoing challenge <ref type="bibr" target="#b20">[21]</ref> in multi-label classification, especially with deep learning models. Deep learning for multilabel classification mostly represents the label space with orthogonal vectors: each label as a one-hot vector and each label set as a multi-hot representation, in general domains <ref type="bibr" target="#b10">[11]</ref> and clinical domains <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1]</ref>. Combined with the sigmoid activation and binary cross-entropy loss, this overall approach, effectively, assumes independence among labels.</p><p>One recent approach to address the problem is through weight initialisation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>: initialising higher weights for dedicated neurons (each encoding a co-occurrence relation among labels) in the final hidden layer. The approach showed performance improvement, however, it is not computationally efficient to assign each neuron in the final hidden layer to represent one of the massive (even the pairwise) patterns of label relations. An alternative method is through regularisation in <ref type="bibr" target="#b14">[15]</ref> to enforce the output layer of the neural network to 4 satisfy constraints on label relations. This requires to further tune the hyper-parameters of the regularisers so that a relatively marginal improvement (0.5-1.5% example-based F 1 on scientific paper abstracts and questions in social Q&amp;A platforms) could be achieved. In this study, we further propose a novel effective weight initialisation approach to tackle the label correlation problem, by initialising pre-trained dense label embeddings instead of the sparse co-occurrence representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>We formalise automated medical coding from clinical notes as a multi-label text classification problem <ref type="bibr" target="#b10">[11]</ref>. With deep learning, multi-label classification mainly contains two, integrated parts, (i) a neural document encoder, representing documents into a continuous representation, and (ii) a prediction layer, matching the document space to the label space. We present the problem formalisation and the deep learning based multi-label classification in Section 3.1. Then, regarding the neural document encoder, we propose the hierarchical labelwise attention network in Section 3.2, followed by the idea of label embedding initialisation in the prediction layer in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation with Deep Learning Models</head><p>Formally multi-label classification can be defined as follows.</p><p>Suppose X denoting the collection of textual sequences (e.g. clinical notes), and Y = {y 1 , y 2 , ..., y |Y| } denotes the full set of labels (i.e. ICD codes) of size |Y|. Each instance x d ? X is a word sequence of a document, where d is the document index.</p><formula xml:id="formula_0">Each x d ? X is associated with a label set Y d ? Y. Each label set Y d can be represented as a |Y|-dimensional multi-hot vector, ? ? Y d = [y d1</formula><p>, y d2 , ..., y d|Y| ] and y dl ? {0, 1}, where a value of 1 indicates that the lth label y l has been used to annotate (is relevant to) the dth instance, and 0 indicates irrelevance. The task is to learn a complex function f : X ? Y based on a training set </p><formula xml:id="formula_1">D = {x d , ? ? Y d |d ? [1, m]},</formula><p>The loss function is commonly the binary cross-entropy loss <ref type="bibr" target="#b10">[11]</ref> as defined in Equation 2, which measures the sum of negative log-likelihood of the predictions p dl of the actual labels. A large deviation between ? ? Y dl and p dl will cause a greater value in the L CE and thus will be penalised during training.</p><formula xml:id="formula_3">L CE = ? d l ( ? ? Y dl log(p dl ) + (1 ? ? ? Y dl ) log(1 ? p dl ))<label>(2)</label></formula><p>For inference, a calibration threshold Th (default as 0.5) is set to assign the label to the document when p dl &gt; Th.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical Label-wise Attention Network</head><p>Following the framework above, the neural document encoder in HLAN (as illustrated in <ref type="figure" target="#fig_2">Figure 1</ref>) takes into input the word sequence x d = {x d1 , x d2 , ..., x dn }, where x di denotes the sequence of tokens in the ith of all n sentences, and output the document representation. The distinction to HAN <ref type="bibr" target="#b9">[10]</ref> is that HLAN represents the same document differently at both the word-level and the sentence-level regarding different labels.</p><p>HLAN extends the contextual vectors in HAN to the label-wise contextual matrices, V w and V s . The document representation also becomes a matrix, C d , where each row (corresponding to each label) has the same dimensionality as h. <ref type="figure" target="#fig_2">Figure 1</ref>, the model consists of an embedding layer, hidden layers (hierarchical label-wise attention layers), and a prediction (or projection) layer. First, the embedding layer transforms the one-hot input representation u di of each token in the sequence of the ith sentence x di into a lowdimensional continuous vector, e di = W e u di , where we used the neural word embedding algorithm, Word2vec <ref type="bibr" target="#b23">[24]</ref>, to pre-train W e for its efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>Second, we applied the Gated Recurrent Unit (GRU) <ref type="bibr" target="#b24">[25]</ref>, a type of RNN unit, to capture long-term dependencies in the clinical narrative. An RNN unit "reads" each token in the sequence one by one, every time producing a new hidden state h (t) , corresponding to the token at time t. Different from the vanilla RNN unit, GRU additionally considers the previous tokens by using a reset gate r (t) and an update gate z <ref type="bibr">(t)</ref> . This allows to model the dependencies among tokens in long sequences. A GRU can be formally defined as in the Equations 3 below, where ? ? h denotes the hidden states through forward processing, ? is a non-linear activation function (e.g. sigmoid function), W hr , W hz , W hh ? R dh?dh are weights, and b r , b z ? R dh represent bias terms. A bi-directional adaptation was applied by concatenating the hidden states at each time after read the sequence both forwardly (?) and backwardly (?) to form a more com-</p><formula xml:id="formula_4">prehensive representation, h (t) = [ ? ? h (t) ; ? ? h (t) ] ? R 2dh .</formula><p>This subarchitecture is generally known as Bi-GRU <ref type="bibr" target="#b24">[25]</ref>.</p><formula xml:id="formula_5">r (t) = ?(W er e (t) + W hr ? ? h (t?1) + b r ) z (t) = ?(W ez e (t) + W hz ? ? h (t?1) + b z ) h (t) = tanh(W eh e (t) + W hh (r (t) ? ? ? h (t?1) )) ? ? h (t) = (1 ? z (t) ) ? ? ? h (t?1) + z (t) ?h (t)<label>(3)</label></formula><p>For simplicity, we use the function h = Bi-GRU(e, ?) to denote the whole process (with bi-directional concatenation of hidden states) above. Instead of applying one single Bi-GRU layer to represent the whole document, we applied a word-level</p><p>Bi-GRU to represent each sentence and then a sentence-level one to represent the whole document, as illustrated in <ref type="figure" target="#fig_2">Figure   1</ref>. This captures the hierarchical structure of the document and relieves the burden of having a too lengthy sequence for each GRU <ref type="bibr" target="#b0">[1]</ref> (e.g. from the original sequence length 2500 in the MIMIC-III discharge summaries to only 100 on the word level and 25 on the sentence level).</p><p>A common way is to represent the whole sequence as the concatenated hidden state h (t) of the last time t. This representation tends to emphasise the ending elements (i.e. words or sentences) and does not discriminate between the elements in a sequence. In fact, the key information for medical cod-ing is contained in a well selected part of the lengthy discharge summary. We therefore use an attention mechanism to learn a weighted average of the hidden states to form a final representation as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>. The attention scores are based on an alignment (or a similarity computation) of each hidden representation in a sequence to a context vector. The context vector is usually shared for all labels as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>, whereas in medical coding, it is essential to interpret the amount of attention paid regarding a specific medical code to the clinical note.</p><formula xml:id="formula_6">h (i) = Bi-GRU(e, ? w ) v (i) = tanh(W w h (i) + b w ) ? (i) wl = exp(V wl ? v (i) ) o?[1,nt] exp(V wl ? v (o) ) C sl = i?[1,nt] ? (i) wl h (i)<label>(4)</label></formula><p>Thus, the adapted, label-wise word-level attention mechanism is defined in Equations 4 above. The context matrix for the word-level attention mechanism is denoted as</p><formula xml:id="formula_7">V w ? R |Y|?dw ,</formula><p>where each row V wl (of attention layer size d w ) is the context vector corresponding to the label y l . The attention score ? (i) wl for the label y l is calculated as a softmax function of the dot product similarity between the vector representation v (i) (transformed from the ith hidden state h (i) with a feed-forward layer) and the context vector V wl for the same label. n t denotes the number of tokens in a sentence. The sentence representation C sl , as a row vector in C s ? R |Y|?2dh , for the label y l , is computed as the weighted average of all the hidden state vectors</p><formula xml:id="formula_8">h (i) .</formula><p>In a similar way, we can compute the label-wise sentencelevel attention mechanism as defined in Equations 5, which encodes each row C sl in the sentence representations C s to a label-wise sentence representation S (r) l , to be non-linearly transformed to U (r) l and aligned to the corresponding row V sl in sentence-level contextual matrix V s ? R |Y|?ds , and outputs the sentence-level attention scores ? sl (for a label y l ) and the document representation matrix C d ? R |Y|?4dh . To note that the dimensionality of S r l and thus C dl are further doubled to 4d h through the Bi-GRU process.</p><formula xml:id="formula_9">S (r) l = Bi-GRU(C sl , ? S ) U (r) l = tanh(W S S (r) l + b S ) ? (r) sl = exp(V sl ? U (r) l ) q?[1,n] exp(V sl ? U (q) l ) C dl = r?[1,n] ? (r) sl S (r) l<label>(5)</label></formula><p>Then, we use a label-wise, dot product projection with logistic sigmoid activation to model the probability of each label to each document, as defined in <ref type="bibr">Equation 6</ref>, adapted from Equation 1. The parameters in w l are row vectors in the projection matrix W.</p><formula xml:id="formula_10">p dl = ?(w l C dl + b l )<label>(6)</label></formula><p>We finally optimise the binary cross-entropy loss function in Equation 2 with L 2 regularisation using the Adam optimiser <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Label Embedding Initialisation</head><p>For automated medical coding, the diagnostic and procedural codes (or labels) have complex semantic relations, and can potentially be leveraged to improve prediction. Clinically, these code relations represent the correlation among diseases and medical procedures from the medical coding practice.</p><p>As we reviewed in Section 2.3, previous studies on weight initialisation to address the label correlation issue mostly focus on a co-occurrence based representation of labels <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Both studies dedicate a neuron in the final hidden layer to initialise one single co-occurrence pattern. There are, however, very limited neurons to be assigned to initialise the massive number of label relations, especially for the large label size in automated coding.</p><p>Instead of encoding the sparse co-occurrence patterns of labels, we learn low-dimensional, dense, label embeddings. For two correlated labels y j and y k , e.g. 486 (Pneumonia) and 518.81 (Acute respiratory failure), one would expect that the prediction of one label has an impact on the other label for some clinical notes, i.e. p d j is correlated or has a similar value to p dk .</p><p>To achieve this, according to Equations 1 or 6, we propose to initialise their corresponding weights w j and w k (corresponding to the labels y j and y k ) in W with a label representation E which reflects the actual label correlation (e.g. similarity between y j and y k ) in a continuous space.</p><p>A straightforward idea is thus to initialise the projection matrix W using E as pre-trained label embeddings, e.g. with a neural word embedding algorithm, learned from the label sets in</p><formula xml:id="formula_11">the training data, { ? ? Y d |d ? [1, m]}.</formula><p>For initialisation, we pre-train the label embeddings E with dimensionality the same as W.</p><p>We used the Continuous Bag of Words algorithm in word2vec <ref type="bibr" target="#b23">[24]</ref> for its efficiency and its power to represent the correlations of the labels. <ref type="figure" target="#fig_4">Figure 2</ref> shows an intuitive visualisation, for which we used an unsupervised technique, T-SNE (t-distributed Stochastic Neighbor Embedding), to reduce the dimensionality of the learned label embeddings while preserving the local similarity and structure of the labels <ref type="bibr" target="#b26">[27]</ref>. It can be observed that the This imposes a tendency for context vectors of the correlated labels to align the Bi-GRU encoded token representation v i in a geometrically similar way. Similarly, we can also initialise the label-wise attention layer in CNN+att <ref type="bibr" target="#b6">[7]</ref> and in HA-GRU <ref type="bibr" target="#b0">[1]</ref>. While the initialised layers are dynamically updated during the training, the tendency that imposed by label embeddings remains for most neural networks; we will empirically demonstrate this in the analysis of initialised layers in Section 4.8.</p><p>For automated coding, the approach can be extended by initialising label embeddings with the clinical ontologies and description texts of ICD codes. However, due to the different nature of the knowledge (i.e. embedded label relations), the external sources may bring contradictory label correlations to the ones in the dataset, as also discussed in <ref type="bibr" target="#b27">[28]</ref>. In this research, we focus on leveraging label relations from the label sets alone as in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, as it directly reflects the label correlation of the coding practice that generated the dataset, and leave the integration of external knowledge for a future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We tested HLAN and several strong baseline models, with label embedding initialisation, on three data sets based on the MIMIC-III database. The main results show the comparative results of HLAN to other state-of-the-art models on the datasets and the consistent improvement with label embedding initialisation. More importantly, through an analysis on model interpretability, we also show that HLAN can provide a more com- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We used the benchmark dataset, MIMIC-III ("Medical Information Mart for Intensive Care") <ref type="bibr" target="#b1">[2]</ref>, which contains clinical data from adult patients admitted to the critical care unit in the Beth Israel Deaconess Medical Center in Boston, Massachusetts between 2001 and 2012, to validate our approach.</p><p>The ICD-9 codes annotated by professionals in the dataset were used as labels. We focused on discharge summaries and followed the preprocessing and data split from <ref type="bibr" target="#b6">[7]</ref>. The preprocessed full MIMIC-III dataset has 8,922 unique codes as labels assigned to 52,724 discharge summaries, where 47,724 of them (from 36,998 patients) were used for training, 1,632 for validation, and 3,372 for testing. We also used the same top-50 setting (termed as "MIMIC-III-50") from <ref type="bibr" target="#b6">[7]</ref>, which narrows down the labels to the top 50 by their frequencies (codes and their frequencies are available in <ref type="table" target="#tab_2">Table S1</ref> in the supplementary material). This has 8,066 discharge summaries for training, 1,573 for validation, and 1,729 for testing.</p><p>We further created a subset of discharge summaries annotated using the COVID-19 shielding related ICD codes. This simulates the application of identifying key patients for shield-ing during the pandemic. We used the ICD-9 codes matched to the ICD-10 codes selected by the NHS to identify patients with medium or high risks during COVID-19. The considered patients were related to solid organ transplant recipients, people with specific cancers, with severe respiratory conditions, with rare diseases and inborn errors of metabolism, on immunosuppression therapies, or who were pregnant with significant congenital heart disease 2 , which is still in active use and under maintenance at the time of writing this paper. While the actual EHR data and the shielded patient list from the NHS are not easy to obtain, the ICD-10 codes are openly available for reuse <ref type="bibr" target="#b2">3</ref> . We thus used MIMIC-III to simulate the task of identifying patients for shielding during COVID-19. We selected those appeared at least 50 times in the MIMIC-III dataset, resulting in 20 ICD-9 codes (out of 79 matched codes), available in <ref type="table" target="#tab_3">Table   S2</ref> in the supplementary material. After filtering the MIMIC-III dataset with the selected ICD-9 codes, there are 4,574 discharge summaries for training, 153 for validation, and 322 for testing.</p><p>We name this dataset as "MIMIC-III-shielding".</p><p>Statistics of the three datasets are in <ref type="table" target="#tab_2">Table 1</ref>  frequencies. This is most pronounced in the full label setting ("MIMIC-III") and also presented in the other two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Settings</head><p>We implemented the proposed Hierarchical Label-wise Attention Network (HLAN) model and the other baselines for comparison:</p><p>1. CNN, Convolutional Neural Network, which is essentially based on <ref type="bibr" target="#b28">[29]</ref> for text classification, and applied in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> for automated medical coding.</p><p>2. CNN+att (or CAML), CNN with a label-wise attention mechanism, proposed in <ref type="bibr" target="#b6">[7]</ref>.</p><p>3. Bi-GRU, Bi-directional Gated Recurrent Unit <ref type="bibr" target="#b24">[25]</ref> for multi-label classification. The document representation is set as the last concatenated hidden state h (t) .</p><p>4. HAN, Hierarchical Attention Network <ref type="bibr" target="#b9">[10]</ref>, which can be considered as a downgraded model of HLAN when the attention mechanisms are shared for all labels (see <ref type="figure" target="#fig_2">Figure   1</ref>, when V w , V s , and C s , C d become vectors, same for all labels).</p><p>5. HA-GRU, Hierarchical Attention bi-directional Gated Recurrent Unit, proposed in <ref type="bibr" target="#b0">[1]</ref>, which can be considered as a downgraded model of HLAN when the word-level attention mechanism is shared for all labels (see <ref type="figure" target="#fig_2">Figure 1</ref>, when V w and C s become vectors, same for all labels, while V s and C d are the same as in HLAN).</p><p>We applied the label embedding initialisation approach (denoted as "+LE") to all the models above. We pre-trained the label embeddings E from the label sets in the training data with the word2vec (Continuous Bag of Words with negative sampling) algorithm <ref type="bibr" target="#b23">[24]</ref>. The label embeddings have the dimension same as the final hidden layer or the label-wise attention layer(s) in each neural network model. We applied the Python Gensim package <ref type="bibr" target="#b30">[31]</ref> to train embeddings, by setting the window size as 5 and minimum frequency threshold ("min count") as 0. Label embeddings were normalised to unit length for initialisation. Xavier initialisation <ref type="bibr" target="#b31">[32]</ref> was used for labels not existing in the training data for faster model convergence. We used the same setting to train and initialise the 100-dimension word embeddings W e from the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9</head><p>The implementations of HLAN and HA-GRU were adapted from our previous implementation 4 of HAN in <ref type="bibr" target="#b14">[15]</ref> using the Python Tensorflow <ref type="bibr" target="#b32">[33]</ref> framework, originated from brightmart's implementation <ref type="bibr" target="#b4">5</ref> , all under the MIT license. We adapted HA-GRU with the sigmoid activation and binary cross-entropy as described in Section 3.1, instead of the softmax activation used in the original paper <ref type="bibr" target="#b0">[1]</ref>, for a controlled comparison with other models. For CNN, CNN+att, and Bi-GRU, we adapted the implementation 6 from <ref type="bibr" target="#b6">[7]</ref> using the PyTorch framework <ref type="bibr" target="#b33">[34]</ref> with the same parameters for MIMIC-III and MIMIC-50 from <ref type="bibr" target="#b6">[7]</ref>. For MIMIC-III-shielding, we used the same hyperparameters as in MIMIC-50. We did not get the results with HA-GRU and HLAN for the MIMIC-III dataset, due to the memory limit caused by the large label size (|Y| = 8, 922), while for MIMIC-III-50 and MIMIC-III-shielding, we obtained the results of all models.</p><p>The input token length for the models was padded to 2,500 as in <ref type="bibr" target="#b6">[7]</ref>. We optimised the precision@k or micro-  <ref type="table" target="#tab_4">Table S3</ref> in the supplementary material.</p><p>We also experimented with BERT as the neural document encoder. Due to the GPU memory limit, we tested the nor-4 https://github.com/acadTags/Automated-Social-Annotation/ tree/master/2%20HAN 5 https://github.com/brightmart/text_classification 6 https://github.com/jamesmullenbach/caml-mimic <ref type="bibr" target="#b6">7</ref> We optimised precision@k for CNN, CNN+att, and Bi-GRU for MIMIC-III and MIMIC-III-50, and micro-F 1 for all other models and for the MIMIC-III-shielding dataset. <ref type="bibr" target="#b7">8</ref> We parsed sentences using the rule-based pipeline component in Spacy with adding double newlines as another rule to segment sentences, see https: //spacy.io/usage/linguistic-features#sbd. mal size of a BERT model, i.e. BioBERT-base <ref type="bibr" target="#b34">[35]</ref>, which had been further pre-trained with PubMed paper abstracts 9 and full texts 10 ; we used a sliding window approach to address the token limit issue (512 tokens) in BERT. Our results from the BioBERT-base model were similar to the results in <ref type="bibr" target="#b18">[19]</ref>, significantly worse than HLAN and CNN <ref type="bibr" target="#b10">11</ref> . We believe further adaptations are necessary for BERT models on automated medical coding and leave the direction for a future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>For comparison, we applied the same set of label-based metrics as in <ref type="bibr" target="#b6">[7]</ref> and according to the evaluation of multi-label classification algorithms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref>. The chosen metrics include micro-and macro-averaging precision (P), recall (R), </p><p>In some clinical application or epidemiological studies, only one type of code (either the diagnosis or the procedure code) is favoured. Thus, for the MIMIC-III full label setting, we also report Micro-F 1 results on the diagnosis codes (F 1 -diag) and procedure codes (F 1 -proc) separately as in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Furthermore, we report the example-based metric, precision@k as in <ref type="bibr" target="#b6">[7]</ref>, averaged over all the documents, <ref type="bibr" target="#b8">9</ref> https://pubmed.ncbi.nlm.nih.gov/ 10 https://www.ncbi.nlm.nih.gov/pmc/ <ref type="bibr" target="#b10">11</ref> We thus do not report the BERT results here but make the implementation details and results available on https://github.com/acadTags/ Explainable-Automated-Medical-Coding.</p><p>where each precision score is the fraction of the true positive in the top-k labels, having highest score p dl , for the document d.</p><p>The idea is to simulate the real-world scenario that the system recommending k predicted medical codes and to evaluate the percentage of them being correct. The number of top-ranked labels k were set as 8 for MIMIC-III, 5 for MIMIC-III-50 to be consistent to the study <ref type="bibr" target="#b6">[7]</ref>, and 1 for MIMIC-III-shielding, near to the average number of labels per document (see <ref type="table" target="#tab_2">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Main Results</head><p>We report the mean and the standard deviation (i.e. the square root of variance) of the testing results of 10 runs with randomly initialised parameters for each model. The results of the MIMIC-III-50, MIMIC-III-shielding, and MIMIC-III datasets are shown in <ref type="table" target="#tab_3">Table 2</ref>, 3, and 4, respectively.</p><p>For the top 50 label dataset (MIMIC-III-50, see <ref type="table" target="#tab_3">Table 2</ref> For code related to high-risk patients for shielding during the COVID-19 pandemic (MIMIC-III-shielding, see <ref type="table" target="#tab_4">Table 3</ref>), results (of Micro-AUC) show that HLAN (96.9%) and HAN (97.6%) performed comparably to the best performed model, CNN (97.9%). HLAN obtained a high value of precision@1, slightly below CNN by 1% (81.2% vs 82.2%), while the difference was not significant (p &gt; 0.05). The better performance of CNN (or HAN) may be because that smaller datasets like MIMIC-III-shielding, with much fewer documents and labels (see <ref type="table" target="#tab_2">Table 1</ref>), tends to favour models with simpler architectures.</p><p>In both MIMIC-III-50 and MIMIC-III-shielding, HA-GRU did not perform better than HLAN, this shows that the labelwise word-level attention mechanisms in HLAN further improved the performance. Also, surprisingly, the HLAN or HAN models with the real sentence split did not perform better (up to 2.8% less Micro-F 1 ) than using text chunk "sentences" (of 25 continuous tokens) in all three datasets. This is probably because, with the sentence split setting, some tokens and sentences were lost during the padding procedure, which could significantly affect the performance.</p><p>For the full label setting ("MIMIC-III"), HAN has better results of Micro-AUC and precision@8 than the vanilla CNN and Bi-GRU, but worse than the CNN+att approach specifically tuned for this dataset. With label embedding initialisation, CNN+att+LE achieved significant best results on MIMIC-III (an Micro-AUC of 98.6%). It is worth to further explore to enhance the scalability of HLAN so that it can process datasets with large label sizes. Also to note that results of the Macro-level metrics (averaging over labels) were dramatically lower than the Micro-level ones (calculated from documentlabel pairs), showing the strong imbalance of labels in MIMIC-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III (see Section 4.1).</head><p>Injecting the code relations through label embedding consistently boosted the performance of automated medical coding. It is clear that most models were improved with label embedding initialisation ("+LE"). Models were affected to different extend by label embedding: CNN+att model was mostly improved with "+LE" (an increase of 6.6% Macro-AUC on MIMIC-IIIshielding), the rest models (CNN, Bi-GRU, HA-GRU) being relatively less affected, while there was no significant improvement for HLAN or HAN on the datasets. This may due to the fact that the prior layers, e.g. hierarchical layers and the label-wise attention layers, could already learn some of the label relations. We thus further analyse the LE-initialised layers in Section 4.8 to understand the effect of label embedding initialisation. Besides, most metrics with the "+LE" models also have higher stability (i.e. reduced variance); and low variance is an essential characteristic to deploy a model in the clinical setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Result for each label</head><p>Apart from the overall performance of the models, it is also essential to see how the models perform regarding each medical code. <ref type="figure" target="#fig_9">Figures 3</ref> show the precision and recall of the five diagnosis codes having the highest and the lowest frequencies in MIMIC-III-50. For this analysis, we selected the three best performing models, CNN, CNN+att, and HLAN, all with label embedding initialisation ("+LE"), in terms of AUC metrics for MIMIC-III-50 (see <ref type="table" target="#tab_3">Table 2</ref>). We provide the full per-label results of HLAN+LE with the MIMIC-III-50 and MIMIC-IIIshielding datasets in <ref type="table" target="#tab_2">Table S1</ref>-S2 in the supplementary material.</p><p>In Figures 3, we can observe that the overall trend of performance is generally consistent to, while not solely dependent on, the label frequency in the training data. For the five most frequent labels, the models achieved around 70%-90% precision and recall. For example, in terms of precision, HLAN obtained The results of better metric score between the model with label embedding initialisation ("+LE") and the model not using LE initialisation are underlined, and the asterisk (*) further marks the paired two-tailed t-tests with .95 significant level (p &lt; 0.05) between them. The best result for each metric (column) is in bold. The AUC, F 1 , and P@5 scores in HLAN models with italics indicates their significantly improved results (p &lt; 0.05) over the second best model category (i.e. HLAN vs. CNN). The model with lower variance is preferred if the average scores are the same. The results of better metric score between the model with label embedding initialisation ("+LE") and the model not using LE initialisation are underlined, and the asterisk (*) further marks the paired two-tailed t-tests with .95 significant level between them. The best result for each metric (column) is in bold. The AUC, F 1 , and P@1 scores in CNN models with italics indicates their significantly improved results (p &lt; 0.05) over the second best model category (i.e. CNN vs. HAN or HLAN). The model with lower variance is preferred if the average scores are the same.</p><p>highest to 91.7% for 427.31 (Atrial fibrillation) and lowest to 71.4% for 584.9 (Acute kidney failure). For the five least frequent labels, the results were much worse due to the fewer training data for the labels and the imbalance issue. For precision, HLAN generally performs better than CNN and CNN+att, especially there is a significant gap for low frequent labels; while for recall, CNN outperforms the other two models. We also note that the precision and recall could be tuned in favour of only one of them through changing the calibration threshold Th (now set as the default value, 0.5), considering the need and the preference of the coding work when deploying the model to support coding professionals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Model Explanation with Hierarchical Label-wise Attention Visualisation</head><p>A critical requirement of the clinical use of automated medical coding systems is their explainability or interpretability.</p><p>We propose to use label-wise word-level and sentence-level attention mechanisms in HLAN to enhance the explainability of the model. The learned word-level and sentence-level attention scores for the label y l are ? wl ? (0, 1) and ? sl ? (0, 1) (see Equations 4 and 5, respectively). For a more concise visualisation, we propose a sentence-weighted word-level attention score? wl to only highlight the words from the salient sentences.</p><p>This adapted word-level attention score is calculated as? wl = ??s l ? wl , where ?s l is the attention score of the sentence where the word is belong to and ? is a hyperparameter to control the The results of better metric score between the model with label embedding initialisation ("+LE") and the model not using LE initialisation are underlined, and the asterisk (*) further marks the paired two-tailed t-tests with .95 significant level between them. The best result for each metric (column) is in bold. The model with lower variance is preferred if the average scores are the same.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Comparison of Model Explanations</head><p>Following the previous section, we further qualitatively analyse and compare the interpretability of the HLAN model and other baseline models. <ref type="table" target="#tab_6">Table 5</ref> shows how CNN, CNN+att, HAN, HA-GRU, and HLAN, all with label embedding initialisation, highlight the "important" part of a random document (number 24) to predict two different labels (427.31 and 428.0).</p><p>The CNN <ref type="bibr" target="#b11">12</ref> and CNN+att chose the most salient n-grams based on the max-pooling and the attention mechanism, respectively <ref type="bibr" target="#b6">[7]</ref>. HLAN and its downgraded models, HA-GRU and HAN, alternatively, highlighted the important sentences and words.</p><p>The distinction is that HAN has the same highlights of the same document for different labels (columns in <ref type="table" target="#tab_6">Table 5</ref>), and HA-GRU has the same word-level but different sentence-level highlights across labels, while HLAN can highlight the most salient words and sentences for different labels. This gives HLAN the most comprehensive interpretability among the models.</p><p>Compared to CNN, we observe that CNN+att generated a more relevant set of n-grams. This is in accordance with the conclusion in <ref type="bibr" target="#b6">[7]</ref>. We also found that the attention weights from CNN are unstable, i.e. the suggested n-grams from CNN were not the same among different runs. Compared to the interpretation with n-grams, highlighting the key sentences and words can produce a more comprehensive interpretation, as the latter is based on the whole hierarchical structure of a document. <ref type="bibr" target="#b11">12</ref> We further normalised the scores of n-grams in CNN based on max-pooling from <ref type="bibr" target="#b6">[7]</ref> to probabilities, to be comparable to the attention scores in other models.</p><p>Especially with the sentence parsing ("HLAN+LE+sent split", see the last row in <ref type="figure">Figure 5</ref>, corresponding to the visualisation in <ref type="figure" target="#fig_10">Figure 4</ref>), we can clearly see which sections of the discharge summary, along with words, contribute more to predict the label.</p><p>It is also interesting to see how the proposed model interpret when it predicted a medical code not previously assigned by the coding professionals. We selected some representative "false positive" results from the HLAN+LE model with sentence splitting in <ref type="table">Table 6</ref>. We presented the prediction results and the highlighted explanations to an experienced clinician to validate and deduce the potential reason for the error. In <ref type="table">Table   6</ref>, we observe that the model can explain the predictions with key sentences and words, therefore it is easier for us to know where there may have been a problem. For example, for the first two rows, "doc-68" and "doc-19" in MIMIC-III-50, the highlighted words and sentences are quite relevant to the noncoded, "false positive" ICD-9 code, indicating that there might have been missed coding or the disease was a past disease of the patient.</p><p>The false positives in "doc-1" and "doc-65" in MIMIC-IIIshielding are errors related to the wrong correlations learned from the data, particularly regarding the high granularity and subtle difference among sub-type diseases. In "doc-1", the highlighted words "htn elev lipids" show that the patient has a certain type of hypertension, but does not necessarily mean the predicted code 416.0 for "Primary pulmonary hypertension".</p><p>In "doc-65", the strongly highlighted word "metastasis" actu- CNN+LE n-gram-1 (0.105): admission date discharge date service surgery allergies patient recorded as having no known... n-gram-1 (0.096): ...surgical intensive care unit she required maximum pressor support to maintain sufficient cardiac index... n-gram-2 (0.083): ...surgical or invasive procedure ex lap r hemicolectomy mucous fistula ileostomy gj tube placement history of present illness... n-gram-2 (0.075): ...past medical history pmhx a fib aortic stenosis chf last ef in osteoporosis reflux... n-gram-3 (0.075): ...presented to location un with perforated viscous hd stable upon transfer to location un... n-gram-3 (0.071):...on ventilation support family meeting at latter evening decided to make patient cmo patient...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN+att+LE</head><p>n-gram-1 (0.026): ...upon arrival past medical history pmhx a fib aortic stenosis chf last ef in osteoporosis reflux doctor first name hx appendectomy many years ago social history non...</p><p>n-gram-1 (0.017): ...pressor support to maintain sufficient cardiac index patient did show signs of distal ischemia to extremities by the afternoon urine output post... n-gram-2 (0.023): ...diagnosis cardiopulmonary arrest perforated colon atrial fibrillation ventilatory support discharge condition death discharge instructions none followup instructions none n-gram-2 (0.011): ...past medical history pmhx a fib aortic stenosis chf last ef in osteoporosis reflux doctor first name hx appendectomy many years ago social history non contributory... * * For CNN and CNN+att, some of the suggested top-3 n-grams were combined together if any of them overlapped; up to five tokens before and after the top n-grams were also displayed. * * * For HAN, HA-GRU, and HLAN, the sentences were selected by those with sentence-level attention scores above 0.1 and the words were selected by those with the word-level attention scores above 0.01. Both HAN and HA-GRU predicted 427.31 but not 428.0. The word-and sentence-level attention weights of HAN are shared for all labels, therefore the interpretation is the same for both columns.</p><p>ally is related to pancreatic cancer, rather than the more common lung cancer as predicted. This wrong correlation may be due to the imbalance of vocabularies in the training data:</p><p>there are 94 (about 2% out of 4,574) discharge summaries in the training data where "pancreatic" and "metasta" appeared together in MIMIC-III-shielding, while there are significant more discharge summaries (661, about 14%) where "lung" and "metasta" appeared together.</p><p>The error in the last example was due to the subtle difference between two codes (280.00 vs 280.03). We noticed some unexpected highlights (e.g. the local oncologist's name code) in the last example ("doc-95" in MIMIC-III-shielding). This may be related to that "hypercalcemia" (appeared in both sent-1 and sent-2) can be caused by cancer, while neutropenia can be caused by treatments like cancer chemotherapy. The word "chemotherapy" was highlighted in another sentence with an attention weight 0.09 (not presented as below the 0.1 threshold) and the word "neutropenia" in the document was not included during the padding process. While it is very likely that the neutropenia was induced by the drug for chemotherapy (that is, 280.03, Drug induced neutropenia), we did not find direct words in the report to point the cause of the disease (thus 280.00, Neutropenia, unspecified, is also appropriate).</p><p>In general, we observe that the label-wise attention mecha-  an essential reference to help coding professionals use the system and help engineers fix the problems for the next system iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Analysis of Label Embedding Initialisation</head><p>We previously visualised the label embedding from the MIMIC-III dataset reduced to two dimensions using T-SNE, in the Jaccard Index to measure the degree of overlap between the two sets for each label, which is the size of the intersection divided by the size of the union of the two sets. We averaged the Jaccard Index over the labels. Thus the final metric reflects how the layers can retain the semantics, i.e. label similarities, of the label embedding E. We also used the models without the LE initialisation as the control group and calculated this averaged Jaccard Index from their layers for comparison.</p><p>The results are displayed in <ref type="figure">Figure 5</ref>. We selected several representative models that either were significantly improved with LE initialisation approach or did not improve with LE according to the results in Tables 2-4. The experiment shows that weights in the final projection layer (and the label-wise attention layer, if applied) with LE initialisation ("+LE") can capture further label similarities from the label embedding. We also observed a strong correlation between the performance improvement with LE (see <ref type="table" target="#tab_3">Tables 2-4</ref>) and the increase of averaged Jaccard Index with LE initialisation (i.e. the extent that the ini-tialised layers captures the semantics of LE after training, as reflected in <ref type="figure">Figure 5</ref>). The models which are more enhanced by LE (for example, CNN+att, with 6.6% improvement of Macro-AUC with "+LE" in <ref type="table" target="#tab_4">Table 3</ref>) have a greater averaged Jaccard Index compared to the models without LE (0.76 vs. 0.42-0.43) in <ref type="figure">Figure 5</ref>. On the contrary, the models which were not improved with LE, e.g. HLAN and HAN, for automated coding, also, was also not affected by LE in terms of the averaged Jaccard Index. The less effect of LE on HLAN and HAN may be because the hierarchical attention layers (especially with the label-wise attention mechanisms) could already model certain label correlations through the document-level matching process. In overall, the analysis supports the idea that LE initialisation, capturing the label correlations, is a key factor to enhance automated coding with deep learning based multi-label classification. Since LE can be visualised after dimensionality reduction (see <ref type="figure" target="#fig_4">Figure 2</ref>), this further serves as a mean to help explain the overall model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We have presented the results on the three datasets and analysed the interpretability of models and the layers initialised The label embedding initialisation approach boosted the performance and reduced the variance of most models. The method is efficient, not requiring further model parameters. It is independent of the neural encoders, and can thus be applied to various deep learning models for multi-label classification. Our analysis on the LE initialised layers show that they can preserve the semantics in the pre-trained label embeddings and therefore can better capture the label similarity from the data. This further contributes to the explainability of the overall approach.</p><p>There are a few exceptions that LE did not improve the performance, this may be due to the fact that the hierarchical layers can already model certain label correlations when optimising the document-label matching.</p><p>In terms of the performance, for MIMIC-III-50, the HLAN model with LE achieved significant better micro-level AUC (91.9%) and F 1 score (64.1%) than the previous state-of-the-art models; for MIMIC-III-shielding, HLAN and HAN performed comparably to CNN (all around 97-98% micro-level AUC); for MIMIC-III, the previous state-of-the-art model CNN+att was significantly boosted by LE initialisation, achieving best AUC and F 1 scores (Micro-level AUC of 98.6% and Micro-level F 1 of 52.5%).</p><p>It is worth nothing that the higher comprehensiveness in explanation from HLAN is at the cost of further memory requirements and the training time <ref type="bibr" target="#b12">13</ref> . Thus, in practice, if there are only limited computational resources (e.g. a single GPU with 12GB memory), we suggest training HLAN with a fewer number of codes, e.g. equal or less than 50, in a sub-disease domain or for specific tasks (i.e. shielding-related diseases during COVID-19) that require higher model explainability for decision making. We also notice that the vanilla CNN can be trained relatively faster with significantly less memory requirement; HAN and HA-GRU can also be applied as "downgraded" alternatives of HLAN for tasks with larger label sizes. It is also worth to explore to optimise the implementation and to distil the model of HLAN to enable its application to large label sizes.</p><p>While training deep learning models can be slow, during the testing phase, the trained models perform reasonably efficient for real-time inference. On average, it requires less than 1/3 second (330 milliseconds) to assign ICD codes with explainable highlights for a discharge summary with a CPU server using HLAN trained from MIMIC-III-50; and the CNN related models can process even faster (see <ref type="table" target="#tab_4">Table S3</ref> in the Supplementary Material). This allows the efficient use of the models in real-time for automated coding.</p><p>Also, the calibration threshold (default as 0.5) could be tuned to adjust the precision and recall of the system when deploying it to a coding department. While high precision is obtained when suggesting a few top-ranked predictions, a system with a higher recall can help the coding professionals to prevent <ref type="figure">Figure 5</ref>: Averaged Jaccard Index between the sets of top-10 similar labels derived from the layers (final projection layer and label-wise attention layers with or without label embedding initialisation) and from the label embedding (LE). The higher the averaged Jaccard Index, the more similar the overall semantics between the layer weights and the pre-trained label embedding. Error bars show the standard deviation over the labels. Representative models are selected for all the three datasets. "+LE" means label embedding initialised for the layer indicated in the closest left bar. A domain for future studies is therefore to investigate few-shot or zero-shot learning for the rare labels and we noticed one recent related work in <ref type="bibr" target="#b36">[37]</ref>, which is based on ICD-9 hierarchies and descriptions to better predict rare labels.</p><p>The results demonstrate the usefulness of label embedding to boost coding performance for most models. In this work, the label embedding was trained with the label sets in the training data using the Continuous Bag of Words algorithm in word2vec. Thus, it encodes the similarity of medical codes derived from the real-world coding practice in the critical care unit of the US hospital. This knowledge is distinct from the ICD-9 hierarchy, as visualised in <ref type="figure" target="#fig_4">Figure 2</ref> and there may be contradictions between them. The advantage of the former is that it directly learned the label correlation from the existing hospital and does not require external knowledge. There are recently more studies on leveraging the hierarchy for medical coding as in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. One future direction is thus to combine the local knowledge with external knowledge for the task.</p><p>The analysis of false positives in the model (see <ref type="bibr">Section 4.6</ref> and <ref type="table">Table 6</ref>) suggests further research in the area of automated medical coding. The errors are likely due to missed coding, past medical history rather than present diseases, nuances of language variations, imbalanced vocabularies, and high label granularity. The highlighted sentences and words helped us better determine the cause of the problems. Since missed coding is very common in real-world practice, as also pointed out recently in <ref type="bibr" target="#b39">[40]</ref>, it is worth to adapt the current algorithms to capture missing labels and emerging new labels. Information on the report template may further help the model select the relevant part of a discharge summary and differentiate a present disease from a past disease. Unable to capture the subtle variations or labels is potentially related to wrong correlations learned from the imbalance of vocabularies and labels in the dataset. This may be addressed by incorporating various external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we examined the existing deep learning based automated coding algorithms and introduced a new architecture, Hierarchical Label-wise Attention Network (HLAN) and a label embedding initialisation approach for automated medical coding. We tested the approaches on the benchmark datasets extracted from the MIMIC-III database, with the simulated task to predict ICD-9 codes related to the high-risk diseases selected by the NHS for shielding during the COVID-19 pandemic. The experiment results showed that HLAN has a more comprehensive explainability and better or comparative results to the previous state-of-the-art, CNN-based approaches and the down- to work on in the future is to adapt automated coding models with human corrections in real-time, which is mostly related to human-in-the-loop machine learning and active learning <ref type="bibr" target="#b40">[41]</ref>.</p><p>Inspire by these, we plan to further test and develop the approach to support the coding department in the NHS. We will consult professionals to identify and address the issues involved in deploying the system to facilitate coding staff and to improve efficiency, accuracy, and overall satisfaction. <ref type="table" target="#tab_2">Table S1</ref>: List of ICD-9 codes in MIMIC-III-50 (50 codes, sorted by frequency in the training data) and per-label prediction results using Hierarchical Label-wise Attention Network with label embedding initialisation (HLAN+LE).  Figure S1: Distribution of label frequency in the training data for the datasets, MIMIC-III, MIMIC-III-50, and MIMIC-III-shielding. "-" denotes that the parameter is inapplicable to the model or the estimated time was not obtained. * Parameter settings for CNN, CNN+att, and Bi-GRU are the same as in Mullenbach et al., 2018. ** All models were trained and tested using a single GeForce GTX TITAN X server. *** For HAN, HA-GRU, and HLAN, testing times on a CPU server (4-core, Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz) were further reported (displayed after the GPU time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIMIC-III-50</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>A formal comparison of the main deep learning based methods for automated coding. Experiments on three datasets based on the MIMIC-III discharge summaries, i.e. full code prediction, top-50 code prediction, and the NHS COVID-19 shielding-related code prediction, show the advantage of the proposed method over the state-ofthe-art methods (CNNs, Bi-GRU) and downgraded baselines (HA-GRU, HAN). Label embedding initialisation significantly improved the performance of neural network models in most evaluation settings. An analysis and comparison of the model interpretability demonstrate the most comprehensive explanations from the HLAN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where m is the number of instances in the training set. Neural document encoders in deep learning models (e.g. CNN, RNN, and BERT, as review in Section 2.2) represent each word sequence x as a continuous vector h, with matrix projection and non-linear activation. The representation h is projected to the label space and turned into p dl ? (0, 1) with the sigmoid function (?(x) = 1 1+e x ), as defined in Equation 1 below, where the weight w l (a row vector in W) and the bias b are parameters to be learned during the training process. The obtained p dl is the probability of the label (e.g. ICD code) y l being related to the document (e.g. discharge summary) d. p dl = ?(w l h + b), or collectively as p d = ?(Wh + b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Hierarchical Label-wise Attention Network (HLAN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>ICD- 9</head><label>9</label><figDesc>code learned from the MIMIC-III training label sets can capture the semantic relations that are distinct from the ICD-9 hierarchy. For example, 486 (Pneumonia) and 518.81 (Acute respiratory failure) appear closely on the bottom while they are not under the same parent in the ICD-9 hierarchy.Besides, the label embedding initialisation can also be applied to the context matrices V w and V s (seeFigure 1) in the label-wise attention mechanisms. Taking the word-level attention mechanisms in Equation 4 as an example, we can initialise V wl with the pre-trained label embedding E l for the label y l .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>The 2-dimensional T-SNE plot of word2vec Continuous Bag of Words label embeddings of the 50 ICD-9 codes in MIMIC-III-50, trained on the whole training label sets, { ? ? Y d |d ? [1, m]}, in MIMIC-III.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>prehensive explanation using the label-wise word and sentencelevel attention mechanisms. Analysis of the layers initialised with label embeddings further reveals the effect of the initialisation approach. The source code of our implementation and the results are openly available at https://github.com/ acadTags/Explainable-Automated-Medical-Coding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>F 1 metrics (defined in Section 4.3) during the training<ref type="bibr" target="#b6">7</ref> , according to the implementation in<ref type="bibr" target="#b6">[7]</ref>. The batch size for CNN, CNN+att, Bi-GRU were set as 16 as in<ref type="bibr" target="#b6">[7]</ref>, for HLAN and HA-GRU as 32, and HAN as 128. For HLAN, HAN, and HA-GRU, we tried both a customised rule-based parsing of real sentences with Spacy 8 and using text chunks of fix length as "sentences"; for both ways, we set the sentence length as 25 and padded the number of sentences to 100. The dimensions of the final document representation were 512, 500, 50, 400 for Bi-GRU, CNN, CNN+att, and HLAN (also HAN and HA-GRU), respectively.All models were trained using a single GeForce GTX TITAN X server, and the trained HLAN, HA-GRU, and HAN models were further tested using a CPU server (4-core, Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz). The detailed hyper-parameter settings, containing learning rate, dropout rate, and CNN specific parameters (kernel size and filter size), with the estimated training and testing times, are in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>F 1 score (F 1 ), area under the receiver operating characteristic curve (AUC), an the precision@k. The micro-averaging metrics treat each document-label as a separate prediction, whereas the macro-averaging metrics are an average of the per-label results. Micro-and macro-averaging applies to all the binary evaluation metrics including precision, recall, AUC. For example, the micro-and macro-averaged precision is defined in Equation 7 below. Recall is calculated in a similar way, but divided by all the true cases (T P l + FN l ), and F 1 is then the harmonic mean of the calculated precision and recall, i.e. F 1 = 2?P?R P+R . The AUC is defined by two metrics, the true positive rate (or recall) on the Y axis and false positive rate on the X axis, depicting the tradeoff between the two metrics when varying the calibration threshold Th [36]. The overall performance of a classifier (with a set of varied Th) can thus be reflected by AUC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>), HLAN performed the best among all experimental settings, achieved significantly better Micro-AUC (91.9%), Micro-F 1 (64.1%), and Precision@5 (62.5%) than the second best model, CNN. This shows the advantage of the hierarchical label-wise attention mechanisms for top-50 code prediction. With the same calibration threshold, the precision of HLAN is better than CNN absolutely by 15% (73.2% vs. 57.7%), while recall is lower with a similar absolute value, indicating that tuning the threshold to balance precision and recall could further improve the F 1 scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 :</head><label>3</label><figDesc>Precision and recall of the five most and the five least frequent ICD-9 diagnosis codes in the MIMIC-III-50 dataset. The bar chart (with the left y-axis) shows the metric score, while the line chart (with the right y-axis) shows the number of occurrences or the frequency ("Freq") of the label in the training data. magnitude of the final weighted attention score. A greater ? will result in highlighting more words in the clinical note and we empirically set ? as 5. We clip the value of? wl to 1 if it is above 1.An example attention visualisation for a random document (number 24) in MIMIC-III-50 using the model HLAN+LE with the parsed sentences ("+sent split") is shown inFigure 4. The two columns on the left visualise the sentence-level attention scores ? sl for the two codes, 427.31 (Atrial fibrillation) and 428.0 (Congestive heart failure, unspecified), respectively. The highlighted sentences are corresponding to the sections "past medical history" and "discharge diagnosis" in the discharge summary. This is in line with our intuition that the key diagnosis information is likely to be contained in the two sections.The words are highlighted according to the adapted word-level attention score? wl . Words related to the code 427.31 is highlighted in yellow and for 428.0 in blue. It is clear that the most salient words are highlighted, and the model successfully recognised the abbreviations and alternative short forms com-monly used by clinicians in the clinical note, for example "a fib" as a short form of atrial fibrillation and "chf" as the abbreviation of Congestive Heart Failure. This shows that the proposed HLAN model can learn to recognise the strongly correlated words (e.g. "a fib") related to the label (e.g. the code 427.31) with label-wise attention mechanisms, even given the fact that the label description (i.e. the knowledge that 427.31 is "Atrial Fibrillation") were not fed into the model during training. Other relevant words are highlighted, e.g., "ef" (short for Ejective Fraction), "pressor", and "extremities", which show a correlation to the code 428.0 while not indicating a causal relation to the diagnosis. We also note that the highlighted words like "age" and "drugs" were too general, which could not be di-rectly related to the diagnosis from a clinician's point of view. This may be related to the peaky distribution of the softmax (normalised exponential) function to form the attention scores (see Equation 4), paying the most of the attention to only a few (one or two) words in a long sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 :</head><label>4</label><figDesc>An example of interpretation using attention visualisation from the Hierarchical Label-wise Attention Network (HLAN), the chosen example is a random document (index 24) in the MIMIC-III-50 dataset with two true positive labels, ICD-9 code 427.31 (Atrial fibrillation) and 428.0 (Congestive heart failure, unspecified). The two red columns show the sentence-level attention scores for the two codes respectively. The tokens highlighted by yellow (for code 427.<ref type="bibr" target="#b30">31)</ref> or blue (for code 428.0) show the importance of them based on the value of sentence-weighted word-level attention scores. The deeper the colour, the higher the (sentence-weighted) attention scores, and thus the more important the highlight words or sentences contributes to the model prediction. Only the first part (11 tokens) of each sentence was shown for a clearer display.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>HAN+LE sent- 1</head><label>1</label><figDesc>(0.34): medical history pmhx a fib(0.374) aortic stenosis chf(0.206) last ef in osteoporosis reflux(0.097) doctor first name hx appendectomy many(0.28) years ago social history non contributory sent-2 (0.18): arthritis fosamax q week(0.039) coumadin(0.282) qd discharge medications none discharge disposition expired discharg diagnosis cardiopulmonary(0.019) arrest(0.109) perforated(0.134) colon(0.119) atrial(0.173) fibrillation(0.023) ventilatory(0.047) support discharge condition death sent-3 (0.11): admission(0.201) date(0.263) discharge(0.05) date(0.055) service(0.075) surgery(0.118) allergies(0.062) patient(0.013) recorded(0.054) as having no known allergies to drugs attending first name3(0.021) lf(0.02) chief complaint perforated bowel(0.046) major HA-GRU+LE sent-1 (0.62): arthritis fosamax q week coumadin(0.06) qd discharge medications none discharge disposition expired discharge diagnosis cardiopulmonary arrest perforated colon atrial fibrillation(0.94) ventilatory support discharge condition death Did not predict 428.0 (i.e. false negative) HLAN+LE sent-1 (0.54): arthritis fosamax q week coumadin qd discharge medications none discharge disposition expired discharge diagnosis cardiopulmonary arrest perforated colon atrial(1.0) fibrillation ventilatory support discharge condition death sent-1 (0.71): medical history pmhx a fib aortic stenosis chf(1.0) last ef in osteoporosis reflux doctor first name hx appendectomy many years ago social history non contributory sent-2 (0.18): medical history pmhx a fib(1.0) aortic stenosis chf last ef in osteoporosis reflux doctor first name hx appendectomy many years ago social history non contributory +sent split sent-1 (0.49): discharge diagnosis cardiopulmonary arrest perforated colon atrial fibrillation(1.0) ventilatory support discharge condition death discharge instructions none followup instructions none sent-1 (0.8): past medical history pmhx a fib aortic stenosis chf(0.888) last ef(0.112) in osteoporosis reflux doctor first name hx appendectomy many years ago social history non sent-2 (0.41): past medical history pmhx a fib(1.0) aortic stenosis chf last ef in osteoporosis reflux doctor first name hx appendectomy many years ago social history non * CNN and CNN+att suggested top n-grams, while HAN, HA-GRU, and HLAN suggested key sentences ("sent-") and words in the sentences. "+sent split" denotes the HLAN model using real sentence splits. The numbers in the parentheses are the attention scores (e.g. for HLAN, ? w and ? s ) in the models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>6 :</head><label>6</label><figDesc>Examples of false positives of HLAN (with label embedding "+LE" and sentence splitting "+sent split") on MIMIC-III-50 and MIMIC-III-shielding Document index (dataset) False positive ICD-9 code Explanation with the most relevant sentences and words by attention scores potential reason doc-68 (MIMIC-III-50) 427.31 (Atrial fibrillation) sent-1 (0.32): discharge diagnosis septic shock due to ascending cholangitis choledocholithiasis atrial fibrillation(1.0) with rapid ventricular response pulmonary emboli deep venous thrombosis upper gi bleed peptic ulcer missed coding sent-2 (0.31): past medical history recent pe dvt afib(1.0) htn hypotension hypothyroidism cad mild chf sent-3 (0.25): she was found to have bilateral pe s and new afib(1.0) and started on coumadin doc-19 (MIMIC-III-50) 401.9 (Hypertension NOS, or Unspecified essential hypertension) sent-1 (0.84): decision made to proceed with primary right total knee arthroplasty past medical history htn(1.0) asthma allergies diabetes social history nc family history nc past disease or missed coding doc-1 (MIMIC-III-shielding) 416.0 (Primary pulmonary hypertension) sent-1 (0.45): brief hospital course year old female with h o mild alzheimer s disease cea in htn(0.177) elev(0.145) lipids(0.659) bladder ca who presents as a transfer subtle difference in language (regarding the type of hypertension) sent-2 (0.36): past medical history mild alzheimer s disease l cea in htn(0.284) elev(0.167) lipids(0.518) bladder ca no known metastasis doc-65 (MIMIC-III-shielding) 197.0 (Secondary malignant neoplasm of lung) sent-1 (0.31): brief hospital course yo man with history of metastatic(1.0) pancreatic cancer was admitted with dyspnea new ascites and profound hyponatremia subtle difference in language (regarding the type of secondary cancer), imbalance of vocabularies or diseases in the training data sent-2 (0.3): history of present illness yo cantonese and spanish speaking male with metastatic(1.0) pancreatic cancer was admitted from the ed with dyspnea altered mental status and sent-3 (0.1): metastatic(1.0) pancreatic cancer evidence of progression of ct abdomen pelvis doc-95 (MIMIC-III-shielding) 280.00 (Neutropenia, unspecified) sent-1 (0.24): she has since been found to have a rising ldh and hypercalcemia and decided with her local(0.538) oncologist dr first name8(0.448) namepattern2 name stitle to subtle difference between the predicted label 280.00 and the ground truth label 280.03 (Drug induced neutropenia) sent-2 (0.17): at presentation on she developed hypercalcemic with a calcium(1.0) of an elevated ldh nisms in the HLAN model can provide a more comprehensive explanation to support the predictions. For wrong or non-coded predictions, the explanations through highlighted sentences and words can help us better understand the problem. This provides</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 2 .</head><label>2</label><figDesc>The visualisation intuitively shows how the label embedding can capture the correlations among ICD-9 codes derived from the coding practice in the clinical setting.It is also interesting to know, after the dynamic update during training, how the weights in the initialised layers (the final projection layer and the attention layer) preserve the semantics of the label embedding, and why, in a few cases, LE did not result in a significant improvement. We thus extracted the weights in the learned layers and measured their similarity to the original label embedding. Based on the idea of label similarity, we calculated the top-10 similar labels for every label based on the pairwise cosine similarity of the rows in the initialised layer weights (e.g. rows such as w j , w k in W in Equation 1 or rows V wl in V w in Equations 4), and also the top-10 similar labels from original label embedding E, and then to see to what extent the two sets of "top-10 similar labels" overlap. We used</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>with label embeddings.The main advantage of HLAN lies in its model explainability, based on the label-wise word-level and sentence-level attention mechanisms. The qualitative comparison of model explanation suggests that the highlighted key words and sentences from HLAN tend to be more comprehensive and more accurate than, those from HAN or HA-GRU and the n-gram explanation from the CNN related models. Such explainable highlights can be particularly helpful when medical coding professionals need to locate the essential part of a long clinical note. When the model suggests a code, its accompanying explanation could be served as a reference for professionals to validate whether the code should be included. This has the potential to build the users' trust in the deep learning model and help identify missed and erroneous coding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>missed coding. A higher recall can be achieved by using a lower calibration threshold, e.g. 0.3-0.4. The results are also highly varied across labels, as seen in the per-label results in Figures 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>graded models, HAN and HA-GRU. The proposed label embedding initialisation effectively boosted the performance of the state-of-the-art deep learning models, capturing label correlations from the dataset, which reflects the coding practice. Analyses on the experiment results of this work suggest that future studies are required in several areas: incorporating external knowledge, learning to capture missed coding, rare labels, and emerging new labels. In particular, automated medical coding work requires to be tested in real-world clinical settings and iteratively improved with inputs from relevant professionals such as coders, nurses and clinicians. Thus an open area</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The rest of the paper is organised as follows. First, we review the related work on automated medical coding with explainability, deep learning methods for multi-label classification, and 1 https://digital.nhs.uk/coronavirus/ shielded-patient-list/methodology label correlation in Section 2. Then, we present the problem formulation, followed by the proposed model, HLAN, and the idea of LE initialisation in Section 3. The experiments, including datasets, experimental settings, main and per-label results,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Denoted by Ave, the average number of labels per document (or label cardinality) in the training set of MIMIC-III, MIMIC-III-50, and MIMIC-III-shielding are 15.88, 5.69, and 1.08, respectively. While all originated from MIMIC-III database, the three datasets represent different case scenarios in automated medical coding with various scales of data and vocabulary size ("vocab"), number of labels to predict, and the average number of labels per document. While the full MIMIC-III dataset has much more training instances, it is more complex as its number of labels |Y| and vocabularies are significantly greater than MIMIC-III-50 and MIMIC-III-shielding.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasetsFigures of ICD-9 code distributions by frequency in the three datasets are available inFigure S1in the supplementary material, along with the list of the selected codes (and their frequencies) in the MIMIC-III and MIMIC-III-shielding datasets. The show a high imbalanced characteristics of the labels in all three data settings. Most label occurrences are from a few labels and there is a long-tail of labels having very low</figDesc><table><row><cell>Dataset MIMIC-III-50 MIMIC-III-shielding MIMIC-III</cell><cell>Vocab 59,168 8,066 1,573 1,729 50 Train Valid Test |Y| 47,979 4,574 153 322 20 140,795 47,724 1,632 3,372 8,922 15.88 Ave 5.69 1.08</cell></row></table><note>2 A clearer description of the "high risk" category is in https: //digital.nhs.uk/coronavirus/shielded-patient-list/ methodology/background3 To see the annexe B in https://digital.nhs.uk/coronavirus/ shielded-patient-list/methodology/annexes</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on MIMIC-III-50 dataset (50 labels)</figDesc><table><row><cell>Model CNN +LE Bi-GRU +LE CNN+att +LE HAN +LE HA-GRU +LE HLAN +LE +sent split 86.9?0.5 AUC 88.1?0.3 88.3?0.3 80.6?1.1 80.9?0.8 88.1?0.0 88.3?0.0  *  87.0?0.4 87.3?0.4 85.3?1.3 86.4?0.7  *  88.4?0.7 88.4?0.5</cell><cell>Macro R 51.5?0.9 P 67.4?1.0 53.0?1.0  *  66.7?1.5 47.2?3.2 36.7?2.6 47.3?2.0 39.2?2.0  *  63.1?0.1 48.4?0.2  *  64.3?0.3  *  46.0?0.1 61.7?2.7 46.3?2.3 61.3?3.2 46.9?2.9 59.3?2.5 43.1?4.0 62.1?1.9  *  44.3?2.3 65.0?1.2 51.0?2.6 65.5?1.5 50.2?1.1 63.6?1.3 47.8?2.4</cell><cell>F 1 58.4?0.5 59.1?0.5  *  41.2?2.3 42.8?1.5 54.8?0.2  *  53.6?0.1 52.8?1.1 53.0?1.1 49.9?3.4 51.7?1.9 57.1?1.6 56.8?0.8 54.5?1.7</cell><cell>AUC 90.9?0.2 91.3?0.1  *  85.5?1.0 85.8?0.7 91.1?0.0 91.3?0.0  *  90.1?0.3 90.3?0.4 89.2?0.9 90.1?0.5  *  91.9?0.4 91.9?0.3 90.4?0.3</cell><cell>Micro R 55.6?1.1 P 71.2?0.9 57.7?1.4  *  70.4?1.4 58.1?3.2 45.8?2.2 57.5?2.2 48.4?2.1  *  70.9?0.2 53.1?0.2  *  71.6?0.1  *  52.5?0.1 68.2?3.1 52.9?2.4 67.9?4.1 54.2?2.8 69.5?0.6 48.7?4.1 71.1?1.2  *  50.7?2.3 72.9?0.8 57.3?2.5 73.2?0.6 56.9?1.0 71.5?1.2 53.8?2.1</cell><cell>F 1 62.4?0.6 63.4?0.5  *  51.2?1.9 52.5?1.3  *  60.7?0.1 60.6?0.1 59.4?0.7 60.1?0.7 57.2?2.8 59.1?1.4  *  64.1?1.4 64.0?0.7 61.4?1.2</cell><cell>Top-k P@5 61.8?0.3 62.1?0.3 51.3?1.7 52.1?1.2 60.8?0.1 61.6?0.1  *  59.5?0.7 59.9?0.8 57.9?1.7 59.5?1.0  *  62.5?0.7 62.4?0.6 60.2?0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Results on MIMIC-III-shielding dataset (20 labels)</cell><cell></cell><cell></cell></row><row><cell>Macro R 59.8?1.2 P 59.6?1.3 60.4?2.8 60.4?2.3 57.4?3.1 43.4?2.2 58.6?1.5 46.8?2.3  CNN+att Model AUC CNN 96.9?0.2  *  +LE 96.7?0.2 Bi-GRU 91.9?1.4 +LE 92.0?1.6 88.9?1.3 46.7?4.6 37.6?2.4 +LE 95.5?0.0  HAN 96.0?1.4 66.4?2.7 58.2?2.0 +LE 96.4?1.3 65.2?2.1 56.5?2.9 HA-GRU 93.4?2.0 60.9?3.9 51.6?2.8 +LE 93.9?2.0 59.2?4.3 49.7?4.2 HLAN 93.5?2.5 59.8?2.9 53.2?2.6 +LE 93.5?1.9 60.5?4.2 52.7?5.0 +sent split 94.5?1.2 60.9?2.1 51.7?3.1</cell><cell>F 1 59.7?0.9 60.4?2.3 49.4?2.2 41.7?3.3 62.0?2.0 60.5?2.3 55.8?3.1 54.0?4.1 56.3?2.4 56.3?4.6 55.8?2.3</cell><cell>AUC 97.9?0.4  *  97.6?0.3 93.6?0.8 93.5?0.2 97.4?0.3 97.6?0.3 96.7?0.4 96.8?0.9 96.9?0.7 96.5?0.4 96.3?0.2</cell><cell>Micro R 80.5?1.3 P 76.2?0.9 78.8?2.5 76.4?1.8 77.9?2.8 58.5?1.9 86.9?1.2  *  52.9?2.8 82.9?1.8 68.7?2.4 83.4?1.2 68.2?2.0 83.0?2.1 65.8?2.2 81.3?4.0 66.3?4.1 81.4?1.8 69.0?2.9  *  81.8?2.8 65.6?4.0 81.4?2.2 64.4?2.5</cell><cell>F 1 78.3?1.0  *  77.5?0.7 66.8?1.2 65.7?2.0 75.1?1.5 75.0?1.2 73.4?1.6 73.0?3.6 74.6?1.6 72.7?3.1 71.9?1.7</cell><cell>Top-k P@1 82.2?0.8 81.6?0.9 72.2?1.6 70.0?2.6 78.1?1.7 79.2?1.7 80.3?1.5 79.1?4.3 81.2?1.2 79.8?3.0 77.8?2.6</cell></row></table><note>* 52.0?1.4* 95.1?0.7* 78.1?2.1 61.8?2.7* 68.9?1.6* 75.1?2.0** 62.1?2.2* 48.4?1.9* 54.4?2.0* 96.1?0.0* 83.3?0.5 61.4?0.6* 70.7?0.3* 77.7?0.3*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results on MIMIC-III dataset (8,922 labels) 0?0.1 51.0?2.8 36.9?1.7 42.8?0.9 41.1?1.0 50.7?0.9 59.6?0.5 +LE 82.4?0.4 * 4.7?0.4 3.6?0.2 4.1?0.2 97.1?0.1 53.0?2.6 36.9?1.2 43.4?0.6 41.7?0.6 51.3?0.9 60.3?0.4 43.9?0.4 51.7?0.1 50.1?0.2 59.8?0.1 69.4?0.2 +LE 90.2?0.0 * 9.3?0.1 * 8.0?0.1 * 8.6?0.1 * 98.6?0.0 * 61.8?0.4 45.6?0.1 * 52.5?0.1 * 50.7?0.1 * 60.7?0.1 * 69.7?0.1 * HAN 88.5?0.1 * 5.4?0.2 * 2.7?0.2 * 3.6?0.2 * 98.1?0.1 63.2?3.3 30.0?1.1 * 40.7?0.7 * 37.0?0.7 * 52.6?0.9 * 61.4?1.3 9?0.1 60.4?2.2 25.3?2.5 35.6?2.6 31.4?2.6 49.1?2.3 56.3?2.0</figDesc><table><row><cell>Macro R 81.8?0.7 4.5?0.4 AUC P 3.7?0.5 97.Bi-GRU Model F 1 AUC CNN 4.1?0.4 83.5?1.6 4.9?0.2 3.6?0.5 4.1?0.4 97.3?0.3 52.8?4.6 34.8?2.2 41.8?1.5 39.3?1.6 51.7?1.3 58.9?2.2 Micro Top-k P R F 1 F 1 -diag F 1 -proc P@8 +LE 84.9?0.7  *  5.0?0.4 3.6?0.5 4.2?0.5 97.6?0.1  *  55.4?4.1 34.8?2.4 42.6?1.5 40?1.6 52.7?1.1 60.3?1.8 CNN+att 88.6?0.2 7.7?0.2 6.4?0.3 7.0?0.2 98.4?0.0 62.8?0.3  +LE 88.2?0.2 5.1?0.2 2.4?0.1 3.3?0.2 98.1?0.0 63.2?1.0 27.6?1.1 38.4?1.0 34.8?1.2 50.6?1.0 59.6?0.6 +sent split 87.4?0.7 4.7?0.6 2.3?0.4 3.1?0.5 97.</cell></row></table><note>***</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of model interpretability across deep learning models of true positive predictions on a random document (index 24) in the MIMIC-III-50 dataset Model doc-24 to predict 427.31 (Atrial fibrillation) doc-24 to predict 428.0 (Congestive heart failure, unspecified)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S2 :</head><label>S2</label><figDesc>List of ICD-9 codes in MIMIC-III-shielding (20 codes, sorted by frequency in the training data) and per-label prediction results using Hierarchical Label-wise Attention Network with label embedding initialisation (HLAN+LE). Short titles of the ICD-9 codes are from https://mimic.physionet.org/mimictables/d_icd_diagnoses/ and https://mimic.physionet.org/mimictables/d_icd_procedures/.</figDesc><table><row><cell>MIMIC-III-shielding ICD-9 code 197.0 745.5 996.81 042 441.2 416.0 746.4 288.00 238.75 996.82 238.71 494.0 288.0 996.85 238.7 770.2 501 288.03 289.59 446.4</cell><cell>Short Title Secondary malig neo lung Secundum atrial sept def Compl kidney transplant Shigella boydii Thoracic aortic aneurysm Prim pulm hypertension Cong aorta valv insuffic Neutropenia NOS Myelodysplastic synd NOS Compl liver transplant Essntial thrombocythemia Bronchiectas w/o ac exac Neutropenia Compl marrow transplant Neoplasm of uncertain behav-ior of other lymphatic and hematopoietic tissues NB interstit emphysema Alastrim Drug induced neutropenia Spleen disease NEC Wegener's granulomatosis</cell><cell>Frequency (train, 4574 documents) 656 592 480 470 430 375 263 196 170 169 164 152 136 133 116 108 103 96 95 49</cell><cell>Frequency (test, 322 documents) 42 41 14 30 36 10 35 39 21 4 28 27 0 8 0 0 5 17 14 1</cell><cell>Precision Recall F 1 82.1 84.8 83.3 95.3 83.7 89.0 94.1 97.1 95.4 97.7 99.3 98.5 86.3 73.9 79.5 54.0 60.0 56.7 94.5 76.3 84.0 51.2 18.5 26.5 87.4 50.5 63.5 49.5 77.5 60.1 74.6 46.8 56.4 87.6 71.1 76.5 0.0 0.0 0.0 71.9 77.5 74.3 0.0 0.0 0.0 0.0 0.0 0.0 44.4 56.0 48.8 47.0 16.5 23.0 71.9 45.0 55.0 20.0 20.0 20.0</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S3 :</head><label>S3</label><figDesc>Model parameters, training time, and testing time from the datasets. Testing time per document, estimated in milliseconds, GPU time / CPU time * * *</figDesc><table><row><cell cols="6">CNN* CNN+att* Bi-GRU* HAN 0.5 0.5 0.5 0.5 0.003 0.0001 0.003 0.01 16 16 16 128 4 10 --2500 2500 2500 2500 ---25 ---100 500 50 512 ----100 Attention layer size (e.g. d w , d s in HLAN) 500 Parameter settings Calibration threshold Th Learning rate Batch size (training and testing) Kernel size (or filter size) # of words per document # of words per sentence n t # of sentences per document n # of filters Hidden size d h 50 -200 Final hidden layer size 500 50 512 400 Dropout rate 0.2 0.2 0 0.5 L 2 penalty 0 0 0 0.0001 0.0001 HA-GRU HLAN 0.5 0.5 0.01 0.01 32 32 --2500 2500 25 25 100 100 --100 100 200 200 400 400 0.5 0.5 0.0001 Training time, estimated in minutes  *  *  From MIMIC-III-50 5 50 40-50 10 30 80 From MIMIC-III-shielding 2.5 8 20-40 10 10-15 25-30 From MIMIC-III 250 1700 100-140 100 --</cell></row><row><cell>From MIMIC-III-50 From MIMIC-III-shielding From MIMIC-III</cell><cell>2 3 2</cell><cell>5 3 3</cell><cell>50 43 50</cell><cell>34 / 40 61 / 160 32 / 40 17 / 30 42 / 40 -</cell><cell>141 / 330 14 / 50 -</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">The estimated training and testing time of the models are inTable S3in the Supplementary Material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank Dr Johnson Alistair in the MIMIC-III team to confirm to display the sentences of discharge summaries in this paper. The authors would also like to thanks comments from Prof Cathierine Sudlow and other </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multilabel classification of patient notes: case study on ICD code assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baumel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nassour-Kassis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">H</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.35</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ICD-9-CM to ICD-10-CM codes: What? why? how?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cartwright</surname></persName>
		</author>
		<idno type="DOI">10.1089/wound.2013.0478</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Wound Care</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="588" to="592" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stewart</surname></persName>
		</author>
		<ptr target="https://www.beckersasc.com/asc-coding-billing-and-collections/icd-11-contains-nearly-4x-as-many-codes-as-icd-10-here-s-what-who-has-to-say.html" />
		<title level="m">ICD-11 contains nearly 4x as many codes as ICD-10: Here&apos;s what WHO has to say</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A systematic literature review of automated clinical coding and classification systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Stanfill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Fenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<idno type="DOI">10.1136/jamia.2009.001024</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association: JAMIA</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="646" to="651" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic diagnosis coding of radiology reports: A comparison of deep learning and conventional classification methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2342</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="328" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explainable prediction of medical codes from clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mullenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1100</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1101" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ethics of artificial intelligence in radiology: summary of the joint european and north american multisociety statement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Geis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ranschaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Jaremko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Kitts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Shields</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Association of Radiologists Journal</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="329" to="334" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">European union regulations on algorithmic decision-making and a &quot;right to explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Flaxman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="57" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Largescale multi-label text classification -revisiting neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loza Menc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>F?rnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>T. Calders, F. Esposito, E. H?llermeier, R. Meo</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A shared task involving multi-label classification of clinical free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Pestian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matykiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hovermale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Duch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing, BioNLP &apos;07</title>
		<meeting>the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing, BioNLP &apos;07<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining multi-label data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-09823-4_34</idno>
	</analytic>
	<monogr>
		<title level="m">Data Mining and Knowledge Discovery Handbook</title>
		<editor>O. Maimon, L. Rokach</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="667" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated social text annotation with joint multilabel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Coenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilabel neural networks with applications to functional genomics and text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2006.162</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An empirical study on large-scale multi-label text classification including few and zero-shot labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fergadiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kotitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01653</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Predicting ICD-9 codes from medical notes -does the magic of BERT applies here?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/reports/custom/report25.pdf,stan-fordCS224NCustomProject" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A tutorial on multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gibaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Survey</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved neural network-based multi-label classification with better initialization leveraging label co-occurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1063</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Initializing neural networks for hierarchical multilabel text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
		<idno>doi:10.18653/ v1/W17-2339</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="307" to="315" />
			<pubPlace>Vancouver, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Distributed representations of words and phrases and their compositionality</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Medical concept embedding with multiple ontological representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IJCAI</publisher>
			<biblScope unit="page" from="4613" to="4619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1181</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foote</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Tyler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">192360</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>?eh??ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16, USENIX Association</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16, USENIX Association<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-Performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno>1234-1240. doi:10.1093/ bioinformatics/btz682</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An introduction to roc analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2005.10.010</idno>
		<ptr target="https://doi.org/10.1016/j.patrec.2005.10.010" />
	</analytic>
	<monogr>
		<title level="m">rOC Analysis in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="861" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Few-shot and zero-shot multi-label learning for structured label spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kavuluru</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1352</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3132" to="3142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ontological attention ensembles for capturing semantic concepts in ICD code prediction from clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Falis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pajak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lisowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schrempf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deckers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mikhael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsaftaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6220</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019), Association for Computational Linguistics</title>
		<meeting>the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019), Association for Computational Linguistics<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">HyperCore: Hyperbolic and co-graph representation for automatic ICD coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.282</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3105" to="3114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Experimental evaluation and development of a silver-standard for the MIMIC-III clinical coding dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Searle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dobson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.bionlp-1.8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing</title>
		<meeting>the 19th SIGBioMed Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Human-in-the-Loop Machine Learning: Active learning and annotation for human-centered AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Monarch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Manning Early Access Program</publisher>
			<pubPlace>Shelter Island, NY</pubPlace>
		</imprint>
	</monogr>
	<note>MEAP Edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
