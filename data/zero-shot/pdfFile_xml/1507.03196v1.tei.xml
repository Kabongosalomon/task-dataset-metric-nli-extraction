<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepFont: Identify Your Font from An Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 26-30, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
							<email>jianchao.yang@snapchat.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Snapchat Inc 4 Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
							<email>jbrandt@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepFont: Identify Your Font from An Image</title>
					</analytic>
					<monogr>
						<title level="m">MM&apos;15</title>
						<meeting> <address><addrLine>Brisbane, Australia</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 26-30, 2015</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/XXX.XXXXXXX</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I47 [Image Processing and Computer Vision]: Fea- ture measurement</term>
					<term>I410 [Image Processing and Com- puter Vision]: Image Representation</term>
					<term>I5 [Pattern Recog- nition]: Classifier design and evaluation General Terms Algorithms, Experimentation Keywords Visual Font Recognition</term>
					<term>Deep Learning</term>
					<term>Domain Adapta- tion</term>
					<term>Model Compression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As font is one of the core design concepts, automatic font identification and similar font suggestion from an image or photo has been on the wish list of many designers. We study the Visual Font Recognition (VFR) problem <ref type="bibr" target="#b3">[4]</ref>, and advance the state-of-the-art remarkably by developing the DeepFont system. First of all, we build up the first available large-scale VFR dataset, named AdobeVFR, consisting of both labeled synthetic data and partially labeled realworld data. Next, to combat the domain mismatch between available training and testing data, we introduce a Convolutional Neural Network (CNN) decomposition approach, using a domain adaptation technique based on a Stacked Convolutional Auto-Encoder (SCAE) that exploits a large corpus of unlabeled real-world text images combined with synthetic data preprocessed in a specific way. Moreover, we study a novel learning-based model compression approach, in order to reduce the DeepFont model size without sacrificing its performance. The DeepFont system achieves an accuracy of higher than 80% (top-5) on our collected dataset, and also produces a good font similarity measure for font selection and suggestion. We also achieve around 6 times compression of the model without any visible loss of recognition accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Typography is fundamental to graphic design. Graphic designers have the desire to identify the fonts they encounter in daily life for later use. While they might take a photo of the text of a particularly interesting font and seek out an expert to identify the font, the manual identification process is extremely tedious and error-prone. Several websites allow users to search and recognize fonts by font similarity, including Identifont, MyFonts, WhatTheFont, and Fontspring. All of them rely on tedious humans interactions and high-quality manual pre-processing of images, and the accuracies are still unsatisfactory. On the other hand, the majority of font selection interfaces in existing softwares are simple linear lists, while exhaustively exploring the entire space of fonts using an alphabetical listing is unrealistic for most users.</p><p>Effective automatic font identification from an image or photo could greatly ease the above difficulties, and facilitate font organization and selection during the design process. Such a Visual Font Recognition (VFR) problem is inherently difficult, as pointed out in <ref type="bibr" target="#b3">[4]</ref>, due to the huge space of possible fonts (online repositories provide hundreds of thousands), the dynamic and open-ended properties of font classes, and the very subtle and character-dependent difference among fonts (letter endings, weights, slopes, etc.). More importantly, while the popular machine learning techniques are data-driven, collecting real-world data for a large collection of font classes turns out to be extremely difficult. Most attainable real-world text images do not have font label information, while the error-prone font labeling task requires font expertise that is out of reach of most people. The few previous approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref> are mostly from the document analysis standpoint, which only focus on a small number of font classes, and are highly sensitive to noise, blur, perspective distortions, and complex backgrounds. In <ref type="bibr" target="#b3">[4]</ref> the authors proposed a large-scale, learning-based solution without dependence on character segmentation or OCR. The core algorithm is built on local feature embedding, local feature metric learning and max-margin template selection. However, their results suggest that the robustness to realworld variations is unsatisfactory, and a higher recognition accuracy is still demanded. Inspired by the great success achieved by deep learning models <ref type="bibr" target="#b9">[10]</ref> in many other computer vision tasks, we develop a VFR system for the Roman alphabets, based on the Convolutional neural networks (CNN), named DeepFont. Without any dependence on character segmentation or content text, the DeepFont system obtains an impressive performance on our collected large real-word dataset, covering  an extensive variety of font categories. Our technical contributions are listed below:</p><p>? AdobeVFR Dataset A large set of labeled real-world images as well as a large corpus of unlabeled real-world data are collected for both training and testing, which is the first of its kind and is publicly released soon. We also leverage a large training corpus of labeled synthetic data augmented in a specific way.</p><p>? Domain Adapted CNN It is very easy to generate lots of rendered font examples but very hard to obtain labeled real-world images for supervised training. This real-to-synthetic domain gap caused poor generalization to new real data in previous VFR methods <ref type="bibr" target="#b3">[4]</ref>. We address this domain mismatch problem by leveraging synthetic data to obtain effective classification features, while introducing a domain adaptation technique based on Stacked Convolutional Auto-Encoder (SCAE) with the help of unlabeled real-world data.</p><p>? Learning-based Model Compression We introduce a novel learning-based approach to obtain a losslessly compressible model, for a high compression ratio without sacrificing its performance. An exact low-rank constraint is enforced on the targeted weight matrix.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain Mismatch between Synthetic and Real-World Data</head><p>To apply machine learning to VFR problem, we require realistic text images with ground truth font labels. However, such data is scarce and expensive to obtain. Moreover, the training data requirement is vast, since there are hundreds of thousands of fonts in use for Roman characters alone. One way to overcome the training data challenge is to synthesize the training set by rendering text fragments for all the necessary fonts. However, to attain effective recognition models with this strategy, we must face the domain mismatch between synthetic and real-world text images <ref type="bibr" target="#b3">[4]</ref>.</p><p>For example, it is common for designers to edit the spacing, aspect ratio or alignment of text arbitrarily, to make the text fit other design components. The result is that characters in real-world images are spaced, stretched and distorted in numerous ways. For example, <ref type="figure" target="#fig_3">Fig. 2</ref> (a) and (b) depict typical examples of character spacing and aspect ratio differences between (standard rendered) synthetic and real-world images. Other perturbations, such as background clutter, perspective distortion, noise, and blur, are also ubiquitous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The AdobeVFR Dataset</head><p>Collecting and labeling real-world examples is notoriously hard and thus a labeled real-world dataset has been absent for long. A small dataset VFRWild325 was collected in <ref type="bibr" target="#b3">[4]</ref>, consisting of 325 real-world text images and 93 classes. However, the small size puts its effectiveness in jeopardy.</p><p>Chen et. al. in <ref type="bibr" target="#b3">[4]</ref> selected 2,420 font classes to work on. We remove some script classes, ending up with a total of 2,383 font classes. We collected 201,780 text images from various typography forums, where people post these images seeking help from experts to identify the fonts. Most of them come with hand-annotated font labels which may be inaccurate. Unfortunately, only a very small portion of them fall into our list of 2,383 fonts. All images are first converted into gray scale. Those images with our target class labels are then selected and inspected by independent experts if their labels are correct. Images with verified labels are then manually cropped with tight bounding boxes and normalized proportionally in size, to be with the identical height of 105 pixels. Finally, we obtain 4,384 real-world test images with reliable labels, covering 617 classes (out of 2,383). Compared to the synthetic data, these images typically have much larger appearance variations caused by scaling, background clutter, lighting, noise, perspective distortions, and compression artifacts. Removing the 4,384 labeled images from the full set, we are left with 197,396 unlabeled realworld images which we denote as VFR real u.</p><p>To create a sufficiently large set of synthetic training data, we follow the same way in <ref type="bibr" target="#b3">[4]</ref> to render long English words sampled from a large corpus, and generate tightly cropped, gray-scale, and size-normalized text images. For each class, we assign 1,000 images for training, and 100 for validation, which are denoted as VFR syn train and VFR syn val, respectively. The entire AdobeVFR dataset, consisting of VFR real test, VFR real u, VFR syn train and VFR syn val, are made publicly available 2 .</p><p>The AdobeVFR dataset is the first large-scale benchmark set consisting of both synthetic and real-world text images, for the task of font recognition. To our best knowledge, so far VFR real test is the largest available set of real-world text images with reliable font label information (12.5 times larger than VFRWild325). The AdobeVFR dataset is super fine-grain, with highly subtle categorical variations, leading itself to a new challenging dataset for object recognition. Moreover, the substantial mismatch between synthetic and real-world data makes the AdobeVFR dataset an ideal subject for general domain adaption and transfer learning research. It also promotes the new problem area of understanding design styles with deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Synthetic Data Augmentation: A First</head><p>Step to Reduce the Mismatch Before feeding synthetic data into model training, it is popular to artificially augment training data using labelpreserving transformations to reduce overfitting. In <ref type="bibr" target="#b9">[10]</ref>, the authors applied image translations and horizontal reflections to the training images, as well as altering the intensities of their RGB channels. The authors in <ref type="bibr" target="#b3">[4]</ref> added moderate distortions and corruptions to the synthetic text images:</p><p>? 1. Noise: a small Gaussian noise with zero mean and standard deviation 3 is added to input ? 2. Blur: a random Gaussian blur with standard deviation from 2.5 to 3.5 is added to input ? 3. Perspective Rotation: a randomly-parameterized affine transformation is added to input ? 4. Shading: the input background is filled with a gradient in illumination.</p><p>The above augmentations cover standard perturbations for general images, and are adopted by us. However, as a very particular type of images, text images have various realworld appearances caused by specific handlings. Based on the observations in <ref type="figure" target="#fig_3">Fig. 2</ref> , we identify two additional fontspecific augmentation steps to our training data:</p><p>? 5. Variable Character Spacing: when rendering each synthetic image, we set the character spacing (by pixel) to be a Gaussian random variable of mean 10 and standard deviation 40, bounded by [0, 50].</p><p>? 6. Variable Aspect Ratio: Before cropping each image into a input patch, the image, with heigh fixed, is squeezed in width by a random ratio, drawn from a uniform distribution between 5 6 and 7 6 . Note that these steps are not useful for the method in <ref type="bibr" target="#b3">[4]</ref> because it exploits very localized features. However, as we show in our experiments, these steps lead to significant performance improvements in our DeepFont system. Overall, our data augmentation includes steps 1-6.</p><p>To leave a visual impression, we take the real-world image <ref type="figure" target="#fig_3">Fig. 2 (a)</ref>, and synthesize a series of images in <ref type="figure" target="#fig_5">Fig. 3</ref>, all with the same text but with different data augmentation ways. Specially, (a) is synthesized with no data augmentation; (b) is (a) with standard augmentation 1-4 added; (c) is synthesized with spacing and aspect ratio customized to be identical to those of <ref type="figure" target="#fig_3">Fig. 2</ref>    <ref type="figure" target="#fig_5">Fig. 3</ref> (e) shows that those augmentations, especially the spacing and aspect ratio changes, reduce the gap between the feature hierarchies of real-world and synthetic data to a large extent. A few synthetic patches after full data augmentation 1-6 are displayed in <ref type="figure" target="#fig_4">Fig. 4</ref>. It is observable that they possess a much more visually similar appearance to real-world data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DOMAIN ADAPTED CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Domain Adaptation by CNN Decomposition and SCAE</head><p>Despite that data augmentations are helpful to reduce the domain mismatch, enumerating all possible real-world degradations is impossible, and may further introduce degradation bias in training. In the section, we propose a learning framework to leverage both synthetic and real-world data, using multi-layer CNN decomposition and SCAE-based domain adaptation. Our approach extends the domain adaptation method in <ref type="bibr" target="#b6">[7]</ref> to extract low-level features that represent both the synthetic and real-world data. We employs a Convolutional Neural Network (CNN) architecture, which is further decomposed into two sub-networks: a "shared" lowlevel sub-network which is learned from the composite set of synthetic and real-world data, and a high-level sub-network that learns a deep classifier from the low-level features.</p><p>The basic CNN architecture is similar to the popular Im-ageNet structure <ref type="bibr" target="#b9">[10]</ref>, as in <ref type="figure" target="#fig_7">Fig. 5</ref>. The numbers along with the network pipeline specify the dimensions of outputs of corresponding layers. The input is a 105 ? 105 patch sampled from a "normalized" image. Since a square window may not capture sufficient discriminative local structures, and is unlikely to catch high-level combinational features when two or more graphemes or letters are joined as a single glyph (e.g., ligatures), we introduce a squeezing operation 3 , that scales the width of the height-normalized image to be of a constant ratio relative to the height (2.5 in all our experiments). Note that the squeezing operation is equivalent to producing "long" rectangular input patches.</p><p>When the CNN model is trained fully on a synthetic dataset, it witnesses a significant performance drop when testing on real-world data, compared to when applied to another synthetic validation set. This also happens with other models such as in <ref type="bibr" target="#b3">[4]</ref>, which uses training and testing sets of similar properties to ours. It alludes to discrepancies between the distributions of synthetic and real-world examples. we propose to decompose the N CNN layers into two sub-networks to be learned sequentially:</p><p>? Unsupervised cross-domain sub-network Cu, which consists of the first K layers of CNN. It accounts for extracting low-level visual features shared by both synthetic and real-world data domains. Cu will be trained in a unsupervised way, using unlabeled data from both domains. It constitutes the crucial step that further minimizes the low-level feature gap, beyond the previous data augmentation efforts.</p><p>? Supervised domain-specific sub-network Cs, which consists of the remaining N ? K layers. It accounts for learning higher-level discriminative features for classification, based on the shared features from Cu. Cs will be trained in a supervised way, using labeled data from the synthetic domain only.</p><p>We show an example of the proposed CNN decomposition in <ref type="figure" target="#fig_7">Fig. 5</ref>. The Cu and Cs parts are marked by red and green colors, respectively, with N = 8 and K = 2. Note that the low-level shared features are implied to be independent of class labels. Therefore in order to address the open-ended problem of font classes, one may keep re-using the Cu subnetwork, and only re-train the Cs part.</p><p>Learning Cu from SCAE Representative unsupervised feature learning methods, such as the Auto-Encoder and the Denoising Auto-Encoder, perform a greedy layer-wise pretraining of weights using unlabeled data alone followed by supervised fine-tuning ( <ref type="bibr" target="#b2">[3]</ref>). However, they rely mostly on fully-connected models and ignore the 2D image structure.</p><p>In <ref type="bibr" target="#b12">[13]</ref>, a Convolutional Auto-Encoder (CAE) was proposed to learn non-trivial features using a hierarchical unsupervised feature extractor that scales well to high-dimensional inputs. The CAE architecture is intuitively similar to the the conventional auto-encoders in <ref type="bibr" target="#b17">[18]</ref>, except for that their weights are shared among all locations in the input, preserving spatial locality. CAEs can be stacked to form a deep hierarchy called the Stacked Convolutional Auto-Encoder (SCAE), where each layer receives its input from a latent representation of the layer below. <ref type="figure" target="#fig_8">Fig. 6</ref> plots the SCAE architecture for our K = 2 case. Training Details We first train the SCAE on both synthetic and real-world data in a unsupervised way, with a learning rate of 0.01 (we do not anneal it through training). Mean Squared Error (MSE) is used as the loss function. After SCAE is learned, its Conv. Layers 1 and 2 are imported to the CNN in <ref type="figure" target="#fig_7">Fig. 5</ref>, as the Cu sub-network and fixed. The Cs sub-network, based on the output by Cu, is then trained in a supervised manner. We start with the learning rate at 0.01, and follow a common heuristic to manually divide the learning rate by 10 when the validation error rate stops decreasing with the current rate. The "dropout" technique is applied to fc6 and fc7 layers during training. Both Cu and Cs are trained with a default batch size of 128, momentum of 0.9 and weight decay of 0.0005. The network training is implemented using the CUDA ConvNet package <ref type="bibr" target="#b9">[10]</ref>, and runs on a workstation with 12 Intel Xeon 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Connections to Previous Work</head><p>We are not the first to look into an essentially "hierarchical" deep architecture for domain adaption. In <ref type="bibr" target="#b14">[15]</ref>, the proposed transfer learning approach relies on the unsupervised learning of representations. Bengio et. al hypothesized in <ref type="bibr" target="#b1">[2]</ref> that more levels of representation can give rise to more abstract, more general features of the raw input, and that the lower layers of the predictor constitute a hierarchy of features that can be shared across variants of the input distribution. The authors in <ref type="bibr" target="#b6">[7]</ref> used data from the union of all domains to learn their shared features, which is different from many previous domain adaptation methods that focus on learning features in a unsupervised way from the target domain only. However, their entire network hierarchy is learned in a unsupervised fashion, except for a simple linear classier trained on top of the network, i.e., K = N ? 1. In <ref type="bibr" target="#b18">[19]</ref>, the CNN learned a set of filters from raw images as the first layer, and those low-level filters are fixed when training higher layers of the same CNN, i.e., K = 1. In other words, they either adopt a simple feature extractor (K = 1), or apply a shallow classifier (K = N ? 1). Our CNN decomposition is different from prior work in that:</p><p>? Our feature extractor Cu and classier Cs are both deep sub-networks with more than one layer (both K and N ? K are larger than 1), which means that both are able to perform more sophisticated learning. More evaluations can be found in Section 5.2.</p><p>? We learn "shared-feature" convolutional filters rather than fully-connected networks such as in <ref type="bibr" target="#b6">[7]</ref>, the former of which is more suitable for visual feature extractions.</p><p>The domain mismatch between synthetic and real-world data on the lower-level statistics can occur in more scenarios, such as real-world face recognition from rendered images or sketches, recognizing characters in real scenes with synthetic training, human pose estimation with synthetic images generated from 3D human body models. We conjecture that our framework can be applicable to those scenarios as well, where labeled real-world data is scarce but synthetic data can be easily rendered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LEARNING-BASED MODEL COMPRES-SION</head><p>The architecture in <ref type="figure" target="#fig_7">Fig. 5</ref> contains a huge number of parameters. It is widely known that the deep models are heavily over-parameterized <ref type="bibr" target="#b4">[5]</ref> and thus those parameters can be compressed to reduce storage by exploring their structure.</p><p>For a typical CNN, about 90% of the storage is taken up by the dense connected layers, which shall be our focus for mode compression.</p><p>One way to shrink the number of parameters is using matrix factorization <ref type="bibr" target="#b5">[6]</ref>. Given the parameter W ? R m?n , we factorize it using singular-value decomposition (SVD):</p><formula xml:id="formula_0">W = U SV T<label>(1)</label></formula><p>where U ? R m?m and V ? R n?n are two dense orthogonal matrices and S ? R m?n is a diagonal matrix. To restore an approximate W , we can utilize U , V and S, which denote the submatrices corresponding to the top k singular vectors in U and V along with the top k eigenvalue in S:</p><formula xml:id="formula_1">W = U S V T<label>(2)</label></formula><p>The compression ratio given m, n, and k is k(m+n+1) mn , which is very promising when m, n k. However, the approximation of SVD is controlled by the decay along the eigenvalues in S. Even it is verified in <ref type="figure" target="#fig_10">Fig. 7</ref> that eigenvalues of weight matrices usually decay fast (the 6-th largest eigenvalue is already less than 10% of the largest one in magnitude), the truncation inevitably leads to information loss, and potential performance degradations, compared to the uncompressed model.  Instead of first training a model then lossy-compressing its parameters, we propose to directly learn a losslessly compressible model (the term "lossless" is referred as there is no further loss after a model is trained). Assuming the parameter matrix W of a certain network layer, our goal is to make sure that its rank is exactly no more than a small constant k. In terms of implementation, in each iteration, an extra hard thresholding operation <ref type="bibr" target="#b10">[11]</ref> is executed on W after it is updated by a conventional back propagation step:</p><formula xml:id="formula_2">W k = U T k (S)V T<label>(3)</label></formula><p>where T k will keep the largest k eigenvalues in S while setting others to zeros. W k is best rank-k approximation of W , as similarly in <ref type="bibr" target="#b1">(2)</ref>. However, different from (2), the proposed method incorporates low-rank approximation into model training and jointly optimize them as a whole, guaranteeing a rank-k weight matrix that is ready to be compressed losslessly by applying <ref type="bibr" target="#b0">(1)</ref>. Note there are other alternatives, such as vector quantization methods <ref type="bibr" target="#b7">[8]</ref>, that have been applied to compressing deep models with appealing performances. We will investigate utilizing them together to further compress our model in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis of Domain Mismatch</head><p>We first analyze the domain mismatch between synthetic and real-world data, and examine how our synthetic data augmentation can help. First we define five dataset variations generated from VFR syn train and VFR real u. These are denoted by the letters N, S, F, R and FR and are explained in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>We train five separate SCAEs, all of the same architecture as in <ref type="figure" target="#fig_8">Fig. 6</ref>, using the above five training data variants. The training and testing errors are all measured by relative MSEs (normalized by the total energy) and compared in <ref type="table" target="#tab_0">Table 1</ref>. The testing errors are evaluated on both the unaugmented synthetic dataset N and the real-world dataset R. Ideally, the better the SCAE captures the features from a domain, the smaller the reconstruction error will be on that domain.</p><p>As revealed by the training errors, real-world data contains rich visual variations and is more difficult to fit. The sharp performance drop from N to R of SCAE N indicates that the convolutional features for synthetic and real data are quite different. This gap is reduced in SCAE S, and further in SCAE F, which validates the effectiveness of adding font-specific data augmentation steps. SCAE R fits the realworld data best, at the expense of a larger error on N. SCAE FR achieves an overall best reconstruction performance of both synthetic and real-world images. <ref type="figure" target="#fig_12">Fig. 8</ref> shows an example patch from a real-world font image of highly textured characters, and its reconstruction outputs from all five models. The gradual visual variations across the results confirm the existence of a mismatch between synthetic and real-world data, and verify the benefit of data augmentation as well as learning shared features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of Network Structure</head><p>Fixing Network Depth N . Given a fixed network complexity (N layers), one may ask about how to best decompose the hierarchy to maximize the overall classification performance on real-world data. Intuitively, we should have sufficient layers of lower-level feature extractors as well as enough subsequent layers for good classification of labeled data. Thus, the depth K of Cu should neither be too small nor too large. <ref type="table" target="#tab_3">Table 3</ref> shows that while the classification training error increases with K, the testing error does not vary monotonically. The best performance is obtained with K = 2 (3   slightly worse), where smaller or larger values of K give substantially worse performance. When K = 5, all layers are learned using SCAE, leading to the worst results. Rather than learning all hidden layers by unsupervised training, as suggested in <ref type="bibr" target="#b6">[7]</ref> and other DL-based transfer learning work, our CNN decomposition reaches its optimal performance when higher-layer convolutional filters are still trained by supervised data. A visual inspection of reconstruction results of a real-world example in <ref type="figure" target="#fig_13">Fig. 9</ref>, using SCAE FR with different K values, shows that a larger K causes less information loss during feature extraction and leads to a better reconstruction. But in the meantime, the classification result may turn worse since noise and irrelevant high frequency details (e.g. textures) might hamper recognition performance. The optimal K =2 corresponds to a proper "content-aware" smoothening, filtering out "noisy" details while keeping recognizable structural properties of the font style.</p><formula xml:id="formula_3">(a) K=1 (b) K=2 (c) K=4 (d) K=5</formula><p>Fixing Cs or Cu Depth. We investigate the influences of K (the depth of Cu) when the depth of Cs (e.g. N ? K) keeps fixed. <ref type="table" target="#tab_4">Table 4</ref> reveals that a deeper Cu contributes little to the results. Similar trends are observed when we fix K and adjust N (and thus the depth ofCs). Therefore, we choose N = 8, K=2 to be the default setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Recognition Performances on VFR Datasets</head><p>We implemented and evaluated the local feature embeddingbased algorithm (LFE) in <ref type="bibr" target="#b3">[4]</ref> as a baseline, and include the four different DeepFont models as specified in <ref type="table" target="#tab_5">Table 5</ref>. The first two models are trained in a fully supervised manner on F, without any decomposition applied. For each of the later two models, its corresponding SCAE (SCAE FR for Deep-Font CAE FR, and SCAE R for DeepFont CAE R) is first trained and then exports the first two convolutional layers to Cu. All trained models are evaluated in term of top-1 and top-5 classification errors, on the VFR syn val dataset for validation purpose. Benefiting from large learning capacity, it is clear that DeepFont models fit synthetic data significantly better than LFE. Notably, the top-5 errors of all DeepFont models (except for DeepFont CAE R) reach zero on the validation set, which is quite impressive for such a fine-grain classification task.</p><p>We then compare DeepFont models with LFE on the original VFRWild325 dataset in <ref type="bibr" target="#b3">[4]</ref>. As seen from <ref type="table" target="#tab_5">Table 5</ref>, while DeepFont S fits synthetic training data best, its performance is the poorest on real-world data, showing a severe overfitting. With two font-specific data augmentations added in training, the DeepFont F model adapts better to realworld data, outperforming LFE by roughly 8% in top-5 error. An additional gain of 2% is obtained when unlabeled real-world data is utilized in DeepFont CAE FR. Next, the DeepFont models are evaluated on the new VFR real test dataset, which is more extensive in size and class coverage. A large margin of around 5% in top-1 error is gained by DeepFont CAE FR model over the second best (DeepFont F), with its top-5 error as low as 18.21%. We will use Deep-Font CAE FR as the default DeepFont model.</p><p>Although SCAE R has the best reconstruction result on real-world data on which it is trained, it has large training and testing errors on synthetic data. Since our supervised training relies fully on synthetic data, an effective feature extraction for synthetic data is also indispensable. The error rates of DeepFont CAE R are also worse than those of DeepFont CAE FR and even DeepFont F on the real-world data, due to the large mismatch between the low-level and high-level layers in the CNN. Another interesting observation is that all methods get similar top-5 errors on VFRWild325 and VFR real test, showing their statistical similarity. However, the top-1 errors of DeepFont models on VFRWild325 are significantly higher than those on VFR real test, with a difference of up to 10%. In contrast, the top-1 error of LFE rises more than 13% on VFR real test than on VFRWild325. For the small VFR-Wild325, the recognition result is easily affected by "bad" examples (e.g, low resolution or highly compressed images) and class bias (less than 4% of all classes are covered). On the other hand, the larger VFR real test dataset dilutes the possible effect of outliers, and examines a lot more classes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluating Font Similarity using DeepFont</head><p>There are a variety of font selection tasks with different goals and requirements. One designer may wish to match a font to the style of a particular image. Another may wish to find a free font which looks similar to a commercial font such as Helvetica. A third may simply be exploring a large set of fonts such as Adobe TypeKit or Google Web Fonts. Exhaustively exploring the entire space of fonts using an alphabetical listing is unrealistic for most users. The authors in <ref type="bibr" target="#b13">[14]</ref> proposed to select fonts based on online crowdsourced attributes, and explore font similarity, from which a user is enabled to explore other visually similar fonts given a specific font. The font similarity measure is very helpful for font selection, organization, browsing, and suggestion.</p><p>Based on our DeepFont system, we are able to build up measures of font similarity. We use the 4096 ? 1 outputs of the fc7 layer as the high-level feature vectors describing font visual appearances. We then extract such features from all samples in VFR syn val Dataset, obtaining 100 feature vectors per class. Next for each class, the 100 feature vectors is averaged to a representative vector. Finally, we calculate the Euclidean distance between the representative vectors of two font classes as their similarity measure. Visualized examples are demonstrated in <ref type="figure" target="#fig_1">Fig. 11</ref>. For each example, the top is the query image of a known font class; the most similar fonts obtained by the font similarity measures are sorted below. Note that although the result fonts can belong to different font families from the query, they share identifiable visual similarities by human perception.</p><p>Although not numerically verified as in <ref type="bibr" target="#b13">[14]</ref>, the DeepFont results are qualitatively better when we look at the top-10 most similar fonts for a wide range of query fonts. The authors of <ref type="bibr" target="#b13">[14]</ref> agree per personal communication with us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">DeepFont Model Compression</head><p>Since the fc6 layer takes up 85% of the total model size, we first focus on its compression. We start from a well-trained DeepFont model (DeepFont CAE FR), and continue tuning it with the hard thresholding (3) applied to the fc6 parameter matrix W in each iteration, until the training/validation errors reach the plateau again. <ref type="table" target="#tab_6">Table 6</ref> compares the DeepFont models compressed using conventional matrix factorization (denoted as the "lossy" method), and the proposed learning based method (denoted as the "lossless" method), under different compression ratios (fc6 and total size counted by parameter numbers). The last column of <ref type="table" target="#tab_6">Table 6</ref> lists the top-5 testing errors (%) on VFR real test. We observe a consistent margin of the "lossless" method over its "lossy" counterpart, which becomes more significant when the compression ratio goes low (more than 1% when k = 5). Notably, when k = 100, the proposed "lossless" compression suffers no visible performance loss, while still maintaining a good compression ratio of 5.79.</p><p>In practice, it takes around 700 megabytes to store all the parameters in our uncompressed DeepFont model, which is quite huge to be embedded or downloaded into most customer softwares. More aggressively, we reduce the output sizes of both fc6 and fc7 to 2048, and further apply the pro-  posed compression method (k = 10) to the fc6 parameter matrix. The obtained "mini" model, with only 9, 477, 066 parameters and a high compression ratio of 18.73, becomes less than 40 megabytes in storage. Being portable even on mobiles, It manages to keep a top-5 error rate around 22%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In the paper, we develop the DeepFont system to remarkably advance the state-of-the-art in the VFR task. A large set of labeled real-world data as well as a large corpus of unlabeled real-world images is collected for both training and testing, which is the first of its kind and will be made publicly available soon. While relying on the learning capacity of CNN, we need to combat the mismatch between available training and testing data. The introduction of SCAE-based domain adaption helps our trained model achieve a higher than 80% top-5 accuracy. A novel lossless model compression is further applied to promote the model storage efficiency. The DeepFont system not only is effective for font recognition, but can also produce a font similarity measure for font selection and suggestion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) (b) Successful VFR examples with the DeepFont system. The top row are query images from VFR real test dataset. Below each query, the results (left column: font classes; right column: images rendered with the corresponding font classes) are listed in a high-to-low order in term of likelihoods. The correct results are marked by the red boxes. (c) More correctly recognized real-world images with DeepFont.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1</head><label>1</label><figDesc>shows successful VFR examples using DeepFont. In (a)(b), given the real-world query images, top-5 font recognition results are listed, within which the ground truth font classes are marked out 1 . More real-world examples are dis-1 Note that the texts are input manually for rendering purposes only. The font recognition process does not need any content information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2Figure 2 :</head><label>2</label><figDesc>http://www.atlaswang.com/deepfont.html (a) the different characters spacings between a pair of synthetic and real-world images. (b) the different aspect ratio between a pair of synthetic and real-world image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 (</head><label>4</label><figDesc>(a); (d) adds standard augmentation 1-4 to (c). We input images (a)-(d) through the trained DeepFont model. For each image, we compare its (a) Synthetic, none (b) Synthetic, 1-Relative CNN layer-wise responses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>The effects of data augmentation steps. (a)-(d): synthetic images of the same text but with different data augmentation ways. (e) compares relative differences of (a)-(d) with the real-world image Fig. 2 (a), in the measure of layer-wise network activations through the same DeepFont model. layer-wise activations with those of the real image Fig. 2 (a) feeding through the same model, by calculating the normalized MSEs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Examples of synthetic training 105 ? 105 patches after pre-processing steps 1-6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>The CNN architecture in the DeepFont system, and its decomposition marked by different colors (N =8, K=2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>The Stacked Convolutional Auto-Encoder (SCAE) architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>The plots of eigenvalues for the fc6 layer weight matrix inFig. 5. This densely connected layer takes up 85% of the total model size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) original (b) SCAE N (c) SCAE S (d) SCAE F (e) SCAE R (f) SCAE FR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>A real-world patch, and its reconstruction results from the five SCAE models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>The reconstruction results of a real-world patch using SCAE FR, with different K values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Failure VFR examples using DeepFont.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :</head><label>11</label><figDesc>Examples of the font similarity. For each one, the top is the query image, and the renderings with the most similar fonts are returned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 10</head><label>10</label><figDesc>lists some failure cases of DeepFont. For example, the top left image contains extra "fluff" decorations along text boundaries, which is nonexistent in the original fonts, that makes the algorithm incorrectly map it to some "artistic" fonts. Others are affected by 3-D effects, strong obstacles in foreground, and in background. Being considerably difficult to be adapted, those examples fail mostly because there are neither specific augmentation steps handling their effects, nor enough examples in VFR real u to extract corresponding robust features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of All VFR Datasets</figDesc><table><row><cell>Dataset name</cell><cell cols="3">Source Label? Purpose</cell><cell>Size</cell><cell>Class</cell></row><row><cell>VFRWild325 [4]</cell><cell>Real</cell><cell>Y</cell><cell>Test</cell><cell>325</cell><cell>93</cell></row><row><cell>VFR real test</cell><cell>Real</cell><cell>Y</cell><cell>Test</cell><cell>4, 384</cell><cell>617</cell></row><row><cell>VFR real u</cell><cell>Real</cell><cell>N</cell><cell>Train</cell><cell>197, 396</cell><cell>/</cell></row><row><cell>VFR syn train</cell><cell>Syn</cell><cell>Y</cell><cell>Train</cell><cell cols="2">2,383, 000 2, 383</cell></row><row><cell>VFR syn val</cell><cell>Syn</cell><cell>Y</cell><cell>Test</cell><cell>238, 300</cell><cell>2, 383</cell></row></table><note>played in (c). Although accompanied with high levels of background clutters, size and ratio variations, as well as per- spective distortions, they are all correctly recognized by the DeepFont system.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>67GHz CPUs and 1 GTX680 GPU. It takes around 1 day to complete the entire training pipeline. Testing Details We adopt multi-scale multi-view testing to improve the result robustness. For each test image, it is first normalized to 105 pixels in height, but squeezed in width by three different random ratios, all drawn from a uniform distribution between 1.5 and 3.5, matching the effects of squeezing and variable aspect ratio operations during training. Under each squeezed scale, five 105 ? 105 patches are sampled at different random locations.</figDesc><table /><note>That constitutes in total fifteen test patches, each of which comes with dif- ferent aspect ratios and views, from one test image. As every single patch could produce a softmax vector through the trained CNN, we average all fifteen softmax vectors to determine the final classification result of the test image.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Training and Testing Errors (%) of Five SCAEs (K = 2)</figDesc><table><row><cell>Methods</cell><cell>Training Data</cell><cell>Train</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell>N</cell><cell>R</cell></row><row><cell>SCAE N</cell><cell>N: VFR syn train, no data augmentation</cell><cell>0.02</cell><cell cols="2">3.54 31.28</cell></row><row><cell>SCAE S</cell><cell>S: VFR syn train, standard augmentation 1-4</cell><cell>0.21</cell><cell cols="2">2.24 19.34</cell></row><row><cell>SCAE F</cell><cell>F: VFR syn train, full augmentation 1-6</cell><cell>1.20</cell><cell cols="2">1.67 15.26</cell></row><row><cell>SCAE R</cell><cell>R:VFR real u, real unlabeled dataset</cell><cell>9.64</cell><cell cols="2">5.73 10.87</cell></row><row><cell>SCAE FR</cell><cell>FR: Combination of data from F and R</cell><cell>6.52</cell><cell cols="2">2.02 14.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">Top-5 Testing Errors (%) for Different</cell><cell></cell></row><row><cell cols="5">CNN Decompositions (Varying K, N = 8)</cell><cell></cell><cell></cell></row><row><cell>K</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>Train</cell><cell>8.46</cell><cell>9.88</cell><cell cols="4">11.23 12.54 15.21 17.88</cell></row><row><cell cols="7">VFR real test 20.72 20.31 18.21 18.96 22.52 25.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">Top-5 Testing Errors (%) for Different</cell></row><row><cell cols="5">CNN Decompositions (Varying K, N = K + 6)</cell></row><row><cell>K</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>Train</cell><cell cols="4">11.46 11.23 10.84 10.86</cell></row><row><cell cols="5">VFR real test 21.58 18.21 18.15 18.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of Training and Testing Errors on Synthetic and Real-world Datasets (%)</figDesc><table><row><cell>Methods</cell><cell cols="3">Training Data Training</cell><cell cols="2">VFR syn val</cell><cell cols="2">VFRWild325</cell><cell cols="2">VFR real test</cell></row><row><cell></cell><cell>Cu</cell><cell>Cs</cell><cell>Error</cell><cell cols="6">Top-1 Top-5 Top-1 Top-5 Top-1 Top-5</cell></row><row><cell>LFE</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>26.50</cell><cell>6.55</cell><cell>44.13</cell><cell>30.25</cell><cell>57.44</cell><cell>32.69</cell></row><row><cell>DeepFont S</cell><cell>/</cell><cell>F</cell><cell>0.84</cell><cell>1.03</cell><cell>0</cell><cell>64.60</cell><cell>57.23</cell><cell>57.51</cell><cell>50.76</cell></row><row><cell>DeepFont F</cell><cell>/</cell><cell>F</cell><cell>8.46</cell><cell>7.40</cell><cell>0</cell><cell>43.10</cell><cell>22.47</cell><cell>33.30</cell><cell>20.72</cell></row><row><cell cols="2">DeepFont CAE FR FR</cell><cell>F</cell><cell>11.23</cell><cell>6.58</cell><cell>0</cell><cell cols="4">38.15 20.62 28.58 18.21</cell></row><row><cell>DeepFont CAE R</cell><cell>R</cell><cell>F</cell><cell>13.67</cell><cell>8.21</cell><cell>1.26</cell><cell>44.62</cell><cell>29.23</cell><cell>39.46</cell><cell>27.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance Comparisons of Lossy and</figDesc><table><row><cell cols="4">Lossless Compression Approaches</cell><cell></cell><cell></cell></row><row><cell></cell><cell>fc6 size</cell><cell cols="4">Total size Ratio Method Error</cell></row><row><cell cols="3">default 150,994,944 177,546,176</cell><cell>NA</cell><cell>NA</cell><cell>18.21</cell></row><row><cell>k=5</cell><cell>204,805</cell><cell>26,756,037</cell><cell>6.64</cell><cell>Lossy Lossless</cell><cell>20.67 19.23</cell></row><row><cell>k=10</cell><cell>409,610</cell><cell>26,960,842</cell><cell>6.59</cell><cell>Lossy Lossless</cell><cell>19.25 18.87</cell></row><row><cell>k=50</cell><cell>2,048,050</cell><cell>28,599,282</cell><cell>6.21</cell><cell>Lossy Lossless</cell><cell>19.04 18.67</cell></row><row><cell>k=100</cell><cell>4,096,100</cell><cell>30,647,332</cell><cell>5.79</cell><cell>Lossy Lossless</cell><cell>18.68 18.21</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note squeezing is independent from the variable aspect ratio operation introduced in Section 2.3, as they are for different purposes.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High-order statistical texture analysis: font recognition applied</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Avil?s-Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rangel-Kuoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes-Ayala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrade-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Escarela-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="145" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning deep architectures for ai. Foundations and trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale visual font recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3598" to="3605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2148" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115</idno>
		<title level="m">Compressing deep convolutional networks using vector quantization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multifont classification using typographical attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="353" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno>arXiv preprint:1009.5055</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gabor filter based multi-class classifier for scanned document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="968" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exploratory font selection using crowdsourced attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O&amp;apos;donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>L?beks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-taught learning: transfer learning from unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel technique for english font recognition using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thaneshwaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Viknesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yuvaraj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ARTCom</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="766" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-linguistic optical font recognition using stroke templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="889" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3304" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Font recognition based on global texture analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
