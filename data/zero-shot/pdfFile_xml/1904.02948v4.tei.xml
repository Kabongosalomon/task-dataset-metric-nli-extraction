<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Center and Scale Prediction: Anchor-free Approach for Pedestrian and Face Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-07">7 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<email>liuwei16@nudt.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI)</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irtiza</forename><surname>Hasan</surname></persName>
							<email>irtiza.hasan@inceptioniai.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
							<email>shengcai.liao@inceptioniai.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Center and Scale Prediction: Anchor-free Approach for Pedestrian and Face Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-07">7 Nov 2021</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Pattern Recognition November 9, 2021</note>
					<note>* Equal contribution * * Corresponding author</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object Detection</term>
					<term>Convolutional Neural Networks</term>
					<term>Feature Detection</term>
					<term>anchor-free</term>
					<term>Anchor-free</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection traditionally requires sliding-window classifier in modern deep learning based approaches. However, both of these approaches requires tedious configurations in bounding boxes. Generally speaking, single-class object detection is to tell where the object is, and how big it is. Traditional methods combine the "where" and "how" subproblems into a single one through the overall judgement of various scales of bounding boxes. In view of this, we are interesting in whether the "where" and "how" subproblems can be separated into two independent subtasks to ease the problem definition and the difficulty of training. Accordingly, we provide a new perspective where detecting objects is approached as a high-level semantic feature detection task. Like edges, corners, blobs and other feature detectors, the proposed detector scans for feature points all over the image, for which the convolution is naturally suited. However, unlike these traditional low-level features, the proposed detector goes for a higher-level abstraction, that is, we are looking for central points where there are objects, and modern deep models are already capable of such a high-level semantic abstraction. Like blob detection, we also predict the scales of the central points, which is also a straightforward convolution. Therefore, in this paper, pedestrian and face detection is simplified as a straightforward center and scale prediction task through convolutions. This way, the proposed method enjoys an anchor-free setting, considerably reducing the difficulty in training configuration and hyper-parameter optimization. Though structurally simple, it presents competitive accuracy on several challenging benchmarks, including pedestrian detection and face detection. Furthermore, a cross-dataset evaluation is performed, demonstrating a superior generalization ability of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pedestrian detection is a very active research topic in the computer vision. It has applications in many different domains, including autonomous driving <ref type="bibr" target="#b0">[1]</ref>, video surveillance <ref type="bibr" target="#b1">[2]</ref>, and action recognition <ref type="bibr" target="#b2">[3]</ref>. Pedestrian detection also plays a critical role as foundation steps in various other computer vision research areas, such as multi-object tracking <ref type="bibr" target="#b3">[4]</ref>, human pose estimation <ref type="bibr" target="#b4">[5]</ref>, person re-identification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, etc. Recently, convolutional neural network (CNNs) based approaches have advanced the field of pedestrian detection a lot, with significantly improved performance.</p><p>For a broader domain, pedestrian detection is a special case of general object detection. Starting from the pioneering work of the Viola-Jones detector <ref type="bibr" target="#b7">[8]</ref>, object detection generally requires sliding-window classifiers in traditional or anchor based predictions in CNN-based methods. These detectors are essentially local classifiers used to judge the pre-defined anchors (windows or anchor-boxes) as being objects or not. However, either of these approaches requires tedious configurations in anchors. Several (but not all) of these configurations, which include the number of scales, the sizes of anchors, the aspect ratios, and the overlap thresholds with ground truth boxes. All of these are task-oriented, and it is difficult to figure out which combination is the optimal one.</p><p>Generally speaking, single-class object detection is to tell where the object is, and how big it is. Traditional methods combine the "where" and "how" subproblems into a single one through the overall judgement of various scales of anchor boxes. In view of this, we are interesting in whether the "where" and "how" subproblems can be separated into two independent subtasks to ease the problem definition and the difficulty of training.</p><p>We resort to traditional feature detection. Feature detection is one of the most fundamental problems in computer vision. It is usually viewed as a low-level technique, with typical tasks including edge detection (e.g. Canny <ref type="bibr" target="#b8">[9]</ref>, Sobel <ref type="bibr" target="#b9">[10]</ref>), corner (or interest point) detection (e.g. SUSAN <ref type="bibr" target="#b10">[11]</ref>, FAST <ref type="bibr" target="#b11">[12]</ref>), and blob (or region of interest point) detection (e.g. LoG <ref type="bibr" target="#b12">[13]</ref>, DoG <ref type="bibr" target="#b13">[14]</ref>, MSER <ref type="bibr" target="#b14">[15]</ref>). Feature detection is of vital importance to a variety of computer vision tasks ranging from image representation, image matching to 3D scene reconstruction. Generally speaking, a feature is defined as an "interesting part" of an image. Therefore, feature detection aims to compute abstractions of image information. Subsequently, feature detection makes local decisions at every image point whether there is an image feature of a given type at that point or not. With the rapid development for computer vision tasks, deep convolutional neural networks (CNN) are believed to be of very good capability to learn high-level image abstractions. CNNs have also been applied for feature detection, and demonstrate attractive successes even in low-level feature detections. For example, there is a recent trend of using CNN to perform edge detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, which has substantially advanced this field. It shows that clean and continuous edges can be obtained by deep convolutions, which indicates that CNN has a stronger capability to learn higher-level abstraction of natural images than traditional methods. This capability may not be limited to low-level feature detection; it may open up many other possibilities of high-level feature detection. Therefore, in this paper, we argue in the favor of posing pedestrian detection as high-level semantic feature detection task. However, unlike traditional low-level feature detectors, the proposed detector goes for a higher-level abstraction, that is, we are looking for central points where there are objects. Besides, similar to the blob detection, we also predict the scales of the central points. However, instead of processing an image pyramid to determine the scale as in traditional blob detection, we also predict object scale with a straightforward convolution in one pass upon a fully convolution network (FCN) <ref type="bibr" target="#b19">[20]</ref>, considering its strong capability. As a result, pedestrian and face detection is simply formulated as a straightforward center and scale prediction task via CNNs. Therefore, the proposed CSP detector separates the "where" and "how" sub- problems into two different convolutions. These design choices enables CSP to enjoy an anchor-free (short for window-free or anchor-anchor-free) setting, considerably reducing the difficulty in training configuration and hyper-parameter optimization. The overall pipeline of the proposed method, denoted as Center and Scale Prediction (CSP) based detector, is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Previously, FCN has already been applied to and made a success in multi-person pose estimation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, where several keypoints are firstly detected merely through responses of full convolutions, and then they are further grouped into complete poses of individual persons. In view of this, recently two inspirational works, CornerNet <ref type="bibr" target="#b22">[23]</ref> and TLL <ref type="bibr" target="#b23">[24]</ref>, successfully go free from windows and anchor-boxes, which perform object detection as convolutional keypoint detections and their associations. Though the keypoint association require additional computations, sometimes complex as in TLL, the keypoint prediction by FCN inspires us to go a step further, achieving center and scale prediction based pedestrian and face detection in full convolutions.</p><p>In summary, the main contributions of this work are as follows: (i) We show a new possibility that pedestrian and face detection can be simplified as a straightforward center and scale prediction task through convolutions, which bypasses the limitations of anchor-box based detectors and gets rid of the complex post-processing of recent keypoint pairing based detectors. (ii) The proposed CSP <ref type="bibr" target="#b24">[25]</ref> detector achieves the new state-of-the-art performance on two challenging pedestrian detection benchmarks, CityPersons <ref type="bibr" target="#b25">[26]</ref> and Caltech <ref type="bibr" target="#b26">[27]</ref>, it also achieves competitive performance on one of the most popular face detection benchmark-WiderFace <ref type="bibr" target="#b27">[28]</ref>. (iii) The proposed CSP detector presents good generalization ability when cross-dataset evaluation is performed.</p><p>This work is built upon CSP <ref type="bibr" target="#b24">[25]</ref>. The major new content is additional experiments on face detection to demonstrate CSP's capability in detecting objects with various aspect ratios. Besides, we also conduct comparative experiments and analysis to demonstrate the generalization ability of the proposed detector. They are summarized as follows:</p><p>? We evaluate the proposed method for face detection on one of the most popular face detection benchmarks, i.e. WiderFace <ref type="bibr" target="#b27">[28]</ref>. The model is trained on the official training subset and evaluated on both the validation and test subsets.</p><p>Comparable performance to other state-of-the-art face detectors on this benchmark are reported and thus demonstrates the proposed method's capability and competitiveness on face detection.</p><p>? To further evaluate the generalizability of the proposed CSP detector, we also conduct two cross-dataset evaluation experiments. For pedestrian detection, we compare the proposed CSP detector with the state-of-the-art anchor-box based pedestrian detector (ALFNet <ref type="bibr" target="#b28">[29]</ref>). Both of the two detectors are trained on the CityPersons <ref type="bibr" target="#b25">[26]</ref> training set and then are directly tested on Caltech <ref type="bibr" target="#b26">[27]</ref>.</p><p>For face detection, we compare the proposed CSP detector with the state-ofthe-art anchor-box based face detector (DSFD <ref type="bibr" target="#b29">[30]</ref>). Both of the two detectors are trained on the WiderFace <ref type="bibr" target="#b27">[28]</ref> training set and then are directly tested on FDDB <ref type="bibr" target="#b30">[31]</ref>, UCCS [32] and DarkFace <ref type="bibr" target="#b31">[33]</ref>. Experimental results show that the proposed CSP detector has a superior generalization ability than the compared methods.</p><p>Besides, we considerably reorganize the whole paper, especially the introduction part, for a better and more clear motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Anchor-box based object detection</head><p>Anchor based object detection is arguably the dominant paradigm in object detection, popularized by <ref type="bibr" target="#b32">[34]</ref>. Architecturally speaking, one of the major component of anchor-box based object detectors are fixed anchor-boxes (pre-defined scales and aspect ratios). Subsequently, these fixed anchor-boxes are used for classification and regression of final object bounding-box. Most two-stage based object detectors, such as Faster R-CNN <ref type="bibr" target="#b32">[34]</ref>, generates proposals (in region proposal network) and progressively classifies and refines them in its downstream classifier (Fast R-CNN branch). Making it end-to-end trainable, in a single unified framework. In contrast to two stage methods, singe-stage detectors, such as SSD <ref type="bibr" target="#b33">[35]</ref>, without the proposal generation step and achieved comparable accuracy while are more efficient (computationally) than twostage detectors.</p><p>Pedestrian detection can be broadly categorized into two sub-categories, 1) from the context of autonomous driving <ref type="bibr" target="#b25">[26]</ref> and 2) surveillance <ref type="bibr" target="#b34">[36]</ref>. In both cases, majority of the methods are direct extensions of Faster R-CNN. For example, <ref type="bibr" target="#b35">[37]</ref> was the first work to use RPN <ref type="bibr" target="#b32">[34]</ref> along with random forest as a downstream classifier. MS-CNN <ref type="bibr" target="#b36">[38]</ref> utilized the Faster R-CNN framework, but additionally exploited the multi-scale feature maps. Moreover, Zhang et al. <ref type="bibr" target="#b25">[26]</ref> used standard faster R-CNN but with added strategies for training, such as high image resolution, larger receptive field, quantization for anchors etc. <ref type="bibr" target="#b34">[36]</ref> suggested a scale-aware Faster R-CNN. Despite achieving high accuracy on non-occluded pedestrians, occluded pedestrian detection stands as one of the major problems in pedestrian detection <ref type="bibr" target="#b37">[39]</ref>, initial works focused on tailoring of the loss function to address the problems <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b39">41]</ref>. Subsequently, Bi-Box <ref type="bibr" target="#b40">[42]</ref> extended Faster R-CNN by proposing a separate branch to predict the visible parts of the pedestrian. In addition to occlusion, small-scale pedestrians are also among the harder cases. <ref type="bibr" target="#b41">[43]</ref> propose a reinforcement learning based approach to address small-scale pedestrians. Moreover, <ref type="bibr" target="#b42">[44]</ref> utilized pose estimation along with detection in a joint optimization for achieving better accuracy. Unlike general object detection, single stage methods <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b44">46]</ref> achieved superior performances than two stage methods in pedestrian detection. For instance, ALFNet <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b45">[47]</ref> proposes the asymptotic localization fitting strategy to progressively refine anchor-boxes for accurate localization. Finally, <ref type="bibr" target="#b46">[48]</ref> ameliorated SSD architecture by focusing on the discriminative feature learning.</p><p>For face detection, it is dominated by the single-stage framework in recent years.</p><p>Most of the advanced face detectors focus on the anchor-box design <ref type="bibr" target="#b47">[49]</ref> and matching strategies, because faces in the wild exhibits a large variation in size. Previously, <ref type="bibr" target="#b47">[49]</ref> proposed framework the relied on cascade of anchors at various scales to better detect faces of different sizes. Furthermore, FaceBoxes <ref type="bibr" target="#b48">[50]</ref> introduces an anchor-box densification strategy to ensure anchor-boxes of different sizes have the same density on an image, and in <ref type="bibr" target="#b49">[51,</ref><ref type="bibr" target="#b50">52]</ref>, the authors propose different anchor-box matching threshold to ensure a certain number of training examples for tiny faces, further DSFD <ref type="bibr" target="#b29">[30]</ref> proposes an improved anchor-box matching strategy to provide better initialization for the regressor. In SSH <ref type="bibr" target="#b51">[53]</ref>, anchor-boxes with two neighboring sizes share the same detection feature map. PyramidBox <ref type="bibr" target="#b52">[54]</ref> designs novel PyramidAnchors to help contextual feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Anchor-free object detection</head><p>Anchor-free detectors bypass the requirement of anchor-boxes and detect objects directly from an image. DeNet <ref type="bibr" target="#b53">[55]</ref> proposes to generate proposals by predict the confidence of each location belonging to four corners of objects. Following the two-stage pipeline, DeNet also appends another sub-network to re-score these proposals. Within the single-stage framework, YOLO <ref type="bibr" target="#b54">[56]</ref> appends fully-connected layers to parse the final feature maps of a network into class confidence scores and box coordinates. Densebox <ref type="bibr" target="#b55">[57]</ref> devises a unified FCN that directly regresses the classification scores and distances to the boundary of a ground truth box on all pixels, and demonstrates improved performance with landmark localization via multi-task learning. Therefore, it is effective pipeline different from Faster R-CNN <ref type="bibr" target="#b32">[34]</ref>. However, DenseBox resizes objects to a single scale during training, thus requiring image pyramids to detect objects of various sizes by multiple network passes during inference. Besides, Densebox defines the central area for each object and thus requires four parameters to measure the distances of each pixel in the central area to the object's boundaries. In contrast, the proposed CSP detector defines a single central point for each object, therefore two parameters measuring the object's scale are enough to get a bounding box, and sometimes one scale parameter is enough given uniform aspect ratio in the task of pedestrian detection. Most recently, CornerNet <ref type="bibr" target="#b22">[23]</ref> also applies a FCN but to predict objects' top-left and bottom-right corners and then group them via associative embedding <ref type="bibr" target="#b21">[22]</ref>. Enhanced by the novel corner pooling layer, CornerNet achieves superior performance on MS COCO object detection benchmark <ref type="bibr" target="#b56">[58]</ref>. Similarly, TLL <ref type="bibr" target="#b23">[24]</ref> proposes to detect an object by predicting the top and bottom vertexes. To group these paired keypoints into individual instances, it also predicts the link edge between them and employs a postprocessing scheme based on Markov Random Field. Applying on pedestrian detection, TLL achieves significant improvement on Caltech <ref type="bibr" target="#b26">[27]</ref>, especially for small-scale pedestrians.</p><p>The proposed methodology is an an anchor-free detection method. However, our work is different from the conventional approaches, as such, that we try to illustrate that a single FCN can be effectively deployed for face and pedestrian detection. In doing so, we also show that a single center point can be used for precise localization without any post-processing methodologies (except NMS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Feature detection</head><p>Feature detection is a widely studied problem with extensive literature in computer vision. Broadly, it primarily includes edge detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, corner detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b58">59]</ref>, blob detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b59">60]</ref> etc. Classical methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> of feature detection mostly relied on local cues, such as the brightness, colors, gradients and textures. However, after the emergence of CNN-based methods, the task of feature detection was greatly advanced. For instance, recently several methods deployed CNN-based methods to perform edge detection, such as, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, which have substantially advanced this field. However, unlike the above mentioned methods that perform low-level feature detection (edge, corners and blobs), the proposed method aims for a higher level of abstraction, that is, our center of attention is localizing central points, where there are pedestrians, for which modern deep models are naturally suited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>Generally, with an input image I, the detection network may generate several feature maps with different resolutions, which can be defined as follows:</p><formula xml:id="formula_0">? i = f i (? i?1 ) = f i (f i?1 (...f 2 (f 1 (I)))),<label>(1)</label></formula><p>where ? i represents feature maps output by the ith stage. These feature maps are generated by f i (.), decreasing in size progressively. Given a network with N downsampling stages, all the generated feature maps can be denoted as ? = {? 1 , ? 2 , ..., ? N }, upon which detection heads are further built.</p><p>We denote these feature maps that are responsible for detection as ? det . Specifically, in a object detector, the feature maps responsible for detection can be represented </p><formula xml:id="formula_1">as ? det = {? L , ? L+1 , ..., ? N }, where 1 &lt; L &lt; N . Besides ? det ,</formula><formula xml:id="formula_2">Dets = H(? det , B) = {cls(? det , B), regr(? det , B)},<label>(2)</label></formula><p>where B is pre-defined according to the corresponding set of feature maps ? det , and H(.) represents the detection head. Generally, H(.) contains two elements, namely cls(.) which predicts the classification scores, and regr(.) which predicts the scaling and offsets of the anchors.</p><p>While in anchor-free detectors, detection is performed merely on the set of feature maps ? det , that is,</p><formula xml:id="formula_3">Dets = H(? det )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overall architecture</head><p>The proposed CSP belongs to the group of anchor-free detectors, the overall architecture is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The backbone network are truncated from a standard network pretrained on ImageNet <ref type="bibr" target="#b60">[61]</ref>.   <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b23">24]</ref>, the dilated convolutions are adopted in stage 5 to maintain the final feature map with a higher resolution. We denote the output of stage 2, 3, 4 and 5 as ? 2 , ? 3 , ? 4 and ? 5 , in which the shallower feature maps can provide more precise localization information, while the coarser ones contain more semantic information with increasing the sizes of receptive fields. We simply fuse these multi-scale feature maps by concatenation, before which a deconvolution layer is adopted to make multi-scale feature maps with the same resolution. Since the feature maps from each stage have different scales, we use L2-normalization to rescale their norms to 10, as adopted in <ref type="bibr" target="#b46">[48]</ref>. To investigate the optimal combination from these multi-scale feature maps, we conduct an ablative experiment in Sec. 4.2. Given an input image of size H ? W , the size of final concatenated feature maps is</p><formula xml:id="formula_4">H/r ? W/r,</formula><p>where r is the downsampling factor. Similarly to <ref type="bibr" target="#b23">[24]</ref>, r = 4 gives the best performance as demonstrated in our experiments, because a larger r means coarser feature maps which struggle on accurate localization. Given its simplicity, more complicated feature fusion strategies in <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref> can be explored to further improve the performance, but it is not in the scope of this work. Detection Head. Upon the concatenated feature maps ? det , a detection head is appended to parse it into detection results. As stated in <ref type="bibr" target="#b64">[65]</ref>, the detection head plays a significant role. In this work, we firstly attach a single 3x3 Conv layer on ? det to reduce its channel dimensions to 256, and then two sibling 1x1Conv layers are appended to produce the scale map and center heatmap, respectively. Also, we do this for simplicity and any improvement of the detection head <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b29">30]</ref> can be flexibly incorporated into this work to be a better detector.</p><p>Optionally, to slightly adjust the center location, an extra offset prediction branch can be appended in parallel with the above two branches. This extra offset branch resolves mismatching of centers that occurs due to downsampled feature maps by learning the shifted offset. We will demonstrate the effectiveness of the extra offset branch in (Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>Ground Truth. The predicted heatmaps are with the same size as the concatenated feature maps (i.e. H/r ? W/r). An illustration example is depicted in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref> to illustrate how to generate the center and scale ground truth by the given bounding box annotations. For the center ground truth, the location where an object's center point falls in is assigned as positive while all others are negatives.</p><p>Scale can be defined as the height and/or width of objects. In pedestrian detection, line annotation is first proposed in <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b25">26]</ref> to generate high-quality ground truth, where tight bounding boxes are automatically generated with a uniform aspect ratio 1 of 0.41.</p><p>In accordance to this annotation, we can merely predict the height of each pedestrian instance and generate the bounding box with the predetermined aspect ratio. Therefore, for the scale ground truth, the kth positive location is assigned with the value of log(h k ). To reduce the center ambiguity, log(h k ) is also assigned to the negatives within a radius 2 of the positives, while all other locations are assigned as zeros. Alternatively, we can also predict the width or height+width, which is required for face detection, because face objects in the wild exhibit a large variation in aspect ratios.</p><p>When the offset prediction branch is appended, the ground truth for the offsets of those centers can be defined as ( x k r ? x k r , y k r ? y k r ). Loss Function. The center prediction can be formulated as a classification task.</p><p>Note that it is difficult to decide an 'exact' center point. In order to reduce the ambiguity of these negatives surrounding the positives, we also apply a 2D Gaussian mask G(.) centered at the location of each positive, which is also adopted in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. An example of the overall mask map M is depicted in <ref type="figure" target="#fig_2">Fig. 3 (c)</ref>. Formally, it is formulated as:</p><formula xml:id="formula_5">M ij = max k=1,2,...,K G(i, j; x k , y k , ? w k , ? h k ), G(i, j; x, y, ? w , ? h ) = e ?( (i?x) 2 2? 2 w + (j?y) 2 2? 2 h ) ,<label>(4)</label></formula><p>where K is the number of objects in an image, (x k , y k , w k , h k ) is the center coordinates, width and height of the kth object, and the variances (? k w , ? k h ) of the Gaussian mask are proportional to the height and width of individual objects. If these masks have overlaps, we choose the maximum values for the overlapped locations. To combat the extreme positive-negative imbalance problem, the focal weights <ref type="bibr" target="#b68">[69]</ref> on hard examples <ref type="bibr" target="#b0">1</ref> Aspect ratio here is defined as width / height are also adopted. Thus, the classification loss can be formulated as:</p><formula xml:id="formula_6">L center = ? 1 K W/r i=1 H/r j=1 ? ij (1 ?p ij ) ? log(p ij ),<label>(5)</label></formula><formula xml:id="formula_7">wherep ij = ? ? ? ? ? p ij if y ij = 1 1 ? p ij otherwise, ? ij = ? ? ? ? ? 1 if y ij = 1 (1 ? M ij ) ? otherwise.<label>(6)</label></formula><p>In the above, p ij ? [0, 1] is the center prediction in the location (i, j), and y ij ? {0, 1} specifies the ground truth label, where y ij = 1 represents the positives and y ij = 0 the negatives. ? is the focusing hyper-parameter, which is set as 2 as suggested in <ref type="bibr" target="#b68">[69]</ref>. To reduce the ambiguity from those negatives surrounding the positives, the ? ij according to the Gaussian mask M is applied to reduce their contributions to the total loss, in which the hyper-parameter ? controls the penalty. Experimentally, ? = 4</p><p>gives the best performance, which is similar in <ref type="bibr" target="#b22">[23]</ref>.</p><p>The scale prediction can be formulated as a regression task via smooth L1 loss <ref type="bibr" target="#b69">[70]</ref>:</p><formula xml:id="formula_8">L scale = 1 K K k=1 SmoothL1(s k , t k ),<label>(7)</label></formula><p>where s k and t k represents the network's prediction and the ground truth of each positive, respectively.</p><p>If the offset prediction branch is appended, the similar smooth L1 loss in Eq. 7 is adopted (denoted as L of f set ).</p><p>To sum up, the full optimization objective is:</p><formula xml:id="formula_9">L = ? c L center + ? s L scale + ? o L offset ,<label>(8)</label></formula><p>where ? c , ? s and ? o are the weights for center classification, scale regression and offset regression losses, which are experimentally set as 0.01, 1 and 0.1, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference</head><p>For inference, CSP simply involves a single forward of the network. Specifically, given the predictions, locations with confidence score above 0.01 in the center heatmap are kept, along with their predicted scale in the scale map. Then bounding boxes are generated and remapped to the original image size, followed by NMS. If the offset prediction branch is adopted, the centers are shifted according to the predicted offsets before being remapped to the original image size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Datasets and evaluation metrics</head><p>We evaluate our approach on two tasks, including pedestrian detection and face detection.</p><p>For pedestrian detection, we choose two of the largest benchmark datasets: Caltech <ref type="bibr" target="#b26">[27]</ref> and CityPersons <ref type="bibr" target="#b25">[26]</ref>. The Caltech dataset contains approximately 2.5 hours of autodriving video with extensively labelled bounding boxes. Following <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">41]</ref>, we use the training data augmented by 10 folds (42782 frames) and the standard test set with 4024 frames, evaluation results are reported based on the new annotations provided by <ref type="bibr" target="#b71">[72]</ref>. The CityPersons dataset is a more challenging benchmark with various occlusion levels. We train the models on the official training set with 2975 images and test on the validation set with 500 images. Evaluation follows the standard Caltech evaluation metric <ref type="bibr" target="#b26">[27]</ref>, that is log-average Miss Rate (MR) over False Positive Per Image (FPPI) ranging in [10 ?2 , 10 0 ] (denoted as M R ?2 ).</p><p>For face detection, we choose one of the most challenging face detection benchmark, i.e. WiderFace <ref type="bibr" target="#b27">[28]</ref>. The main reason we choose this dataset is due to its large variability of scale, aspect ratios, illumination and occlusions. The dataset defines three levels of difficulty: Easy, Medium and Hard by the detection rate of EdgeBox <ref type="bibr" target="#b72">[73]</ref>. We train the proposed CSP merely on the training subset and test on both validation and testing subsets by the Average Precision (AP).</p><p>We first conduct the ablative study on the Caltech dataset, and then compare the proposed CSP detector with the state of the arts on all the above benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Training details</head><p>The proposed method is implemented in Keras 2 . We use ResNet-50 <ref type="bibr">[</ref> iterations. Similar data augmentations in PyramidBox <ref type="bibr" target="#b52">[54]</ref> is applied to increase the proportion of small faces during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We perform ablation study of the key components of CSP on the Caltech dataset.    How important is the Feature Combination? It is revealed in <ref type="bibr" target="#b23">[24]</ref> that multiscale representation is vital for detection of various scales. In this part, we consider different combinations of multi-scale feature maps from the backbone. In practice we choose the output of stage 2 (? 2 ) as a start point and the downsampling factor r is fixed as 4. In spite of the ResNet-50 <ref type="bibr" target="#b73">[74]</ref> with stronger feature representation, a light-weight network like MobileNetV1 <ref type="bibr" target="#b76">[77]</ref> is also choosen. Results in <ref type="table" target="#tab_6">Table 4</ref> reveal that the much shallower feature maps like ? 2 result in poorer accuracy, while deeper feature maps like ? 4 and ? 5 are of great importance for superior performance, and the middlelevel feature maps ? 3 are indispensable to achieve the best results. For ResNet-50, the best performance comes from the combination of {? 3 , ? 4 , ? 5 }, while {? 3 , ? 4 } is the optimal one for MobileNetV1. With the model initialized from CityPersons <ref type="bibr" target="#b25">[26]</ref>, CSP achieves a new state of the art of 3.8%, compared to 4.0% of RepLoss <ref type="bibr" target="#b38">[40]</ref>. A mere pre-training on CityPersons further   boosts the performances because Caltech has lower person per images density compared to CityPersons 3 . It presents the superiority on detecting pedestrians of various scales and occlusion levels as demonstrated in <ref type="figure" target="#fig_4">Fig . 4 (b)</ref>. Moreover, <ref type="figure" target="#fig_4">Fig. 4 (c)</ref> shows that CSP also performs very well for heavily occluded pedestrians, outperforming Re-pLoss <ref type="bibr" target="#b38">[40]</ref> and OR-CNN <ref type="bibr" target="#b39">[41]</ref> which are explicitly designed for occlusion cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the State of the</head><p>CityPersons. <ref type="table" target="#tab_8">Table 5</ref> shows the comparison with previous state of the arts on CityPersons. Following <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b25">26]</ref>, results on subsets with different occlusion levels and various scale ranges are also reported. It can be observed that CSP beats the <ref type="bibr" target="#b2">3</ref> CityPersons has roughy 6 persons/image whereas Caltech has 0.3 <ref type="bibr" target="#b77">[78]</ref> Methods M R ?2 (%)</p><p>RetinaNet <ref type="bibr" target="#b68">[69]</ref> 63.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSP[ours]</head><p>62.1 competitors and performs fairly well on occlusion cases even without any specific occlusion-handling strategies <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b39">41]</ref>. On the Reasonable subset, CSP performs the best with a gain of 1.0% M R ?2 upon the closest competitor (ALFNet <ref type="bibr" target="#b28">[29]</ref>), while the speed is comparable on the same running environment with 0.33 second per image of 1024x2048 pixels. Additionally, we also compare CSP with state of the arts (Reti-naNet <ref type="bibr" target="#b68">[69]</ref> and CornerNet <ref type="bibr" target="#b22">[23]</ref>) in generic object detection, results presented in <ref type="table" target="#tab_8">Table   5</ref> demonstrate the superority of CSP in pedestrian detection.</p><p>CrowdHuman. We further validated our approach on a recently collected largescale general purpose (curated from web-crawling) person detection dataset, Crowd-Human <ref type="bibr" target="#b77">[78]</ref>. Unlike CityPersons and Caltech, CrowdHuman dataset does not have a fixed aspect ratio. CrowdHuman benchmark is a more diverse and dense dataset than CityPerson and Caltech in terms of person per image and unique pedestrians. We compare with a single stage object detector RetinaNet <ref type="bibr" target="#b68">[69]</ref> in <ref type="table" target="#tab_9">Table 6</ref>. CSP outperforms RetinaNet <ref type="bibr" target="#b68">[69]</ref> on a general person detection benchmark by more than 1 M R ?2 (%), illustrating the robustness of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Face Detection</head><p>WiderFace. The model trained on the training subset of WiderFace are evaluated on both the validation and test subsets, and the multi-scale testing is also performed in a similar way as in <ref type="bibr" target="#b52">[54]</ref>. Comparisons with the state-of-the-art face detectors on WiderFace are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generalization ability of the proposed method</head><p>To further demonstrate the generalization ability of the proposed CSP detector, we perform cross-dataset evaluation on two tasks, i.e. pedestrian detection and face detection. Specifically, models trained on the source dataset are directly tested on the target dataset without further finetuning. Furthermore, unlike humans, faces occur in different scale and aspect ratio, as shown in <ref type="table" target="#tab_3">Table 11</ref>. In <ref type="table" target="#tab_3">Table 11</ref>, we illustrate that in order to achieve high localization accuracy for faces, CSP needs to predict both height and width jointly, as oppose to simply predicting height (as in pedestrian detection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Cross-dataset evalutaion for Pedestrian Detection</head><p>For pedestrian detection, we compare the proposed CSP detector with the state-ofthe-art anchor-box based pedestrian detector (ALFNet <ref type="bibr" target="#b28">[29]</ref>  Mean image size 377x399 5184x3456 1080x720 are trained on the CityPersons <ref type="bibr" target="#b25">[26]</ref> training subset and then are directly tested on the Caltech <ref type="bibr" target="#b26">[27]</ref> test subset. For ALFNet <ref type="bibr" target="#b28">[29]</ref>, we use the source code and models provided by the authors <ref type="bibr" target="#b3">4</ref> . Results shown in <ref type="table" target="#tab_12">Table 7</ref> are based on the reasonable setting, and the evaluation metric is log average Miss Rate (MR). It can be seen that the gap between the two detectors on the source dataset (CityPersons) is merely 1%, but the gap on the target dataset (Caltech) increases to 5.9%, which gives the evidence that the proposed CSP detector generalizes better to another dataset than the anchor-box based competitor, i.e.</p><p>ALFNet <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Cross-dataset evalutation for Face Detection</head><p>For face detection, models trained on the WiderFace <ref type="bibr" target="#b27">[28]</ref> training subset are directly tested on three other face detection datasets, i.e. FDDB <ref type="bibr" target="#b30">[31]</ref>, UCCS [32] and DarkFace <ref type="bibr" target="#b31">[33]</ref>. Detailed statistics about these three datasets for testing are listed in <ref type="table" target="#tab_13">Table 8</ref>. It can be seen that these three datasets exhibit a large difference in the mean size of face objects. FDDB <ref type="bibr" target="#b30">[31]</ref> is also a widely adopted face detection benchmark. Comparisons with other advanced face detectors on this benchmark are reported in <ref type="figure" target="#fig_6">Fig. 6</ref>, results of other face detectors are from FDDB's official website <ref type="bibr" target="#b4">5</ref> . As shown in <ref type="figure" target="#fig_6">Fig. 6</ref>  The average size of the face objects in this dataset is merely 16x17 pixels. In the official website <ref type="bibr" target="#b5">6</ref> , it totally released 6000 images, on which we test both models and report the results.</p><p>For UCCS [32] and DarkFace <ref type="bibr" target="#b31">[33]</ref>, we compare the proposed CSP with the stateof-the-art anchor-box based face detector (DSFD <ref type="bibr" target="#b29">[30]</ref>). For DSFD, we use the source code and models provided by the authors <ref type="bibr" target="#b6">7</ref> . Results are given in <ref type="table" target="#tab_15">Table 9</ref>, and the evaluation metric is the Average Precision (AP). As can be seen from   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Further discussions</head><p>Note that CSP only requires object centers and scales for training, though generating them from bounding box or central line annotations is more feasible since centers are not always easy to annotate. Besides, the model may be puzzled on ambiguous centers during training. To demonstrate this, we also conduct an ablative experiment on Caltech, in which object centers are randomly disturbed in the range of [0,4] and [0,8] pixels during training. Results in <ref type="table" target="#tab_3">Table 10</ref> show that performance drops with increasing annotation noise. For Caltech, we also apply the original annotations but with inferior performance to another anchor-free detector, TLL <ref type="bibr" target="#b23">[24]</ref>. A possible reason is that TLL includes a series of post-processing strategies in keypoint pairing. As evaluations of TLL on Caltech with new annotations <ref type="bibr" target="#b71">[72]</ref> are not reported in <ref type="bibr" target="#b23">[24]</ref>, comparison to TLL is given in <ref type="table" target="#tab_8">Table 5</ref> on the CityPersons, which shows the superiority of CSP. Therefore, the proposed method may be limited for annotations with ambiguous centers, e.g. the traditional pedestrian bounding box annotations affected by limbs. In view of this, applying CSP to generic object detection requires further improvement.</p><p>When compared with anchor-box based methods, the advantage of CSP lies in two aspects. Firstly, CSP does not require tedious configurations on anchor-boxes specifi- is more robust to occluded objects as shown in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Deviating from traditional paradigms for feature detection, in this work, we argue in the favor of posing pedestrian detection as a high-level semantic feature detection task through straightforward convolutions for center and scale predictions. This enables a complete anchor.free settings and is also free from complex post-processing </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overall pipeline of the proposed CSP detector. The final convolutions have two channels, one is a heatmap indicating the locations of the centers (red dots), and the other serves to predict the scales (yellow dotted lines) for each detected center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture of CSP, which mainly comprises two components, i.e. the feature extraction module and the detection head. The feature extraction module concatenates feature maps of different resolutions into a single one. The detection head merely contains a 3x3 convolutional layer, followed by three prediction layers, for the offset prediction, center location and the the corresponding scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) is the bounding box annotations commonly adopted by anchor-box based detectors. (b) is the center and scale ground truth generated automatically from (a). Locations of all objects' center points are assigned as positives, and negatives otherwise. Each pixel is assigned a scale value of the corresponding object if it is a positive point, or 0 otherwise. We only show the height information of the two positives for clarity. (c) is the overall Gaussian mask map M defined in Eq.4 to reduce the ambiguity of these negatives surrounding the positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Arts 4.3.1. Pedestrian Detection Caltech. Extensive comparisons are conducted on three settings: Reasonable, All and Heavy Occlusion. As shown in Fig. 4, CSP achieves M R ?2 of 4.5% on the Reasonable setting, outperforming the best competitor (5.0 of RepLoss [40]) by 0.4%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Comparisons with the state of the arts on Caltech using new annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Precision-recall curves on WIDER FACE validation and testing subsets. these face detectors are anchor-box based. Therefore, the results indicate a superiority of the proposed CSP detector when complex default anchor-box design and anchor-box matching strategies are abandoned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Comparisons of ROC results on the FDDB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Different type of challenges present in pedestrian detection such as person-over-person occlusion (left), occlusion due to cars and objects (middle) and low illumination (right). CSP is robust enough to handle such challenging scenarios cally for each dataset. Secondly, anchor-box based methods detect objects by overall classifications of each anchor-box where background information and occlusions are also included and will confuse the detector's training. However, CSP overcomes this drawback by scanning for pedestrian centers instead of anchor-boxes in an image, thus</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>strategies as in recent keypoint-pairing based detectors. Consequently, the proposed CSP detector achieves the new state-of-the-art performance on two challenging pedestrian detection benchmarks, namely CityPersons and Caltech. Due to the generic architecture of the CSP detector, we further evaluate it for face detection on the most popular face detection benchmark, i.e. WiderFace. The comparable performance to other advanced anchor-box based face detectors also shows the proposed CSP de-tector's competitiveness. Besides, experiments on cross-dataset evaluation for both pedestrian detection and face detection further demonstrate CSP's superior generalization ability over anchor-box based detectors. For future possibilities, it is interesting to further explore CSP's capability in general object detection. Given its superiority on cross-dataset evaluation, it is also interesting to see CSP's potential when domain adaptation techniques are further explored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Why do we use the Center Point? The center point is capable of locating an individual object. A coming question is how about other high-level feature points. To answer this, we choose two other high-level feature points adopted in<ref type="bibr" target="#b23">[24]</ref>, i.e. the top and bottom vertexes. Comparisons are reported inTable.1. It is shown that both the two vertexes can succeed in detection but underperform the center point by approximately 2%-3% under IoU=0.5, and the performance gap is even larger under the stricter IoU=0.75. This is probably because the center point is advantageous to perceive the full body information. Comparisons of different high-level feature points. Bold number indicates the best result.</figDesc><table><row><cell>Point</cell><cell cols="2">M R ?2 (%)</cell></row><row><cell>Prediction</cell><cell cols="2">IoU=0.5 IoU=0.75</cell></row><row><cell>Center point</cell><cell>4.62</cell><cell>36.47</cell></row><row><cell>Top vertex</cell><cell>7.75</cell><cell>44.70</cell></row><row><cell>Bottom vertex</cell><cell>6.52</cell><cell>40.25</cell></row><row><cell>Scale</cell><cell cols="2">M R ?2 (%)</cell></row><row><cell>Prediction</cell><cell cols="2">IoU=0.5 IoU=0.75</cell></row><row><cell>Height</cell><cell>4.62</cell><cell>36.47</cell></row><row><cell>Width</cell><cell>5.31</cell><cell>53.06</cell></row><row><cell>Height+Width</cell><cell>4.73</cell><cell>41.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of different definitions for scale prediction. Bold number indicates the best result.How important is the Scale Prediction? Scale prediction is another indespensible component of CSP. In practice, we merely predict the height for each detected center. To demonstrate the generality of CSP, we have also tried to predict Width or Height+Width for comparison. For Height+Width, the only difference lies in that the scale prediction branch has two channels responsible for the height and width respectively. It can be observed inTable 2that Width and Height+Width prediction can also achieve comparable but suboptimal results to Height prediction. This result may be attributed to the line annotation style<ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b25">26]</ref> which provides accurate Height information, while Width is automatically generated by a fixed aspect ratio and thus is not able to provide additional information for training. Anyway, CSP is potentially feasible for other detection tasks requiring both height and width, which will be demonstrated in the following experiments for face detection.How important is the Feature Resolution? In the proposed method, the final set of feature maps (denoted as ? r det ) is downsampled by r w.r.t the input image. To explore the influence from r, we train the models with r = 2, 4, 8, 16 respectively. For</figDesc><table><row><cell>Feature for</cell><cell>Test Time</cell><cell cols="2">M R ?2 (%)</cell><cell cols="2">?M R ?2 (%)</cell></row><row><cell>+Offset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Detection</cell><cell>(ms/img)</cell><cell cols="4">IoU=0.5 IoU=0.75 IoU=0.5 IoU=0.75</cell></row><row><cell>? 2 det</cell><cell>69.8</cell><cell>5.32</cell><cell>30.08</cell><cell>-</cell><cell>-</cell></row><row><cell>? 4 det</cell><cell>58.2 59.6</cell><cell>4.62 4.54</cell><cell>36.47 28.80</cell><cell>+0.08</cell><cell>+7.67</cell></row><row><cell>? 8 det</cell><cell>49.2 50.4</cell><cell>7.00 6.08</cell><cell>54.25 32.93</cell><cell>+0.92</cell><cell>+21.32</cell></row><row><cell>? 16 det</cell><cell>42.0 42.7</cell><cell>20.27 7.41</cell><cell>75.17 33.87</cell><cell>+12.86</cell><cell>+41.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of different downsampling factors of the feature maps, which are denoted as ? r det downsampled by r w.r.t the input image. Test time is evaluated on the image with size of 480x640 pixels. ?M R ?2 means the improvement from the utilization of the offset prediction. Bold numbers indicate the best result.</figDesc><table><row><cell>Feature Maps</cell><cell>ResNet-50[74]</cell><cell></cell><cell cols="2">MobileNetV1[77]</cell><cell></cell></row><row><cell>? 2 ? 3 ? 4 ? 5 # Parameters</cell><cell>Test Time</cell><cell cols="2">M R ?2 (%) # Parameters</cell><cell>Test Time</cell><cell>M R ?2 (%)</cell></row><row><cell>4.7MB</cell><cell>36.2ms/img</cell><cell>9.96</cell><cell>2.1MB</cell><cell>27.3ms/img</cell><cell>34.96</cell></row><row><cell>16.1MB</cell><cell>44.5ms/img</cell><cell>5.68</cell><cell>6.0MB</cell><cell>32.3ms/img</cell><cell>8.33</cell></row><row><cell>37.4MB</cell><cell>54.4ms/img</cell><cell>5.84</cell><cell>10.7MB</cell><cell>34.5ms/img</cell><cell>10.03</cell></row><row><cell>16.7MB</cell><cell>46.0ms/img</cell><cell>6.34</cell><cell>6.3MB</cell><cell>33.3ms/img</cell><cell>8.43</cell></row><row><cell>40.0MB</cell><cell>58.2ms/img</cell><cell>4.62</cell><cell>12.3MB</cell><cell>38.2ms/img</cell><cell>9.59</cell></row><row><cell>40.6MB</cell><cell>61.1ms/img</cell><cell>4.99</cell><cell>12.6MB</cell><cell>40.5ms/img</cell><cell>9.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of different combinations of multi-scale feature representations defined in Sec. 3.2.? 2 , ? 3 , ? 4 and ? 5 represent the output of stage 2, 3, 4 and 5 of a backbone network, respectively. Bold numbers indicate the best results. r = 2, ? 2 det are up-sampled from ? 4 det by deconvolution. For r = 4, 8, 16, the offset prediction branch is alternatively appended. Stricter evaluations under IoU=0.75 are included to verify the effectiveness of additional offset prediction. As can be seen fromTable. 3, without offset prediction, ? 4 det presents the best result under IoU=0.5, but performs poorly under IoU=0.75 when compared with ? 2 det , which indicates that finer feature maps are beneficial for precise localization. Not surprisingly, a larger r witnesses a significant performance drop, which is mainly because coarser feature maps result in poor localization. In this case, additional offset prediction can substantially improve the detector upon ? 16 det by 12.86% and 41.30% under the IoU threshold of 0.5 and 0.75, respectively. It also brings an improvement of 7.67% under IoU=0.75 for the detector upon ? 4 det , with negligible extra computation cost, approximately 1ms per image of 480x640 pixels.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the state of the arts on CityPersons<ref type="bibr" target="#b25">[26]</ref>. Results test on the original image size (1024x2048 pixels) are reported. Red and green indicate the best and second best performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison of CSP with another single stage object detector on general person detection dataset,</figDesc><table><row><cell>CrowdHuman).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Comparisons of the generalization ability for pedestrian detection (Evaluation metric: log averageMiss Rate; the lower, the better).</figDesc><table><row><cell>Dataset</cell><cell cols="3">FDDB [31] UCCS [32] DarkFace [33]</cell></row><row><cell>Num. of images</cell><cell>2485</cell><cell>5232</cell><cell>6000</cell></row><row><cell>Num. of faces</cell><cell>5171</cell><cell>11109</cell><cell>43849</cell></row><row><cell>Mean face size</cell><cell>95x141</cell><cell>225x407</cell><cell>16x17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Statistics of three face detection datasets for cross-dataset evaluation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>, the proposed CSP detector achieves competitive results on both discontinuous and continuous ROC curves, with the true positive rate of 99.2% and 86.0% when the number of false positives equals to 1000, while the results of the most recent anchor-box based face detector (DSFD [30]) are 99.1% and 86.2%, respectively. Since face images of both FDDB and WiderFace are obtained from the Internet, they are basically similar. Therefore, both CSP and DSFD detectors trained on WiderFace perform quite good on FDDB, and there is little performance difference between them.However, when evaluated on the other two quite different face datasets, UCCS[32]    and DarkFace<ref type="bibr" target="#b31">[33]</ref>, it is interesting to see some difference. UCCS [32], with the full name of UnConstrained College Students (UCCS), is a recently published dataset collected by a long-range high-resolution surveillance camera. The significant difference between UCCS [32] and other face detection datasets is that the data are collected unconstrainedly in surveillance scenes. People walking on the sidewalk did not aware that they were being recorded. As the annotations of the test subset is publicly unavailable, results on the validation subset are reported.DarkFace<ref type="bibr" target="#b31">[33]</ref> is a recently published face detection dataset collected during nighttime, which exhibits an extreme light condition compared to other face detection datasets.</figDesc><table><row><cell cols="3">WiderFace ? WiderFace</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell></cell><cell cols="2">WiderFace ? UCCS WiderFace ? DarkFace</cell></row><row><cell cols="3">Hard Medium Easy</cell><cell></cell><cell></cell></row><row><cell>DSFD [30] 90.0</cell><cell>95.3</cell><cell>96.0</cell><cell>7.6</cell><cell>25.9</cell></row><row><cell>CSP[ours] 89.9</cell><cell>94.4</cell><cell>94.9</cell><cell>11.3</cell><cell>28.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Comparisons on generalization ability of face detectors (Evaluation metric: Average Precision (AP); the higher, the better).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9</head><label>9</label><figDesc>, though the proposed CSP slightly underperforms DSFD on the WiderFace test subset, it achieves a significant gain over DSFD on these two cross-dataset evaluations, with 3.7% and 2.1% on UCCS [32] and DarkFace<ref type="bibr" target="#b31">[33]</ref>, respectively. Due to the substantial domain gaps between UCCS, DarkFace and WiderFace, both models trained on WiderFace perform unsurprisingly poor on UCCS and DarkFace, but the proposed CSP detector still outperforms the anchor-box based DSFD, which gives the evidence that CSP generalizes better to unknown domains than the anchor-box based competitor. It is possible that the default configurations of anchor-boxes in anchor-based detectors can not adapt to new scenes especially when the scales and aspect ratios of objects have a large difference as shown inTable 8. In contrast, the proposed detector simply predicts the centers and scales of objects without any considerations of priors of the objects in the dataset, thus shows a better generalization ability. Disturbance (pixels) M R ?2 (%) ?M R ?2 (%)</figDesc><table><row><cell>0</cell><cell>4.62</cell><cell>-</cell></row><row><cell>[0, 4]</cell><cell>5.68</cell><cell>? 1.06</cell></row><row><cell>[0, 8]</cell><cell>8.59</cell><cell>? 3.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Performance drop with disturbances of the centers.</figDesc><table><row><cell cols="3">Method CSP H + W CSP H</cell></row><row><cell>Easy</cell><cell>0.961</cell><cell>0.903</cell></row><row><cell>Medium</cell><cell>0.952</cell><cell>0.898</cell></row><row><cell>Hard</cell><cell>0.907</cell><cell>0.840</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>CSP with H+W vs. CSP with H only.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/fchollet/keras</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/liuwei16/ALFNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://vis-www.cs.umass.edu/fddb/results.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://flyywh.github.io/CVPRW2019LowLight/ 7 https://github.com/TencentYoutuResearch/FaceDetection-DSFD</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gpu-based pedestrian detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Campmany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Moure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2377" to="2381" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning scene-specific pedestrian detectors without real data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3819" to="3827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1112" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bridging the gap between detection and tracking: A unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3999" to="4009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining detection and tracking for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11088" to="11096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person reidentification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anchor-free person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7690" to="7699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Camera models and machine perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Department</title>
		<imprint>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
	<note type="report_type">Technion</note>
	<note>Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Susan: a new approach to low level image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="78" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine learning for high-speed corner detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scale selection properties of generalized scale-space interest point detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and vision</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="210" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust wide-baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3982" to="3991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepedge: A multi-scale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4380" to="4389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08050</idno>
		<title level="m">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Cornernet: Detecting objects as paired keypoints</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Small-scale pedestrian detection based on topological line localization and temporal feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-level semantic feature detection: A new perspective for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5187" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05693</idno>
		<title level="m">Citypersons: A diverse dataset for pedestrian detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning efficient single-stage pedestrian detectors by asymptotic localization fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10220</idno>
		<title level="m">Dsfd: Dual shot face detector</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep retinex decomposition for low-light enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">155</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Oriented spatial transformer network for pedestrian detection using fish-eye camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Is faster r-cnn doing well for pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalizable pedestrian detection: The elephant in the room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11328" to="11337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07752</idno>
		<title level="m">Repulsion loss: Detecting pedestrians in a crowd</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Occlusion-aware r-cnn: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bi-box regression for pedestrian detection and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Too far to see? not really!-pedestrian detection with scale-aware localization policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3703" to="3715" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Accurate pedestrian detection by human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1591" to="1605" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Centermask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13906" to="13915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient single-stage pedestrian detector by asymptotic localization fitting and multi-scale context encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1413" to="1425" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graininess-aware deep feature learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Anchor cascade for efficient face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2490" to="2501" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Faceboxes: A cpu real-time face detector with high accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Central Banking</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">S3fd: Single shot scaleinvariant face detector, International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Seeing small faces from robust anchor&apos;s perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5127" to="5136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<title level="m">Ssh: Single stage headless face detector, International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4885" to="4894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pyramidbox: A context-assisted single shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Denet: Scalable real-time object detection with directed sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tychsen-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="428" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<title level="m">Densebox: Unifying landmark localization with end to end object detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Faster and better: A machine learning approach to corner detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="105" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Principal curvaturebased region detector for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database, in: Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03144</idno>
		<title level="m">Feature pyramid networks for object detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Parallel feature pyramid network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Kook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep feature pyramid reconfiguration for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Light-head r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07264</idno>
	</analytic>
	<monogr>
		<title level="m">defense of two-stage object detector</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Towards reaching human performance in pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="973" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<title level="m">Focal loss for dense object detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1259" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Crowdhuman</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<title level="m">A benchmark for detecting human in a crowd</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
