<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Natural Synthetic Anomalies for Self-Supervised Anomaly Detection and Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><forename type="middle">M</forename><surname>Schl?ter</surname></persName>
							<email>hannah.schlueter17@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Hou</surname></persName>
							<email>benjamin.hou11@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kainz</surname></persName>
							<email>b.kainz@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Friedrich-Alexander-Universit?t Erlangen-N?rnberg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Natural Synthetic Anomalies for Self-Supervised Anomaly Detection and Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>image anomaly localization, self-supervised learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a simple and intuitive self-supervision task, Natural Synthetic Anomalies (NSA), for training an end-to-end model for anomaly detection and localization using only normal training data. NSA integrates Poisson image editing to seamlessly blend scaled patches of various sizes from separate images. This creates a wide range of synthetic anomalies which are more similar to natural sub-image irregularities than previous data-augmentation strategies for self-supervised anomaly detection. We evaluate the proposed method using natural and medical images. Our experiments with the MVTec AD dataset show that a model trained to localize NSA anomalies generalizes well to detecting real-world a priori unknown types of manufacturing defects. Our method achieves an overall detection AUROC of 97.2 outperforming all previous methods that learn without the use of additional datasets. Code available at https://github.com/hmsch/natural-synthetic-anomalies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Anomaly detection is a binary classification task where the aim is to separate normal data from anomalous examples. There are different types of anomaly detection depending on what training data and labels are available. A difficult yet realistic setting is to detect and localize unknown types of anomalies while only having access to normal data during training. To be useful in real applications, an automated system must be able to detect subtle and rare anomalies; irregularities that are either impossible to spot for humans because of contextual uniformity or get lost due to task-remote stimuli that lead to inattentional blindness <ref type="bibr" target="#b15">[16]</ref>.</p><p>Attempting to detect rare anomalies often means that it is impossible to acquire sufficient amounts of human-annotated training data for a supervised method. Obtaining precise ground-truth annotations is time-consuming and requires expert knowledge depending on the application domain. Anomaly detection based on only normal data has applications in many areas including defect detection in industrial production pipelines <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref>, unsupervised lesion detection in medical images <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref>, or finding unusual events in surveillance videos <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>The main challenge of unsupervised approaches is designing a training setup that will encourage the model to learn features relevant to anomaly detection without having any prior knowledge of the types of anomalies to expect. A common theme among top-performing approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> for anomaly detection in natural images, specifically on the MVTec AD benchmark, is to sidestep this challenge by relying on deep features from pre-trained ImageNet models.</p><p>We find approaches that learn from scratch more interesting. They are more widely applicable to other domains where the usefulness of ImageNet pre-training is limited, such as medical imaging <ref type="bibr" target="#b21">[22]</ref>. Many rely on learning a compressed representation of normal data and use these embeddings or reconstructions derived from them to define an anomaly score. Self-supervised learning is thus becoming a prominent strategy in anomaly detection. By designing an appropriate task, self-supervision can be an effective proxy for supervised learning, bypassing the need for labeled data. While various self-supervised tasks, such as context prediction <ref type="bibr" target="#b6">[7]</ref> or estimating geometric transformations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, can be used to learn a compressed representation of the data, recent works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref> show that data-augmentation strategies mimicking real defects are particularly effective for sub-image anomaly detection. These methods create synthetic anomalies by replacing or blending image patches with content from other images or image locations. However, recently proposed synthesis strategies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36]</ref> feature obvious discontinuities. This is raising concerns that the model may overfit to prior assumptions that are inherently encoded in synthetic manipulations. To prevent this, one can either resort to simpler encoders <ref type="bibr" target="#b13">[14]</ref>, which impedes end-to-end localization, or add additional networks for reconstruction <ref type="bibr" target="#b35">[36]</ref>, which greatly increases model size, computational costs, and training time. Even methods that linearly interpolate similar patches to create more subtle irregularities can suffer from the same problem <ref type="bibr" target="#b27">[28]</ref>. <ref type="bibr" target="#b28">[29]</ref> solves this problem by using Poisson image editing <ref type="bibr" target="#b20">[21]</ref>, but the resulting anomalies are so subtle that they may represent variations of the normal class rather than true anomalies.</p><p>Contribution. We introduce a simple and intuitive self-supervised method for sub-image anomaly detection and localization. Our Natural Synthetic Anomalies (NSA), are a) more natural than the current state-of-the-a-art CutPaste <ref type="bibr" target="#b13">[14]</ref>, FPI <ref type="bibr" target="#b27">[28]</ref>, or DRAEM <ref type="bibr" target="#b35">[36]</ref> due to the use of Poisson image editing, b) more diverse than CutPaste, FPI, or PII anomalies due to rescaling, shifting and a new Gamma-distribution-based patch shape sampling strategy, and c) more relevant to the task by imposing background constraints and using pixel-level labels derived from the resulting difference to the normal image rather than interpolation factors as used in FPI and PII. Like FPI and PII, NSA can be used to train an end-to-end model for anomaly detection and localization rather than generating compressed representations for a multi-stage pipeline.</p><p>We evaluate the proposed method on the MVTec AD dataset <ref type="bibr" target="#b1">[2]</ref> which contains normal training data and both normal and anomalous test data for a wide range of natural and manufacturing defects for 10 object and 5 texture classes. NSA achieves the new state-of-the-art localization (96.3 AUROC) and dectection (97.2 AUROC) performance among methods that do not use additional datasets.</p><p>It also performs comparably to the best methods that use additional data and much bigger models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref>. <ref type="bibr" target="#b5">[6]</ref> uses a model pre-trained on ImageNet. Compared to the large amount of data in ImageNet, our method only uses MVTec AD data, which contains between 60 and 391 training images per class.</p><p>NSA is a very general method for creating diverse and realistic synthetic anomalies in images. Since it does not rely on pre-training with ImageNet or any other dataset, it can easily be adapted to domains beyond natural images. Thus, we also evaluate NSA using a curated subset of a public chest X-ray dataset <ref type="bibr" target="#b32">[33]</ref> where it outperforms other state-of-the-art self-supervised methods for disease detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Reconstruction-based anomaly detection establishes pixel-level and imagelevel anomaly scores from the pixel-wise reconstruction error using variational autoencoders (VAE) <ref type="bibr" target="#b37">[38]</ref>, Bayesian autoencoders <ref type="bibr" target="#b19">[20]</ref>, generative adversarial networks (GAN) <ref type="bibr" target="#b26">[27]</ref>, or the restoration distance using a vector-quantized VAE (VQ-VAE) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref> trained with normal data. Anomaly scores can be improved by leveraging additional information derived from the model, such as the discriminator output when using a GAN <ref type="bibr" target="#b26">[27]</ref>, the KL-divergence of the latent representation of a VAE for image-level scores or its gradient for pixel-level scores <ref type="bibr" target="#b37">[38]</ref>, or the likelihood of the latent representation under a learnt prior using a VQ-VAE <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref> or latent space autoregression <ref type="bibr" target="#b0">[1]</ref>. A downside of these approaches is that it is difficult to control the capacity of the model. Depending on regularization, the model cannot reconstruct all details of normal examples well or may be able to reconstruct anomalous regions too. Embedding-based anomaly detection derives an anomaly score from the distance between embedding vectors of normal training images and test examples. An embedding-similarity metric can be defined using any method for one-class classification such as support vector data description (SVDD) used in <ref type="bibr" target="#b33">[34]</ref>, Gaussian distributions used in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>, or nearest neighbour search used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>. Features for the embedding vectors are often extracted from pre-trained deep neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, but can also be learned from scratch using a self-supervised task <ref type="bibr" target="#b13">[14]</ref> or together with the one-class classification objective in Deep-SVDD <ref type="bibr" target="#b25">[26]</ref> and Deep One-Class Classification (DOCC) <ref type="bibr" target="#b24">[25]</ref> or a combination thereof <ref type="bibr" target="#b33">[34]</ref>. When using embeddings of the entire image, embedding-based approaches can perform detection but not localization and are hence less interpretable. To circumvent this issue, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref> work with patch-level embeddings to create an anomaly map while <ref type="bibr" target="#b4">[5]</ref> compares test images to their nearest neighbors from the training set at the pixel-level. In a similar vein, flow-based methods can perform density estimation <ref type="bibr" target="#b9">[10]</ref>. However, these methods can sometimes assign higher likelihood to outlier samples <ref type="bibr" target="#b17">[18]</ref> and typically need pre-trained feature extractors to achieve better performance <ref type="bibr" target="#b34">[35]</ref>. Self-supervised learning. A supervisory signal from a proxy task defined based on unlabeled data, such as predicting the relative position of patches <ref type="bibr" target="#b6">[7]</ref> or estimating geometric transformations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, can help the model learn useful features for a downstream task. While <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> use features learned from the proxy task to discover different object classes, self-supervised learning has also been successfully applied to sub-image anomaly detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. In <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, the output for the self-supervised task is a prediction of the interpolation factor where a foreign patch from another observation in the training distribution has been blended into the current image. This output is used directly as an anomaly score without any further training step. We also employ this general setup for our method. In <ref type="bibr" target="#b35">[36]</ref>, the foreign patches are first removed by a reconstructive sub-network and then a discriminative sub-network produces an anomaly map by comparing the input image with foreign patches to the reconstruction. Poisson image editing. Pasting part of one image into another causes obvious discontinuities. <ref type="bibr" target="#b20">[21]</ref> developed a method to seamlessly clone an object from one image into another image. For a source image given by g and a destination image given by f * , we seek an interpolant f over the interior of a region ? with boundary ?? that solves the minimization problem given by <ref type="bibr" target="#b0">(1)</ref>. According to <ref type="bibr" target="#b20">[21]</ref>, this has the unique solution of the Poisson partial differential equations <ref type="bibr" target="#b1">(2)</ref> with Dirichlet boundary conditions given by the destination image.</p><formula xml:id="formula_0">f = arg min f ? |?f ? v| 2 with f | ?? = f * | ?? (1) ?f = divv over ?, with f | ?? = f * | ?? (2)</formula><p>[21] gives two options for defining the guidance field v: a) use the source image gradient (3) or b) a mix of source and destination gradients <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_1">v = ?g (3) ?x ? ?, v(x) = ?f * (x), if |?f * (x)| &gt; |?g(x)|, ?g(x), otherwise<label>(4)</label></formula><p>In practice, a finite difference discretization of (2) is solved numerically. Seamless cloning is implemented in the OpenCV library <ref type="bibr" target="#b2">[3]</ref> which we use in our selfsupervised task. A Poisson image editing approach has also recently been used for anomaly detection in medical images <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NSA Self-supervised task</head><p>As only normal data is available at training time, the model needs to be trained using a proxy task. In our case, the task is to localize synthetic anomalies created from normal data by blending a patch from a source image into the destination image as follows:</p><p>1. Select a random rectangular patch in the source image. 2. Randomly resize the patch and select a different destination location. 3. Seamlessly blend the patch into the destination image.</p><p>4. Optionally, repeat steps 1-3 to add multiple patches to the same image. 5. Create a pixel-wise label mask.</p><p>Patch sampling and constraints: Given two normal W ? H training images x s and x d , we select a random rectangular patch p s with width w and height h sampled from a truncated Gamma distribution, and center (c x , c y ) in the source image x s sampled from a uniform distribution:</p><formula xml:id="formula_2">w = W min (max (w min , 0.06 + r w ) , w max ) , with r w ? Gamma(2, 0.1) (5) h = H min (max (h min , 0.06 + r h ) , h max ) , with r h ? Gamma(2, 0.1) (6) c x ? U W w min 2 , W ? W w min 2 , c y ? U H h min 2 , H ? H h min 2<label>(7)</label></formula><p>Sampling the width and height from a truncated Gamma distribution means we assume anomalies are local (small) but want the model to be able to recognize larger irregularities too. Hence, some long slim rectangles and occasionally large patches are generated. The width and height bounds are selected based on the dimensions of the object. For images containing an object and a plain background, we calculate object masks m s and m d by thresholding the pixel-wise absolute difference to the background brightness b. For each pixel i the masks are given by:</p><formula xml:id="formula_3">m (i) s = |x (i) s ? b| &lt; t brightness , m (i) d = |x (i) d ? b| &lt; t brightness<label>(8)</label></formula><p>We apply <ref type="bibr" target="#b6">(7)</ref> repeatedly until (p s ? m s )/(wh) &gt; t object to ensure the patch contains part of the object. Then we resize the patch to obtain p ? s with width w ? = sw and height h ? = sh. We select a destination patch p d in the destination image x d with the same dimensions and center (c ? x , c ? y ) where:</p><formula xml:id="formula_4">s = max w min w , h min h , min r s , w max w , h max h with r s ? N (1, 1/4) (9) c ? x ? U W w ? 2 , W ? W w ? 2 , c ? y ? U H h ? 2 , H ? H h ? 2<label>(10)</label></formula><p>To prevent creating many examples of patches floating in the background, we apply (10) repeatedly until</p><formula xml:id="formula_5">(p d ? m d )/(w ? h ? ) &gt; t object (contains part of the object),<label>(11)</label></formula><formula xml:id="formula_6">(m p d ? m p ? s )/|m p ? s | &gt; t overlap</formula><p>(object portions of patch and destination image overlap)</p><p>where m p d and m p ? s are the object masks of the source and destination patches. We seamlessly blend p ? s into x d at location (c ? x , c ? y ) to obtain the training sample x. After blending the first patch, we add up to n ? 1 further patches by flipping n?1 coins whether to add another patch or not. <ref type="figure" target="#fig_0">Fig. 1</ref> shows a simplified outline of how the synthetic anomalies are created. Hyperparameters as assumptions: The patch selection procedure has a number of hyperparameters which characterize our synthetic out-distribution. Ideally, we would want this synthetic out-distribution to match the real out-distribution as closely as possible but since the real distribution is unknown at training time, we cannot use it to select the hyperparameters and sampling distributions. As <ref type="bibr" target="#b36">[37]</ref> proved for generative models, no test statistic is useful for all possible outdistributions and some assumptions must be made to increase the likelihood that the test is useful for relevant or probable out-distributions. For our method, we try to keep these assumptions as broad as possible by generating many different sizes, locations, quantities, and shapes of anomalies and only disregarding those which are obviously irrelevant due to extreme sizes compared to the object or little overlap with the object. Labels: We use the local intensity differences where a foreign patch has been introduced to create a pixel-wise label y which is either a) binary: whether there is a difference or not, b) continuous based on the mean absolute intensity difference across C color channels, or c) a logistic function of the previous. All labels are median filtered to be more coherent. Before filtering, the label values at each pixel i are calculated as follows:</p><formula xml:id="formula_8">y (i) binary = 1, if x (i) ? = x (i) d 0, otherwise , y (i) continuous = 1 C C c=1 | x (i,c) ? x (i,c) d | (13) y (i) logistic = y (i) binary 1 + exp ?k y (i) continuous ? y 0<label>(14)</label></formula><p>In contrast, FPI <ref type="bibr" target="#b27">[28]</ref> and PII <ref type="bibr" target="#b28">[29]</ref> use the patch interpolation factor as a label. This is somewhat ill-posed because the interpolation factor cannot be determined without knowing the pixel intensities of both the source and destination patches.</p><p>Our labels are directly related to the change in intensity (created by the patch blending) and therefore provide a more consistent training signal.</p><p>Loss: When using bounded labels ( y binary or y logistic ) we define our pixel-wise regression objective using binary cross-entropy loss. For unbounded labels ( y cont. ) we use mean squared error loss. The loss is given in (15)- <ref type="bibr" target="#b15">(16)</ref> where y = f ( x) is the output of a deep convolutional encoder-decoder.  <ref type="bibr" target="#b13">[14]</ref> and more diverse than interpolating patches from two separate images at corresponding locations as in FPI <ref type="bibr" target="#b27">[28]</ref> and PII <ref type="bibr" target="#b28">[29]</ref> although still noticeably artificial to a human observer ( <ref type="figure">Fig. 2</ref>).</p><formula xml:id="formula_9">L bce = 1 W ? H i ? y (i) bounded log y (i) ? (1 ? y (i) bounded ) log 1 ? y (i)<label>(15)</label></formula><formula xml:id="formula_10">L mse = 1 W ? H i y (i) continuous ? y (i) 2<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare end-to-end detection and localization models trained using our selfsupervised task to end-to-end models trained using our implementations of FPI <ref type="bibr" target="#b27">[28]</ref>, PII <ref type="bibr" target="#b28">[29]</ref>, and CutPaste augmentation <ref type="bibr" target="#b13">[14]</ref> on the MVTec AD dataset <ref type="bibr" target="#b1">[2]</ref> and a curated subset of a public chest X-ray dataset <ref type="bibr" target="#b32">[33]</ref>. We assess performance using the area under the receiver operating characteristic curve (AUROC).</p><p>Datasets: MVTec AD <ref type="bibr" target="#b1">[2]</ref> contains normal training data and normal and anomalous test data featuring various types of natural and manufacturing defects for 10 object and 5 texture classes. The NIH chest X-ray dataset <ref type="bibr" target="#b32">[33]</ref> contains normal images as well as 14 different types of pathological patterns. There is a lot of natural variation in the normal class which is challenging for unsupervised methods. However, the most obvious differences are easily explained by the different views and the gender of the patients. We reduce this variation by reducing the curated subset defined in <ref type="bibr" target="#b30">[31]</ref> further to only posteroanterior (back-to-front) view images of patients aged over 18 and separating them by gender. This leaves us with 1973 normal training images, 299 normal and 139 abnormal test images of male patients. For female patients, we have 1641 normal training, 244 normal and 123 abnormal test images. We call this dataset re-curated chest X-ray (rCXR) in the following. Note that the authors of PII <ref type="bibr" target="#b28">[29]</ref> used the full NIH chest X-ray rather than the curated subset defined in <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network architecture and training setup</head><p>We use an encoder-decoder architecture with ResNet-18 <ref type="bibr" target="#b11">[12]</ref> without the classification layers as the encoder, two 1x1 convolutions in the bottleneck to reduce the number of channels and a simpler ResNet-based decoder. The final activation is sigmoid and we use binary-crossentropy loss for all models besides NSA (continuous) for which we use ReLU activation and mean squared error loss as the labels are unbounded. The models are trained on batches of size 64 using Adam <ref type="bibr" target="#b12">[13]</ref> with a cosine-annealing learning rate <ref type="bibr" target="#b14">[15]</ref> that decays from 10 ?3 to 10 ?6 over 320 epochs. For non-aligned objects, the loss takes longer to converge, so we use 560 epochs for the hazelnut, metal nut, and screw classes in the MVTec AD dataset. For rCXR, we use 240 epochs. The same training hyperparameters are used for all variants of the self-supervised task. Hyperparameters for the self-supervised task are given in the supplementary material. Note that in our implementation of FPI, PII, and CutPaste we use object masks and the patch sizes are sampled from a truncated Gamma distribution rather than a uniform distribution <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> to allow for a more fair comparison with NSA. We call our CutPaste baseline CutPaste (end-to-end) to distinguish it from the multi-stage framework used in <ref type="bibr" target="#b13">[14]</ref>.</p><p>The MVTec AD images have high resolutions of up to 1024 ? 1024 pixels and use the RGB color scheme. We resize object images to 256 ? 256 pixels, apply a random rotation of up to 5 degrees for non-aligned and rotation invariant objects (bottle, hazelnut, metal nut, screw), center-crop to 230 ? 230 pixels and crop a random 224 ? 224 part of the image before creating self-supervised training examples to achieve slight rotation and translation invariance. When testing we use 224 ? 224 center-crops of 256 ? 256 object images. For texture classes, we use random 256 ? 256 crops of 264 ? 264 images for training and 256 ? 256 images for testing. Intensities are normalized using the mean and standard deviation of ImageNet as commonly used before feeding them into the model.</p><p>The rCXR images have a resolution of 1024 ? 1024 pixels in grayscale. We resize them to 256 ? 256 pixels for training and apply a random rotation of up to 3 degrees, center-crop to 230 ? 230 pixels and take a random crop of 224 ? 224 pixels. For testing we use 224 ? 224 center-crops of 256 ? 256 resampled images. Implementation: We use PyTorch <ref type="bibr" target="#b18">[19]</ref> V1.8.1 and train each model on an Nvidia GeForce GTX 1080 GPU while the self-supervised examples are created in parallel using 8 processes on an Intel Core i7-7700K CPU. The code is available at https://github.com/hmsch/natural-synthetic-anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and evaluation</head><p>Defect detection. In Tab. 1 we compare the detection performance of our models trained using variations of NSA, FPI <ref type="bibr" target="#b27">[28]</ref>, PII <ref type="bibr" target="#b28">[29]</ref>, and CutPaste <ref type="bibr" target="#b13">[14]</ref> to CutPaste (3-way) from <ref type="bibr" target="#b13">[14]</ref> which was previously the top-performing approach for defection detection in the MVTec AD dataset <ref type="bibr" target="#b1">[2]</ref> without using additional datasets. For NSA, we report the mean and standard error of the detection AUROC for each class as well as the object, texture, and overall averages across five different random seeds. Our best method, NSA (logistic), achieves an overall image-level AUROC of 97.2 outperforming CutPaste (3-way) <ref type="bibr" target="#b13">[14]</ref> by 2.0 which is well outside of the standard error range. A single NSA (logistic) model is also better than an ensemble of 5 CutPaste (3-way) models <ref type="bibr" target="#b13">[14]</ref> (96.1 AUROC) and comparable to EfficientNet <ref type="bibr" target="#b29">[30]</ref> pre-trained with ImageNet and finetuned with CutPaste (3-way) <ref type="bibr" target="#b13">[14]</ref> (97.1 AUROC).</p><p>Methods using pre-trained ImageNet models, such as PaDiM <ref type="bibr" target="#b5">[6]</ref> (97.9 AU-ROC), do not provide a fair comparison to our from-scratch approach. DRAEM <ref type="bibr" target="#b35">[36]</ref> is a more relevant competitor. It uses the describable textures dataset (DTD) <ref type="bibr" target="#b3">[4]</ref> not for pre-training but for creating its synthetic anomalies. DRAEM's training objective consists of localizing and correcting the synthetic anomalies, while our model performs anomaly localization. DRAEM (98.0 AUROC) outperforms our approach slightly overall, however both approaches give comparable results for many classes despite the fact that our models only have around 11 million parameters while DRAEM's two components add up to over 97 million parameters. We note that no standard errors were reported for DRAEM's results. The authors of DRAEM claim that realism of the synthetic anomalies is not important for their method. However, their own ablation study indicates that using plain solid colors instead of real textures from DTD, causes localization AUROC and AP to drop from 97.1 and 68.4 down to 92.6 and 56.5, respectively <ref type="bibr" target="#b35">[36]</ref>. As such, external data from DTD, which is designed to span a range of textures found in the wild <ref type="bibr" target="#b3">[4]</ref>, may help to produce synthetic anomalies that overlap more with real anomalies. Furthermore, when using a similar training setup as ours, i.e., without its reconstructive sub-network, DRAEM performs worse (93.9 AUROC) than NSA which uses more realistic synthetic anomalies. <ref type="table">Table 1</ref>: Image-level AUROC % for MVTec AD and standard error across five different random seeds. For our models, the image-level score is the average pixel score across the image. Best scores between DRAEM <ref type="bibr" target="#b35">[36]</ref>, CutPaste (3way) <ref type="bibr" target="#b13">[14]</ref>, and NSA within standard error are bold-faced. Note that DRAEM uses additional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOTA</head><p>Our Experiments DRAEM <ref type="bibr" target="#b35">[36]</ref> CutPaste (3-way) <ref type="bibr">[</ref> Synthetic anomalies should be as diverse and realistic as possible. In our experiments, models trained with self-supervised examples created using Poisson blending clearly outperform models trained with simpler data-augmentation strategies like CutPaste <ref type="bibr" target="#b13">[14]</ref> and FPI <ref type="bibr" target="#b27">[28]</ref>. Examples created with Poisson blending are more visually similar to real-world defects as they do not have artificial discontinuities ( <ref type="figure">Fig. 2</ref>) and according to Tab. 1, the corresponding models generalize better to real defects. Although PII <ref type="bibr" target="#b28">[29]</ref> performs well for most texture classes, NSA, which also shifts and resizes the patches, performs much better for objects. Since PII and FPI use the same source and destination location for the patches, their synthetic anomalies are very subtle for aligned object classes and much subtler than the real defects in the MVTec AD dataset. In classes with lower AUROC, the synthetic training anomalies may have less similarity to the real test anomalies. These classes also tend to have higher standard error. In contrast, classes with high AUROC have low standard error. This could indicate that for these classes the self-supervised task can sensitize the network to the distribution of real anomalies reliably. It may also be possible to use the variance between random seeds to gauge the reliability of predictions. Labels should approximate the degree of abnormality. Aside from abnormal irregularities there can also be natural variation between and within the images of each class. When training a model with binary labels, the final activations tend to saturate and the predictions do not give any measure of how anomalous the regions with high scores are. Training a model with continuous labels teaches the model to differentiate between different degrees of anomalies. When using unbounded continuous labels, training is less stable and the AUROC scores have a high standard error. Models trained with bounded continuous labels outperform the binary ones most in classes with high inherent variation such as cable, hazelnut, transistor, carpet, and wood (Tab. 1) when using a threshold independent metric such as AUROC. PII <ref type="bibr" target="#b28">[29]</ref> also uses continuous labels; pixels corresponding to the foreign patch are assigned to a uniform value equivalent to the interpolation factor. However, NSA (logistic) outperforms PII for some textures and most objects, including unaligned objects. For aligned objects, NSA creates more diverse anomalies than PII because the location of the source and destination patches can be different. But for unaligned objects, this advantage is negligible. Despite the similarity in generated anomalies, NSA still yields higher performance in these classes. A possible explanation is that the labels for NSA (logistic) are inhomogeneous and based on the outcome of the blending rather than its setup and hence more accurately represent the degree of abnormality. Defect localization. In Tab. 2 we report the pixel-level performance of the models from Tab. 1. Although NSA performs well for objects on the imagelevel when trained with unbounded continuous labels, pixel-wise performance is much better for NSA with bounded binary or continuous labels. NSA (logistic) achieves a 96.3 average pixel-level AUROC performing similarly to CutPaste (3way) <ref type="bibr" target="#b13">[14]</ref> (96.0 AUROC). It also reaches similar performance to DRAEM <ref type="bibr" target="#b35">[36]</ref> for many classes although DRAEM achieves a higher overall score (97.3 AUROC). <ref type="figure" target="#fig_1">Fig. 3</ref> shows that NSA (logistic) can localize a wide range of real-world defects accurately including anomalies that are very different from the synthetic anomalies seen during training (e.g., white writing on hazelnuts, misplaced transistors, stained tiles and carpet). Medical imaging. In Tab. 3, we compare the performance of NSA and endto-end models trained using other self-supervised tasks for the task of binary classification of rCXR images into healthy (normal) and pathological (abnormal) categories. Models trained using NSA clearly outperform end-to-end models trained with FPI <ref type="bibr" target="#b27">[28]</ref> or CutPaste <ref type="bibr" target="#b13">[14]</ref>. NSA also outperforms PII <ref type="bibr" target="#b28">[29]</ref>. However, the type of label used for NSA is less important for this dataset. Since there is high inter-sample variability in the normal data, it is possible that all synthetic anomalies created by NSA would be considered abnormal. So, approximating the degree of abnormality with a continuous label does not improve over the binary labels when evaluating the model with real anomalies. We do not report pixel-level metrics, as we only have very rough bounding boxes for less than 10% of the test set. <ref type="figure">Fig. 4</ref> shows example predictions for  <ref type="bibr" target="#b13">[14]</ref> here as there is no reference implementation available. Limitations. The 9th example in <ref type="figure">Fig. 4</ref> shows cardiomegaly, i.e., an enlarged heart, for which the bounding box contains the entire heart. But the model only activates for portions of the heart that exceed the normal size found in healthy patients. In these cases, the model lacks the semantic understanding that radiologists use to categorize diseases. In the 10th example, the model activates more for the tubes on the patient's right side than for the finding  inside of the bounding box. Unlike a human radiologist, an anomaly detection model cannot be expected to automatically classify clinically correctly placed lines, tubes, or cardiac devices as normal if they are not expected in the healthy training distribution. Our method sometimes fails to detect very small defects (see capsule, hazelnut, screw, and wood examples in <ref type="figure" target="#fig_2">Fig. 5</ref>), predicts too large regions or has false positives (see bottle, cable, zipper, carpet, leather in <ref type="figure" target="#fig_2">Fig. 5</ref>). The transistor class has several examples of misplaced or missing transistors. These are detected but the predicted localization does not match the large human annotation ( <ref type="figure" target="#fig_2">Fig. 5</ref>) resulting in a low localization AUROC (Tab. 2). Patch-wise localization, as used for CutPaste <ref type="bibr" target="#b13">[14]</ref> and PaDiM <ref type="bibr" target="#b5">[6]</ref> among others, can detect the missing transistor for each patch and hence produce a segmentation map closer to the human annotation. However, for these large anomalies the type of defect is immediately obvious to a human inspector once detected so we argue that precise localization would not be necessary for most applications.</p><p>Since the model does not see any real anomalies during training, it may predict statistically unlikely normal variations as abnormal and fails to recognize subtle anomalies that are very different from the synthetic anomalies seen during training. Hence, predictions from a self-supervised anomaly detection model should not be used on their own for decision making. Such models can however be useful as an instant second observer and for quality control. As far as we are aware, there are no further potential negative societal impacts of this work. <ref type="figure">Fig. 4</ref>: Example localization predictions for chest X-ray disease detection using models trained with NSA (binary). From top to bottom: input images, rough bounding box from a radiologist, heatmap of pixel-level predictions. Each case is labeled with pathology keywords that <ref type="bibr" target="#b32">[33]</ref> mined from radiologist reports. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a self-supervised task that creates diverse and realistic synthetic anomalies. These training examples are generated under controlled conditions that help to produce relevant and subtle anomalies. This provides a more consistent training signal and results in better detection of real anomalies. The formulation of the loss and synthetic labels yields an effective and computationally efficient training task. This helps NSA outperform state-of-the-art methods on both natural and medical imaging datasets, demonstrating its generalizability. In the future, additions such as quantifying uncertainty or exploiting classes of known anomalies could help facilitate the use of NSA in critical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional experiment details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Hyperparameters</head><p>The hyperparameters for the self-supervised tasks used in our experiments are given in Tab. 4. n max is the maximum number of patches added to each image. h min , h max , w min , w max ? [0, 1] are the bounds on the patch dimensions relative to the image size. b ? [0, 225] is the background brightness. All pixels with absolute brightness distance less than t brightness to the background brightness are assigned to the background. t object , t overlap ? [0, 1] are used for the object and overlap conditions for the patches (see Section 3). The patch resize-scale is clipped to the range [s min , s max ] in addition to the conditions given in Section 3. y 0 and k give the midpoint and steepness of the logistic function used for creating the labels for NSA (logistic).</p><p>These parameters encode our assumptions about the unknown real outdistribution (see Sec. 3). Thus they were not tuned in a data-driven way as no validation set containing all possible types of real anomalies was available. These assumptions were chosen based on visual inspection of the input images and selfsupervised examples. E.g., for objects that have larger width than height, w max is higher than h max and vice versa; for classes with high perceived natural variation y 0 should be larger and k smaller.  For FPI (Poisson), we used mixed gradients for seamless cloning. For NSA, we use mixed gradients for all rCXR data and for MVTec AD texture classes. For MVTec AD object classes, we find that OpenCV's <ref type="bibr" target="#b2">[3]</ref> seamless cloning method causes artifacts when there are sharp contrast changes (e.g., at the boundary from the object to the background) near the edges of the patch boundary more frequently when using mixed gradients than source gradients. Thus, we only use source gradients for these classes for NSA.  <ref type="bibr" target="#b13">[14]</ref>, FPI <ref type="bibr" target="#b27">[28]</ref>, PII <ref type="bibr" target="#b28">[29]</ref>, and NSA self-supervised tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Comparison of self-supervised tasks</head><p>CutPaste <ref type="bibr" target="#b13">[14]</ref> FPI <ref type="bibr" target="#b27">[28]</ref> PII <ref type="bibr">[</ref>   <ref type="bibr" target="#b13">[14]</ref>, FPI <ref type="bibr" target="#b27">[28]</ref>, PII <ref type="bibr" target="#b28">[29]</ref>, and our method. We use our patch-selection procedure for our re-implementations of CutPaste, FPI, and PII. See Sec. 3 for more details on our method. D Mask the patches with a union of 5 random ellipses to achieve non-rectangular patch shapes.</p><p>For these experiments we report image-level and pixel-level AUROC % (Tab. 7). The results back-up our design choices as the final version outperforms all three variants. Specifically, the experiments show that A using foreground constraints is most important for classes where the images contain a lot of background due to the shape of the objects (e.g., screw and capsule), B using a random number of patches performs slightly better than using a single patch, C our patch-selection procedure leads to much better overall performance of NSA than the patch selection procedure described in <ref type="bibr" target="#b13">[14]</ref>, and D beyond the diverse sizes and aspect ratios the shape of the patches is not important. This could be due to the fact that because of Poisson blending rectangular patches due not necessarily create rectangular anomalies, so NSA with rectangular patches already creates various non-rectangular anomalies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Per-region overlap</head><p>The AU-PRO 0.3 metric is defined as the area under the per-region overlap (PRO) curve for false positive rates up to 30 % <ref type="bibr" target="#b1">[2]</ref>. To calculate PRO, we decompose the ground-truth label maps into M connected components such that C j,k gives the set of anomalous pixels in a connected component k of label map j. Let P j denote the predicted anomalous pixels when using a threshold t.</p><p>[2] defines PRO as:</p><formula xml:id="formula_11">PRO = 1 M j k |P j ? C j,k | |C j,k |<label>(17)</label></formula><p>Unlike pixel-level AUROC, AU-PRO assigns equal weight to small and large anomalies. This is desirable for practical applications where precise localization of small anomalies is at least as important as localization of large anomalies.</p><p>In Tab. 8 we report AU-PRO 0.3 scores for our models from Tab. 2 as well as the scores for PaDiM <ref type="bibr" target="#b5">[6]</ref> for reference. Note that unlike our method, PaDiM relies on ImageNet pretraining, so this is not a fair comparison. The authors of CutPaste <ref type="bibr" target="#b13">[14]</ref> and DRAEM <ref type="bibr" target="#b35">[36]</ref> did not report AU-PRO for their method but we hope that future methods that learn from scratch can compare their localization performance to our AU-PRO scores. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>NSA anomalies are created by seamlessly cloning a patch from a normal training image into another normal training image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Examples of defect localization in the MVTec AD dataset using models trained with NSA (logistic). From top to bottom: input images, human annotation, heatmap of pixel-level predictions. Best viewed in a digital version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Failure cases for MVTec AD defect localization using models trained with NSA (logistic). Examples include false pos./negatives and incorrect localization. From left to right: input, human annotation, heatmap of pixel-level predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>hmax wmin, wmax b tbrightness tobject toverlap smin, smax y0 k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Synthetic anomalies created with CutPaste, FPI, PII, and NSA. scale of the patches, this method dynamically creates a wide range of synthetic anomalies during training. The examples feature changes in size, shape, texture, location, and color of local image components as well as missing components by blending in a patch containing some background, while staying true to the overall distribution of the images and avoiding obvious discontinuities. Hence, these examples are a more realistic approximation of natural sub-image anomalies than CutPaste augmentations constructed by simply pasting patches at different locations</figDesc><table><row><cell>Original</cell><cell>CutPaste</cell><cell>CutPaste</cell><cell>FPI</cell><cell>FPI</cell><cell>PII</cell><cell>PII</cell><cell>NSA</cell><cell>NSA</cell></row><row><cell>Fig. 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>By varying size, aspect ratio, source and destination location, and resizing the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Pixel-level AUROC % with standard error for the MVTec AD models from Tab. 1. Note that DRAEM uses additional data. ? 0.2 93.1 ? 1.1 96.3 ? 0.4 several healthy and pathological cases. The localization predictions are good for examples 1-5, 7, and 8, but the model fails to detect any abnormal findings in the 6th example and disagrees with the bounding box annotation for bottom examples 9 and 10. We did not compare to CutPaste (3-way)</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SOTA</cell><cell></cell><cell cols="3">Our Experiments</cell></row><row><cell></cell><cell></cell><cell>DRAEM [36]</cell><cell>CutPaste (3-way) [14]</cell><cell>CutPaste (end-to-end)</cell><cell>FPI PII</cell><cell>NSA (binary)</cell><cell>NSA (continuous)</cell><cell>NSA (logistic)</cell></row><row><cell></cell><cell>bottle</cell><cell>99.1</cell><cell>97.6 ? 0.1</cell><cell>97.7</cell><cell cols="3">91.8 93.1 98.4 ? 0.2 97.3 ? 0.5 98.3 ? 0.1</cell></row><row><cell></cell><cell>cable</cell><cell>94.7</cell><cell>90.0 ? 0.2</cell><cell>81.0</cell><cell cols="3">66.5 70.2 93.3 ? 3.4 91.0 ? 2.8 96.0 ? 1.4</cell></row><row><cell></cell><cell>capsule</cell><cell>94.3</cell><cell>97.4 ? 0.1</cell><cell>97.5</cell><cell cols="3">95.9 90.2 98.1 ? 0.2 91.6 ? 5.6 97.6 ? 0.9</cell></row><row><cell></cell><cell>hazelnut</cell><cell>99.7</cell><cell>97.3 ? 0.1</cell><cell>94.8</cell><cell cols="3">89.8 97.0 97.2 ? 0.6 97.7 ? 0.6 97.6 ? 0.6</cell></row><row><cell>object</cell><cell>metal nut pill screw</cell><cell>99.5 97.6 97.6</cell><cell>93.1 ? 0.4 95.7 ? 0.1 96.7 ? 0.1</cell><cell>68.1 98.1 90.7</cell><cell cols="3">96.2 95.4 98.2 ? 0.2 97.3 ? 0.3 98.4 ? 0.2 62.3 95.3 98.5 ? 0.2 97.1 ? 2.7 98.5 ? 0.3 90.4 92.8 96.7 ? 0.4 92.3 ? 5.3 96.5 ? 0.1</cell></row><row><cell></cell><cell>toothbrush</cell><cell>98.1</cell><cell>98.1 ? 0.0</cell><cell>95.7</cell><cell cols="3">81.8 81.3 95.6 ? 0.6 94.5 ? 0.7 94.9 ? 0.7</cell></row><row><cell></cell><cell>transistor</cell><cell>90.9</cell><cell>93.0 ? 0.2</cell><cell>85.9</cell><cell cols="3">78.5 86.9 87.8 ? 1.9 80.2 ? 3.3 88.0 ? 1.8</cell></row><row><cell></cell><cell>zipper</cell><cell>98.8</cell><cell>99.3 ? 0.0</cell><cell>92.9</cell><cell cols="3">91.8 93.8 94.2 ? 0.2 90.7 ? 1.5 94.2 ? 0.3</cell></row><row><cell></cell><cell>average</cell><cell>97.0</cell><cell>95.8 ? 0.1</cell><cell>90.2</cell><cell cols="3">84.5 89.6 95.8 ? 0.4 93.0 ? 1.7 96.0 ? 0.4</cell></row><row><cell></cell><cell>carpet</cell><cell>95.5</cell><cell>98.3 ? 0.0</cell><cell>83.3</cell><cell cols="3">70.8 97.2 94.5 ? 4.1 81.8 ? 6.8 95.5 ? 2.3</cell></row><row><cell>texture</cell><cell>grid leather tile wood</cell><cell>99.7 98.6 99.2 96.4</cell><cell>97.5 ? 0.1 99.5 ? 0.0 90.5 ? 0.2 95.5 ? 0.1</cell><cell>97.6 96.4 72.7 84.0</cell><cell cols="3">94.2 98.9 99.1 ? 0.0 98.0 ? 0.3 99.2 ? 0.1 88.3 99.2 99.6 ? 0.0 99.5 ? 0.2 99.5 ? 0.1 65.0 98.0 99.0 ? 0.2 97.4 ? 0.7 99.3 ? 0.0 71.1 91.1 94.0 ? 0.8 90.6 ? 3.7 90.7 ? 1.9</cell></row><row><cell></cell><cell>average</cell><cell>97.9</cell><cell>96.3 ? 0.1</cell><cell>86.8</cell><cell cols="3">77.9 96.9 97.3 ? 0.7 93.5 ? 0.9 96.8 ? 0.7</cell></row><row><cell cols="3">overall average 97.3</cell><cell>96.0 ? 0.1</cell><cell>89.1</cell><cell cols="2">82.3 92.0 96.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Image-level AUROC % for rCXR and standard error across five different random seeds. Best scores per row within standard error are bold-faced.</figDesc><table><row><cell></cell><cell>CutPaste (end-to-end)</cell><cell>FPI</cell><cell>PII</cell><cell>NSA (binary)</cell><cell>NSA (continuous)</cell><cell>NSA (logistic)</cell></row><row><cell>male</cell><cell>59.8</cell><cell cols="5">73.7 91.7 ? 0.6 94.0 ? 0.5 93.4 ? 0.3 94.0 ? 0.6</cell></row><row><cell>female</cell><cell>56.2</cell><cell cols="5">67.4 92.8 ? 0.4 94.3 ? 0.6 93.0 ? 0.4 94.0 ? 0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for the self-supervised tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of CutPaste</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the original patch-selection procedures for CutPaste</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Do not use foreground constraints for any classes. Note that in the original, foreground constraints were not applied to cable, transistor, and textures so these experiments do not need to be duplicated.</figDesc><table><row><cell cols="2">B Additional results</cell><cell></cell><cell></cell></row><row><cell cols="3">B.1 Additional ablation studies</cell><cell></cell></row><row><cell cols="4">To validate our design choices we conducted several ablation studies beyond</cell></row><row><cell cols="4">comparing different label definitions and comparing NSA to simpler baseline</cell></row><row><cell cols="4">synthetic anomalies (see Sec. 4.2). Additional variants of NSA (logistic) consid-</cell></row><row><cell>ered were:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">A B Only use a single patch per training example instead of a random number</cell></row><row><cell>of patches.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">C Generate patch shapes as for CutPaste [14]:</cell><cell></cell></row><row><cell cols="4">1. sample the area ratio between the patch and the full image from (0.02, 0.15),</cell></row><row><cell cols="4">2. determine the aspect ratio by sampling from (0.3, 1) ? (1, 3.3),</cell></row><row><cell cols="4">3. sample location such that patch is contained entirely within the image.</cell></row><row><cell cols="4">(Use single patch, uniform distributions, no foreground constraints, no re-</cell></row><row><cell>sizing.)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CutPaste [14]</cell><cell>FPI [28] and PII [29]</cell><cell>Ours</cell></row><row><cell>Patch size</cell><cell>area ratio between patch and image</cell><cell>width and height relative to im-</cell><cell>width and height relative to image</cell></row><row><cell></cell><cell>sampled from (0.02, 0.15)</cell><cell>age dimensions sampled from</cell><cell>dimensions sampled from truncated</cell></row><row><cell></cell><cell></cell><cell>U (0.1, 0.4)</cell><cell>Gamma(2, 0.1)</cell></row><row><cell>Patch aspect ratio</cell><cell>sampled from (0.3, 1) ? (1, 3.3)</cell><cell>square, except when truncated by</cell><cell>any ratio resulting from the above</cell></row><row><cell></cell><cell></cell><cell>the image boundary</cell><cell></cell></row><row><cell>Location restrictions</cell><cell>entire patch must appear in the full</cell><cell>patch center must lie within the</cell><cell>patch must contain part of the ob-</cell></row><row><cell></cell><cell>image</cell><cell>core 80% of the image dimensions</cell><cell>ject and object portions at source</cell></row><row><cell></cell><cell></cell><cell></cell><cell>and destination must overlap</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Image-level and pixel-level AUROC % for MVTec AD and standard error across five different random seeds for NSA (logistic) variants. ? 0.3 97.5 ? 0.5 97.1 ? 0.4 97.1 ? 0.7 98.4 ? 0.2 98.3 ? 0.1 98.4 ? 0.1 97.9 ? 0.1 97.4 ? 0.2 98.9 ? 0.0 cable 94.5 ? 1.0 -96.1 ? 1.0 92.4 ? 2.0 91.8 ? 1.4 96.0 ? 1.4 -94.7 ? 2.7 96.7 ? 0.5 86.8 ? 2.4 capsule 95.2 ? 1.7 91.7 ? 1.9 89.7 ? 1.3 84.6 ? 0.8 95.6 ? 0.7 97.6 ? 0.9 96.4 ? 0.4 95.2 ? 1.4 92.7 ? 0.9 97.1 ? 0.2 hazelnut 94.7 ? 1.1 93.1 ? 0.8 92.2 ? 2.7 85.2 ? 2.5 94.1 ? 2.0 97.6 ? 0.6 97.5 ? 0.7 93.6 ? 0.9 94.5 ? 1.2 97.5 ? 0.4 metal nut 98.7 ? 0.7 99.0 ? 0.6 97.7 ? 1.0 94.4 ? 1.0 99.3 ? 0.3 98.4 ? 0.2 98.5 ? 0.1 96.5 ? 0.9 97.0 ? 0.3 98.2 ? 0.4 pill 99.2 ? 0.6 99.0 ? 0.2 97.8 ? 0.5 94.5 ? 1.6 96.9 ? 1.0 98.5 ? 0.3 97.5 ? 0.2 90.5 ? 4.5 92.8 ? 2.2 97.1 ? 1.0 screw 90.2 ? 1.4 77.8 ? 3.3 85.3 ? 3.4 56.3 ? 1.8 90.3 ? 1.0 96.5 ? 0.1 92.9 ? 0.6 95.6 ? 0.8 82.6 ? 1.6 96.2 ? 0.2 toothbrush 100.0 ? 0.0 100.0 ? 0.0 100.0 ? 0.0 99.7 ? 0.2 100.0 ? 0.0 94.9 ? 0.7 93.8 ? 1.1 91.7 ? 2.8 94.4 ? 0.9 95.3 ? 0.2 transistor 95.1 ? 0.2 -93.7 ? 1.5 91.2 ? 1.7 93.2 ? 0.8 88.0 ? 1.8 -83.8 ? 0.9 83.1 ? 2.2 86.0 ? 1.1 zipper 99.8 ? 0.1 100.0 ? 0.0 99.8 ? 0.3 98.9 ? 1.1 99.9 ? 0.1 94.2 ? 0.3 94.1 ? 0.2 94.0 ? 0.3 94.0 ? 0.3 94.9 ? 0.1 ? 2.8 87.4 ? 5.7 97.2 ? 1.3 95.5 ? 2.3 -95.8 ? 5.0 88.3 ? 4.5 98.0 ? 0.7 grid 99.9 ? 0.1 -100.0 ? 0.0 98.6 ? 0.8 100.0 ? 0.0 99.2 ? 0.1 -98.4 ? 0.7 92.5 ? 2.0 99.4 ? 0.0 leather 99.9 ? 0.1 -99.9 ? 0.1 100.0 ? 0.0 100.0 ? 0.0 99.5 ? 0.1 -99.3 ? 0.5 99.5 ? 0.1 99.7 ? 0.0 tile 100.0 ? 0.0 -100.0 ? 0.0 100.0 ? 0.0 99.9 ? 0.1 99.3 ? 0.0 -98.5 ? 0.4 95.6 ? 2.1 98.2 ? 0.4 wood 97.5 ? 1.5 -91.4 ? 4.3 91.4 ? 2.5 98.0 ? 0.3 90.7 ? 1.9 -86.5 ? 3.9 ?856 ? 2.0 92.4 ? 0.6 average 98.6 ? 0.3 -96.0 ? 0.7 95.5 ? 1.5 99.0 ? 0.3 96.8 ? 0.7 -95.7 ? 1.6 92.3 ? 1.1 97.5 ? 0.1 ? 0.5 91.4 ? 0.5 97.0 ? 0.2 96.3 ? 0.4 -94.1 ? 0.8 92.3 ? 0.5 95.7 ? 0.2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Image-level AUROC %</cell><cell></cell><cell></cell><cell cols="3">Pixel-level AUROC %</cell></row><row><cell cols="2">NSA (logistic) variants</cell><cell>final</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>final</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell></row><row><cell>object</cell><cell cols="2">bottle 97.7 average 96.5 ? 0.3</cell><cell>-</cell><cell cols="4">94.9 ? 0.6 89.4 ? 0.4 96.0 ? 0.2 96.0 ? 0.4</cell><cell>-</cell><cell cols="3">93.3 ? 0.9 92.3 ? 0.5 94.8 ? 0.3</cell></row><row><cell>texture</cell><cell cols="4">carpet 88.7 overall average 95.6 ? 0.6 -97.2 ? 0.3 -95.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>AU-PRO 0.3 % for MVTec AD defect localization and standard error across five different random seeds. Scores are calculated for 256 ? 256 resampled image and mask. Best scores between PaDiM-WR50-Rd550 [6] and NSA within standard error are bold-faced. Note that PaDiM uses pretrained ImageNet features. 73.9 89.8 ? 0.8 84.8 ? 2.8 90.4 ? 0.5 texture carpet 96.2 50.4 21.6 93.5 84.0 ? 11.8 71.1 ? 8.2 85.0 ? 6.2 grid 94.6 91.5 86.0 95.9 96.5 ? 0.1 94.2 ? 0.8 96.8 ? 0.4 leather 97.8 83.7 84.1 98.1 98.9 ? 0.1 98.6 ? 0.4 98.7 ? 0.1 tile 86.0 54.4 42.0 83.2 93.9 ? 0.9 90.3 ? 2.5 95.3 ? 0.5 wood 91.1 64.0 41.7 81.7 89.2 ? 2.4 86.1 ? 5.7 85.3 ? 3.7 average 93.2 68.8 55.1 90.5 92.5 ? 2.0 88.1 ? 1.3 92.2 ? 1.4 overall average 92.1 74.1 62.3 79.4 90.7 ? 0.4 85.9 ? 2.1 91.0 ? 0.6</figDesc><table><row><cell></cell><cell></cell><cell>SOTA</cell><cell></cell><cell cols="3">Our Experiments</cell></row><row><cell></cell><cell></cell><cell>PaDiM [6]</cell><cell>CutPaste (end-to-end)</cell><cell>FPI PII</cell><cell>NSA (binary)</cell><cell>NSA (continuous)</cell><cell>NSA (logistic)</cell></row><row><cell></cell><cell>bottle</cell><cell>94.8</cell><cell>91.2</cell><cell cols="3">66.0 79.0 93.0 ? 0.9 89.9 ? 1.1 92.9 ? 0.3</cell></row><row><cell></cell><cell>cable</cell><cell>88.8</cell><cell>59.8</cell><cell cols="3">51.9 55.7 87.6 ? 3.4 85.4 ? 2.1 89.9 ? 1.0</cell></row><row><cell></cell><cell>capsule</cell><cell>93.5</cell><cell>83.5</cell><cell cols="3">79.9 67.6 91.8 ? 0.8 79.9 ? 9.0 91.4 ? 2.2</cell></row><row><cell></cell><cell>hazelnut</cell><cell>92.6</cell><cell>81.3</cell><cell cols="3">71.4 90.9 93.6 ? 0.4 93.1 ? 1.3 93.6 ? 0.9</cell></row><row><cell>object</cell><cell>metal nut pill screw</cell><cell>85.6 92.7 94.4</cell><cell>54.4 83.1 72.6</cell><cell cols="3">72.2 91.5 94.9 ? 0.2 90.8 ? 1.1 94.6 ? 0.6 50.4 65.2 93.7 ? 0.9 92.5 ? 3.5 96.0 ? 0.5 69.8 78.4 90.6 ? 1.3 80.6 ? 10.3 90.1 ? 0.3</cell></row><row><cell></cell><cell cols="2">toothbrush 93.1</cell><cell>88.1</cell><cell cols="3">60.3 66.8 91.2 ? 0.6 89.0 ? 1.8 90.7 ? 1.0</cell></row><row><cell></cell><cell>transistor</cell><cell>84.5</cell><cell>68.5</cell><cell cols="3">55.4 57.4 72.6 ? 4.4 63.3 ? 1.2 75.3 ? 2.4</cell></row><row><cell></cell><cell>zipper</cell><cell>95.9</cell><cell>84.9</cell><cell cols="3">81.2 86.6 88.9 ? 0.5 83.6 ? 3.3 89.2 ? 0.3</cell></row><row><cell></cell><cell>average</cell><cell>91.6</cell><cell>76.7</cell><cell>65.8</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The mvtec anomaly detection dataset: A comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1038" to="1059" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The OpenCV Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Padim: A patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition. ICPR International Workshops and Challenges -Virtual Event</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12664</biblScope>
			<biblScope unit="page" from="475" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems. p. 9781-9791. NIPS&apos;18</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems. p. 9781-9791. NIPS&apos;18<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FFJORD: free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Gudovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishizaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kozuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1819" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Inattentional blindness. MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Anomaly detection through latent space restoration using vector quantized variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Marimont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tarroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1764" to="1767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>G?r?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised lesion detection in brain ct using bayesian convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Digby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Coles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>OpenReview</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2003 Papers</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
	<note>SIGGRAPH &apos;03</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transfusion: Understanding transfer learning for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling the distribution of normal data in pre-trained deep features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Milan, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6726" to="6733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards total recall in industrial anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="14318" to="14328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A unifying review of deep and shallow anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<meeting>the 35th International Conference on Machine Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">f-anogan: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting outliers with foreign patch interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning for Biomedical Imaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting outliers with poisson image interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2021</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="581" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<meeting>the 36th International Conference on Machine Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automated abnormality classification of chest radiographs using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Redd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Brandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image anomaly detection using normal data only by latent space resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">23</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Patch svdd: Patch-level svdd for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/2111.07677</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Draem-a discriminatively trained reconstruction embedding for surface anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Skocaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8330" to="8339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Understanding failures in out-ofdistribution detection with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<idno>PMLR (18-24</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>Meila, M., Zhang, T.</editor>
		<meeting>the 38th International Conference on Machine Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12427" to="12436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly localization using variational auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zimmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2019 -22nd International Conference</title>
		<meeting><address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
