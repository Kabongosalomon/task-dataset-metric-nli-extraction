<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeVIS: Making Deformable Transformers Work for Video Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Caelles</surname></persName>
							<email>adria.caelles@estudiantat.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Catalonia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
							<email>tim.meinhardt@tum.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Bras?</surname></persName>
							<email>guillem.braso@tum.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
							<email>leal.taixe@tum.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeVIS: Making Deformable Transformers Work for Video Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>video instance segmentation, deformable transformers</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video Instance Segmentation (VIS) jointly tackles multiobject detection, tracking, and segmentation in video sequences. In the past, VIS methods mirrored the fragmentation of these subtasks in their architectural design, hence missing out on a joint solution. Transformers recently allowed to cast the entire VIS task as a single set-prediction problem. Nevertheless, the quadratic complexity of existing Transformerbased methods requires long training times, high memory requirements, and processing of low-single-scale feature maps. Deformable attention provides a more efficient alternative but its application to the temporal domain or the segmentation task have not yet been explored. In this work, we present Deformable VIS (DeVIS), a VIS method which capitalizes on the efficiency and performance of deformable Transformers. To reason about all VIS subtasks jointly over multiple frames, we present temporal multi-scale deformable attention with instance-aware object queries. We further introduce a new image and video instance mask head with multi-scale features, and perform near-online video processing with multi-cue clip tracking. DeVIS reduces memory as well as training time requirements, and achieves state-of-the-art results on the YouTube-VIS 2021, as well as the challenging OVIS dataset. Code is available at https://github.com/acaelles97/DeVIS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video Instance Segmentation (VIS) simultaneously aims to detect, segment, and track multiple classes and object instances in a given video sequence. Thereby, VIS provide rich information for scene understanding in applications such as autonomous driving, robotics, and augmented reality.</p><p>Early methods <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">25]</ref> took inspiration from the multi-object tracking (MOT) field by applying tracking-by-detection methods to VIS, e.g., <ref type="bibr" target="#b25">[24]</ref> extends Mask R-CNN <ref type="bibr" target="#b9">[9]</ref> with an additional tracking head. The frame-by-frame mask prediction and track association allow for real-time processing but fail to capitalize on temporal consistencies in video data. Hence, more recent VIS methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17]</ref> moved towards an offline or near-online processing of clips by treating instance segmentations as 3D spatio-temporal volumes.</p><p>The recent success of Transformers <ref type="bibr" target="#b20">[20]</ref> in object recognition inspired a new generation of VIS approaches. Both VisTR <ref type="bibr" target="#b21">[21]</ref> and IFC <ref type="bibr" target="#b11">[11]</ref> deploy an encoderdecoder Transformer architecture, and closely mirror DETR's <ref type="bibr" target="#b4">[5]</ref> approach to object detection by formulating the VIS task as a set-prediction problem. In this paradigm, instance masks are obtained in a single end-to-end trainable forward pass for all frames in a clip. While their formulation is simple and appealing, both methods are limited by the quadratic complexity of full attention. VisTR achieves communication between frames by concatenating and encoding the pixels of all frames jointly. Such a multi-frame processing only amplifies the expensive attention computation and makes <ref type="bibr" target="#b21">[21]</ref> suffer from long training times and high memory requirements. IFC <ref type="bibr" target="#b11">[11]</ref> tries to mitigate these issues by introducing inter-frame memory tokens to encode each frame individually. However, <ref type="bibr" target="#b11">[11]</ref> still relies on full attention for single frames and hence inherits the limitations of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">21]</ref> to only process low-and single-scale feature maps.</p><p>Deformable attention <ref type="bibr" target="#b28">[27]</ref> resolves many of DETR's efficiency and performance issues, and circumvents the quadratic computational complexity for its inputs by subsampling attention keys around a spatial reference point assigned to each query. As shown in our experiments, a naive deformabilization of VisTR, i.e., replacing full attention with deformable attention in the encoder-decoder of <ref type="figure" target="#fig_3">Figure 1</ref>, does not achieve satisfactory train time nor segmentation performance. This is largely due to two problems: (i) learnable reference point offsets and attention weights introduce an unfeasible amount of new parameters for long clip sizes, and (ii) attention with spatially local reference points is not well-suited for the detection and tracking of objects moving through a sequence.</p><p>To make deformable attention work for VIS, we present Deformable VIS (DeVIS), a Transformer encoder-decoder which applies temporal deformable attention over multiple frames. The reduction in computational complexity reduces training time and makes the processing of high-res feature maps at multiple scales feasible. Furthermore, we motivate the alignment of reference points for decoder object queries individually for each object instance. Our newly proposed image and video instance segmentation head takes full advantage of multi-scale features and improves mask quality significantly. To run sequences with arbitrary lengths, we also introduce an improved multi-cue clip tracking. The presented DeVIS method achieves state-of-the-art results on the challenging YouTube-VIS <ref type="bibr" target="#b25">[24]</ref> 2021 and OVIS <ref type="bibr" target="#b19">[19]</ref> datasets and substantially reduces training time with respect to VisTR.</p><p>In summary, our key contributions are:</p><p>-We present Deformable VIS (DeVIS), a VIS method which introduces temporal multi-scale deformable attention and instance-aware object queries. -We present a new instance mask prediction head which takes full advantage of deformable attention and encoded multi-scale feature maps. -Our improved multi-cue clip tracking incorporates mask and class information to connect overlapping clips to sequences of arbitrary length. -Our method provides efficient training and achieves state-of-the-art performance on YouTube-VIS 2021 and the challenging OVIS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>We discuss VIS methods following the progression from tracking-by-detection to clip-level approaches culminating in modern Transformer-based architectures.</p><p>Tracking-by-detection. The inception of the VIS task with the YouTube-VIS 2019 <ref type="bibr" target="#b25">[24]</ref> dataset also created Mask-Track R-CNN <ref type="bibr" target="#b25">[24]</ref>. As it is common in the multi-object-tracking community, Mask-Track R-CNN processes sequences frame-by-frame in an online tracking-by-detection manner. To this end, <ref type="bibr" target="#b25">[24]</ref> extends Mask R-CNN <ref type="bibr" target="#b9">[9]</ref> with a tracking branch which allows it to not only detect and segment objects, but also assign instance identities via similarity matching of instance embeddings in the current frame and a memory queue. Sip-Mask <ref type="bibr" target="#b3">[4]</ref> applies the same tracking head but with a single-stage detector and light-weight spatial mask preservation module. The crossover learning scheme of CrossVIS <ref type="bibr" target="#b26">[25]</ref> allows for a localization of pixel instance features in other frames. As a clip-level method, DeVIS processes sequences in clips, which greatly improves quality and robustness of instance mask as well as identity predictions. Clip-level. Clip-level processing allows for offline or near-online VIS methods. The latter clip a sequence into multiple parts and hence rely on an additional clip tracking step. The STEm-Seg <ref type="bibr" target="#b1">[2]</ref> method took inspiration from offline trackers and is the first to model object instances as 3D spatio-temporal volumes by predicting pixel embeddings with Gaussian variances. For a hybrid tracking-bydetection and clip-level method, the authors of MaskProp <ref type="bibr" target="#b2">[3]</ref> extend Mask R-CNN with a mask propagation branch that operates between all frames in a video clip. The Produce-Reduce heuristic applied in SeqMask-RCNN <ref type="bibr" target="#b14">[14]</ref> generates object instance proposals and reduces redundant identities based on multiple key frames. STMask <ref type="bibr" target="#b13">[13]</ref> and SG-Net <ref type="bibr" target="#b17">[17]</ref>, on the other hand, move beyond Mask R-CNN by applying improved one-stage detection methods.</p><p>Albeit their early success, these clip-level methods usually tackle the detect, segment, and track subtasks with separately trainable multi-stage pipelines. Our DeVIS approach enjoys the advantages of clip-level methods but in a unified and end-to-end trainable manner through the application of Transformers. Clip-level with Transformers. The VisTR <ref type="bibr" target="#b21">[21]</ref> method introduced Transformers <ref type="bibr" target="#b20">[20]</ref> to VIS by extending the DETR <ref type="bibr" target="#b4">[5]</ref> object detector to the temporal domain. Its unified Transformer encoder-decoder architecture concatenates and encodes all frames in a clip by computing attention between all pixels. The decoder reasons about detection and tracking via multi-frame cross-attention between object queries and the encoded pixels, and produces instance masks with a subsequent segmentation head. To avoid the expensive computation multi-frame pixel attention, the authors of <ref type="bibr" target="#b11">[11]</ref> encode each frame separately and introduce memory tokens for a high-level inter-frame communication.</p><p>Our proposed DeVIS method mitigates the aforementioned efficiency issues while still benefiting from the simultaneous encoding of multiple frames at once through the application of temporal multi-scale deformable attention with instance-aware reference point sampling. We further propose a new segmentation head which takes full advantage of the Transformer encoded multi-scale feature maps, and an improved multi-cue clip-tracking. <ref type="figure" target="#fig_3">Fig. 1</ref>. An overview of our DeVIS method which applies temporal multi-scale deformable attention in a Transformer encoder-decoder architecture. The encoder computes deformable attention between pixels across scales and frames in a given clip without suffering from quadratic complexity of full attention. In the decoder, object queries attend to multiple frames, thereby providing consistent identity predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DeVIS</head><p>In this section, we present Deformable VIS (DeVIS), a near-online end-to-end trainable Transformer encoder-decoder architecture for VIS. We details its key components: (i) Temporal multi-scale deformable attention on a (ii) Transformer architecture with instance-aware object queries, (iii) a new multi-scale deformable mask head, and (iv) multi-cue clip tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Clip-level VIS with Transformers</head><p>In this section, we present an overview of our DeVIS method, shown in <ref type="figure" target="#fig_3">Figure 1</ref>, which follows the general Transformer-based clip processing pipeline of <ref type="bibr" target="#b21">[21]</ref>. Clip-level VIS. Given a video with T frames, the goal of VIS is to detect, track, and segment all K objects in a sequence. This is achieved by providing a set of instance predictions</p><formula xml:id="formula_0">? = {y i,t } with y i,t = {c i,t , b i,t , m i,t }.</formula><p>A single prediction consists of the class c, bounding box b, and mask m for instance identity i at frame t. The VIS task expects a constant c i,t for all t. If an object instance with identity j is not present for the entire sequence, the final set ? can include less than T predictions with y j,t . A clip-level VIS method processes clips with ? frames and first provides subsets of instance predictions ? k with distinct sets of identities. Usually, clips include overlapping frames to perform a final clip tracking/stitching step, which is responsible for merging identities of overlapping clip instance predictions.</p><p>VIS with Transformers. To generate a set of clip predictions ? k , Transformer encoder-decoder methods first extract feature maps for each frame independently with a convolutional neural network (CNN). Feature maps are then concatenated to form a clip and temporal-positional encoding is added. Treating each pixel as an input query, the subsequent Transformer encoder shares information spatially and temporally between frames via self-attention <ref type="bibr" target="#b20">[20]</ref>. A set of learned object queries <ref type="bibr" target="#b4">[5]</ref> computes self-attention and cross-attention with all encoded pixels in the Transformer decoder. The total amount of object queries is equally distributed over the frames, hence, a query attends to all pixels in the clip but is responsible for the predictions on a fixed frame. The decoder outputs a set of object query embeddings which are passed through separate multi-layer perceptrons to predict c i,t and b i,t for each frame in the clip. The instance mask predictions m i,t are obtained by computing attention maps for each object embedding, and feeding these together with the backbone feature maps into an additional instance mask head. The entire model is trained end-to-end by matching the predicted outputs y i,t via a Hungarian cost matrix to the ground truth. We refer to <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b21">[21]</ref> for more details on the matching and loss computation. Instance identities i are predicted by matching a fixed set of queries each from a different frame to the same identity during training. At inference, all objects detected and segmented by one of these query sets are then assumed to belong to the same identity. Intuitively, object queries in DETR learn to detect objects in certain regions of the image. For VIS, each set of queries is responsible for certain types of spatio-temporal object trajectories through the clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Roadmap to temporal deformable attention</head><p>The authors of <ref type="bibr" target="#b28">[27]</ref> introduced deformable attention to DETR, thereby reducing the computational footprint and training time substantially. This allowed <ref type="bibr" target="#b28">[27]</ref> to improve single-image object detection performance by running the Transformer encoder-decoder on multiple feature scales. While they only operated on single images, we present deformable attention for the temporal domain which simultaneously captures spatial and temporal dependencies across multiple scales.</p><p>Multi-Head Attention. The original Transformer <ref type="bibr" target="#b20">[20]</ref> applies full attention between two sets of input queries ? q and keys ? k . An element of each of these sets is denoted by k and q with feature representations z q ? R C and x k ? R C of hidden size C, respectively. We denote the set of all x k as X. The Multi-Head Attention (MHA) for query q and M attention heads is then computed via:</p><formula xml:id="formula_1">MHA(z q , X) = M m=1 W m k?? k A mqk ? W m x k ,<label>(1)</label></formula><p>with learnable weight matrices W m ? R Cv?C and W m ? R C?Cv where C v = C/M . The attention weights A mqk are computed via dot product between z q and x k and are normalized over all keys k?? k A mqk = 1. The case where ? q = ? k is usually referred to as self-attention. The Transformer encoder in DETR <ref type="bibr" target="#b4">[5]</ref> applies self-attention, where every entry in the feature map corresponds to a query. Due to the computation of A mqk , which scales quadratically with the number of queries/keys, it is only feasible to run <ref type="bibr" target="#b4">[5]</ref> on a single feature scale.</p><p>Deformable attention. To mitigate DETR's computational issues around attention, the authors of <ref type="bibr" target="#b28">[27]</ref> suggest deformable attention which works on subsets of queries and weight computation of linear complexity. To this end, each query q is assigned a reference point p q ? R 2 in the feature map domain. A subset of K queries is sampled around the reference point based on sample offsets ?p mqk which are learnable via linear projection over the query feature z q . For simplicity we omit the summation over multiple attention heads and denote the resulting Deformable Attention (DA) for a single attention head m as:</p><formula xml:id="formula_2">DA(z q , p q , X) = W m K k=1 A mqk ? W m x(p q + ?p mqk ) .<label>(2)</label></formula><p>The attention weights are normalized over the sample points k?? k A mqk = 1 and also obtained via linear projection which avoids the expensive computation of dot product between queries. Furthermore, <ref type="bibr" target="#b28">[27]</ref> present a multi-scale version of Equation 2 which computes deformable attention across feature maps.</p><p>Temporal multi-scale deformable attention. To encode spatio-temporal dependencies, which are crucial for for VIS, we present multi-scale deformable attention for the temporal domain. That is, the sets of queries and keys include pixels from multiple scales and all frames in a clip. Hence, we re-define X = {X l } L l=1 to be the stack of L multi-scale backbone features with ? frames where X l ? R C?? ?H l ?W l . Intuitively, a query q from frame t has the ability to compute attention with sampled keys from all frames and feature levels in a clip. In comparison to full attention between all pixels, the sampling with offsets around a query's reference pointp q reduces the computational effort substantially. The number of keys is independent of the input resolution and only scales linearly with the number of feature scales and frames. We define Temporal Multi-Scale Deformable Attention (TMSDA) module for a single m over a clip as:</p><formula xml:id="formula_3">TMSDA(z q ,p q , X) = W m ? t=1 L l=1 K(t) k=1 A mtlqk ? W m x lt (? l (p q ) + ?p mtlqk ) . (3)</formula><p>As in <ref type="bibr" target="#b28">[27]</ref>, each reference point is represented with normalized coordinate? p q ? [0, 1] 2 and re-scaled by ? l to allow for a sampling across feature maps l with different resolutions. The scalar attention weight A mtlqk is normalized by ? t=1 L l=1 K(t) k=1 A mtlqk = 1. We introduce K(t) which adapts the number of keys sampled from a given frame and present two scenarios depending on whether a query z q samples keys from its corresponding or other temporal frames:</p><formula xml:id="formula_4">K(t) = K curr if z q ? Z lt K temp else.<label>(4)</label></formula><p>Adding the temporal dimension allows each query to simultaneously sample keys from all feature scales as well as its current, and temporal frames. Such a design is particularly beneficial for a consistent detection and identity prediction of objects moving and changing size over the sequence. K temp = 0 removes all temporal connections and reverts back to the original deformable attention from <ref type="bibr" target="#b28">[27]</ref>. In the following paragraphs, we give further details on how our temporal deformable attention is applied in our Transformer encoder-decoder architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TMSDA for VIS Transformers</head><p>Following the formal introduction of Temporal Multi-Scale Deformable Attention (TMSDA), we detail its integration into our Transformer encoder-decoder and present a novel design of instance-aware reference points for object queries.</p><p>Transformer encoder. We replace the common full self-attention between all pixels in the encoder with TMSDA as in Equation <ref type="bibr" target="#b2">3</ref>. The sampling design of deformable attention allows each pixel to only connect to a subset of other pixels close to spatial location of the reference points p q for the current and other frames. The amount of temporal information being considered is controlled by the clip size ? and the number of temporal sampling points K temp . Computing deformable attention allows us to encode all L = 4 ResNet [10] backbone feature scales, thereby replacing the role of a FPN <ref type="bibr" target="#b15">[15]</ref>. We apply an additive encoding to each pixel which indicates it spatial, temporal, and feature scale position. The spatial locality of sampled keys limits the available temporal information potentially required for long clips with large object motions. We believe our deformable approach is superior to <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b11">11]</ref> as it allows for a more fine-grained inter-frame communication on multiple scales in a unified formulation.</p><p>Transformer decoder. Our Transformer decoder computes two attention steps for its object queries: self-attention and cross-attention between queries and the encoded frame features. For single-image detection <ref type="bibr" target="#b4">[5]</ref>, the decoder self-attention helps to avoid duplicate object detections. But for VIS Transformers, object queries also need to communicate about instance identities. As explained in Section 3.1, a subset of object queries are assigned to each frame of the clip. To extract object information with an object query from its corresponding and future as well as past frames, we compute temporal deformable multi-scale crossattention. In contrast to the Transformer encoder, the reference points of object queries required for the cross-attention are learnable. Each object query can leverage not only meaningful, local information from its particular object on its assigned frame, but from each of the other frames of the input clip. This helps improve consistent mask and identity predictions over the sequence. Furthermore, we apply the same bounding box refinement as <ref type="bibr" target="#b28">[27]</ref> at each layer of the decoder. This allows the initial reference point, i.e., sampling area, of a query to be adapted to the currently assumed coordinates of the object bounding box.</p><p>Instance-aware object queries. Each object query of the decoder is by design able to learn different sampling reference points for different frames in the clip. This not only allows to distinguish between a query's assigned frame and other frames, but to adjust the reference points on other frames according to its instance identity. Hence, we introduce Instance-aware object queries which exploit the identity consistency across queries to adapt their reference points on other frames to the predicted bounding boxes belonging to their respective object identity. This instance-aware reference point sampling is applied before each Transformer decoder layer, see <ref type="figure" target="#fig_3">Figure 1</ref>, and works in conjunction with the bounding box refinement of the reference point on the query frame. If an object query successfully predicts the object bounding box on its own frame t, other object queries belonging to the same instance but from other frames will benefit from an improved sampling on frame t. Instance-aware object queries provide an additional communication between object queries of different frames and improve track consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-scale deformable mask head</head><p>Processing multi-scale features currently only benefits the detection and tracking performance of the Transformer encoder-decoder. We further explore the potential of Transformer encoded feature maps for mask prediction, and present a new multi-scale deformable instance mask segmentation head applicable to both image and video segmentation.</p><p>In <ref type="figure" target="#fig_1">Figure 3</ref>, we present an overview of its architecture. Our encoder-decoder architecture outputs encoded backbone feature maps and object embeddings per clip. As shown in <ref type="figure" target="#fig_3">Figure 1</ref>, the mask head predicts binary instance masks by generating attention maps for each of the embeddings and applying a series of convolutions and upsampling operations. Attention maps are generated by computing Multi-Head Attention (MHA), as in Equation 1, between an embedding an the encoded feature maps of its corresponding frame in the clip. At different scales of the upsampling process, corresponding backbone feature maps are added to improve segmentation performance. The mask head predicts all instance masks in a clip at once and does not consider any explicit temporal information. In particular, each query only computes attention maps for its own frame as opposed to the deformable cross-attention computation in the decoder.</p><p>To take full advantage of the encoded multi-scale features, we propose the following design changes in order to improve training time and segmentation performance for VIS and image instance segmentation: Training only positive matches. During the training of DETR, the detection loss is computed via Hungarian matching between the ground truth and the object embeddings produced by the decoder. We reduce the mask head training time by only computing masks and their corresponding losses embeddings which receive a positive matching with a ground truth object. Since object queries are designed to exceed the number of objects per frame by a large margin, this results in a substantial reduction of training time. Encoded feature maps. The deformable attention allows us to encode all but the largest (H/4 x W/4) backbone feature maps. While <ref type="bibr" target="#b4">[5]</ref> only encodes the lowest (H/32 x W/32) feature map and hence must use raw backbone features in its mask head, we are able to add and upsample multiple Transformer-encoded feature maps as visualized in <ref type="figure" target="#fig_1">Figure 3</ref>. The application of encoded feature maps allows for a direct connection between the mask head and Transformer encoder. Multi-scale attention maps. To obtain informative attention maps, the object embeddings must compute MHA with the feature maps in the preceding Transformer encoder-decoder. Therefore, we are able to generate attention maps not only for a single but multiple scales and concatenate these at the corresponding stages of the upsampling process. The original mask head only generates attention maps for the smallest scale (H/32 x W/32) and is not able to benefit from the additional connections to the object embeddings, i.e., the Transformer decoder, during the upsampling process. It should be noted, that although our Transformer encoder-decoder computes deformable attention as in <ref type="bibr">Equation 3</ref>, the attention maps are generated via regular attention as in Equation 1.</p><p>MDC. Furthermore, we replace the convolutions in the mask head with Modulated Deformable Convolutions (MDC) <ref type="bibr" target="#b27">[26]</ref>. This not only boosts performance and convergence time but presents a more unified deformable approach. End-to-end training. The additional connections of our mask head in to the preceding encoder-decoder blocks, see colored lines in <ref type="figure" target="#fig_1">Figure 3</ref>, result in a significant performance boost for an end-to-end training of the full model, i.e., including the backbone and encoder-decoder. Without these connections, the authors of <ref type="bibr" target="#b4">[5]</ref> did not observe any improvement for a similar end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multi-cue clip tracking</head><p>For DeVIS to run on sequences with arbitrary length, we apply a near-online clip tracking/stitching similar to <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. To this end, we sequentially match instance identities from consecutive and overlapping clips. The final set of tack predictions ? is computed by matching the current ? with the next ? k via the Hungarian algorithm <ref type="bibr" target="#b12">[12]</ref>. Instances in ? k without any match in ? start a new instance identity. For tracking-by-detection/online methods it is common to perform the data association with multiple cues <ref type="bibr" target="#b25">[24]</ref>. We are the first to extend this idea to clip-level tracking and compute multiple cost terms for identities i and j in ? and ? k , respectively:</p><p>-Mask cost via negative volumetric soft IoU <ref type="bibr" target="#b11">[11]</ref> between consecutive overlapping sets of masks m i,t and m j,t . The contribution of each cue is controlled by their corresponding weights ? mask , ? class and ? score . In cases of strong occlusion and imperfect mask predictions, the identity matching can additionally rely on consistent class and score information which further improves the identity preservation across a sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section provides the most relevant details of our implementation, and the experimental setup for the following ablation studies and benchmark evaluations. If not otherwise specified all results in the paper are obtained with a ResNet-50 <ref type="bibr" target="#b10">[10]</ref> backbone and follow the hyperparameters of <ref type="bibr" target="#b28">[27]</ref>. For additional implementation and training details we refer to the appendix. Multi-scale deformable mask head. We train our mask head jointly with a pre-trained Deformable DETR <ref type="bibr" target="#b28">[27]</ref> on COCO <ref type="bibr" target="#b16">[16]</ref> for 24 epochs, decaying the learning by 0.1 after epoch 15. For batch size 2, we use 8 GPUs with 32GB memory for 2 days (345 GPU hours). The initial learning rates of the backbone, encoder-decoder, and mask head are 1e ?5 , 2e ?5 , and 2e ?4 , respectively. DeVIS. We initialize our DeVIS model from the preceding end-to-end instance mask head training and then fine-tune for additional 10 epochs on the respective VIS dataset. With one clip per GPU, we use 4 GPUs for 1.5 days (120 GPU hours) with 18GB memory for YouTube-VIS 2019 <ref type="bibr" target="#b25">[24]</ref> dataset. The increased number of objects per frame in both YouTube-VIS 2021 <ref type="bibr" target="#b25">[24]</ref> and OVIS <ref type="bibr" target="#b19">[19]</ref> require 24GB of memory. In contrast to <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b11">11]</ref>, we are able to train with data augmentations on different input scales and apply a learned additive temporal encoding. If not otherwise specified, all models use clip size ? = 6, stride S = 4 and reference point keys K curr = K temp = 4. We run multi-cue clip tracking with ? mask = ? class = ? score = 1. For the ablations, we report best validation scores after training 10 epochs. Datasets and metrics. We evaluate results on the YouTube-VIS 2019/2021 <ref type="bibr" target="#b25">[24]</ref> datasets which contain 2883 and 3859 high quality videos with 40 unique object categories, respectively. The latter provides improved ground truth annotations. The OVIS <ref type="bibr" target="#b19">[19]</ref> dataset includes severe occlusion scenarios on 901 sequences. We measure the Average Precision (AP) as well as Average Recall (AR). For instance mask prediction on images, we report the AP based on mask IoU on the common COCO <ref type="bibr" target="#b16">[16]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation studies</head><p>We present ablations demonstrating the effectiveness of our contributions for DeVIS and provide detailed insights on the new mask head and clip tracking. Making DeVIS work for VIS. In <ref type="table" target="#tab_0">Table 1</ref>, we demonstrate the shortcomings of a naive deformabilization, i.e., replacement of VisTR's <ref type="bibr" target="#b21">[21]</ref> full attention with deformable attention. We indicate full attention over all pixels wit K temp = All. Running temporal deformable attention offline, i.e., with clip size ? = 36, reduces training time compared to VisTR, but converges with an unsatisfactory performance of 34.2. This is due to the incapability of spatially restricted reference points to compute meaningful temporal attention connections over large clip sizes. This assumption is supported by the 1.1 improvement of an offline Deformable VisTR without any temporal connections, i.e., K temp = 0. A reduction of the clip size to ? = 6 in a near-online fashion mitigates the reference point issues while also requiring only a fraction of the original training time (155 vs. 48 GPU hours). To further support our hypothesis, we demonstrate how removing temporal connections K temp = 0 for smaller clip sizes ? = 6 does indeed deteriorate the performance. However, without fully capitalizing on the efficiency of deformable attention its best version with 34.0 is still inferior to VisTR. The first DeVIS row demonstrates the potential gains (1.9) from increasing the number of feature scales to L = 4 and training on higher input resolutions. The transition to multiple scales is a prerequisite for the application of our new mask head and only feasible to train on smaller clip sizes. The same model with full attention (second row) results in out-of-memory (OOM) errors. Naturally, this increases the total number of parameters and training time but both remain far below VisTR. Instance-aware object queries come with a neglectable increase in training time but provide a 1.1 AP boost. The individual contributions of the mask head and clip tracking additions are ablated in <ref type="table" target="#tab_1">Table 2</ref> and 5, respectively. Both result in additional performance boosts without substantially increasing the computational costs. A cascaded weighting of the decoder auxiliary loss terms as applied in <ref type="bibr" target="#b0">[1]</ref> increases results further. Our final model benefits from deformable attention with low training times and parameter counts surpassing a naive Deformable VisTR approach by 11.1 points.</p><p>In <ref type="table" target="#tab_7">Table 3</ref>, we ablate different clip sizes ? and number of temporal sampling keys K temp for our final configuration. We used the optimal clip size ? = 6 for our DeVIS ablations in <ref type="table" target="#tab_0">Table 1</ref>. Both smaller and larger clip sizes resulted in worse performance either due to the lack of temporal connections or the aforementioned problem of spatially local reference points in the encoder, respectively. Running all L = 4 feature scales was only possible for a clip length of up to ? = 12. Furthermore, we ablate the removal of temporal connections which resulted in a large relative drop for the challenging OVIS <ref type="bibr" target="#b19">[19]</ref> dataset. Multi-scale mask head. We evaluate our contributions on the mask head in <ref type="table" target="#tab_1">Table 2</ref> on COCO <ref type="bibr" target="#b16">[16]</ref> instance segmentation. The baseline represents a straightforward application of the original DETR <ref type="bibr" target="#b4">[5]</ref> mask head with the De-  <ref type="table" target="#tab_7">Table 3</ref>. Removing temporal connections with Ktemp = 0 results in performance drops across all datasets. We observe an optimal clip size of ? = 6.   formable DETR <ref type="bibr" target="#b28">[27]</ref> detector, which is not only 9.6 points worse than <ref type="bibr" target="#b4">[5]</ref> (see <ref type="table" target="#tab_3">Table 4</ref>), but suffers from an unfeasible long training time largely due to the increased number of object queries in <ref type="bibr" target="#b28">[27]</ref>. By computing instance masks only for queries positively matched with a ground truth object, we are able to reduce the training time 4-fold. The following two additions take full advantage of the encoded multi-scale features of <ref type="bibr" target="#b28">[27]</ref> and result in a mask AP of 29.2. For top performance, we further add MDC <ref type="bibr" target="#b27">[26]</ref> and train the entire model end-to-end. Our mask head without end-to-end training is still inferior to DETR. This can be attributed to the sparse computation of deformable attention which makes the generated attention maps less suitable for pixel-level segmentation. However, the end-to-end training fully realizes the potential of the additional connections between our mask head and the encoder-decoder. The increased training time is justified by the overall 4.7 point improvement over DETR. Multi-cue clip tracking. To improve the track consistency between clips, we introduce additional cues to the common mask-based clip tracking. The clip tracking row in <ref type="table" target="#tab_0">Table 1</ref> replaces the original VisTR mask IoU cost term with a combination of volumetric soft mask IoU, class, and score costs. After tuning the cost weighting parameters, we ablate their individual contributions in <ref type="table" target="#tab_4">Table 5</ref>. The new class and score terms provide an overall boost of 2.3 AP points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmark evaluation</head><p>Video instance segmentation. To demonstrate the effectiveness of DeVIS in comparison with other VIS methods, we report results on the YouTube-VIS 2019 and 2021 <ref type="bibr" target="#b25">[24]</ref> dataset in <ref type="table" target="#tab_7">Table 6</ref>. Our method achieves state-of-the-art performance with respect to all previous methods on the YouTube-VIS 2021 dataset by <ref type="table" target="#tab_7">Table 6</ref>. Comparison of VIS methods on the YouTube-VIS 2019/2021 <ref type="bibr" target="#b25">[24]</ref> validation sets. FPS measurements denoted with * are extracted from <ref type="bibr" target="#b8">[8]</ref>. With * * and ? the we denote a joint training with COCO <ref type="bibr" target="#b16">[16]</ref> and unpublished methods, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone YT-VIS 19 <ref type="bibr" target="#b25">[24]</ref> YT-VIS 21 <ref type="bibr" target="#b25">[24]</ref> OVIS <ref type="bibr" target="#b19">[19]</ref> FPS AP AP50 AP75 AR1 AR10 AP AP50 AP75 AR1 AR10 AP AP50 AP75</p><p>Online significant margins of 2.5 and 1.8 for ResNet-50 <ref type="bibr" target="#b10">[10]</ref> and Swin-L <ref type="bibr" target="#b18">[18]</ref> backbones, respectively. The benefits of our temporal connections and multi-scale feature encoding in conjunction with the mask head are most apparent by our 7.9 improvement over IFC <ref type="bibr" target="#b11">[11]</ref>. Both VisTR and IFC achieve lower runtimes mirroring the relation between Deformable DETR <ref type="bibr" target="#b28">[27]</ref> and DETR <ref type="bibr" target="#b4">[5]</ref>. Furthermore, both methods rely on an expensively pretrained DETR limiting their adaptability dramatically. In addition to YouTube-VIS, we are the first Transformer-based method to present results on the OVIS <ref type="bibr" target="#b19">[19]</ref> dataset. We surpass the previous best method <ref type="bibr" target="#b26">[25]</ref> by 5.7 points for ResNet-50. Due to its pixel-level encoding of image features, DeVIS excels on OVIS' challenging occlusions scenarios. Image instance segmentation. The new multi-scale mask head does not only boost VIS performance but also excels on image segmentation. In <ref type="table" target="#tab_3">Table 4</ref>, we evaluate performance on COCO <ref type="bibr" target="#b16">[16]</ref> even surpassing Mask R-CNN <ref type="bibr" target="#b9">[9]</ref>. We present this as contribution beyond the VIS community and regard our mask head as a valuable completion of Deformable DETR. Interestingly, DeVIS is superior to Mask2Former despite the inferiority of our mask head for image segmentation. We regard this as a strong sentiment for our DeVIS video approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a novel VIS method which applies a Transformer encoderdecoder architecture to clips of frames in a near-online fashion. To mitigate the efficiency issues of previous Transformer-based methods suffering from quadratic input complexity, we propose temporal deformable attention with instance-aware object queries. Deformable attention allows us to benefit from multi-scale feature maps and led to the introduction of a new powerful instance mask head. Furthermore, we present multi-cue tracking with class and score terms. Our DeVIS method achieves state-of-the-art results on two VIS datasets and hopefully paves the way for future applications of deformable attention for VIS.</p><p>This section provides additional material for the main paper: ?A contains further implementation details for our DeVIS method and its training. In ?B, we discuss further ablations on the effect of the clip size and temporal sampling points. Furthermore, we complement the qualitative results of the main paper with selected illustrations in ( ?C). These include qualitative results, more detailed attention maps and failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation details</head><p>Multi-scale mask head To improve convergence of the end-to-end full model training, we increase the dice and mask loss weights to ? DICE = ? M ASK = 8. Furthermore, we add both loss terms to the auxiliary losses of the 3 rd decoder layer. For trainings of the new mask head with DeVIS, we keep the original mask loss weights but also add the corresponding terms to the 3 rd auxiliary loss. To further speed up the inference, we reduce the top k from 100 as in <ref type="bibr" target="#b28">[27]</ref> to 50.</p><p>DeVIS We train our model with a total of 60 object queries for YouTube-VIS 2019 and 180 object queries for YouTube-VIS 2021 and OVIS. With a clip size of ? = 6 this assigns 10 or 30 queries to each frame. OVIS includes sequences with up to 44 unique instances and hence requires an increased number of object queries. The class head is solely responsible for categorization of each object query. This means, the mask head predicts a single mask for each object query and does not produce per-category outputs as Mask R-CNN <ref type="bibr" target="#b9">[9]</ref>. In order to associate a single class with a trajectory of queries, we compute the mean score over each class. This results in a total of number of classes times number of object queries per frame trajectories. The final output is selected in a top-k manner from the total set of trajectories. It should be noted, if k is larger than the number of queries per frame, a single trajectory is associated with multiple labels. To work as similar as possible to VisTR <ref type="bibr" target="#b21">[21]</ref>, our ablation experiments apply k = 10. For our benchmark experiments, we increase the value to k = 20 and k = 30 for YouTube-VIS 2019 and YouTube-VIS 2019/OVIS, respectively. For the auxiliary loss weighting <ref type="bibr" target="#b0">[1]</ref>, we use the following incremental weights from the first to the final layer: 1/2, 5/30, 4/30, 3/30, 2/30 and 1/30. Such a weighting decreases the influence of early decoder layers with an emphasized focus on optimizing the outputs of the final layer. In contrast to <ref type="bibr" target="#b0">[1]</ref>, we keep the same weighting during the entire training. However, the weighting is not applied to the mask auxiliary loss on the third layer. As the total contribution from nonmask losses is heavily reduced, we use ? DICE = ? M ASK = 1, since we did not observe any benefit in this case from increasing its contribution. Furthermore, we use ? CLASS = 1 weight for the class cost and matching, following <ref type="bibr" target="#b21">[21]</ref>. In comparison to <ref type="bibr" target="#b28">[27]</ref>, our encoder-decoder model introduces only changes to the dimensions of a few parameters, namely, object queries and linear projections for sample offset and attention weight. We train the object queries, the introduced temporal learned embedding and the classification head from scratch. For the linear projections, we duplicate the existing pre-trained weights for each new temporal position, adapted to the number of points K temp . We apply initial learning rates of 1e ?5 and 1e ?4 for the backbone, and rest of the model including the mask head, respectively. We drop these by 0.1 at epoch 3 and 7 for YouTube-VIS 2019, 4 and 8 for YouTube-VIS 2021 and 6 and 10 for OVIS. Learnable sample offset and attention weight parameters To circumvent the quadratic input complexity of regular attention, deformable attention <ref type="bibr" target="#b28">[27]</ref> learns linear projections which infer the sample offsets ?p mqk and attention weights A mqk for a query q, sampling point k and attention head m. In our temporal deformable attention formulation, we separately learn linear projections for the current and temporal frames, see <ref type="table" target="#tab_7">Table A</ref>.1 for their dimensions. This allows to individually set K curr and K temp and ablate configurations without temporal connections. Given a clip with ? = 6, the queries assigned to a frame t apply their ? ? 1 learned temporal projections to the remaining frames in the following frame order:</p><p>We follow VIS convention to run inference on reduced input resolutions with 360 pixels but upsample to the required benchmark resolution before the clip tracking. The runtime frames per second (FPS) measurement do not include this upsampling. This requires the same projection parameters to predict offsets/weights with different temporal distances to the current frame which is possible due to the learned temporal encoding added to each query. Furthermore, the explicit discrimination between the current and temporal frames with respect to a query allows decoder object queries to focus on predictions for their frame while taking additional temporal information under consideration. It should also be noted that all the terms in <ref type="table" target="#tab_7">Table A</ref>.1 scale linearly with the number of frames in a clip and and do not depend on the input resolution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Ablation studies</head><p>In <ref type="table" target="#tab_7">Table A</ref>.2, we extend the analysis of the main paper on the effect of adding more or less temporal information by altering the number of K temp for the encoder and decoder separately. The ablation of different K temp require individually trained models with differing number of parameters. Given a trained DeVIS model, the final runtime and performance can be modulated by adjusting the clip stride S, i.e., instance overlap, during inference. For experiments with differing clip size ? , we adjust the stride to keep the number of overlapping frames between constant. We set K curr = 4 as in <ref type="bibr" target="#b28">[27]</ref> for all experiments and obtain the same optimal value for K temp .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative results</head><p>In <ref type="figure">Figure A</ref>.1 and A.2, we present additional qualitative results for the YouTube-VIS 2019/2021 and OVIS, respectively. In <ref type="figure">Figure A</ref>.5, we demonstrate the instanceaware object queries and their reference point alignment. To this end, we visualize the attention maps with reference points (cross) at the first, third and last layer for the query from the third frame. The reference point from that query on other frames aligns with the bounding box positions on these respective frames. Finally, we present failure cases in <ref type="figure" target="#fig_1">Figure A.3</ref> and A.4.     <ref type="bibr" target="#b25">[24]</ref> dataset. We also struggle sometimes to segment overlapping instances from the same category on YouTube-VIS dataset. We do a better job on similar scenarios A.2 on OVIS, and we argue it is because the model sees a lot more of these examples during training in this other dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. A.5.</head><p>Visualization of the instance-aware object queries and their alignment of reference points for temporal frames on example sequences from <ref type="bibr" target="#b25">[24]</ref>. To this end, we plot the not yet aligned reference points applied in the first decoder layer (first row) and their subsequent alignment to the predicted bounding box centers after the 2 nd and 6 th layer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Attention map visualization of all frames in a clip for two object queries assigned to detect and segment object on the first, second and third frame. The temporal attention computed on other frames successfully follows each object and hence provides consistent identity predictions. Furthermore, we visualize the instance-aware reference point alignment (red dot) which adjusts the reference point of each first-frame query to the position of the respective object in the other frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of our new multi-scale mask head for video and image instance segmentation. The upsampling of Transformer-encoded feature maps and multi-scale attention maps boosts performance significantly. Attention maps are generated by computing multi-head attention between feature maps and object queries. We indicate the hidden size and reduced set of object queries with N * and C, respectively. New connections to the decoder (blue) and encoder (red) are shown colored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>-</head><label></label><figDesc>Class cost C(c i , c j ) = ?1 if c i = c j , and else 0, rewards consistent categories. -Score cost S(s i , s j ) = |s i ? s j | matches clips with similar confidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. A. 1 .</head><label>1</label><figDesc>Qualitative results on the YouTube-VIS 2019/2021<ref type="bibr" target="#b25">[24]</ref> datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. A. 2 .</head><label>2</label><figDesc>Qualitative results on the OVIS<ref type="bibr" target="#b19">[19]</ref> dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. A. 3 .</head><label>3</label><figDesc>Failure predictions due to the category. Most of our errors on YouTube-VIS 2019/2021<ref type="bibr" target="#b25">[24]</ref> datasets are from failed categories, specially the ones that are more under-represented on the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. A. 4 .</head><label>4</label><figDesc>Segmentation failure on YouTube-VIS 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation of our main DeVIS contributions and the incremental built from a naive Deformable VisTR to our final model. Increase spatial inputs denotes multi-scale training on higher input resolutions.</figDesc><table><row><cell>Method</cell><cell>Clip size ?</cell><cell cols="2">Kcurr Ktemp</cell><cell>Feature scales</cell><cell cols="2">AP ? AP</cell><cell>Training GPU hours</cell><cell>#params</cell></row><row><cell>VisTR</cell><cell>36 6</cell><cell>All All</cell><cell>All All</cell><cell>1 4</cell><cell>36.1 -</cell><cell>--</cell><cell>350 OOM</cell><cell>57M 64M</cell></row><row><cell></cell><cell>36</cell><cell>4</cell><cell>4</cell><cell>1</cell><cell>34.2</cell><cell>-</cell><cell>260</cell><cell>47M</cell></row><row><cell>Deformable</cell><cell>36</cell><cell>4</cell><cell>0</cell><cell>1</cell><cell>35.3</cell><cell>-</cell><cell>155</cell><cell>36M</cell></row><row><cell>VisTR</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>1</cell><cell>34.0</cell><cell>-</cell><cell>48</cell><cell>41M</cell></row><row><cell></cell><cell>6</cell><cell>4</cell><cell>0</cell><cell>1</cell><cell>32.4</cell><cell>-</cell><cell>30</cell><cell>36M</cell></row><row><cell>DeVIS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ Increase spatial inputs</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell cols="2">35.9 +1.9</cell><cell>93</cell><cell>48M</cell></row><row><cell>+ Instance-aware object queries</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell cols="2">37.0 +1.1</cell><cell>112</cell><cell>48M</cell></row><row><cell>+ Multi-scale mask head</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell cols="2">40.2 +3.2</cell><cell>120</cell><cell>48M</cell></row><row><cell>+ Multi-cue clip tracking</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell cols="2">41.9 +1.7</cell><cell>120</cell><cell>48M</cell></row><row><cell>+ Auxiliary loss weighting</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell cols="2">44.0 +2.1</cell><cell>120</cell><cell>48M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation for the multi-scale mask head on COCO<ref type="bibr" target="#b16">[16]</ref>. The baseline applies the original mask head as in DETR with Deformable DETR<ref type="bibr" target="#b28">[27]</ref>.</figDesc><table><row><cell>Mask head</cell><cell>Mask mAP</cell><cell>Training GPU hours</cell></row><row><cell>Baseline as in [5] with [27]</cell><cell>23.7</cell><cell>242</cell></row><row><cell cols="2">+ Train only positive matches 24.5</cell><cell>58</cell></row><row><cell>+ Encoded feature maps</cell><cell>25.0</cell><cell>56</cell></row><row><cell>+ Multi-scale attention maps</cell><cell>29.2</cell><cell>59</cell></row><row><cell>+ MDC</cell><cell>31.1</cell><cell>78</cell></row><row><cell>+ End-to-end training</cell><cell>38.0</cell><cell>345</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="6">Comparison of instance</cell></row><row><cell cols="8">segmentation results on COCO [16].</cell></row><row><cell cols="7">Mask R-CNN is from detectron2 [23].</cell><cell></cell></row><row><cell>Methods</cell><cell cols="7">AP AP50 AP75 APl APm APs FPS</cell></row><row><cell>DETR [5]</cell><cell cols="6">33.3 56.5 33.9 53.1 36.8 13.5</cell><cell>-</cell></row><row><cell>IFC [11]</cell><cell>35.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="8">Mask R-CNN [9] 37.2 58.5 39.8 53.3 39.4 18.6 21.4</cell></row><row><cell cols="2">Mask2Former [6] 43.7</cell><cell>-</cell><cell>-</cell><cell cols="4">64.8 47.2 23.4 13.5</cell></row><row><cell>Ours</cell><cell cols="7">38.0 61.4 40.1 59.8 41.4 17.9 12.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Contribution of the additional class and score cost terms in our multi-cue clip tracking.</figDesc><table><row><cell>Clip tracking cues</cell><cell>AP AP50 AP75</cell></row><row><cell>Vol. soft mask IoU</cell><cell>42.1 63.2 46.7</cell></row><row><cell>Vol. soft mask IoU + Score</cell><cell>42.8 65.1 46.9</cell></row><row><cell>Vol. soft mask IoU + Class</cell><cell>43.1 66.0 47.2</cell></row><row><cell cols="2">Vol. soft mask IoU + Score + Class 44.4 66.8 48.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A .</head><label>A</label><figDesc>1. Dimensions of learnable sample offset and attention weight parameters with object queries N , attention heads m, feature scales L and clip size ? .? M ? L ? Kcurr ? 1 ? 1 N ? m ? L ? Ktemp ? ? ? 1 ? 1 Sampling offsets N ? M ? L ? Kcurr ? 1 ? 2 N ? m ? L ? Ktemp ? ? ? 1 ? 2</figDesc><table><row><cell>Current frame</cell><cell>Temporal frames</cell></row><row><cell>Attention weights N</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table A.2. Removing temporal connections with Ktemp = 0 results in performance drops across all datasets. We observe an optimal clip size of ? = 6.</figDesc><table><row><cell>Clip</cell><cell>Ktemp</cell><cell cols="6">YT-VIS 19 [24] YT-VIS 21 [24] OVIS [19]</cell></row><row><cell>size ?</cell><cell></cell><cell>AP</cell><cell>? AP</cell><cell>AP</cell><cell>? AP</cell><cell cols="2">AP ? AP</cell></row><row><cell>6</cell><cell>4</cell><cell>44.4</cell><cell>-</cell><cell>43.1</cell><cell>-</cell><cell>23.8</cell><cell>-</cell></row><row><cell>3</cell><cell>4</cell><cell>41.0</cell><cell>-3.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>9</cell><cell>4</cell><cell>42.4</cell><cell>-2.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>12</cell><cell>4</cell><cell>41.6</cell><cell>-2.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>6</cell><cell>0</cell><cell>41.2</cell><cell>-3.2</cell><cell>39.5</cell><cell>-3.6</cell><cell cols="2">19.7 -4.1</cell></row><row><cell>6</cell><cell>1</cell><cell>41.2</cell><cell>-3.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>6</cell><cell>2</cell><cell>43.4</cell><cell>-1.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>6</cell><cell>3</cell><cell>43.6</cell><cell>-0.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatiotemporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>O?ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. (2020) 1, 3</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. (2020) 1, 3</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mask2former for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/ARXIV.2112.10764</idno>
		<ptr target="https://arxiv.org/abs/2112.10764" />
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Compfeat: Comprehensive feature aggregation for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">VISOLO: grid-based space-time aggregation for efficient online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video instance segmentation using interframe communication transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logist. Quart</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial feature calibration and temporal fusion for effective one-stage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video instance segmentation with a proposereduce paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft coco: Common objects in context</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sg-net: Spatial granularity network for onestage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Occluded video instance segmentation: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01558</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>2021) 2, 3, 4, 5, 8, 11</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Seqformer: a frustratingly simple model for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/ARXIV.2112.08275</idno>
		<ptr target="https://arxiv.org/abs/2112.0827514" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crossover learning for fast online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11168</idno>
		<title level="m">Deformable convnets v2: More deformable, better results</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
