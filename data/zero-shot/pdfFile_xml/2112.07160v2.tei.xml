<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A New Perspective on the Effects of Spectrum in Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqi</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
						</author>
						<title level="a" type="main">A New Perspective on the Effects of Spectrum in Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many improvements on GNNs can be deemed as operations on the spectrum of the underlying graph matrix, which motivates us to directly study the characteristics of the spectrum and their effects on GNN performance. By generalizing most existing GNN architectures, we show that the correlation issue caused by the unsmooth spectrum becomes the obstacle to leveraging more powerful graph filters as well as developing deep architectures, which therefore restricts GNNs' performance. Inspired by this, we propose the correlation-free architecture which naturally removes the correlation issue among different channels, making it possible to utilize more sophisticated filters within each channel. The final correlation-free architecture with more powerful filters consistently boosts the performance of learning graph representations. Code is available at https://github.com/qslim/ gnn-spectrum.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Although graph neural network (GNN) communities are in a rapid development of both theories and applications, there is still a lack of a generalized understanding of the effects of the graph's spectrum in GNNs. As we can see, many improvements can finally be unified into different operations on the spectrum of the underlying graph, while their effectiveness is interpreted by several well-accepted isolated concepts: <ref type="bibr" target="#b45">(Wu et al., 2019;</ref><ref type="bibr" target="#b57">Zhu et al., 2021;</ref><ref type="bibr" target="#b23">Klicpera et al., 2019a;</ref><ref type="bibr" target="#b8">Chien et al., 2021;</ref><ref type="bibr" target="#b1">Balcilar et al., 2021)</ref> explain it in the perspective of simulating low/high pass filters; <ref type="bibr" target="#b31">(Ming Chen et al., 2020;</ref><ref type="bibr" target="#b47">Xu et al., 2018;</ref><ref type="bibr" target="#b30">Liu et al., 2020;</ref><ref type="bibr" target="#b29">Li et al., 2018)</ref> interpret it as ways of alleviating oversmoothing phenomenon in deep architectures;  adopts the conception of normalization operation in neural 1 Dalian University of Technology, China 2 Peng Cheng Laboratory, China.</p><p>Correspondence to: Yanming Shen &lt;shen@dlut.edu.cn&gt;.</p><p>Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). networks and applies it to graph data. Since these improvements all indirectly operate on the spectrum, it motivates us to study the potential connections between the GNN performance and the characteristics of the graph's spectrum. If we can find such a connection, it would provide a deeper and generalized insight into these seemingly unrelated improvements associated with the graph's spectrum (low/high pass filter, oversmoothing, graph normalization, etc), and further identify potential issues in existing architectures. To this end, we first consider the simple correlation metric: cosine similarity among signals, and study the relations between it and the graph's spectrum in the graph convolution operation. It provides a new perspective that in existing GNN architectures, the distribution of eigenvalues of the underlying graph matrix controls the cosine similarity among signals. An ill-posed unsmooth spectrum would easily make signals over-correlated which is evidence of information loss.</p><p>Compared with oversmoothing studies <ref type="bibr" target="#b38">Oono &amp; Suzuki, 2020;</ref><ref type="bibr">Rong et al., 2019;</ref><ref type="bibr" target="#b18">Huang et al., 2020)</ref>, the correlation analysis associated with the graph's spectrum further indicates that the correlation issue is essentially caused by the graph's spectrum. In other words, for graph topologies with an unsmooth spectrum, the issue can appear even with a shallow architecture, and a deep model further makes the spectrum less smooth and eventually exacerbates this issue. Meanwhile, the correlation analysis also provides a unified interpretation of the effectiveness of various existing improvements associated with the graph's spectrum since they all implicitly impose some constraints on the spectrum to alleviate the correlation issue. However, these improvements are trade-offs between alleviating the correlation issue and applying more powerful graph filters: since a filter implementation directly reflects on the spectrum, a more appropriate filter for relevant signal patterns may correspond to an ill-posed spectrum, which in return will not gain performance improvements. Hence, in general GNN architectures, the correlation issue becomes the obstacle to applying more powerful filters. As we can see, although one can approximate more sophisticated graph filters by increasing the order k of the polynomial theoretically <ref type="bibr" target="#b39">(Shuman et al., 2013)</ref>, in the popular models, simple filters, e.g. low-pass filter <ref type="bibr" target="#b22">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b45">Wu et al., 2019)</ref>, or the fixed filter coefficients <ref type="bibr" target="#b23">(Klicpera et al., 2019a;</ref> serve as the practical applicable choice. arXiv:2112.07160v2 <ref type="bibr">[cs.</ref>LG] 19 Jun 2022 With all the above understandings, the key solution is to decouple the correlation issue from the filter design, which results in our correlation-free architecture. In contrast to existing approaches, it allows to focus on exploring more sophisticated filters without the concern of the correlation issue. With this guarantee, we can improve the approximation abilities of polynomial filters to better approximate the desired more complex filters <ref type="bibr" target="#b15">(Hammond et al., 2011;</ref><ref type="bibr" target="#b10">Defferrard et al., 2016)</ref>. However, we also find that it cannot be achieved by simply increasing the number of polynomial bases as the basis characteristics implicitly restrict the number of available bases in the resulting polynomial filter. For this reason, commonly used (normalized) adjacency or Laplacian matrix where its spectrum serves as the basis cannot effectively utilize high-order bases. To address this issue, we propose new graph matrix representations, which are capable of leveraging more bases and learnable filter coefficients to better respond to more complex signal patterns. The resulting model significantly boosts performance on learning graph representations. Although there are extensive studies on the polynomial filters including the fixed coefficients and learnable coefficients <ref type="bibr" target="#b10">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b27">Levie et al., 2019;</ref><ref type="bibr" target="#b8">Chien et al., 2021;</ref>, to the best of our knowledge, they all focus on the coefficients design and use the (normalized) adjacency or Laplacian matrix as a basis. Therefore, our work is well distinguished from them. Our contributions are summarized as follows:</p><p>? We show that general GNN architectures suffer from the correlation issue and also quantify this issue with spectral smoothness;</p><p>? We propose the correlation-free architecture that decouples the correlation issue from graph convolution;</p><p>? We show that the spectral characteristics also hinder the approximation abilities of polynomial filters and address it by altering the graph's spectrum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Let G " pV, Eq be an undirected graph with node set V and edge set E. We denote n " |V| the number of nodes, A P A n?n the adjacency matrix and H P R n?d the node feature matrix where d is the feature dimensionality. h P R n is a graph signal that corresponds to one dimension of H.</p><p>Spectral Graph Convolution <ref type="bibr" target="#b15">(Hammond et al., 2011;</ref><ref type="bibr" target="#b10">Defferrard et al., 2016)</ref>. The definition of spectral graph convolution relies on Fourier transform on the graph domain. For a signal h and graph Laplacian L " U ?U J , we have Fourier transformx " U J x and inverse transform x " Ux. Then, the graph convolution of a signal h with a filter g ? is</p><formula xml:id="formula_0">g ??h " U``U J g ??d`U J h??" U? ? 1 U J h,<label>(1)</label></formula><p>where? ? 1 denotes a diagonal matrix in which the diagonal corresponds to spectral filter coefficients. To avoid eigendecomposition and ensure scalability,? ? 1 is approximated by a truncated expansion in terms of Chebyshev polynomials T k p?q up to the k-th order <ref type="bibr" target="#b15">(Hammond et al., 2011)</ref>, which is also the polynomials of ?,</p><formula xml:id="formula_1">G ? 1 p?q ? k ? i"0 ? 1 i T i p?q " k ? i"0 ? i ? i ,<label>(2)</label></formula><p>where? " 2 ?max ??I n . Now the convolution in Eq. 1 is</p><formula xml:id="formula_2">U? ? 1 U J h ? U?k ? i"0 ? i ? i?U J h " k ? i"0 ? i L i h. (3)</formula><p>Note that this expression is k-localized since it is a k-order polynomial in the Laplacian, i.e., it depends only on nodes that are at most k hops away from the central node.</p><p>Graph Convolutional Network (GCN) <ref type="bibr" target="#b22">(Kipf &amp; Welling, 2017)</ref>. GCN is derived from 1-order Chebyshev polynomials with several approximations. The authors further introduce the renormalization trickD?1 2?D?1 2 with? " A`I n andD ii " ? j? ij . Also, GCN can be generalized to multiple input channels and a layer-wise model:</p><formula xml:id="formula_3">H pl`1q " ??D?1 2?D?1 2 H plq W plq?,<label>(4)</label></formula><p>where W is learnable matrix and ? is nonlinear function.</p><p>Graph Diffusion Convolution (GDC) <ref type="bibr" target="#b24">(Klicpera et al., 2019b)</ref>. A generalized graph diffusion is given by the diffusion matrix:</p><formula xml:id="formula_4">H " 8 ? k"0 ? k T k ,<label>(5)</label></formula><p>with the weight coefficients ? k and the generalized transition matrix T . T can be T rw " AD?1, T sym " D?1 2 AD?1 2 or others as long as they are convergent. GDC can be viewed as a generalization of the original definition of spectral graph convolution, which also applies polynomial filters but not necessarily the Laplacian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Revisiting Existing GNN Architectures</head><p>We first generalize existing spectral graph convolution as follows</p><formula xml:id="formula_5">H " ?`p ? pSqf ? pHq?,<label>(6)</label></formula><p>where S is the graph matrix, e.g. adjacency or Laplacian matrix and their normalized forms. p ? : R n?n ? R n?n is the polynomial of graph matrices with coefficients ? P R k for a k-order polynomial. f ? : R d ? R d 1 is the feature transformation neural network with the learnable parameters ?. In SGC <ref type="bibr" target="#b45">(Wu et al., 2019)</ref>, GDC <ref type="bibr" target="#b24">(Klicpera et al., 2019b)</ref>, SSGC <ref type="bibr" target="#b56">(Zhu &amp; Koniusz, 2020)</ref>, and GPR (Chien  <ref type="bibr">, 2021)</ref>, p ? is implemented as the general polynomial, i.e. p ? pSq " ? k i"0 ? i S i . Their differences are identified by the coefficients ?. For example, SGC corresponds to a very simple form with ? i " 0, i ? k and ? k " 1. By removing the nonlinear layer in GCNII <ref type="bibr" target="#b31">(Ming Chen et al., 2020)</ref>, APPNP <ref type="bibr" target="#b23">(Klicpera et al., 2019a)</ref> and GCNII share the similar graph convolution layer as</p><formula xml:id="formula_6">H plq " p1??qSH pl?1q`? Z, H p0q " Z, Z " f ? pXq,</formula><p>where ? P p0, 1q and X P R n?d is the input node features. By deriving its closed-form, we reformulate it with Eq. 6 as p ? pSq " ? k?1 i"0 ?p1??q i S i`p 1??q k S k . In ChebyNet <ref type="bibr" target="#b10">(Defferrard et al., 2016)</ref>, CayleNet <ref type="bibr" target="#b27">(Levie et al., 2019)</ref> and BernNet , p ? corresponds to Chebyshev, Cayle and Bernstein polynomials respectively. GPR, CayleNet and BernNet apply learnable coefficient ?, where ? is learned as the coefficients of general, Cayle and Bernstein basis respectively. Therefore, with our formulation in Eq. 6, general graph convolutions are mainly different from p ? as summarized in Tab. 1 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Correlation Analysis in the Lens of Graph's Spectrum</head><p>Based on the generalized formulation of Eq. 6, we conduct correlation analysis on existing graph convolution in the perspective of the graph's spectrum. We denote S " p ? pSq for simplicity. h P R n denotes one channel in f ? pHq.</p><p>Then the convolution on h is represented as Sh. The cosine similarity between h and the i-th eigenvector p i of S is</p><formula xml:id="formula_7">cos`xh, p i y?" h J p i b ? n j"1`h J p j?2 " ? i b ? n j"1 ? 2 j .<label>(7)</label></formula><p>? i " h J p i is the weight of h on p i when representing h with the set of orthonormal bases p i , i P rns. The cosine similarity between Sh and p i is</p><formula xml:id="formula_8">cos`xSh, p i y?" ? i ? i b ? n j"1 ? 2 j ? 2 j .<label>(8)</label></formula><p>1 Here, we follow the naming convention in GCNII called initial residual connection. GCN and GCNII interlace nonlinear computations over layers, making them difficult to reformulate all layers with Eq. 6. But one can represent them with the recursive form as H plq " ?`p?pSqf?pH pl?1q q?. For example, in GCN, we have p?pSq " S and f?pH pl?1q q " H pl?1q ? with S "D?1 2?D?1 2 .</p><p>The detailed derivations of Eq. 7 and Eq. 8 are given in Appendix A.</p><p>Eq. 8 builds the connection between the cosine similarity and the spectrum of the underlying graph matrix. We say the spectrum is smooth if all eigenvalues have similar magnitudes. By comparing Eq. 7 and Eq. 8, it shows that the graph convolution operation with the unsmooth spectrum, i.e., dissimilar eigenvalues, results in signals correlated (a higher cosine similarity) to the eigenvectors corresponding to larger magnitude eigenvalues and orthogonal (a lower cosine similarity) to the eigenvectors corresponding to smaller magnitude eigenvalues. In the case where 0 eigenvalue is involved in the spectrum, signals would lose information in the direction of the corresponding eigenvectors. In the deep architecture, this problem would further be exacerbated:</p><p>Proposition 3.1. Assume S P R n?n is a symmetric matrix with real-valued entries. |? 1 | ? |? 2 | ?, . . . , ? |? n | are n real eigenvalues, and p i P R n , i P rns are corresponding eigenvectors. Then, for any given h, h 1 P R n , we have (i) | cospxS k`1 h, p 1 yq| ? | cospxS k h, p 1 yq| and | cospxS k`1 h, p n yq| ? | cospxS k h, p n yq| for k " 0, 1, 2, . . . ,`8;</p><formula xml:id="formula_9">(ii) If |? 1 | ? |? 2 |, lim k?8 | cospxS k h, p 1 yq| " lim k?8</formula><p>| cospxS k h, S k h 1 yq| " 1, and the convergence speed is decided by | ?2 ?1 |.</p><p>We prove Proposition 3.1 in Appendix B. Proposition 3.1 shows that a deeper architecture violates the spectrum's smoothness, which therefore makes the input signals more correlated to each other. 2 Finally, Rankpph 1 , h 2 , . . . , h d qq " 1, and the information within signals would be washed out. Note that all the above analysis does not impose any constraint to the underlying graph such as connectivity.</p><p>Revisiting oversmoothing via the lens of correlation is-2 Here, nonlinearity is not involved in the propagation step. This meets the case of the decoupling structure where a multilayer GNN is split into independent propagation and prediction steps <ref type="bibr" target="#b45">Wu et al., 2019;</ref><ref type="bibr" target="#b23">Klicpera et al., 2019a;</ref><ref type="bibr" target="#b56">Zhu &amp; Koniusz, 2020;</ref><ref type="bibr" target="#b54">Zhang et al., 2021)</ref>. The propagation involving nonlinearity remains unexplored due to its high complexity, except for one case of ReLU as nonlinearity <ref type="bibr" target="#b38">(Oono &amp; Suzuki, 2020)</ref>. Most convergence analyses (such as over-smoothing) only study the simplified linear case <ref type="bibr" target="#b30">Liu et al., 2020;</ref><ref type="bibr" target="#b45">Wu et al., 2019;</ref><ref type="bibr" target="#b23">Klicpera et al., 2019a;</ref><ref type="bibr" target="#b55">Zhao &amp; Akoglu, 2020;</ref><ref type="bibr" target="#b47">Xu et al., 2018;</ref><ref type="bibr" target="#b31">Ming Chen et al., 2020;</ref><ref type="bibr" target="#b56">Zhu &amp; Koniusz, 2020;</ref><ref type="bibr" target="#b24">Klicpera et al., 2019b;</ref><ref type="bibr" target="#b8">Chien et al., 2021).</ref> sue. In the well-known oversmoothing analysis, the convergence is considered as lim k?8? k sym H p0q " H p8q where each row of H p8q only depends on the degree of the corresponding node, provided that the graph is irreducible and aperiodic <ref type="bibr" target="#b47">(Xu et al., 2018;</ref><ref type="bibr" target="#b30">Liu et al., 2020;</ref><ref type="bibr" target="#b55">Zhao &amp; Akoglu, 2020;</ref><ref type="bibr" target="#b8">Chien et al., 2021)</ref>. Our analysis generalizes this result. In our analysis, the convergence of the cosine similarity among signals does not limit a graph to be connected or normalized that is required in the oversmoothing analysis analogical to the stationary distribution of the Markov chain, and even does not require a model to be necessarily deep : it is essentially caused by the bad distributions of eigenvalues, while the deep architecture exacerbates it. Interestingly, inspired by this perspective, the correlation problem actually relates to the specific topologies since different topologies correspond to different spectrum. There exists topologies inherently with bad distributions of eigenvalues, and they will suffer from the problem even with a shallow architecture. Also, by taking the symmetry into consideration, Proposition 3.1(i) shows that the convergence of cosine similarity with respect to k is also monotonous. In contrast that existing results only discuss the theoretical infinite depth case, this provides more concrete evidence in the practical finite depth case that a deeper architecture can be more harmful than a shallow one.</p><p>Revisiting graph filters via the lens of correlation issue. The graph filter is approximated by a polynomial in the theory of spectral graph convolution <ref type="bibr" target="#b15">(Hammond et al., 2011;</ref><ref type="bibr" target="#b10">Defferrard et al., 2016)</ref>. Although theoretically, one can approximate any desired graph filter by increasing the order k of the polynomial <ref type="bibr" target="#b39">(Shuman et al., 2013)</ref>, most GNNs cannot gain improvements by enlarging k. Instead, the simple low-pass filter studied by many improvements on spectral graph convolution acts as the practical effective choice <ref type="bibr" target="#b39">(Shuman et al., 2013;</ref><ref type="bibr" target="#b45">Wu et al., 2019;</ref><ref type="bibr" target="#b37">NT &amp; Maehara, 2019;</ref><ref type="bibr">Muhammet et al., 2020;</ref><ref type="bibr" target="#b24">Klicpera et al., 2019b)</ref>. Although there are studies involving high-pass filters to better process high-frequency signals recently, the low-pass is always required in graph convolution <ref type="bibr" target="#b56">(Zhu &amp; Koniusz, 2020;</ref><ref type="bibr" target="#b57">Zhu et al., 2021;</ref><ref type="bibr" target="#b1">Balcilar et al., 2021;</ref><ref type="bibr">Bo et al., 2021;</ref><ref type="bibr" target="#b12">Gao et al., 2021)</ref>. This can be explained in the perspective of correlation analysis. As we have shown, the graph convolution is sensitive to the spectrum. A more proper filter to better respond to relevant signal patterns may result in an unsmooth spectrum, making different channels correlated to each other after convolution. In contrast, although a low-pass filter has limited expressiveness, it corresponds to a smoother spectrum, which alleviates the correlation issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Correlation-free Architecture</head><p>The correlation analysis via the lens of graph's spectrum shows that in general GNN architectures, the unsmooth spectrum leads to correlation issue and therefore acts as the obstacle to developing deep architectures as well as leveraging more expressive graph filters. To overcome this issue, a natural idea is to assign the graph convolution in different channels of f ? pHq with different spectrums, which can be viewed as a generalization of Eq. 6 as follows</p><formula xml:id="formula_10">H " f ?`" p ?1 pSqf ?1 pHq, . . . , p ? d 1 pSqf ? d 1 pHq ??. (9) Both f ? : R d ? R d 1 and f ? : R d 1 ? R d 2</formula><p>are the feature transformation neural networks with the learnable parameters ? and ? respectively. p ?i is the i-th polynomial with the learnable coefficients</p><formula xml:id="formula_11">? i P R k . f ?i pHq P R n is the i-th channel of f ? pHq P R n?d 1</formula><p>. We denote h i " f ?i pHq for simplicity. Then the convolution operation on h i in Eq. 9 is</p><formula xml:id="formula_12">p ?i pSqh i " k ? j"0 ? i,j S j h i " U k ? j"0 ? i,j ? j U J h i (10) with the filter diagpg ?i q " ? k j"0 ? i,j ? j . We denote ? " ? 1 , ? 2 , . . . , ? n?J P R n . Then, g ?i " k ? j"0 ? i,j ? j "`? 1 , ? 2 , . . . , ? k??? i " V?? i , (11) where V "`? 1 , ? 2 , . . . , ? k?P R n?k . If ? i ? ? j for any i ? j, i.e.</formula><p>, the algebraic multiplicity of all eigenvalues is 1, V is a Vandermonde matrix with RankpV q " minpn, kq. V j " ? j , j P rks serve as a set of k bases, where each filter g ?i is a linear combination of V j . Hence, a larger k helps to better approximate the desired filter. When k " n, V is a full-rank matrix and g ?i is sufficient to represent any desired filter with proper assignments of ? i . Note that n is much smaller in real-world graph-level tasks than that in node-level tasks, making k " n more tractable.</p><p>By considering the columns of a Vandermonde matrix, i.e. ? j , j P rks as bases, we can see that when increasing k (aka applying more bases), ? k i with |? i | ?? 1 goes diminishing and ? k i with |? i | ?? 1 goes divergent. To balance the diminishing and divergence problems when applying a larger k, we need to carefully control the range of the spectrum close to 1 or?1. General approaches have ? P r0, 1s n 3 . Although there is no concern of divergence problems, ? k i , especially for a small ? i , inclines to 0 when increasing k, making the higher-order basis ineffective in the practical limited precision condition.</p><p>On the other hand, general approaches are less likely to learn the coefficients of polynomial filters in a completely 3 General approaches use the (symmetry) normalized A, i.e. D?1 2?D?1 2 ,D?1? to guarantee its spectrum is bounded by r?1, 1s <ref type="bibr" target="#b22">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b24">Klicpera et al., 2019b)</ref> or the (symmetry) normalized L, i.e. I?D?1 2?D?1 2 to ensure the boundary [0, 2] and then rescale it to r0, 1s . free manner <ref type="bibr" target="#b24">(Klicpera et al., 2019b;</ref>. The specially designed coefficients to explicit modify spectrum, i.e. Personalized PageRank (PPR), heat kernel <ref type="bibr" target="#b24">(Klicpera et al., 2019b)</ref>, etc or the coefficients learned under the constrained condition, i.e. Chebyshev <ref type="bibr" target="#b10">(Defferrard et al., 2016)</ref>, Cayley <ref type="bibr" target="#b27">(Levie et al., 2019)</ref>, Bernstein  polynomial, etc act as the practical applicable filters. This is probably because the polynomial filter relies on sophisticated coefficients to maintain spectral properties. Learning them from scratch would easily fall into an ill-posed filter . However, by modifying the filter bases, it would relax the requirement on the coefficients, making it more suitable for learning coefficients from scratch.</p><p>Finally, although the new architecture in Eq. 9 decouples the correlation issue from developing more powerful filters, general filter bases are less qualified for approximating more complex filters. Hence, we still need to explore more effective filter bases to replace existing ones. To this end, we will introduce two different improvements on filter bases in the following sections whose effectiveness will serve as a verification of our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Spectral Optimization on Filter Basis</head><p>One can directly apply a smoothing function on the spectrum of S, which helps to narrow the range of eigenvalues close to 1 or -1. There can be various approaches to this end, and in this paper, we propose the following eigendecompositionbased method for a symmetric matrix S " P ?P J 4</p><formula xml:id="formula_13">S ? " P diagpf ? p? i qqP J , f ? p?q " "?p?? q ? , ? ? 0 ? ? , ? ? 0,<label>(12)</label></formula><p>where i P rns. ? P p0, 1q, ? ? " e ? ln ? . S ? serves as the polynomial bases in Eq. 10. Unlike general spectral approaches, S is not required to be a bounded spectrum. It can leverage more bases while alleviating both the diminishing and divergence problems by controlling ??k in a small range. Therefore, S ? can be considered as a basis-augmentation technique as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. There can be other transformations on the spectrum, e.g., P`Sigmoid`??`??P J , which have a similar effect to S ? . Note that the injectivity of f ? also influences the approximation ability, which is discussed in more details in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generalized Normalization on Filter Basis</head><p>Eq. 12 directly operates on the spectrum, which can achieve an accurate control on the range of the spectrum but requires eigendecomposition. To avoid eigendecomposition, we alternatively study the effects of graph normalization on the spectrum. We generalize the normalized adjacency matrix as follows</p><formula xml:id="formula_14">D ?D " pD`?Iq pA`?IqpD`?Iq ,<label>(13)</label></formula><p>where P r?0.5, 0s is the normalization coefficient and ? P r0, 1s is the shift coefficient. Widely-usedD?1 2?D?1 2 corresponds to "?0.5 and ? " 1.</p><p>Proposition 4.1. Let ? 1 ? ? 2 ????? ? n be the spectrum of A and ? 1 ? ? 2 ????? ? n be the spectrum of pD? Iq pA`?IqpD`?Iq , then for any i P rns, we have</p><formula xml:id="formula_15">p? i`? qpd max`? q 2 ? ? i ? p? i`? qpd min`? q 2 ,</formula><p>where d min and d max are the minimum and maximum degrees of nodes in the graph.</p><p>We prove Proposition 4.1 in Appendix D. Proposition 4.1 extends the results in <ref type="bibr" target="#b41">(Spielman, 2007)</ref>, showing that the normalization has a scaling effect on the spectrum: a smaller is likely to lead to a smaller ? i , while a larger is likely to lead to a larger ? i . When " 0, the upper and lower bounds coincide with ? i " ? i`? .</p><p>To further investigate the effects of the normalization on the spectrum, we fix ? " 0 and empirically evaluate as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. When fixing ,D ?D shrinks the spectrum of A with different degrees on different eigenvalues. For eigenvalues with small magnitudes (in the middle area of the spectrum), it has a small shrinking effect, while for eigenvalues with large magnitudes, it has a relatively large shrinking effect. Hence,D ?D can be used as a spectral smoothing method. Also, different results in different shrinking effects, which is consistent with the results in Proposition 4.1. Widely-usedD?1 2?D?1 2 with the spectrum bounded by r?1, 1s may not be a good choice since the diminishing problem. Intuitively, to utilize more bases, we should narrow the range of the spectrum close to 1 (or -1) to avoid both the diminishing and divergence problems in higher-order bases. This can vary from different datasets and we should carefully balance and k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Many improvements on GNNs can be unified into the spectral smoothing operations, e.g. low-pass filter <ref type="bibr" target="#b45">(Wu et al., 2019;</ref><ref type="bibr" target="#b57">Zhu et al., 2021;</ref><ref type="bibr" target="#b23">Klicpera et al., 2019a;</ref><ref type="bibr" target="#b8">Chien et al., 2021;</ref><ref type="bibr" target="#b1">Balcilar et al., 2021)</ref>, alleviating oversmoothing <ref type="bibr" target="#b31">(Ming Chen et al., 2020;</ref><ref type="bibr" target="#b47">Xu et al., 2018;</ref><ref type="bibr" target="#b30">Liu et al., 2020;</ref><ref type="bibr" target="#b29">Li et al., 2018)</ref>, graph normalization , etc, to evaluate the shrinking effects ofD ?D on the spectrum. We randomly sample 5 graphs in each of three datasets ZINC, MolPCBA and NCI1 respectively. In the first three figures, we use the fixed "?0.3 on all 5 graphs. In the fourth figure, we use "?0.1,?0.2,?0.3,?0.4,?0.5 respectively on one graph, which corresponds to the 5 lines from top to bottom. More visualization results on other datasets can be found in Appendix E. our analysis on the relations of the correlation issue and the spectrum of underlying graph's matrix provides a unified interpretation on their effectiveness.</p><p>ChebyNet <ref type="bibr" target="#b10">(Defferrard et al., 2016)</ref>, CayleNet <ref type="bibr" target="#b27">(Levie et al., 2019)</ref>, APPNP <ref type="bibr" target="#b23">(Klicpera et al., 2019a)</ref>, SSGC <ref type="bibr" target="#b56">(Zhu &amp; Koniusz, 2020)</ref>, GPR <ref type="bibr" target="#b8">(Chien et al., 2021)</ref>, BernNet , etc explore various polynomial filters and use the normalized adjacency or Laplacian matrix as basis. We improve the approximation ability of polynomial filters by altering the spectrum of filter bases. The resulting bases allow leveraging more bases to approximate more sophisticated filters and are more suitable for learning coefficients from scratch.</p><p>We note that the concurrent work <ref type="bibr" target="#b20">(Jin et al., 2022)</ref> has also pointed out the overcorrelation issue in the infinite depth case, without further discussion on the reason (e.g. graph's spectrum) behind this phenomenon. In contrast, we show that correlation is inherently caused by the unsmooth spectrum of the underlying graph filter, and also quantify this effect with spectral smoothness. It allows to analyze the correlation across all layers instead of only the theoretical infinite depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We conduct experiments on TUDatasets <ref type="bibr" target="#b49">(Yanardag &amp; Vishwanathan, 2015;</ref><ref type="bibr" target="#b21">Kersting et al., 2016)</ref>, OGB <ref type="bibr" target="#b17">(Hu et al., 2020)</ref> which involve graph classification tasks and ZINC <ref type="bibr" target="#b11">(Dwivedi et al., 2020)</ref> which involves graph regression tasks. Then, we evaluate the effects of our proposed new graph convolution architecture and two filter bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results</head><p>Settings. We use the default dataset splits for OGB and ZINC. For TUDatasets, we follow the standard 10-fold cross-validation protocol and splits from <ref type="bibr" target="#b53">(Zhang et al., 2018)</ref> and report our results following the protocol described in  <ref type="bibr" target="#b48">(Xu et al., 2019;</ref><ref type="bibr" target="#b51">Ying et al., 2018)</ref>. Following all baselines on the leaderboard of ZINC, we control the number of parameters around 500K. The baseline models include: GK (Shervashidze et al., 2009), RW <ref type="bibr" target="#b44">(Vishwanathan et al., 2010)</ref>, PK , FGSD <ref type="bibr" target="#b43">(Verma &amp; Zhang, 2017)</ref>, AWE <ref type="bibr" target="#b19">(Ivanov &amp; Burnaev, 2018)</ref>, DGCNN <ref type="bibr" target="#b53">(Zhang et al., 2018)</ref>, PSCN <ref type="bibr" target="#b36">(Niepert et al., 2016)</ref>, DCNN <ref type="bibr" target="#b0">(Atwood &amp; Towsley, 2016)</ref>, ECC <ref type="bibr" target="#b40">(Simonovsky &amp; Komodakis, 2017)</ref>, DGK <ref type="bibr" target="#b49">(Yanardag &amp; Vishwanathan, 2015)</ref>, CapsGNN <ref type="bibr" target="#b46">(Xinyi &amp; Chen, 2019)</ref>, DiffPool <ref type="bibr" target="#b51">(Ying et al., 2018)</ref>, GIN <ref type="bibr" target="#b48">(Xu et al., 2019)</ref>, k-GNN <ref type="bibr" target="#b32">(Morris et al., 2019)</ref>, GraphSage <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b42">(Veli?kovi? et al., 2018)</ref>, GatedGCN-PE <ref type="bibr" target="#b5">(Bresson &amp; Laurent, 2017)</ref>, MPNN (sum) <ref type="bibr" target="#b13">(Gilmer et al., 2017)</ref>, DeeperG , PNA <ref type="bibr" target="#b9">(Corso et al., 2020)</ref>, DGN <ref type="bibr" target="#b2">(Beani et al., 2021)</ref>, GSN <ref type="bibr" target="#b4">(Bouritsas et al., 2020)</ref>, GINE-VN <ref type="bibr" target="#b6">(Brossard et al., 2020)</ref>, GINE-APPNP <ref type="bibr" target="#b6">(Brossard et al., 2020)</ref>, PHC-GNN <ref type="bibr" target="#b26">(Le et al., 2021)</ref>, SAN <ref type="bibr" target="#b25">(Kreuzer et al., 2021)</ref>, Graphormer <ref type="bibr" target="#b50">(Ying et al., 2021)</ref>. Spec-GN denotes the proposed graph convolution in Eq. 9 with the smoothed filter basis by spectral transformation in Eq. 12.  <ref type="figure">Figure 3</ref>. A visualization of the learned filters on ZINC. We tested on three bases with each basis randomly sampling 9 filters. Dots represent the eigenvalues of each basis. More visualization results on other datasets can be found in Appendix F.</p><p>Norm-GN denotes the proposed graph convolution in Eq. 9 with the smoothed filter basis by graph normalization in Eq. 13. Results. Tab. 2 and 3 summarize performance of our approaches comparing with baselines on TUDatasets, ZINC and MolPCBA. For TUDatasets, we report the results of each model in its original paper by default. When the results are not given in the original paper, we report the best testing results given in <ref type="bibr" target="#b53">(Zhang et al., 2018;</ref><ref type="bibr" target="#b19">Ivanov &amp; Burnaev, 2018;</ref><ref type="bibr" target="#b46">Xinyi &amp; Chen, 2019)</ref>. For ZINC and MolPCBA, we report the results of their public leaderboards. TUDatasets involves small-scale datasets. NCI1 and NCI109 are around 4K graphs. ENZYMES and PTC MR are under 1K graphs. General GNNs easily suffer from overfitting on these smallscale data, and therefore we can see that some traditional kernel-based methods even get better performance. However, Spec-GN and Norm-GN achieve higher classification accuracies by a large margin on these datasets. The results on TUDatasets show that although Spec-GN and Norm-GN achieve more expressive filters, it does not lead to overfitting on learning graph representations. Recently, Transformerbased models are quite popular in learning graph representations, and they significantly improve the results on large-scale molecular datasets. On ZINC, Spec-GN and Norm-GN outperform these Transformer-based models by a large margin. And on MolPCBA, they are also competitive compared with SOTA results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation Studies</head><p>We perform ablation studies on the proposed architecture and the filter bases? ? (by setting S "? in Eq. 12) and D ?D on ZINC. We use "idp" and "shd" to respectively represent the correlation-free architecture (also known as independent filter architecture) in Eq. 9 and the general shared filter architecture in Eq. 6. Both architectures learn the filter coefficients from scratch.</p><p>Correlation-free architecture and different filter bases. In <ref type="figure">Fig. 3</ref>, we visualize the learned filters in the correlationfree on three bases, i.e.D?1 2?D?1 2 ,? ? andD ?D . The visualizations show that each channel indeed learns a different filter on all three bases.D?1 2?D?1 2 has the bounded spectrum r?1, 1s that is slightly close to 1 due to the involvement of self-loop. The filters learn a similar response on all range which corresponds to different frequencies in frequency domain.? ? andD ?D have the spectrum close to 1 or?1 while the filters learn diverse responses on these areas, which corresponds to more complex patterns on different frequencies. Tab. 4 shows that the correlation-free always outperforms the shared filter by a large margin on all tested bases. BothD?1 2?D?1 2 andD?1? have the bounded spectrum r?1, 1s and they have similar performance.? ? andD ?D narrow the range of the spectrum close to 1 or?1 through completely different strategies, but they have similar performance that is much better tha? D?1 2?D?1 2 andD?1?. This validates our analysis on the filter basis. Meanwhile,? ? achieves more accurate control on the spectrum, and correspondingly, it slightly outper-formsD ?D .</p><p>Do more bases gain improvements? In <ref type="figure">Fig. 4</ref>, we systematically evaluate the effects of the number of bases on Valid MAE shd+D 1 2 AD 1 2 idp+D 1 2 AD 1 2 idp+A = 1/3 idp+A = 1/6 idp+A = 1/7 idp+A = 1/8 <ref type="figure">Figure 4</ref>. Ablation study results on ZINC with different number of bases k.</p><p>learning graph representations, includingD?1 2?D?1 2 and our? ? with ? " 1{3, 1{6, 1{7, 1{8. The shared filter case, i.e. shd`D?1 2?D?1 2 cannot well leverage more bases (a larger k) as the MAE stops decreasing at 0.150 which is also reported by several baselines in Tab. 3. In contrast, both correlation-free cases idp`D?1 2?D?1 2 and idp`? ? outperform the shared filter case by a large margin and they continuously gain improvements when increasing k. The MAE of idp`D?1 2?D?1 2 stops decreasing at the test MAE close to 0.09 and the valid MAE close to 0.11. By replac-ingD?1 2?D?1 2 with? ? , the best test MAE is below 0.07, and the best valid MAE is close to 0.088. The bases in? ? are controlled by both ? and k. We use the tuple p?, kq to denote a combination of ? and k. By fixing ?, the curves corresponding to ? " 1{3 and ? " 1{6 show that increasing k gains improvements. By fixing the upper bound of ??k to be 1, p1{6, 6q involves 3 more bases than p1{3, 3q and outperforms p1{3, 3q. The same results are also reflected in the comparison of p1{6, 12q and p1{3, 6q. For the comparison of p1{6, 18q and p1{3, 9q, both settings achieve the lowest MAE and the difference is less obvious.</p><p>The effects of model depth. <ref type="figure" target="#fig_3">Fig.5</ref> shows the performance comparisons between correlation-free and shared filter as depth increases. Each architecture is tested with the default basisD 1 2?D 1 2 and our proposed? ? . We set the same number of bases in all resulting models, and each model is tested with the number of layers (depth) equal to t5, 10, 15, 20, 25u. The results show that the correlationfree can preserve the performance as depth increases. The shared filter cases perform quite unstable and drop dra-matically when depth ? 20. Also, across all depths, the correlation-free almost always outperforms the shared filter and has low variance among different runs. In Appendix G, we also test cosine similarities of different layers in a deep model.</p><p>Stability. We also found that the correlation-free is more stable in different runs than the shared filter case as reflected in the standard deviation in Tab. 4. This is probably because different channels may pose different patterns, which causes interference among each other in the shared filter case. While the correlation-free well avoids this problem. Also, the results of? ? andD ?D are more stable tha? D?1 2?D?1 2 andD?1? in different runs. ForD?1 2?D?1 2 andD?1?, the difference between the best and the worst runs can be more than 0.02. While for? ? andD ?D , this difference is less than 0.01. More results are given in Appendix H. The instability ofD?1 2?D?1 2 andD?1? is probably because learning filter coefficients from scratch without any constraints is difficult to maintain spectrum properties and therefore easily falls into an ill-posed filter . In contrast,? ? andD ?D inherently with smoother spectrum alleviate this problem and make them more appropriate in the scenario of learning coefficients from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We study the effects of spectrum in GNNs. It shows that in existing architectures, the unsmooth spectrum results in the correlation issue, which acts as the obstacle to developing deep models as well as applying more powerful graph filters. Based on this observation, we propose the correlation-free architecture which decouples the correlation issue from filter design. Then, we show that the spectral characteristics also hinder the approximation abilities of polynomial filters and address it by altering the graph's spectrum. Our extensive experiments show the significant performance gain of correlation-free architecture with powerful filters.</p><p>Rong, Y., <ref type="bibr">Huang, W., Xu, T., and Huang, J. Dropedge:</ref> Towards deep graph convolutional networks on node classification. In International Conference on Learning <ref type="bibr">Representations, 2019.</ref> Shervashidze, N., Vishwanathan, S., Petri, T., Mehlhorn, K., and Borgwardt, K. Efficient graphlet kernels for large graph comparison. In Artificial Intelligence and <ref type="bibr">Statistics, pp. 488-495, 2009</ref>.</p><formula xml:id="formula_16">Then, lim k?8 | cospxS k h, S k h 1 yq| " lim k?8?? n i"1 ? i ? i ? 2k ib ? n i"1 ? 2 i ? 2k i b ? n i"1 ? 2 i ? 2k i " lim k?8?? n i"1 ? i ? i ?i ?1 2kb ? n i"1 ? 2 i ?i ?1 2k b ? n i"1 ? 2 i ?i ?1 2k " lim k?8?? 1 ? 1`? n i"2 ? i ? i ?i ?1 2kb ? 2 1`? n i"2 ? 2 i ?i ?1 2k b ? 2 1`? n i"2 ? 2 i ?i ?1 2k "??? 1 ? 1? ? 2 1 a ? 2 1 " 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Discussions of Spectral Optimization on Filter Basis</head><p>We use E pS,?q to denote the eigenspace of S associated with ? such that E pS,?q " tv : pS??Iqv " 0u.</p><p>Proposition C.1. Given a symmetric matrix S P R n?n with S " P ?P J where ? " diagp? 1 , ? 2 , . . . , ? n q, and P can be any eigenbasis of S, let S ? " P ?p?qP J , where ?p?q is an entry-wise function applied on ?. Then we have</p><formula xml:id="formula_17">(i) E pS,?iq ? E pS ? ,?p?iqq , i P rns; (ii) Meanwhile, if ?p?q is injective, E pS,?iq " E pS ? ,?p?iqq and F ? pSq " P ?p?qP J is injective.</formula><p>Proof. Let P " pp 1 , p 2 , . . . , p n q. S " P ?P J is equivalent to Sp i " ? i p i , i P rns. For any i P rns, the geometric multiplicity of any ? i is equal to its algebraic multiplicity, and E pS,?iq " Spanptp k |? k " ? i , k P rnsuq. S ? " P ?p?qP J and S ? p i " ?p? i qp i , i P rns. Similarly, for any i P rns, E pS ? ,?p?iqq " Spanptp k |?p? k q " ?p? i q, k P rnsuq. Note that tp k |? k " ? i , k P rnsu ? tp k |?p? k q " ?p? i q, k P rnsu for any i P rns. Hence Spanptp k |? k " ? i , k P rnsuq ? Spanptp k |?p? k q " ?p? i q, k P rnsu. As a result, E pS,?iq ? E pS ? ,?p?iqq for any i P rnsq.</p><p>If ?p?q is injective, tp k |? k " ? i , k P rnsu " tp k |?p? k q " ?p? i q, k P rnsu for any i P rns. Thus E pS,?iq " E pS ? ,?p?iqq .</p><p>We use ?pSq to denote the generalisation of the set of all eigenvalues of S (Slso known as the spectrum of S). Let S " P ? 1 P J and B " Q? 2 Q J . Suppose S ? B, to prove S ? " F ? pSq ? B ? " F ? pBq, we discuss two cases respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 1: ?pSq ? ?pBq</head><p>Then ?pS ? q ? ?pB ? q. The characteristic polynomials of S ? and B ? are different. Therefore, S ? ? B ? .</p><p>Case 2: ?pSq " ?pBq</p><formula xml:id="formula_18">Then ? 1 " ? 2 " ?. We prove the equivalent proposition "S ? " B ? ? S " B". If S ? " B ? , P ?p?qP J " Q?p?qQ J .</formula><p>For any ? i with geometric multiplicity k, we can find the corresponding eigenvectors p 1 , p 2 , . . . , p k according to P ?p?qP J . Similarly, we can find the corresponding eigenvectors q 1 , q 2 , . . . , q k according to Q?p?qQ J . Note that the eigen-decomposition is unique in terms of eigenspaces. Thus, E pS ? ,?p?iqq " Spanpp 1 , p 2 , . . . , p k q " Spanpq 1 , q 2 , . . . , q k q " E pB ? ,?p?iqq . Therefore, for any ? i , E pS,?iq " E pB,?iq (As given in Proposition C.1). Correspondingly, S " P ?P J " Q?Q J " B.</p><p>Proposition C.1 shows that the eigenspace of S ? involves the eigenspace of S. Therefore, S ? is invariant to the choice of eigenbasis, i.e., S ? " P ?p?qP J " P 1 ?p?qP 1J for any eigenbases P and P 1 of S. Hence, S ? is unique to S for a given ?p?q. Consistently, we denote the mapping F ? pSq " F ? pP ?P J q " P ?p?qP J .</p><p>When F ? is injective, F ? pSq and S share the same algebraic multiplicity. Otherwise, F ? pSq has a larger algebraic multiplicity on the corresponding eigenvalues, which may weaken the approximation ability based on the understanding of Vandermonde matrix. Also, the injectivity of F ? serves as a guarantee that the transformation is reversible with no information loss.</p><p>F ? p?q is also equivariant to graph isomorphism. For any two graphs G 1 and G 2 with matrix representations S 1 and S 2 (e.g., adjacency matrix, Laplacian matrix, etc.), G 1 and G 2 are isomorphic if and only if there exists a permutation matrix M such that M S 1 M J " S 2 . We denote IpSq " M SM J . Then</p><formula xml:id="formula_19">Claim 1. F ? p?q is equivariant to graph isomorphism, i.e. F ? pIpSqq " IpF ? pSqq. Proof. F ? pIpSqq " F ? pM SM J q " F ? pM pP ?P J qM J q " F ? ppM P q?pM P q J q " pM P q?p?qpM P q J " M pP ?p?qP J qM J " IpF ? pSqq Hence, for a specific GNN model f GNN , f GNN pF ? pIpSqqq " f GNN pIpF ? pSqqq " f GNN pF ? pSqq.</formula><p>The learned representation is invariant to graph isomorphism (also known as permutation invariance <ref type="bibr" target="#b52">(Zaheer et al., 2017;</ref><ref type="bibr" target="#b34">Murphy et al., 2019)</ref>) when introducing F ? p?q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Proposition 4.1</head><p>Proof. Let? " pD`?Iq pA`?IqpD`?Iq . According to Courant-Fischer theorem,</p><formula xml:id="formula_20">? i " min dimpSq"i max xPS x J? x x J x .</formula><p>Let y " pD`?Iq x. As the change of variables y " pD`?Iq x is non-singular, this is equivalent to</p><formula xml:id="formula_21">? i " min dimpT q"i max yPT y J pA`?Iqy y J pD`?Iq?2 y .</formula><p>Therefore,</p><formula xml:id="formula_22">? i " min dimpT q"i max yPT y J pA`?Iqy y J pD`?Iq?2 y ? min dimpT q"i max yPT y J pA`?Iqy pd max`? q?2 y J y " pd max`? q 2 `m in dimpT q"i max yPT y J Ay y J y`?" p? i`? qpd max`? q 2 .</formula><p>Similarly, we can prove ? i ? p? i`? qpd min`? q 2 .  <ref type="figure">Figure 6</ref>. We randomly sample 5 graphs in each of three datasets NCI109, ENZYMES and PTC MR respectively. And we use the fixed "?0.3 to see the effects of the normalization on all graphs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visualizations of the Learned Filters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. The Correlation Issue of Deep Models</head><p>We test the absolute value of cosine similarities in different layers for a depth=25 model. For each graph, we compute the mean of all hidden signal pairs. The final visualized results in <ref type="figure">Fig.8</ref> are the mean of all graphs within a randomly selected batch. To be consistent with the definition of spectral graph convolution as well as our correlation analysis, the test runs do not utilize edge features of ZINC.</p><p>The results show that on both bases, the cosine of the shared filter case converges to 1, while the correlation-free converges to 0.8 forD 1 2?D 1 2 and 0.7 for? ? . (We also found that it easily leads to a large cosine similarity on ZINC, which is mainly because graphs are small such that n ?? d, where n is the number of nodes and d is the number of hidden features.) These results do show that general GNNs suffer from the correlation issue as depth increases, while our correlation-free architecture enjoys a relatively stable performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Assume ? ? 0 and ? " 0. 9 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>We use the metric ?D ?D ??</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Ablation study results on ZINC with different number of layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Visualizations of the learned filters on MolPCBA, NCI1, NCI109, ENZYMES and PTC MR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>The curves of 5 runs on ZINC with the number of basis k " 9, 18, 21, 24.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>A summary of p? in Eq. 6 in general graph convolutions.</figDesc><table><row><cell></cell><cell>GCN</cell><cell>SGC</cell><cell>APPNP</cell><cell>GCNII</cell><cell>GDC</cell><cell>SSGC</cell><cell>GPR</cell><cell>ChebyNet</cell><cell>CayleNet</cell><cell>BernNet</cell></row><row><cell>Poly-basis</cell><cell cols="6">General General Residual Residual General General</cell><cell>General</cell><cell>Chebyshev</cell><cell>Cayle</cell><cell>Bernstein</cell></row><row><cell>Poly-coefficient</cell><cell>Fixed</cell><cell>Fixed</cell><cell>Fixed</cell><cell>Fixed</cell><cell>Fixed</cell><cell>Fixed</cell><cell>Learnable</cell><cell>Fixed</cell><cell cols="2">Learnable Learnable</cell></row><row><cell>et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on TUDatasets. Higher is better. Spec-GN 84.79?1.63 83.62?0.75 72.50?5.79 68.05?6.41 Norm-GN 84.87?1.68 83.50?1.27 73.33?7.96 67.76?4.52</figDesc><table><row><cell>dataset</cell><cell>NCI1</cell><cell>NCI109</cell><cell>ENZYMES</cell><cell>PTC MR</cell></row><row><cell>GK</cell><cell cols="4">62.49?0.27 62.35?0.3 32.70?1.20 55.65?0.5</cell></row><row><cell>RW</cell><cell>-</cell><cell>-</cell><cell cols="2">24.16?1.64 55.91?0.3</cell></row><row><cell>PK</cell><cell>82.54?0.5</cell><cell>-</cell><cell>-</cell><cell>59.5?2.4</cell></row><row><cell>FGSD</cell><cell>79.80</cell><cell>78.84</cell><cell>-</cell><cell>62.8</cell></row><row><cell>AWE</cell><cell>-</cell><cell>-</cell><cell>35.77?5.93</cell><cell>-</cell></row><row><cell>DGCNN</cell><cell>74.44?0.47</cell><cell>-</cell><cell>51.0?7.29</cell><cell>58.59?2.5</cell></row><row><cell>PSCN</cell><cell>74.44?0.5</cell><cell>-</cell><cell>-</cell><cell>62.29?5.7</cell></row><row><cell>DCNN</cell><cell>56.61?1.04</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ECC</cell><cell>76.82</cell><cell>75.03</cell><cell>45.67</cell><cell>-</cell></row><row><cell>DGK</cell><cell cols="4">80.31?0.46 80.32?0.3 53.43?0.91 60.08?2.6</cell></row><row><cell>GraphSag</cell><cell>76.0?1.8</cell><cell>-</cell><cell>58.2?6.0</cell><cell>-</cell></row><row><cell cols="2">CapsGNN 78.35?1.55</cell><cell>-</cell><cell>54.67?5.67</cell><cell>-</cell></row><row><cell>DiffPool</cell><cell>76.9?1.9</cell><cell>-</cell><cell>62.53</cell><cell>-</cell></row><row><cell>GIN</cell><cell>82.7?1.7</cell><cell>-</cell><cell>-</cell><cell>64.6?7.0</cell></row><row><cell>k-GNN</cell><cell>76.2</cell><cell>-</cell><cell>-</cell><cell>60.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results</figDesc><table><row><cell></cell><cell cols="2">on ZINC (Lower is better) and MolPCBA (Higher</cell></row><row><cell>is better).</cell><cell></cell><cell></cell></row><row><cell>method</cell><cell>ZINC MAE</cell><cell>MolPCBA AP</cell></row><row><cell>GCN</cell><cell>0.367?0.011 p505kq</cell><cell>24.24?0.34 p2.02mq</cell></row><row><cell>GIN</cell><cell>0.526?0.051 p510kq</cell><cell>27.03?0.23 p3.37mq</cell></row><row><cell>GAT</cell><cell>0.384?0.007 p531kq</cell><cell>-</cell></row><row><cell>GraphSage</cell><cell>0.398?0.002 p505kq</cell><cell>-</cell></row><row><cell cols="2">GatedGCN-PE 0.214?0.006 p505kq</cell><cell>-</cell></row><row><cell>MPNN</cell><cell>0.145?0.007 p481kq</cell><cell>-</cell></row><row><cell>DeeperG</cell><cell>-</cell><cell>28.42?0.43 p5.55mq</cell></row><row><cell>PNA</cell><cell>0.142?0.010 p387kq</cell><cell>28.38?0.35 p6.55mq</cell></row><row><cell>DGN</cell><cell>0.168?0.003 NA</cell><cell>28.85?0.30 p6.73mq</cell></row><row><cell>GSN</cell><cell>0.101?0.010 p523kq</cell><cell>-</cell></row><row><cell>GINE-VN</cell><cell>-</cell><cell>29.17?0.15 p6.15mq</cell></row><row><cell>GINE-APPNP</cell><cell>-</cell><cell>29.79?0.30 p6.15mq</cell></row><row><cell>PHC-GNN</cell><cell>-</cell><cell>29.47?0.26 p1.69mq</cell></row><row><cell>SAN</cell><cell>0.139?0.006 p509kq</cell><cell>-</cell></row><row><cell>Graphormer</cell><cell>0.122?0.006 p489kq</cell><cell>-</cell></row><row><cell>Spec-GN</cell><cell cols="2">0.0698?0.002 p503kq 29.65?0.28 p1.74mq</cell></row><row><cell>Norm-GN</cell><cell cols="2">0.0709?0.002 p500kq 29.51?0.33 p1.74mq</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study results on ZINC with different settings.</figDesc><table><row><cell></cell><cell cols="2">Architecture</cell><cell>Basis</cell><cell>test MAE</cell><cell>valid MAE</cell></row><row><cell></cell><cell>shd</cell><cell cols="3">idpD?1 2?D?1 2D?1?? ?D ?D</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1415?0.00748 0.1568?0.00729</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1439?0.00900 0.1569?0.00739</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1061?0.01018 0.1294?0.01454</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1133?0.01711 0.1316?0.02057</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0944?0.00379 0.1100?0.00787</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0982?0.00417 0.1172?0.00666</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0698?0.00200 0.0884?0.00319</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0709?0.00176 0.0929?0.00445</cell></row><row><cell></cell><cell>0.200</cell><cell></cell><cell>0.200</cell></row><row><cell></cell><cell>0.175</cell><cell></cell><cell>0.175</cell></row><row><cell>Test MAE</cell><cell>0.100 0.125 0.150</cell><cell></cell><cell>0.100 0.125 0.150</cell></row><row><cell></cell><cell>0.075</cell><cell></cell><cell>0.075</cell></row><row><cell></cell><cell>5</cell><cell></cell><cell>10 15 20 25 k</cell><cell>5</cell><cell>10 15 20 25 k</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Although the computation of S? requires eigendecomposition, S is always a symmetric matrix and the eigendecomposition on it is much faster than a general matrix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported in part by the National Key Research and Development Program of China (no. 2021ZD0112400), and also in part by the National Natural Science Foundation of China under grants U1811463 and 62072069.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since S P R n?n is a symmetric matrix, assume the eigendecomposition S " P ?P J with P "`p 1 , p 2 , . . . , p n?a nd }p i } " 1, i P rns.</p><p>Proof. (i) As S k " P ? k P J and Eq. 8, for k " 0, 1, 2, . . . ,`8, we have</p><p>Similarly, we can prove that | cospxS k h, p n yq| ? | cospxS k`1 h, p n yq|.</p><p>(ii) Since | cospxS k h, p n yq| monotonously increases with respect to k and has the upper bound 1, | cospxS k h, p n yq| must be convergent.</p><p>As |? 1 | ? |? 2 | ?, . . . , ? |? n |, we have lim k?8 ? n i"2 ? 2 i`? i ?1?2 k " 0 and the convergence speed is decided by | ?2 ?1 |. Therefore lim k?8 | cospxS k h, p 1 yq| " 1. cos`xSh, Sh 1 y?"`S h?JSh 1</p><p>?`P J h??J`P ?`P J h??b`P ?`P J h 1??J`P ?`P J h 1?"`P J h?J? 2 P J h 1 b`P J h?J? 2`P J h?b`P J h 1?J ? 2`P J h 1" ? J ? 2 ? ? ? J ? 2 ? a ? J ? 2 ? " ? n i"1 ? i ? i ? 2 i a ? n i"1 ? 2 i ? 2 i a ? n i"1 ? 2 i ? 2</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Renton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=-qh0M9XWxnv" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Directional graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="748" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond lowfrequency information in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dehaene</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.15069</idno>
		<title level="m">Graph convolutions that can finally model local structure</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graphnorm: A principled approach to accelerating graph neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=n6jl7fLxrP" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09910</idno>
		<title level="m">Message passing in graph convolution networks via adaptive filter banks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning arbitrary graph spectral filters via bernstein approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Tackling over-smoothing for general graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09864</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anonymous walk embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/ivanov18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards feature overcorrelation in deeper graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Mi9xQBeZxY5" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13354" to="13366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tossou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03893</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16584</idno>
		<title level="m">Parameterized hypercomplex graph neural networks for graph classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cayleynets</surname></persName>
		</author>
		<idno>doi: 10.1109/ TSP.2018.2879624</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">When spectral domain meets spatial domain in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Muhammet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>S?bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtyseventh International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Workshop on Graph Representation Learning and Beyond. GRL+ 2020</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJluy2RcFm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1ldO2EFPr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spectral graph theory and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<idno type="DOI">10.1109/FOCS.2007.56</idno>
	</analytic>
	<monogr>
		<title level="m">48th Annual IEEE Symposium on Foundations of Computer Science (FOCS&apos;07)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Capsule graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xinyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Byl8BnRcYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05234</idno>
		<title level="m">Do transformers really perform bad for graph representation? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="https://proceedings" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An end-toend deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Lite geometry enhanced molecular representation learning for quantum property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litegem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pairnorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkecl1rtwB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Simple spectral graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Interpreting and unifying graph neural networks with an optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1215" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
