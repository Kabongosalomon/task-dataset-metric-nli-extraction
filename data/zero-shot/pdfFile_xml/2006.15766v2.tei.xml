<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 HETEROSKEDASTIC AND IMBALANCED DEEP LEARN- ING WITH ADAPTIVE REGULARIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
							<email>kaidicao@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Toyota Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Toyota Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
							<email>tengyuma@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 HETEROSKEDASTIC AND IMBALANCED DEEP LEARN- ING WITH ADAPTIVE REGULARIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world large-scale datasets are heteroskedastic and imbalanced -labels have varying levels of uncertainty and label distributions are long-tailed. Heteroskedasticity and imbalance challenge deep learning algorithms due to the difficulty of distinguishing among mislabeled, ambiguous, and rare examples. Addressing heteroskedasticity and imbalance simultaneously is under-explored. We propose a data-dependent regularization technique for heteroskedastic datasets that regularizes different regions of the input space differently. Inspired by the theoretical derivation of the optimal regularization strength in a one-dimensional nonparametric classification setting, our approach adaptively regularizes the data points in higher-uncertainty, lower-density regions more heavily. We test our method on several benchmark tasks, including a real-world heteroskedastic and imbalanced dataset, WebVision. Our experiments corroborate our theory and demonstrate a significant improvement over other methods in noise-robust deep learning. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In real-world machine learning applications, even well-curated training datasets have various types of heterogeneity. Two main types of heterogeneity are: (1) data imbalance: the input or label distribution often has a long-tailed density, and (2) heteroskedasticity: the labels given inputs have varying levels of uncertainties across subsets of data stemming from various sources such as the intrinsic ambiguity of the data or annotation errors. Many deep learning algorithms have been proposed for imbalanced datasets (e.g., see <ref type="bibr" target="#b5">Cui et al., 2019;</ref><ref type="bibr" target="#b20">Liu et al., 2019)</ref> and the reference therein). However, heteroskedasticity, a classical notion studied extensively in the statistical community <ref type="bibr" target="#b28">(Pintore et al., 2006;</ref><ref type="bibr" target="#b37">Wang et al., 2013;</ref><ref type="bibr" target="#b35">Tibshirani et al., 2014)</ref>, has so far been under-explored in deep learning. This paper focuses on addressing heteroskedasticity and its interaction with data imbalance in deep learning.</p><p>Heteroskedasticity is often studied in regression analysis and refers to the property that the distribution of the error varies across inputs. In this work, we mostly focus on classification, though the developed technique also applies to regression. Here, heteroskedasticity reflects how the uncertainty in the conditional distribution y | x, or the entropy of y | x, varies as a function of x. Real-world datasets are often heteroskedastic. For example, <ref type="bibr" target="#b18">Li et al. (2017)</ref> shows that the WebVision dataset has a varying number of ambiguous or true noisy examples across classes. <ref type="bibr">2</ref> Conversely, we consider a dataset to be homoscedastic if every example is mislabeled with a fixed probably , as assumed by many prior theoretical and empirical works on label corruption <ref type="bibr" target="#b7">(Ghosh et al., 2017;</ref><ref type="bibr" target="#b10">Han et al., 2018;</ref><ref type="bibr" target="#b14">Jiang et al., 2018;</ref><ref type="bibr" target="#b24">Mirzasoleiman et al., 2020)</ref>. We note that varying uncertainty in y | x can come from at least two sources: the intrinsic semantic ambiguity of the input, and the (data-dependent) mislabeling introduced by the annotation process. Our approach can handle both types of noisy examples in a unified way, but for the sake of comparisons with past methods, we call them "ambiguous examples" and "mislabeled examples" respectively, and refer to both of them as "noisy examples".  <ref type="figure">Figure 2</ref>: Real-world datasets have various sources of heterogeneity and it could be hard to distinguish one from another. They require mutuallyexclusive reweighting strategy, but they all benefit from stronger regularization.</p><p>Overparameterized deep learning models tend to overfit more to the noisy examples <ref type="bibr" target="#b0">(Arpit et al., 2017;</ref><ref type="bibr" target="#b6">Fr?nay &amp; Verleysen, 2013;</ref>. To address this issue, a common approach is to detect noisy examples by selecting those with large training losses, and then remove them from the (re-)training process. However, an input's training loss can also be big because it is rare or ambiguous <ref type="bibr" target="#b9">(Hacohen &amp; Weinshall, 2019;</ref>, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Noise-cleaning methods could fail to distinguish mislabeled from rare/ambiguous examples (see Section 3.1 for empirical proofs). Though dropping the former is desirable, dropping the latter loses important information. Another popular approach is reweighting methods that reduce the contribution of noisy examples in optimization. However, failing to distinguish between mislabeled and rare/ambiguous examples makes the decision of the weights tricky -mislabeled examples require small weights, whereas rare / ambiguous examples benefit from larger weights <ref type="bibr" target="#b32">Shu et al., 2019)</ref>.</p><p>We propose a regularization method that deals with noisy and rare examples in a unified way. We observe that mislabeled, ambiguous, and rare examples all benefit from stronger regularization . We apply a Lipschitz regularizer <ref type="bibr" target="#b40">(Wei &amp; Ma, 2019a;</ref><ref type="bibr">b)</ref> with varying regularization strength depending on the particular data point. Through theoretical analysis in the one-dimensional setting, we derive the optimal regularization strength for each training example. The optimal strength is larger for rarer and noisier examples. Our proposed algorithm, heteroskedastic adaptive regularization (HAR), first estimates the noise level and density of each example, and then optimizes a Lipschitz-regularized objective with input-dependent regularization with strength provided by the theoretical formula.</p><p>In summary, our main contributions are: (i) we propose to learn heteroskedastic imbalanced datasets under a unified framework, and theoretically study the optimal regularization strength on onedimensional data. (ii) we propose an algorithm, heteroskedastic adaptive regularization (HAR), which applies stronger regularization to data points with high uncertainty and low density. (iii) we experimentally show that HAR achieves significant improvements over other noise-robust deep learning methods on simulated vision and language datasets with controllable degrees of data noise and data imbalance, as well as a real-world heteroskedastic and imbalanced dataset, WebVision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ADAPTIVE REGULARIZATION FOR HETEROSKEDASTIC DATASETS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">BACKGROUNDS</head><p>We first introduce general nonparametric tools that we use in our analysis, and review the dependency of optimal regularization strength on the sample size and noise level.</p><p>Over-parameterized neural networks as nonparametric methods. We use nonparametric method as a surrogate for neural networks because they have been shown to be closely related. <ref type="bibr">Recent work (Savarese et al., 2019)</ref> shows that the minimum norm two-layer ReLU network that fits Ground Truth Weak Unif-reg Strong Unif-reg Adapt-reg (HAR) <ref type="figure">Figure 3</ref>: A one-dimensional example with a three-layer neural network in heteroskedastic and imbalanced regression setting. The curve in blue is the underlying ground truth and the dots are observations with heteroskedastic noise. This example shows that uniformly weak regularization overfits on noisy and rare data (on the right half), whereas uniformly strong regularization causes underfitting on the frequent and oscillating data (on the left half). The adaptive regularization does not underfit the oscillating data but still denoise the noisy data. We note that standard nonparametric methods such as cubic spline do not work here because they also use uniform regularization.</p><p>the training data is in fact a linear spline interpolation. <ref type="bibr" target="#b25">Parhi &amp; Nowak (2019)</ref> extend this result to a broader family of neural networks with a broader family of activations.</p><p>Given a training dataset {(x i , y i )} n i=1 , nonparametric method with penalty works as follows. Let F : R ? R be a twice-differentiable model family. We aim to fit the data with smoothness penalty</p><formula xml:id="formula_0">min f 1 n n i=1 (f (x i ), y i ) + ? (f (x)) 2 dx (1)</formula><p>Lipschitz regularization for neural networks. Lipschitz regularization has been shown to be effective for deep neural networks as well. <ref type="bibr" target="#b40">Wei &amp; Ma (2019a)</ref> proves a generalization bound of neural networks dependent on the Lipschitzness of each layer with respect to all intermediate layers on the training data, and show that, empirically, regularizing the Lipschitzness improve the generalization. <ref type="bibr" target="#b33">Sokoli? et al. (2017)</ref> shows similar results in data-limited settings. In Section 2.3, we extend the Lipschitz regularization technique to heteroskedastic setting.</p><p>Regularization strength as a function of noise level and sample size. Finally, we briefly review existing theoretical insights on the optimal choice of regularization strength. Generally, the optimal regularization strength for a given model family increases with the label noise level and decreases in the sample size. As a simple example, consider linear ridge regression min ? 1 n n i=1 (x i ? ? y i ) 2 + ? ? 2 , where x i , ? ? R d and y i ? R. We assume y i = x i ? * + ? for some ground truth parameter ? * , and ? ? N (0, ? 2 ). Then the optimal regularization strength ? opt = d? 2 /n ? * 2 2 . Results of similar nature can also be found in nonparametric statistics <ref type="bibr" target="#b37">(Wang et al., 2013;</ref><ref type="bibr" target="#b35">Tibshirani et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">HETEROSKEDASTIC NONPARAMETRIC CLASSIFICATION ON ONE-DIMENSIONAL DATA</head><p>We consider a one-dimensional binary classification problem where X = [0, 1] ? R and Y = {?1, 1}. We assume Y given X follows a logistic model with ground-truth function f , i.e.</p><formula xml:id="formula_1">Pr [Y = y|X = x] = 1 1 + exp(?yf (x))</formula><p>.</p><p>( <ref type="formula">2)</ref> The training objective is cross-entropy loss plus Lipschitz regularization, i.e.</p><formula xml:id="formula_2">f = argmin f L(f ) 1 n n i=1 (f (x i ), y i ) + ? 1 0 ?(x)(f (x)) 2 dx,<label>(3)</label></formula><p>where (a, y) = ? log(1 + exp(?ya)), and ?(x) is a smoothing parameter as a function of the noise level and density of x. Let I(x) be the fisher information matrix conditioned on the input, i.e.</p><formula xml:id="formula_3">I(x) E[? 2 a (a, Y )| a=f (X) |X = x].</formula><p>When (X, Y ) follows the logistic model in equation 2,</p><formula xml:id="formula_4">I(x) = 1 (1 + exp(f (x))(1 + exp(?f (x)) = Var(Y |X = x).</formula><p>Therefore, I(x) captures the aleatoric uncertainty of x. For example, when Y is deterministic conditioned on X = x, we have I(x) = 0, indicating perfect certainty.</p><p>Define the test metric as the mean-squared-error on the test set</p><formula xml:id="formula_5">{(x i , y i )} n i=1 , i.e., 1 MSE(f ) E {(xi,yi)} n i=1 1 0 (f (t) ? f (t)) 2 dt<label>(4)</label></formula><p>Our main goal is to derive the optimal choice of ?(x) that minimizes the MSE. We start with an analytical characterization of the test error. Let W 2 2 = {f is absolute continuous and f ? L 2 [0, 1]}. We denote the density of X as q(x). The following theorem analytically computes the MSE under the regularization strength ?(?), building upon <ref type="bibr" target="#b37">(Wang et al., 2013)</ref> for regression problems. The proof of the Theorem is deferred to Appendix A.</p><formula xml:id="formula_6">Theorem 1. Assume that f , q, I ? W 2 2 . Let r(t) = ?1/(q(t)I(t)) and L 0 = ? ?? 1 4 exp(?2|t|)dt. If we choose ? = C 0 n ?2/5 for some constant C 0 &gt; 0, the asymptotic mean squared error is lim n?? MSE(f ) = C n 1 0 ? 2 r 2 (t) d dt (?(t)(f * ) (t)) 2 + L 0 r(t) 1/2 ?(t) ?1/2 dt</formula><p>in probability, where C n is a scalar that only depends on n.</p><p>Using the analytical formula of the test error above, we want to derive an approximately optimal choice of ?(x). A precise computation is infeasible, so we restrict ourselves to consider only ?(x) that is constant within groups of examples. We introduce an additional structure -we assume the data can be divided into k groups [a 0 , a 1 ), [a 1 , a 2 ), ? ? ? , [a k?1 , a k ). Each group [a j , a j+1 ) consists of an interval of data with approximately the same aleatoric uncertainty. We approximate ?(t) is constant on each of the group [a i , a i+1 ) with value ? i . Plugging this piece-wise constant ? into the asymptotic MSE in Theorem 1, we obtain</p><formula xml:id="formula_7">lim n?? MSE(f ) = j ? 2 j aj+1 aj r 2 (t) d 2 dt 2 f (t) 2 dt + ? ?1/2 j L 0 aj+1 aj r 1/2 (t)dt .</formula><p>Minimizing the above formula over ? 1 , . . . , ? k separately, we derive the optimal weights, ? j = L0 a j+1 a j</p><formula xml:id="formula_8">r(t) 1/2 dt 4 a j+1 a j r 2 (t) d 2 dt 2 f (t) 2 dt 2/5</formula><p>. In practice, we do not know f and q(x), so we make the following simplifications. We assume that q(t) and I(t) are constant on each interval [a j , a j+1 ]. In other words, we assume that q(t) = q j and I(t) = I j for all t ? [a j , a j+1 ]. We further assume that d 2 dt 2 f (t) is close to a constant on the entire space, because estimating the curvature in high dimension is difficult.</p><formula xml:id="formula_9">This simplification yields ? j ? q ?1/2 j I ?1/2 j q ?2 j I ?2 j 2/5 = q 3/5 j I 3/5</formula><p>j . We find the simplification works well in practice.</p><p>Adaptive regularization with importance sampling. It is practically infeasible to implement the integration in equation 3 for high-dimensional data. We use importance sampling to approximate the integral:</p><formula xml:id="formula_10">minimize f L(f ) 1 n n i=1 (f (x i ), y i ) + ? n i=1 ? i f (x i ) 2 (5) Suppose x i ? [a j , a j+1 ),</formula><p>we have that ? i should satisfy that ? i q j = ? j so that the expectation of the regularization term in equation 5 is equal to that in equation 3. Hence,</p><formula xml:id="formula_11">? i = I 3/5 j q ?2/5 j = I(x i ) 3/5 q(x i ) ?2/5 .</formula><p>Adaptive regularization for multi-class classification and regression. In fact, the proof of Theorem 1 is proved for general loss (a, y). Therefore, we can directly generalize it to multiclass classification and regression problems. For a regression problem, (a, y) is the square loss: (y, a) = 0.5(y ? a) 2 , the Fisher information I(x) = 1. Therefore, for a regression problem, we can choose regularization weight ? i = q(x i ) ?2/5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">PRACTICAL IMPLEMENTATION ON NEURAL NETWORKS WITH HIGH-DIMENSIONAL DATA</head><p>We heuristically extend the Lipschitz regularization technique discussed in Section 2.2 from nonparametric models to over-parameterized deep neural networks. Let (x, y) be an example and f ? be an r-layer neural network. We denote by h (j) the j-th hidden layer of the network, by</p><formula xml:id="formula_12">J (j) (x) ? ?h (j) L(f (x), y), i.e., the Jacobian of the loss w.r.t h (j) . We replace the regularization term f (x) 2 in equation 5 by R(x) = r j=1 J (j) (x) 2 F 1/2</formula><p>, which was proposed by <ref type="bibr" target="#b40">(Wei &amp; Ma, 2019a)</ref>. As a proof of concept, we visualize the behavior of our algorithm in <ref type="figure">Figure 3</ref>, where we observe that the rare and noisy examples have significantly improved error due to stronger regularization. In contrast, a uniform regularization either overfits or underfits different subsets.</p><p>Note that the differences from the 1-D case include the following three aspects. 1. The derivative is taken w.r.t to all the hidden layers for deep models, which has been shown to have superior generalization guarantees for neural networks by <ref type="bibr" target="#b40">(Wei &amp; Ma, 2019a;</ref><ref type="bibr">b)</ref>. 2. An additional square root is taken in computing R(x). This modified version may have milder curvature and be easier to tune. 3. We take the derivative of the loss instead of the derivative of the model, which outputs k numbers for multi-class classification. This is because the derivative of the model requires k times more time to compute. The regularized training objective is consequently</p><formula xml:id="formula_13">minimize f L(f ) 1 n n i=1 ( (f (x i ), y i ) + ?? i R(x i )) ,<label>(6)</label></formula><p>where ? i is chosen to be ? i = I(x i ) 3/5 /q(x i ) 2/5 following the formula equation 5 in Section 2.2 and ? is a hyperparameter to control the overall scale of the regularization strength. We note that we do not expect this choice of ? i to be optimal for the high-dimensional case with all the modifications above -the optimal choice does depend on the nuances. However, we also observe that the empirical performance is not sensitive to the form of ? as long as it's increasing in I(x) and decreasing in q(x).</p><p>That is, the more uncertain or rare an example is, the stronger regularization should be applied.</p><p>In order to estimate the relative regularization strength ? i , the key difficulty lies in the estimation of uncertainty I(x). As in the 1-D setting, we divide the examples into k groups G 1 , . . . , G k (e.g., each group can correspond to a class), and estimate the uncertainty on each group. In the binary setting,</p><formula xml:id="formula_14">I(x) = Var(Y |X = x) = Pr[Y = 1 | X] ? Pr[Y = 0 | X] can be approximated b? I(x) = 1 ? max k?{0,1} Pr[Y = k | X = x]</formula><p>up to a factor of at most 2. We use the same formula for multi-class setting as the approximation of the uncertainty. (As a sanity check, when Y is concentrated on a single outcome, the uncertainty is 0.) Note that?(x) is essentially the minimum possible error of any deterministic prediction on the data point x. Assume that we have a sufficiently accurate pre-trained model, we can use its validation error to estimate?(x):</p><p>Then for all x ? G j , we estimate q(x) and I(x) by ?x ? G j , q(x) ? |G j |, I(x) ? average validation error of a pre-trained model f? on G j</p><p>The whole training pipeline is summarized in Algorithm 1.</p><p>Algorithm 1 Heteroskedastic Adaptive Regularization (HAR) </p><formula xml:id="formula_16">Require: Dataset D = {(x i , y i )} n i=1 . A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We experimentally show that our proposed algorithm HAR(Algorithm 1) improves the test performance of the noisier and rarer groups of examples (by stronger regularization) without negatively affecting the training and test performance of the other groups. We evaluate our algorithms on three vision datasets and one NLP dataset: CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b16">(Krizhevsky et al., 2009</ref>), IMDB-review ) (see Appendix C.1), and WebVision <ref type="bibr" target="#b18">(Li et al., 2017)</ref>, a real-world heteroskedastic and imbalanced dataset. Please refer to Appendix B for low-level implementation details.</p><p>Baselines. We compare our proposed HAR with the following baselines. The simplest one is (1) Empirical risk minimization (ERM): the vanilla cross-entropy loss with all examples having the same weights of losses. We select two representatives from the noise-cleaning line of approach.</p><p>(2) Co-teaching <ref type="bibr" target="#b10">(Han et al., 2018)</ref>: two deep networks are trained simultaneously. Each network aims to identify clean data points that have small losses and use them to guide the training of the other network.</p><p>(3) INCV <ref type="bibr" target="#b3">(Chen et al., 2019)</ref>: it extends Co-teacing to an interative version to estimate the noise ratio and select data. We consider three representatives from the reweighting-based methods, including two that learn the weighting using meta-learning. (4) MentorNet <ref type="bibr" target="#b14">(Jiang et al., 2018)</ref>: it pretrains a teacher network that outputs weights for examples that are used to train the student network with reweighting. (5) L2RW <ref type="bibr" target="#b29">(Ren et al., 2018)</ref>: it directly optimizes weights of each example in the training set by minimizing its corresponding loss on a small meta validation set. (6) MW-Net <ref type="bibr" target="#b32">(Shu et al., 2019)</ref>: it extends L2RW by explicitly defining a weighting function which depends only on the loss of the example. We also compare against two representatives from the robust loss function. (7) GCE <ref type="bibr" target="#b45">(Zhang &amp; Sabuncu, 2018)</ref>: it generalizes mean average error and cross-entropy loss to obtain a new loss function. (8) DMI : it designs a new loss function based on generalized mutual information. In addition, as an essential ablation study, we consider vanilla uniform regularization. (9) Unif-reg: we apply the Jacobian regularizer on all examples with equal strength, and tune the strength to get the best possible validation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SIMULATING HETEROSKEDASTIC AND IMBALANCED DATASETS ON CIFAR</head><p>Setting. Unlike previous works that test on uniform random or asymmetric noise, which is often not the case in reality, in this paper we test our method on more realistic noisy settings, as suggested by <ref type="bibr" target="#b27">Patrini et al. (2017)</ref>; <ref type="bibr" target="#b45">Zhang &amp; Sabuncu (2018)</ref>. In order to simulate heteroskedasticity, we only corrupt semantically-similar classes. For CIFAR-10, we exchange 40% of the labels between classes 'cat' and 'dog', and between 'truck' and 'automobile'. CIFAR-100 has 100 classes grouped into 20 super classes. For each class of the 5 classes under the super class 'vehicles 1' and 'vehicles 2', we corrupt the labels with 40% probability uniformly randomly to the rest of four classes under the same super class. As a result, the 10 classes under super class 'vehicle 1' and 'vehicle 2' have high label noise level and the corruption are only within the same super class. Heteroskedasticity of the labels and imbalance of the inputs commonly coexist in the real world settings. HAR can take both of them into account. To understand the challenge imposed by the entanglements of heteroskedasticity and imbalance, and compare HAR with the aforementioned baselines, we inject data imbalance concurrently with the heteroskedastic noise. We remove samples from the corrupted classes to simulate the most difficult scenario -the rare and noisy groups overfit significantly. (A more benign interaction between the noises and imbalance is that the rare classes have lower noise level, we defer it to Appendix C.3.) We use the imbalance ratio to denote the frequency ratio between the frequent (and clean) classes to the rare (and corrupted) classes. We consider imbalance ratio to be 10 and 100.</p><p>Result. <ref type="table" target="#tab_1">Table 1</ref>  Reweighting-based methods tend to suffer from the loss of accuracy in other more frequent classes, which is aligned with the findings in . While the aforementioned baselines struggle to deal with heteroskedasticity and imbalance together, HAR is able to put them under the same regularization framework and achieve significant improvements. Notably, HAR also shows improvement over uniform regularization with optimally tuned strength. This clearly demonstrates the importance of introducing adaptive regularization among all examples for a better trade-off. A more detailed ablation study on the trade-off between training accuracy and validation accuracy can be found in Section 3.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ABLATION STUDY ON CIFAR</head><p>We disentangle the problem setting to show the effectiveness of our unified framework.</p><p>Simulating heteroskedastic noise on CIFAR. We study the uncertainty part of HAR by testing under the setting with only heteroskedastic noise. The type of noise injection is the same as Section 3.1.</p><p>We report the top-1 validation accuracy of various methods in <ref type="table" target="#tab_2">Table 2</ref>. Aligned with our analysis in Section 4, we observe that both noise-cleaning and reweighting based methods don't get a comparable accuracy on noisy classes with applying strong regularization (? = 0.1) under this heteroskedastic setting. We observe the behavior that too strong regularization impede the model from fitting informative samples, thus it could lead to a decrease on clean classes' accuracy. On the contrary, too weak regularization leads to overfitting the noisy examples thus the accuracy on noisy classes do not reach the optimal.</p><p>Interestingly, we find that even the well-studied CIFAR-100 dataset has intrinsic heteroskedasticity and HAR can improve over uniform regularization to some extent. Please refer to Appendix C.2 for the results on CIFAR-100 and Appendix C.1 for results on IMDB-review. Simulating data imbalance on CIFAR. We study the density part of HAR by testing under the setting with only data imbalance. We follow the same setting as  to create imbalanced CIFAR. Long-tailed imbalance follows an exponential decay in sample sizes across different classes. For step imbalance setting, all rare classes have the same sample size, as do all frequent classes. Our approach achieves better results than LDAM-DRW and is comparable to recent state-of-the-art methods under the imbalanced setting.  <ref type="bibr">(Zhou et al., 2020) 20.18 11.68 21.64 11.99 57.44 40.88 57.44 40.36 HAR-DRW 20.46 10.62 20.27 11.58 55.35 38.98 51.73 37.54</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">EVALUATION ON WEBVISION WITH REAL-WORLD HETEROGENEITY</head><p>WebVision <ref type="bibr" target="#b18">(Li et al., 2017)</ref> contains 2.4 million images crawled from Google and Flickr using 1,000 labels shared with the ImageNet dataset. Its training set is both heteroskedastic and imbalanced (detailed statistics can be found in <ref type="bibr" target="#b18">(Li et al., 2017)</ref>), and it is considered as a popular benchmark for noise robust learning. As the full dataset is very large, we follow <ref type="bibr" target="#b14">(Jiang et al., 2018)</ref> to use a mini version, which contains the first 50 classes of the Google subset of the data. Following the standard protocol <ref type="bibr" target="#b14">(Jiang et al., 2018)</ref>, we test the trained model on the WebVision validation set and the ImageNet validation set. We use ResNet-50 for ablation study and InceptionResNet-v2 for a fair comparison with the baselines. We report results comparing against other state-of-the-art approaches in <ref type="table" target="#tab_5">Table 5</ref>. Strikingly, HAR achieves significant improvement.</p><p>Ablation study. We demonstrate the trade-off between training accuracy and validation accuracy on mini WebVision with various uniform regularization strength and HAR in <ref type="table" target="#tab_4">Table 4</ref>. It's evident that when we gradually increase the overall uniform regularization strength, the training accuracy continues to decrease, and the validation accuracy reaches its peak at 5e-2. While a strong regularization could improve generalization, it reduces deep networks' capacity to fit the training data. However, with our proposed HAR, we only enforce strong regularization on a subset so that we improve the generalization on noisier groups while maintaining the overall training accuracy not affected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Our work is closely related to the following methods and directions.</p><p>Noise-cleaning. The key idea of noise-cleaning is to identify and remove (or re-label) examples with wrong annotations. The general procedure for identifying mislabeled instances has a long history <ref type="bibr" target="#b1">(Brodley &amp; Friedl, 1999;</ref><ref type="bibr" target="#b42">Wilson &amp; Martinez, 1997;</ref><ref type="bibr" target="#b46">Zhao &amp; Nishida, 1995)</ref>. Some recent works tailored this idea for deep neural networks. <ref type="bibr" target="#b36">Veit et al. (2017)</ref> trains a label cleaning network on a small set of data with clean labels, and uses this model to identify noises in large datasets. To circumvent the requirement of a clean subset, <ref type="bibr" target="#b22">Malach &amp; Shalev-Shwartz (2017)</ref> train two networks simultaneously and perform update steps only in case of disagreement. Similarly, in co-teaching <ref type="bibr" target="#b10">(Han et al., 2018)</ref>, each network selects a certain number of small-loss samples and feeds them to its peer network. <ref type="bibr" target="#b3">Chen et al. (2019)</ref> further extends the co-training strategy and comes up with an iterative Reweighting. Reweighting training data has shown its effectiveness on noisy data <ref type="bibr" target="#b19">(Liu &amp; Tao, 2015)</ref>. Its challenge lies in the difficulty of weights estimation. <ref type="bibr" target="#b29">Ren et al. (2018)</ref> proposes a meta-learning algorithm to assign weights to training examples based on its gradient direction with the one on a clean validation set. Recently, <ref type="bibr" target="#b32">Shu et al. (2019)</ref> proposes to learn an explicit loss-weight function to mitigate the optimizing issue of <ref type="bibr" target="#b29">(Ren et al., 2018)</ref>. Another line of work resorts to curriculum learning by either designing an easy-to-hard strategy of training <ref type="bibr" target="#b8">(Guo et al., 2018)</ref> or introducing an extra network <ref type="bibr" target="#b14">(Jiang et al., 2018)</ref> to assign weights.</p><p>Noise-cleaning and reweighting methods usually rely on the empirical loss to determine if a sample is noisy. However, when the dataset is heteroskedastic, each example's training/validation loss no longer correlates well with its noise level. In such cases, we argue that changing the strength of regularization is a more conservative adaption and suffers less from uncertain estimation, compared to changing the weights of losses (Please refer to Section C.4 for empirical justifications).</p><p>Robust loss function. Another line of works has attempted to design robust loss functions <ref type="bibr" target="#b7">(Ghosh et al., 2017;</ref><ref type="bibr" target="#b45">Zhang &amp; Sabuncu, 2018;</ref><ref type="bibr" target="#b27">Patrini et al., 2017;</ref><ref type="bibr" target="#b4">Cheng et al., 2017;</ref><ref type="bibr" target="#b23">Menon et al., 2016)</ref>. They usually rely on prior assumption about latent transition matrix that might not hold in practice. On the contrary, we focus on more realistic settings.</p><p>Regularization. Regularization based techniques have also been explored to combat label noise. <ref type="bibr" target="#b17">Li et al. (2019)</ref> proves that SGD with early stopping is robust to label noise.  provides theoretical analysis of two additional regularization methods. While these methods consider a uniform regularization on all training examples, our work emphasizes on adjusting the weights of regularizers in search of a better generalization than uniform assignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose a unified framework (HAR) for training on heteroskedastic and imbalanced datasets. Our method achieves significant improvements over the previous state-of-the-arts on a variety of benchmark vision and language tasks. We provide theoretical results as well as empirical justifications by showing that ambiguous, mislabeled, and rare examples all benefit from stronger regularization. We further provide the formula for optimal weighting of regularization. Heteroskedasticity of datasets is a fascinating direction worth exploring, and it is an important step towards a better understanding of real-world scenarios in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 SIMULATING HETEROSKEDASTIC AND IMBALANCED DATASETS ON CIFAR</head><p>As mentioned in Section 3.1, we consider another variant of heteroskedastic and imbalanced dataset such that the rare classes have low noise level. To simulate this setting, we make the clean classes have fewer labels than the corrupted classes on the heteroskedastic CIFAR-10 we created in Section ??. <ref type="table" target="#tab_6">Table 8</ref> summarizes the results. For the setting of imbalance ratio equals 10, INCV automatically drops 34.1% of examples from the clean and rare classes, which results in a decrease of mean accuracy on the rare and clean classes. HAR is able to achieve improvements on both noisy classes and rare classes by enforcing the optimal regularization.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Histogram of the distributions of losses on an imbalanced and noisy CIFAR-10 dataset. Clean but rare examples tend to have larger losses, similar to the noisy examples in frequent classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>parameterized model f ? 1: Split training set D into D train and D val 2: f? ? Standard SGD Training on D train 3: Estimate I(x), q(x) with equation 7 using f? on D val , and compute ? i = I(x i ) 3/5 /q(x i ) the model parameters ? randomly 6: f ? ? SGD with the regularized objective as in equation 6 on the full dataset D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualizations of per-class top-1 error and regularization strength of HAR on mini WebVision dataset. The classes are sorted in the ascending order of applied regularization strength.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>summarizes the results. Since examples from rare classes tend to have larger training and validation loss regardless of whether the labels are correct or not, noise-cleaning based methods might drop excessive examples with correct labels. We examined the noise ratio of dropped samples for INCV under the setting of imbalance ratio equals 10. Among all dropped examples, there is only 19.2% of true noise examples. In addition, the rare class examples selected still have 29.8% of label noise. This explains that the significant decrease of accuracies of Co-teaching and INCV on corrupted and rare classes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Top-1 validation accuracy (averaged over 3 runs) of ResNet-32 on heteroskedastic and imbalanced CIFAR-10. HAR significantly improves noisy and rare classes, while keeping the accuracy on other classes almost unaffected.</figDesc><table><row><cell>Imbalance ratio</cell><cell>10</cell><cell></cell><cell>100</cell><cell></cell></row><row><cell>Method</cell><cell>Noisy&amp;Rare Cls.</cell><cell>Clean Cls.</cell><cell>Noisy&amp;Rare Cls.</cell><cell>Clean Cls.</cell></row><row><cell>ERM</cell><cell>52.9 ? 1.2</cell><cell>94.4 ? 0.1</cell><cell>18.9 ? 1.0</cell><cell>94.2 ? 0.1</cell></row><row><cell>Co-teaching</cell><cell>30.2 ? 2.3</cell><cell>88.9 ? 0.3</cell><cell>15.4 ? 2.8</cell><cell>86.4 ? 0.7</cell></row><row><cell>INCV</cell><cell>48.9 ? 1.7</cell><cell>94.0 ? 0.2</cell><cell>25.8 ? 1.8</cell><cell>93.8 ? 0.2</cell></row><row><cell>MentorNet</cell><cell>54.1 ? 1.0</cell><cell>90.3 ? 0.5</cell><cell>28.3 ? 1.5</cell><cell>90.2 ? 0.4</cell></row><row><cell>L2RW</cell><cell>44.3 ? 2.0</cell><cell>90.1 ? 0.5</cell><cell>31.2 ? 1.9</cell><cell>89.7 ? 0.7</cell></row><row><cell>MW-Net</cell><cell>55.4 ? 1.1</cell><cell>91.7 ? 0.5</cell><cell>35.6 ? 1.6</cell><cell>92.3 ? 0.5</cell></row><row><cell>GCE</cell><cell>48.2 ? 0.6</cell><cell>91.6 ? 0.3</cell><cell>14.1 ? 2.0</cell><cell>91.7 ? 0.4</cell></row><row><cell>DMI</cell><cell>44.7 ? 2.3</cell><cell>90.7 ? 0.8</cell><cell>14.0 ? 2.1</cell><cell>91.8 ? 0.6</cell></row><row><cell>Unif-reg (optimal)</cell><cell>53.9 ? 0.9</cell><cell>92.1 ? 0.2</cell><cell>36.7 ? 1.0</cell><cell>92.4 ? 0.3</cell></row><row><cell>Ours (HAR)</cell><cell>63.5 ? 0.8</cell><cell>94.3 ? 0.2</cell><cell>42.4 ? 0.7</cell><cell>94.0 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Top-1 validation accuracy (averaged over 3 runs) of ResNet-32 on heteroskedastic CIFAR-10 and CIFAR-100 for the noisy classes and the clean classes.</figDesc><table><row><cell>Dataset</cell><cell cols="2">CIFAR-10</cell><cell cols="2">CIFAR-100</cell></row><row><cell>Method</cell><cell cols="4">Avg. Noisy Cls. Avg. Clean Cls. Avg. Noisy Cls. Avg. Clean Cls.</cell></row><row><cell>ERM</cell><cell>68.6 ? 0.2</cell><cell>93.6 ? 0.2</cell><cell>65.3 ? 0.3</cell><cell>67.8 ? 0.2</cell></row><row><cell>Co-teaching</cell><cell>64.7 ? 0.4</cell><cell>89.1 ? 0.3</cell><cell>59.8 ? 0.4</cell><cell>65.3 ? 0.3</cell></row><row><cell>INCV</cell><cell>76.7 ? 0.6</cell><cell>93.0 ? 0.2</cell><cell>66.2 ? 0.3</cell><cell>68.6 ? 0.2</cell></row><row><cell>MentorNet</cell><cell>71.1 ? 0.4</cell><cell>93.7 ? 0.2</cell><cell>65.9 ? 0.3</cell><cell>67.5 ? 0.3</cell></row><row><cell>L2RW</cell><cell>70.1 ? 0.3</cell><cell>92.5 ? 0.3</cell><cell>65.1 ? 0.5</cell><cell>67.0 ? 0.3</cell></row><row><cell>MW-Net</cell><cell>75.0 ? 0.3</cell><cell>94.4 ? 0.2</cell><cell>65.7 ? 0.3</cell><cell>69.1 ? 0.2</cell></row><row><cell>GCE</cell><cell>62.6 ? 1.1</cell><cell>90.2 ? 0.2</cell><cell>61.2 ? 0.6</cell><cell>66.9 ? 0.2</cell></row><row><cell>DMI</cell><cell>73.2 ? 0.7</cell><cell>90.8 ? 0.2</cell><cell>64.8 ? 0.5</cell><cell>67.1 ? 0.2</cell></row><row><cell>Unif-reg (? = 0.1)</cell><cell>77.5 ? 0.6</cell><cell>92.3 ? 0.2</cell><cell>69.3 ? 0.5</cell><cell>66.6 ? 0.3</cell></row><row><cell>Unif-reg (optimal)</cell><cell>75.3 ? 0.3</cell><cell>94.1 ? 0.2</cell><cell>68.5 ? 0.3</cell><cell>68.6 ? 0.2</cell></row><row><cell>Ours (HAR)</cell><cell>80.7 ? 0.3</cell><cell>94.5 ? 0.2</cell><cell>74.2 ? 0.3</cell><cell>69.3 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Top-1 validation errors of ResNet-32 on imbalanced CIFAR-10 and CIFAR-100.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">Imbalanced CIFAR-10</cell><cell></cell><cell cols="3">Imbalanced CIFAR-100</cell><cell></cell></row><row><cell>Imbalance Type</cell><cell cols="2">long-tailed</cell><cell>step</cell><cell></cell><cell cols="2">long-tailed</cell><cell>step</cell><cell></cell></row><row><cell>Imbalance Ratio</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell></row><row><cell>ERM</cell><cell cols="8">29.64 13.61 36.70 17.50 61.68 44.30 61.45 45.37</cell></row><row><cell>Focal</cell><cell cols="8">29.62 13.34 36.09 16.36 61.59 44.22 61.43 46.54</cell></row><row><cell>CB Focal</cell><cell cols="8">25.43 12.90 39.73 16.54 63.98 42.01 80.24 49.98</cell></row><row><cell>LDAM-DRW</cell><cell cols="8">22.97 11.84 23.08 12.19 57.96 41.29 54.64 40.54</cell></row><row><cell>BBN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Validation accuracy of ResNet-50 when tuning the regularization strength on mini WebVision. HAR stands out of the trade-off constraint of fitting and generalization.</figDesc><table><row><cell></cell><cell>Train Acc</cell><cell>Val Acc</cell></row><row><cell>Reg Strength</cell><cell cols="2">Top 1 Top 5 Top 1 Top 5</cell></row><row><cell>0</cell><cell cols="2">69.01 88.64 59.40 80.84</cell></row><row><cell cols="3">Unif-reg (? = 0.01) 68.96 88.54 64.32 86.11</cell></row><row><cell cols="3">Unif-reg (? = 0.02) 67.02 87.51 64.40 85.92</cell></row><row><cell cols="3">Unif-reg (? = 0.05) 65.11 86.33 65.80 86.84</cell></row><row><cell>Unif-reg (? = 0.1)</cell><cell cols="2">63.35 84.98 65.04 86.56</cell></row><row><cell>Adaptive (HAR)</cell><cell cols="2">69.12 88.41 69.20 88.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Validation accuracy of InceptionResNet-v2 on WebVision and ImageNet validation sets. HAR demonstrates significant improvements over the previous state-of-the-arts.</figDesc><table><row><cell>Train</cell><cell></cell><cell cols="2">mini WebVision</cell><cell></cell><cell></cell><cell cols="2">full WebVision</cell><cell></cell></row><row><cell>Test</cell><cell cols="2">WebVision</cell><cell cols="2">ImageNet</cell><cell cols="2">WebVision</cell><cell cols="2">ImageNet</cell></row><row><cell>Method</cell><cell cols="8">Top 1 Top 5 Top 1 Top 5 Top 1 Top 5 Top 1 Top 5</cell></row><row><cell>ERM</cell><cell>62.5</cell><cell>80.8</cell><cell>58.5</cell><cell>81.8</cell><cell>69.7</cell><cell>87.0</cell><cell>62.9</cell><cell>83.6</cell></row><row><cell>Co-teaching</cell><cell>63.6</cell><cell>85.2</cell><cell>61.5</cell><cell>84.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>INCV</cell><cell>65.2</cell><cell>85.3</cell><cell>61.6</cell><cell>85.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MentorNet</cell><cell>63.0</cell><cell>81.4</cell><cell>57.8</cell><cell>79.9</cell><cell>70.8</cell><cell>88.0</cell><cell>62.5</cell><cell>83.0</cell></row><row><cell>Ours (HAR)</cell><cell>75.5</cell><cell>90.7</cell><cell>70.3</cell><cell>90.0</cell><cell>75.0</cell><cell>90.6</cell><cell>67.1</cell><cell>86.7</cell></row><row><cell cols="9">version that performs even better empirically. Recently Song et al. (2020) discovers that it is not</cell></row><row><cell cols="9">necessary to maintain two networks. Removing examples whose training loss exceeds a certain</cell></row><row><cell cols="7">threshold before learning rate decay can also get robust performance.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Top-1 validation accuracy (averaged over 3 runs) of ResNet-32 on heteroskedastic and imbalanced CIFAR-10.</figDesc><table><row><cell>Imbalance ratio</cell><cell>10</cell><cell></cell><cell>100</cell><cell></cell></row><row><cell>Method</cell><cell>Noisy Cls.</cell><cell>Rare Cls.</cell><cell>Noisy Cls.</cell><cell>Rare Cls.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at https://github.com/kaidic/HAR. 2 SeeFigure 4of<ref type="bibr" target="#b18">(Li et al., 2017)</ref>, the number of votes for each example indicates the level of uncertainty of that example.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that we integrate the error without weighting because we are interested in the balanced test performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>Toyota Research Institute ("TRI") provided funds and computational resources to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. YC is supported by Stanford Graduate Fellowship. TM acknowledges support of Google Faculty Award. The work is also partially supported by SDSI and SAIL at Stanford.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS OF THEOREM 1</head><p>We prove a general theorem here. In particular, we have the general theorem below. Theorem 2. Assume that f , q, I ? W 2 2 . Suppose (1) (a, y) is convex and three times continously differentiable with respect to a, (2) there exist constants 0 &lt; c &lt; C &lt; ? such that c ? I(X) ? C almost surely, and ? a (f (X), Y ) satisfies E[ |X] = 0, E[ 2 |X] = I(X) and E[ 4 |X] &lt; ? almost surely. Let r(t) = ?1/(q(t)I(t)) and L 0 = ? ?? 1 4 exp(?2|t|)dt. If we choose ? = C 0 n ?2/5 for some constant C 0 &gt; 0, the asymptotic mean squared error off by equation 4 is</p><p>in probability, where C n is a scalar that only depends on n.</p><p>It is easy to check that the logistic loss satisfies the condition of the theorem.</p><p>The proof strategy of Theorem 2 is adopted from the proof of Theorem 2 of <ref type="bibr" target="#b37">(Wang et al., 2013)</ref> by generalizing it from the least square loss to logistic loss.</p><p>The high level idea is to reformulatef as solutions to ordinary differential equations. Let (? v , h v ) be the (normalized) solution of the following equation</p><p>We define the the leading term off ? f as S n,? (f ) as</p><p>By Proposition 2.1 and Theorem 3.4 of <ref type="bibr" target="#b31">Shang et al. (2013)</ref>, we have</p><p>Following the same proof of Theorem 2 of <ref type="bibr" target="#b37">(Wang et al., 2013)</ref>, we can simplify the definition of K t and W ? as</p><p>where</p><p>Plugging equation 11 into equation 10, we then have</p><p>We develop our core algorithm in PyTorch <ref type="bibr" target="#b26">(Paszke et al., 2017)</ref>.</p><p>Implementation details for CIFAR. We follow the simple data augmentation used in <ref type="bibr" target="#b11">(He et al., 2016)</ref> with only random crop and horizontal flip. We use ResNet-32 as our base network and repeat all experiments for 3 runs. We use standard SGD with momentum of 0.9, weight decay of 1 ? 10 ?4 for training. The model is trained with a batch size of 128 for 120 epochs. We anneal the learning rate by a factor of 10 at 80 and 100 epochs. We group the data by class labels, and by default we split D equally and randomly into D train and D val . As for the Jacobian regularizer, we sum over the frobenius norm of the gradients of all normalization layers' (BN by default) activations with respect to the classification loss. For experiments of HAR, we tune ? so that the largest enforced regularization strength (?? i ) is 0.1. We train each model with 1 NVIDIA GeForce RTX 2080 Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours (HAR)</head><p>76.1 ? 0.8 72.1 ? 1.0 73.0 ? 1.6 26.1 ? 0.8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 COMPARING THE EFFECT OF WEIGHTS ON LOSSES AND REGULARIZERS</head><p>As discussed in Section 4, we train several classifiers with alternative weights selection scheme which are not optimal. We consider the following two alternatives.</p><p>(1) random: we draw the weights from a uniform distribution with the same mean as the weights of MW-Net and HAR.</p><p>(2) inverse: we take the inverse of the weights learned by MW-Net and HAR and then normalize them to ensure the average reweighting/regularization strength remains the same.</p><p>We conducted experiments on the heteroskedastic CIFAR-10 introduced in Section ?? and the results are summarized in <ref type="table">Table 9</ref>. We could conclude that changing the weights of the regularizer is a more conservative adaption and less susceptible to uncertain estimation than reweighting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 VISUALIZATION</head><p>In order to better understand how the proposed HAR works on real-world heteroskedastic datasets, we plot the per-class key statistics used by HAR and validation errors in <ref type="figure">Figure 4</ref>. We observe that HAR outperforms the tuned uniform regularization baseline on the majority of the classes.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Identifying mislabeled training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Carla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="131" to="167" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1565" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">Ben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning with bounded instance-and label-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotagiri</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03768</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Fr?nay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="845" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinglong</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the power of curriculum learning in training deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2535" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple and effective regularization methods for training on noisily labeled data with generalization guarantee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11680</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning from binary labels with instance-dependent corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.00751</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coresets for robust training of neural networks against noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Baharan Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Parhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nowak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02333</idno>
		<title level="m">Minimum&quot; norm&quot; neural networks are splines</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatially adaptive smoothing splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pintore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Speckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">C</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="125" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How do infinite width bounded norm networks look in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Evron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Local and global asymptotic inference in smoothing spline models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuofeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2608" to="2638" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meta-weightnet: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1917" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust large margin deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sokoli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R D</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4265" to="4280" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Robust and on-the-fly dataset denoising for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lunjia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10647</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive piecewise polynomial estimation via trend filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="285" to="323" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Smoothing splines with varying smoothing parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglai</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="955" to="970" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic curriculum learning for imbalanced data classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5017" to="5026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Data-dependent sample complexity of deep neural networks via lipschitz augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9722" to="9733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improved sample complexities for deep networks and robust classification via an all-layer margin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04284</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Instance pruning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="400" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">L_dmi: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6222" to="6233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Using qualitative hypotheses to identify inaccurate data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toyoaki</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="119" to="145" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9719" to="9728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">2015) with 256 units followed with 0.5 dropout before the linear classifier. The network is trained for 20 epochs with Adam optimizer (Kingma &amp; Ba, 2014). For HAR, we tune ? so that the largest enforced regularization strength</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Implementation details for IMDB-review. We train a two-layer bidirectional LSTM</title>
		<imprint/>
	</monogr>
	<note>?? i ) is 0.1. We train each model with 1 NVIDIA GeForce RTX 2080 Ti</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">For mini WebVision, We train the network for 90 epochs using standard SGD with a batch size of 128. The initial learning rate is 0.1 and is annealed by a factor of 10 at epoch 60 and 90. For full WebVision, We train the network for 50 epochs using standard SGD with a batch size of 256. The initial learning rate is 0.1 and is annealed by a factor of 10 at epoch 30 and 40. For experiments of HAR, we tune ? so that the largest enforced regularization strength</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>2016) including random crop and horizontal flip. ?? i ) is 0.1. We train each model with 8 NVIDIA Tesla V100 GPUs</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Because the pre-trained model only trains on half of the training data and is only done once, the run-time of HAR is at most twice of the time for ERM. Many baselines in our paper use sophisticated pipelines and are slower than HAR. For example, INCV trains 2 models simultaneously for 4 times from random initialization to get a clean training set. MW-Net has a very slow convergence rate</title>
		<imprint/>
	</monogr>
	<note>Runtime analysis. which is a common issue for meta-learning</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Additional</forename><surname>Results</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">C.1 SIMULATING HETEROSKEDASTIC NOISE ON IMDB-REVIEW</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">To simulate heteroskedastic noise for this binary classification problem, we project 5% of the labels of negative reviews to positive, and 40% in the reverse direction. Table 6 summarizes the results. The proposed HAR outperforms the ERM baseline with various strength of uniform regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IMDB-review dataset has a total of</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
	<note>000 positive and 25,000 negative reviews) movie reviews for binary sentiment classification</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Top-1 validation accuracy (averaged over 3 runs) on heteroskedastic IMDB-review dataset</title>
	</analytic>
	<monogr>
		<title level="j">Reg Strength Acc. of neg. reviews Acc. of pos. reviews Mean Acc</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">There are also noisy examples that contain multiple objects, or are more ambiguous in terms of indentity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EVALUATION ON CIFAR-100 WITH REAL-WORLD HETEROSKEDASTICITY It is acknowledged that CIFAR-100 training set contains noisy examples</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>We find that HAR can improve over uniform regularization on the well-studied CIFAR-100. due to its heteroskedasticity and the results are reported in Table 7</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<title level="m">Top-1 validation accuracy (average over 3 runs) of ResNet-32 on the original CIFAR-100</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
