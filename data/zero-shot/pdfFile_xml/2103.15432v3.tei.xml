<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards High Fidelity Monocular Face Reconstruction with Rich Reflectance using Self-supervised Learning and Ray Tracing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Dib</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">InterDigital R&amp;I</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Th?bault</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">InterDigital R&amp;I</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Ahn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">InterDigital R&amp;I</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe-Henri</forename><surname>Gosselin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">InterDigital R&amp;I</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max-Planck-Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Chevallier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">InterDigital R&amp;I</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards High Fidelity Monocular Face Reconstruction with Rich Reflectance using Self-supervised Learning and Ray Tracing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Equal contribution sonalized diffuse and specular albedos, a more sophisticated illumination model and a plausible representation of self-shadows. This enables to take a big leap forward in reconstruction quality of shape, appearance and lighting even in scenes with difficult illumination. With consistent face attributes reconstruction, our method leads to practical applications such as relighting and self-shadows removal. Compared to state-of-the-art methods, our results show improved accuracy and validity of the approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. A deep neural network trained in a self-supervised manner with a differentiable ray tracer estimates a complete set of facial attributes -3D head pose, geometry, personalized albedo (diffuse and specular) from unconstrained monocular image. Reconstruction based on these attributes enables a variety of applications, such as relighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Robust face reconstruction from monocular image in general lighting conditions is challenging. Methods combining deep neural network encoders with differentiable rendering have opened up the path for very fast monocular reconstruction of geometry, lighting and reflectance. They can also be trained in self-supervised manner for increased robustness and better generalization. However, their differentiable rasterization-based image formation models, as well as underlying scene parameterization, limit them to Lambertian face reflectance and to poor shape details. More recently, ray tracing was introduced for monocular face reconstruction within a classic optimization-based framework and enables state-of-the art results. However, optimization-based approaches are inherently slow and lack robustness. In this paper, we build our work on the aforementioned approaches and propose a new method that greatly improves reconstruction quality and robustness in general scenes. We achieve this by combining a CNN encoder with a differentiable ray tracer, which enables us to base the reconstruction on much more advanced per-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fast and accurate image-based face reconstruction has many applications in several domains including rig based realistic videoconferencing, interactive AR/VR experiences and special effects for professionals like facial attribute manipulation/transfer or relighting. Also, supporting unconstrained pose and in-the-wild capture conditions without specific hardware, such as multi-view setup, allows for enhanced flexibility and extended applicability. However, captured images reflect the complex interaction between light and faces including shadows and specularities, which poses a real challenge for face reconstruction. Speed is also a key factor for interactive scenario and other real-time use cases. Great multi-view approaches exist ( <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>), but they may not be easily applied in many applications such as VR or movies / special effects. Significant progress has been made on monocular face reconstruction where most methods resort to some form of parametric prior; high-quality analysis-by-synthesis monocular optimization methods exist ( <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>), but besides being rather slow, they would fail for difficult head poses and lighting conditions. More recently, and to improve this robustness against lighting conditions, <ref type="bibr" target="#b8">[9]</ref> introduced ray tracing for face reconstruction within an optimization-based framework. But the quality of their reconstruction remains sensitive to the landmarks used for initialization.</p><p>Real-time analysis-by-synthesis approaches have also been presented, however they often sacrifice reconstruction details. To increase reconstruction efficiency, CNN based approaches ( <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>) that directly regress 3D reconstruction parameters from images have been investigated. To overcome the challenge of creating large amounts of labeled data, while enabling reconstruction on the basis of meaningful scene parameters, methods combining CNNs with differentiable image formation models trained in a selfsupervised way have been presented ( <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>). They enable reconstruction performance in the range of milliseconds, and can be applied to more general scenes and subjects ( <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>). However, even the best of these highly efficient monocular reconstruction methods fall short of the quality and robustness requirements expected in professional visual effects (VFX) pipelines. They rely on simple parametric diffuse reflectance models based on lowfrequency spherical harmonics (SH) illumination model, whereas more detailed models would be needed to reconstruct at the level of quality that is typically required. The inability to model self-shadows is also a prime reason for their instability under challenging scene conditions.</p><p>To overcome these limitations, we present a new approach that is the first to jointly provide the following capabilities: it enables monocular reconstruction of detailed face geometry, spatially varying face reflectance and complex scene illumination at very high speed on the basis of semantically meaningful scene parameters. To achieve this, our model resorts to a parametric face and scene model that represents geometry using 3DMM statistical model, illumination as high-order spherical harmonics and reflectance model with diffuse and specular components. Our new CNN-based approach can be trained in a self-supervised way on unlabeled image data. It features a CNN encoder projecting the input image into the parametric scene representation. We also use an end-to-end differentiable ray tracing image formation model which, in contrast to earlier rasterization-based models, provides a more accurate light-geometry interaction and is able to synthesize images with complex illumination accounting for self-shadows. To the best of our knowledge, this is the first time a differentiable ray tracer is used for deep-based face reconstruction in an inverse-rendering setup. While ray tracing enables <ref type="bibr" target="#b8">[9]</ref> to improve the state of the art, their method is based on costly and slow iterative optimization and the final reconstruction remains sensitive to the quality of the landmarks. Our method overcomes these limitations: it absorbs the complexity of ray tracing at training time, achieves robust and competitive results with near real-time performance, and, being completely independent of landmarks at test time, is more suitable for in-the-wild conditions. Finally, with an appropriate training strategy, our method is the first self-supervised method to achieve robust face reconstruction in challenging lighting conditions and captures person-specific shadow-free albedo details (such as facial hair or makeup) not restricted by the 3DMM space. Our rich and consistent facial attributes reconstruction naturally allows various types of applications such as relighting, light/albedo edit and transfer. Our comparison with recent state-of-the-art methods shows improved robustness, accuracy and versatility of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>While methods such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> produce highquality face reconstruction from multi-view camera and/or from multi-light illumination setup, they are not applicable for in-the-wild images. We focus therefore on monocular image-based face reconstruction approaches that do not require any external hardware setup.</p><p>Statistical morphable model To make the highly ill-posed problem of monocular face reconstruction tractable, statistical priors have been introduced <ref type="bibr" target="#b14">[15]</ref> such as 3D morphable models (3DMMs) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. 3DMM was the main building block for most recent approaches because of its efficiency. However, 3DMM limits face reconstruction to a low-dimensional space and does not capture person specific details (such as beards and makeup), and its statistical albedo prior bakes some illumination in it. Recently, to overcome these limitations, <ref type="bibr" target="#b18">[19]</ref> proposes a drop-in replacement to 3DMM statistical albedo model with more sophisticated diffuse and specular albedo priors. In this work, we base our reconstruction on the 3DMM geometry and albedo prior (diffuse and specular) of <ref type="bibr" target="#b18">[19]</ref> and we learn an increment, similarly to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9]</ref>, on top of the albedo prior to capture more person-specific details outside of the statistical prior space.</p><p>Optimization-based approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9]</ref> use optimization formulation for face (geometry and diffuse albedo) reconstruction. In <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>, a 3DMM based statistical priors act as optimization regularizer while minimizing a photo-consistency loss function. While such methods work well for controlled scene conditions, often they do not generalize well for in-the-wild images scenarios and can be computationally expensive.</p><p>Illumination and reflectance models The aforementioned methods generally use low-order spherical harmonics (SH) to model light and assume Lambertian surface, while our method uses higher-order SH in order to better model light interaction with non-Lambertian surface (with diffuse and specular). While <ref type="bibr" target="#b21">[22]</ref> extracts diffuse and specular albedos from a single image using SH illumination, they do not explicitly model self-shadows and show only results in controlled conditions. <ref type="bibr" target="#b8">[9]</ref> introduced a novel virtual light stage to model area lights and used a Cook-Torrance BRDF (diffuse, specular and roughness) to model skin reflectance.</p><p>Differentiable rendering A simple and efficient vertexwise differentiable rendering is proposed in <ref type="bibr" target="#b9">[10]</ref>. Two shortcomings of this approach are the simple Lambertian BRDF and its inability to handle self-shadows. To address these limitations, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref> introduce the use of differentiable ray tracing for face reconstruction. It can handle more complex illumination and BRDF models, and naturally accounts for self-shadows. However, in addition to being computationally expensive, differentiable ray tracing exhibits noisy gradients on the geometry edges as they are sampled by very few points (solutions, such as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, exist but remain computationally expensive).</p><p>Deep learning based approaches When trained on large corpus, convolution neural networks (CNNs) are well proven face deep representation decoders and encoders such as <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. Several novel approaches have been proposed ( <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>). <ref type="bibr" target="#b9">[10]</ref> was among the first to show the use of self-supervised autoencoder-like inverse rendering based architecture to infer semantic attributes. These unsupervised learning approaches have the potential to leverage vast amount of face images. They are however limited by their differentiable rasterizer, relying on a low-order spherical harmonics parametrization and a pure-Lambertian skin reflectance model, which limits their performance and reconstruction quality under challenging lighting conditions. [10] relies on diffuse albedo obtained from 3DMM, which restricts generalization of the true diversity in face reflectance. <ref type="bibr" target="#b10">[11]</ref> uses self-supervision to learn a corrective space to capture more person-specific albedo/geometry details outside of the 3DMM space. More recently, <ref type="bibr" target="#b29">[30]</ref> learns user-specific expression blendshapes and dynamic albedo maps by predicting personalized corrections on top of a 3DMM prior. However, these methods do not separate diffuse and specular albedos, which get mixed in their final estimated reflectance. They also do not show reconstruction under challenging lighting conditions. Our method uses self-supervision to train a CNN and extracts personalized diffuse and specular albedos, out-side of the 3DMM space. In contrast to the aforementioned methods, our method uses high-order SH for better light approximation and an image formation layer based on a differentiable ray tracing that handle advanced lighting and self-shadows. All this contributes to a robust face reconstruction even in scenes with challenging lighting conditions. Our loss functions and training strategy allow us to obtain personalized diffuse and specular albedos, outside the span of 3DMM, with faithful separation between them avoiding baking residual self-shadows in the albedo. More recently, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> use supervised training to learn a nonlinear 3DMM model to produce more detailed albedo and geometry. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> show vast improvements in geometry reconstruction. <ref type="bibr" target="#b33">[34]</ref> improves on these approaches further by inferring mesoscopic facial attributes given monocular facial images, an attribute we do not model in our final reconstruction. Finally, <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b35">[36]</ref> use image-to-image translation networks to directly regress diffuse and specular albedos but they do not model light. Self-shadows can be observed in the albedo they obtain, whereas we model self-shadows implicitly. Additionally, these methods require ground-truth data to train the generative model, which is not easy to acquire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our method is inspired by the work of <ref type="bibr" target="#b8">[9]</ref>, which achieves face reconstruction with personalized albedos outside the statistical albedo prior space and in challenging lighting conditions using ray-tracing within an optimization framework. While their method achieves state-of-the art results, it lacks robustness against initial starting point and is inherently slow, which makes their method not suitable for in-the-wild conditions. Our method overcomes these limitations by leveraging the generalization capacity of selfsupervised learning.</p><p>Our method is composed of two stages as shown in Figure 2. In stage I, an input image I R is passed through a deep network E 1 (a pre-trained ResNet-152) followed by a fully connected layer to predict a semantic attribute vector ? for 3DMM shape (?), expression blendshapes (?), camera pose (? = {R, T }, composed of rotation and translation), light (?) modeled with 9 spherical harmonics bands, albedo prior (?), which encodes diffuse and specular priors from <ref type="bibr" target="#b18">[19]</ref>. Statistical diffuse D and specular S textures are obtained from ?. These parameters are fed to a differentiable ray tracer to generate a ray traced image I S 1 . This encoder E is trained end-to-end in a non-supervised manner to obtain a 'base' reconstruction. At this level, the estimated albedos only capture low-frequency skin attributes. In stage II, to enhance these albedo priors, we train two additional decoders, D 1 and D 2 , in a self-supervised way to estimate The challenges is to avoid mixing the diffuse and specular parts and baking unexplained shadows (residual) in the personalized albedos. In the next section, we briefly describe the scene attributes used for image formation, and then we discuss the training of the different networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scene attributes</head><p>Geometry We use <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37]</ref>'s statistical face model, where identity is given by e = a s + ? s ?. e a vector of face geometry vertices with N vertices. The identity-shape space is spanned by ? s ? R 3N ?Ks composed of K s = 80 principal components of this space. ? ? R Ks describes weights for each coefficient of the 3DMM and a s ? R 3N is the average face mesh. We use linear blendshapes to model face expressions over the neutral identity e. v = e + ? e ?, where v is the final vertex position displaced from e by blendshape weights vector ? ? R Ke and ? e ? R 3N ?Ke composed of K e = 75 principal components of the expression space.</p><p>Reflectance A simplified Cook-Torrance BRDF <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, with a constant roughness term, is used to model skin reflectance. In contrast to Lambertian BRDF, the Cook-Torrance BRDF can model specular reflections, and thus defines for each vertex v i a diffuse c i ? R 3 and a specular s i ? R 3 albedos. The statistical diffuse albedo c ? R 3N is derived from 3DMM as c = a r + ? r ?, where ? r ? R 3N ?Kr defines the PCA diffuse reflectance with K r = 80 and ? ? R Kr the coefficients. a r is the average skin diffuse reflectance. Similarly to <ref type="bibr" target="#b8">[9]</ref>, we employ the statistical specular prior introduced by <ref type="bibr" target="#b18">[19]</ref> to model the specular reflectance:</p><formula xml:id="formula_0">s = a b + ? b ? where ? b ? R 3N</formula><p>?Kr defines the PCA specular reflectance. a b is the average specular reflectance. We use the same coefficients ? to sample for the diffuse and specular albedos as suggested by <ref type="bibr" target="#b18">[19]</ref>. In unwrapped (UV) image texture space, D ? R M ?M ?3 and S ? R M ?M ?3 are the statistical diffuse and specular albedos, with M ? M being the texture resolution.</p><p>Illumination Specular reflections are much more sensitive to light direction than diffuse reflections. So while in the literature 3-order spherical harmonics have been widely used in conjunction with Lambertian BRDF, we resort to nine SH bands with the Cook-Torrance model. We actually found SH to be better suited to our deep learning framework than an explicit spatial representation as the one used by <ref type="bibr" target="#b8">[9]</ref>. We show in section B that high-order SH produces better shadows estimation than low-order SH. To use with the ray tracer, an environment map of 64 ? 64 is derived from this light representation. We define ? ? R 9?9?3 as the light coefficients to be predicted by the network E.</p><p>Camera We use the pinhole camera model with rotation R ? SO(3) and translation T ? R 3 . We define ? = {T, R} the parameters predicted by E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>First stage (Base reconstruction) In this stage, we use statistical geometry and albedo priors to obtain a first estimation of geometry, reflectance, light and camera attributes. We define ? = {?, ?, ?, ?, ?} as the semantic attribute vector used by the differentiable ray tracer to obtain a ray traced image I S 1 . We train a deep encoder E to directly regress ?. We use a pixel-wise photo-consistency loss between I S 1 and the input image I R :</p><formula xml:id="formula_1">E ph (?) = i?I |p S i (?) ? p R i | (1) Here, p S i , p R i ? R 3</formula><p>are ray traced and real image pixel colors, respectively. Rendered pixel colors are given by</p><formula xml:id="formula_2">p S i = F(?),</formula><p>where F is the Monte Carlo estimator of the rendering equation <ref type="bibr" target="#b39">[40]</ref>. We also define a sparse landmark loss E land , which measures the distance between the projection of L = 68 facial landmarks and their corresponding pixel projections z l on the input image (more details in <ref type="bibr" target="#b8">[9]</ref>). These landmarks are obtained using an off-the-shelf landmarks detector <ref type="bibr" target="#b40">[41]</ref>. During training, we minimize the following energy function: argmin</p><formula xml:id="formula_3">(?,?,?,?,?) E d (?) + E p (?, ?) + E b (?) (2) E d (?) = E ph (?) + ? 1 E land (?)</formula><p>and E p is the statistical face (shape and albedo) prior ( <ref type="bibr" target="#b8">[9]</ref>) that regularizes against implausible face geometry and reflectance deformations.</p><formula xml:id="formula_4">E b (?) is a soft-box constraint that restricts ? to range [0, 1].</formula><p>Second stage (personalized albedos) In <ref type="figure" target="#fig_1">Figure 3</ref> (yellow boxes) we show the final reconstruction together with the estimated statistical albedos (D and S) obtained by the base reconstruction of stage I. This result misses subject specific skin features, like beards and make-up, as 3DMM does not support them. We aim to personalize the albedo. However, the challenges are to avoid mixing the diffuse and specular parts and to avoid baking unexplained shadows (residual) in the personalized albedos. We train two additional networks D 1 and D 2 , which take as input the latent space of E and estimate a diffuse and specular increments ? d and ? s which are added to D and S, respectively.D = D +? d and S = S + ? s are then used by the differentiable ray tracer to generate a new synthetic image I S 2 . Predicting an increment on statistical texture priors instead of directly estimating a complete texture is important to force E to produce a good albedo prior while at the same time estimating a good increment over these priors. We define? = {?, ?, ?, ?,D,?} which is used in the photo-consistency loss (eq 7). The following energy function is minimized:</p><formula xml:id="formula_5">argmin (? d ,?s) E d (?) + w 1 (E s (D) + E s (?))+ w 2D E c (D, D) + w 2S E c (?, S)+ w 3 (E m (D) + E m (?))+ w 4 (E b (D) + E b (?))<label>(3)</label></formula><p>where E b is the soft box constraint that restricts the albedos to remain in an acceptable range [0, 1]. E m is a constraint term which ensures local smoothness at each vertex, with respect to its first ring neighbors in the UV space, and is given by</p><formula xml:id="formula_6">E m (?) = xj ?Nx i ||(?(x j ) ??(x i )|| 2 2 , where N xi is 4-pixel neighborhood of pixel x i .</formula><p>We use regularization similar to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9]</ref> to prevent residual shadows to leak into the albedos. E s (?) = i?M |?(x i ))?flip(?(x i )))| 1 is a symmetry constraint, where flip() is the horizontal flip operator. E c (?, A) is a consistency regularizer, which weakly regularizes? with respect to the previously optimized albedo A based on the chromaticity ? of each pixel in the texture given by Training strategy Obtaining faithful and personalized diffuse and specular albedos (outside of the statistical prior space) in uncontrolled lighting conditions is an ill-posed problem. For instance, one can easily overfit the diffuse albedo increment ? d to explain the remaining information in the input image. To avoid this case, we proceed with the following strategy to separate the diffuse and specular albedos: After training E for few epochs, we fix E and start training D 1 and D 2 with a high regularization weight w 2D for the diffuse consistency regularizer in order to keepD closer to D. Next, we progressively relax the w 2D constraint during the training to let the diffuse increment ? s capture more details. Finally, we train for all networks jointly (E, D 1 and D 2 ). So, for a given image, we generate two images I S 1 and I S 2 and minimize the energy functions in 2 and 3 respectively and back-propagate over the whole attributes. This last step is important to stress E to produce better priors, light and pose while at the same time pushing D 1 and D 2 to capture more details in the albedos. </p><formula xml:id="formula_7">E c (?, A) = i?M |?(?(x i )) ? ?(A(x i ))| 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>For training, we used a total of 250K images, partially from CelebA dataset <ref type="bibr" target="#b41">[42]</ref>. For E, we use a pre-trained ResNet-152, and both D 1 and D 2 networks use a cascade of 7 convolution layers. Ray tracing is based on the method of <ref type="bibr" target="#b24">[25]</ref>. Because ray tracing is very memory consuming, we use 256 ? 256 as resolution for the texture and the input images. The inference takes 54 ms (47 ms for E and 7 ms for D 1 , D 2 ). During training, we use 8 samples per pixels for ray tracing the images. Other implementation details can be found in the supplementary material (section I). Please note that images used in the figures were not used for training. The attributes estimated by our method are compatible with most rendering engines, nevertheless all results shown in the paper were rendered using ray tracing. <ref type="figure" target="#fig_1">Figure 3</ref> (red boxes) shows reconstruction results from inthe-wild images for different subjects with various face attributes, head pose and lighting conditions. Our method successfully captures the facial hair and the lipstick -outside of statistical albedo prior space -for subjects on the left and in the middle respectively. Subject on the right is under challenging lighting conditions. Our method robustly estimates meaningful albedos and avoid baking residual shadows in the final albedos. We also show on <ref type="figure" target="#fig_1">Figure 3</ref> the effectiveness of albedo personalization provided by the networks D 1 and D 2 (red box) to refine the estimated statistical priors obtained by E (yellow box) from outside of the statistical albedo prior space. More results are shown on <ref type="figure">Figure 1</ref> and in supplementary material (section 5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation</head><p>Low-order SH parameterization In this experiment, we compare 9-order with 3-order SH. Estimated illuminations are shown in <ref type="figure" target="#fig_2">Figure 4</ref> (first row). This shows that using higher-order spherical harmonics results in better shadow reconstruction, which prevents baking residual shadows in the albedo. In a similar experiment, 9-order SH has proven itself to be slightly better than 7-order.</p><p>Dual networks In this experiment, we trained a single network to regress diffuse ? d and specular ? s increments. As shown in <ref type="figure" target="#fig_2">Figure 4</ref> (second row), a single network produces poor albedo separation compared to our approach that uses two separate networks, possibly because the diffuse and specular components capture different skin features that interfere when using a single network. Vertex-based renderer In this experiment, we train the same architecture, except that we used a 'vertex-based' renderer, with the same illumination model (9-order SH) <ref type="bibr" target="#b42">[43]</ref> and the simplified Cook-Torrance BRDF (more details in supp. material, section 2). <ref type="figure" target="#fig_2">Figure 4</ref> (last row) shows that ray tracing produces smoother and natural projected-shadows (especially around the nose). This is because ray tracing can naturally models self-shadows while vertex-based renderer cannot. For instance, spherical harmonics (SH) coefficients are converted to an environment map (EM) for the use with ray tracing. Each pixel in EM acts as a light source at infinity. Shadows rays (rays shot from a surface point (P) towards a light source sampled from EM) are used to calculate a visibility mask for P. On the other hand, vertex-based renderers do not naturally model visibility of light sources. Work such as <ref type="bibr" target="#b43">[44]</ref> tries to solve for this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symmetry and consistency regularizers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Comparison</head><p>In the next, we compare, qualitatively and quantitatively, our method against recent methods and to the ground-truth Digital Emily project and NoW benchmark <ref type="bibr" target="#b44">[45]</ref>. <ref type="bibr">Figure 7</ref>. Comparison between methods - <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, and ours. The red arrows indicate the shadow baked in diffuse albedo. <ref type="figure" target="#fig_3">Figure 5</ref> shows comparison results of our method against <ref type="bibr" target="#b10">[11]</ref> (results and images are taken from authors paper). Both methods achieve visually comparable results on the final reconstructed images. However, our method separates diffuse and specular albedos while <ref type="bibr" target="#b10">[11]</ref> only estimates a reflectance map, which mixes diffuse and specular components. This makes our method more suitable for applications such as relighting. <ref type="figure" target="#fig_4">Figure 6</ref> shows comparison against <ref type="bibr" target="#b8">[9]</ref>. For the left subject, their method infers incorrect shape (around the mouth), head pose and illumination estimation. For the right subject, their estimated shape is inaccurate (right part of the head). This illustrates the sensitivity of <ref type="bibr" target="#b8">[9]</ref> to the landmarks quality (as reported by the authors). Our method, completely independent of landmarks, is more robust to the lighting conditions and generates more convincing results. In the supp. material (section 4), we show that our method achieves visually comparable results ([9] being slightly better) while being an order of magnitude faster (54 ms vs. 6.4 min). We note, that similarly to <ref type="bibr" target="#b45">[46]</ref>, combining both approaches by using our predicted attributes as initialization for the optimization produces better reconstruction quality. Finally, we note that <ref type="bibr" target="#b8">[9]</ref> can capture shadows projected by point lights, while our method can only handle lights at infinite distance; nevertheless, our training strategy, together with carefully designed loss functions helps our method from baking unexplained shadows in the final personalized albedos at the expense of some albedo details (please refer to supp. material for more results in challenging lighting conditions). <ref type="figure">Figure 7</ref> shows comparison against <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>. Results for <ref type="bibr" target="#b18">[19]</ref> are obtained using their open-source implementations. For <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, results are from original authors. <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b35">[36]</ref> methods do not estimate light and thus final reconstruction is not available. Because <ref type="bibr" target="#b34">[35]</ref> does not model light, they bake some shadows in the diffuse albedo for both subjects. <ref type="bibr" target="#b18">[19]</ref> estimates light using 3-order SH but their reconstruction is bounded by the statistical albedo space and cannot capture personalized albedos outside of this space. Compared to <ref type="bibr" target="#b12">[13]</ref>, our method has 'visually' better light estimation and smoother shadows (for first subject). Additionally, their diffuse and geometry have some artifacts visible around the nose. Also, their method does <ref type="table">Table 1</ref>. Vertex position and normal error ? and standard deviation ? for each method on all subjects. <ref type="bibr" target="#b34">[35]</ref> [36] </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometric comparison</head><p>We evaluate the quality of the reconstructed geometry on 23 images with 3D ground truth (GT) mesh from 3DFAW <ref type="bibr" target="#b46">[47]</ref>, AFLW2000 fitting package <ref type="bibr" target="#b47">[48]</ref>[49] <ref type="bibr" target="#b49">[50]</ref>, and from the wikihuman project 2 <ref type="bibr" target="#b50">[51]</ref>. We compared our method to <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9]</ref>  <ref type="figure">(Figure 8</ref> and <ref type="table">Table  1</ref>), using vertex position and normal errors calculated with respect to the GT meshes. <ref type="table">Table 1</ref> reports, for each method on all subjects, the average error (?) and standard deviation (?) for vertex position and normal direction. For vertexposition error, <ref type="bibr" target="#b8">[9]</ref> scores the best average error (0.174), while <ref type="bibr" target="#b12">[13]</ref> reports the second lowest measure. Comparing to the above methods, our method shows similar performance (0.181). We note that <ref type="bibr" target="#b12">[13]</ref> learns a non-linear 3DMM model to improve the geometry while our method, which only uses 3DMM geometry, achieves comparable results. Our score is also on par with optimization-based method <ref type="bibr" target="#b8">[9]</ref>, while being order of magnitude faster. For normal error, our method reports the second-best error (0.148), very close to <ref type="bibr" target="#b8">[9]</ref>. Also, our score is better than <ref type="bibr" target="#b12">[13]</ref> (0.159), since the mesh estimated by the latter has noise that appears around the nose (visible in <ref type="figure">Figure 7</ref> and <ref type="figure">Figure 8</ref>). We also evaluate our method on the NoW benchmark <ref type="bibr" target="#b44">[45]</ref> that only evaluates neutral mesh (no expression, albedo and pose evaluation). Nevertheless, we obtain very competitive results: 1.26/1.57/1.31mm (median/mean/std).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Digital Emily</head><p>As shown in <ref type="figure" target="#fig_7">Fig.9</ref>, we compare our method with Digital Emily <ref type="bibr" target="#b50">[51]</ref> ground truth (GT). We note that our method bakes some albedo in the estimated light (as shown in the recovered environment map). <ref type="figure">Figure 8</ref>. Vertex position and normal error for each method (from left <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and Ours) compared to GT mesh.</p><p>We also compare quantitatively our image reconstruction quality against state-of-the-art (see <ref type="table" target="#tab_0">Table 2</ref>). For each method, we compute SSIM <ref type="bibr" target="#b51">[52]</ref> and PSNR scores versus GT, for final render, diffuse, and specular. Since each method uses a different UV mapping, we compare the projection of the albedo on the input image (using the GT camera pose) and not on the unwrapped texture. For the 'Final' rendered image, our method is on par with the method of <ref type="bibr" target="#b8">[9]</ref> and achieves better performance than <ref type="bibr" target="#b18">[19]</ref>. Since <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b35">[36]</ref> do not estimate scene light, they do not have a final render image, so no comparison is available for these methods. For diffuse and specular albedos, <ref type="bibr" target="#b34">[35]</ref> is globally better than the other methods except for 'Diffuse SSIM' and 'Specular SSIM', where <ref type="bibr" target="#b8">[9]</ref> and our method measure higher similarity, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitations, Future works and Conclusion</head><p>Limitations Our data-driven method inherits the bias of the training data, which does not provide high diversity in lighting and facial expression (most subjects in the dataset are with smiling faces, eyes opened and well lit). This leads to sub-optimal reconstruction for less frequent expression/lighting (incorrect light estimation for left subject in <ref type="figure" target="#fig_8">Figure 10</ref>). This limitation can be mitigated by using a more balanced dataset while the method remains the same. Also, with images with extreme shadows some artifacts may appear in the estimated albedos (right subject in <ref type="figure" target="#fig_8">Figure 10</ref>).</p><p>Disentangling light color from skin color from a single image is an ill-posed problem and is not solved in this work. The limitations of 3DMM statistical albedo prior (unable to model all skin types e.g. non-Caucasian) make this separation even more difficult. Adding light color regularization, together with a more complete albedo prior can mitigate this. Future works Our method could be extended by using a non-linear morphable model such as <ref type="bibr" target="#b52">[53]</ref> to improve the geometry at finer level. Using a more complex skin reflectance model such as BSSRDF/dielectric materials <ref type="bibr" target="#b53">[54]</ref> is also interesting. Our method could also benefit from a work such as <ref type="bibr" target="#b54">[55]</ref>, which tackles the foreign (external) shadows challenge. Finally, our method can naturally extend to video-based reconstruction <ref type="bibr" target="#b13">[14]</ref>, which should improve the accuracy of the estimated facial attributes. Conclusion In this work, we address the problem of face reconstruction under general illumination conditions, an important challenge to tackle for in-the-wild face reconstruction. For this, we introduced the first deep-based and self-supervised method that achieves state-of-the art monocular face reconstruction in challenging lighting conditions. We build our work on recent methods, namely <ref type="bibr" target="#b9">[10]</ref> which combines deep neural networks with differentiable rendering and <ref type="bibr" target="#b8">[9]</ref> which uses ray tracing for face reconstruction within an optimization-based framework. Our method solves the limitations of <ref type="bibr" target="#b9">[10]</ref> by using a better light and BRDF models and captures personalized diffuse and specular albedos outside of 3DMM space, while being robust against harsh shadows. Our approach also solves the limitations of <ref type="bibr" target="#b8">[9]</ref> and achieves near-real time performance while at the same time being completely independent of landmarks at test time. Our method naturally benefits from large-scale unlabeled data-sets. By comparing to recent approaches, we achieve better results in terms of robustness in scenes with challenging lighting conditions, while producing plausible reconstruction of subject-specific albedos. Beyond its robustness to lighting conditions, the rich reflectance decomposition produced by our method is compatible with existing rendering engines and allows for several style -illumination and albedo -transfer, edit applications, avatar creation and relighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>We implemented the architecture using PyTorch <ref type="bibr" target="#b55">[56]</ref> with a GPU-enabled backend. Ray tracing is based on the method of <ref type="bibr" target="#b24">[25]</ref>, and for training we used Adam <ref type="bibr" target="#b56">[57]</ref> as optimizer with default parameters. We used images from CelebA dataset <ref type="bibr" target="#b41">[42]</ref> in addition to 40K images collected from the web for a total of 250K images. We keep 2K images for the validation. Images are aligned and cropped to a resolution of 256 ? 256. We trained E for 10 epochs, then we fixed E and trained D 1 and D 2 for 5 epochs. Finally we trained all networks jointly for 5 epochs. We set our regularization weights as following: landmarks weight ? 1 = 1, w i = 0.002, w c = 0.01, symmetry regularizer w 1 = 20, w 2S = 0.01, smoothness regularizer w 3 = 0.0001; and for w 2D , we start with w 2D = 0.5, and decrease it by a factor of 2 at each epoch. For E, we use a pre-trained ResNet-152 with latent space dimension equal to 1000. Both D 1 and D 2 networks use a cascade of 7 convolution layers. Because ray tracing is very memory consuming, we use a texture resolution of 256 ? 256 with batch size equal to 8 and input image of resolution 256 ? 256 to fit the GPU memory (12GB on a NVIDIA GeForce RTX 2080 Ti). For the learning rates, we use 1e ?6 for E and 1e ?7 for D 1 and D 2 . For training, it takes 15 hours to do a single epoch. During training, we use 8 samples per pixels for ray tracing the images. We experimented with different numbers of samples per pixel (spp) for ray tracing (8, 16 and 32 spp), but we did not obtain substantial improvements when using more than 8 spp, even though using 16 spp already made the training much slower. Additionally, as skin is generally not a highly specular surface, in our experiments, modeling self-geometry ray bounces did not lead to substantial gain in accuracy; thus we did not use it for training. The inference takes 54 ms (47 ms for E and 7 ms for D 1 , D 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vertex-based renderer implementation</head><p>In this section we provide implementation details of the vertex-based renderer that we used to compare against the ray tracer (please refer to section 5 in primary document).</p><p>The vertex based renderer computes the irradiance by evaluating spherical harmonics (SH) for each vertex of the face mesh. To model skin reflectance, we use a simplified Cook-Torrance BRDF, thus the final irradiance is the sum of diffuse and specular irradiance terms. For the diffuse term, a spatial convolution with the half-cosine is applied to the SH light representation. This corresponds to a multiplication of the SH coefficients (B lm ) of the light representation with SH coefficients (A l ) of the half-cosine function ( <ref type="bibr" target="#b42">[43]</ref>). For each vertex, the diffuse irradiance, B d , is obtained by evaluating the resulting SH:</p><formula xml:id="formula_8">B d (n i , c i ) = c i ? 8 l=0 l m=?l A l ? B lm ? Y lm (n i )<label>(4)</label></formula><p>where c i ? R 3 is the diffuse albedo of a vertex. n i ? R 3 is the vertex normal. The specular term is similarly obtained using a spatial convolution of the SH light representation with the BRDF kernel corresponding to the roughness (which is constant in the simplified Cook-Torrance BRDF model we use). The specular irradiance, B s , is obtained by evaluating the resulting SH:</p><formula xml:id="formula_9">B s (R i ) = 8 l=0 l m=?l S l ? B lm ? Y lm (R i )<label>(5)</label></formula><p>where R i is the reflection direction of the viewing vector W i according to the surface normal, and S l are the SH coefficients of the BRDF function corresponding to the roughness <ref type="bibr" target="#b42">[43]</ref>. The final irradiance B is equal to the sum of the diffuse and specular terms weighted by the specular intensity s i :</p><formula xml:id="formula_10">B(n i , c i , R i ) = (1 ? s i ) ? B d (n i , c i ) + s i ? B s (R i ) (6)</formula><p>s i ? R is the specular albedo. Finally, We use the following vertex-based photoconsistency loss to minimize during the training:</p><formula xml:id="formula_11">E ph (?) = N i=1 |B(n i , c i , R i ) ? I R (? ? C(v i ))|<label>(7)</label></formula><p>where N is the number of vertices, C(v i ) is the projection of vertex v i in the real image, equal to: R ?1 (v i ? T). ? is the perspective camera matrix that maps a 3D vertex to a 2D pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mesh difference</head><p>In this section, we provide more details on how the geometric error is calculated for each method (please refer to <ref type="table">Table 1</ref> in primary document).</p><p>The mean difference error is computed per-vertex on the entire mesh. We implement a 3D mesh evaluation protocol similar to <ref type="bibr" target="#b46">[47]</ref>. For computing the mesh difference, we first align a reconstructed mesh towards a ground truth (GT) mesh. Several feature points, namely, sparse correspondence points are defined on both the GT and reconstructed facial meshes, where vertices are minimally affected by the facial muscles. With the corresponding points ready on both meshes, we use a traditional least-square estimation introduced by <ref type="bibr" target="#b57">[58]</ref> to align the two meshes. After this alignment, we compute the distance from each vertex of a mesh to the other via a fast ray-triangle intersection method <ref type="bibr" target="#b58">[59]</ref>. The average error is computed for final difference between two meshes. <ref type="figure">Figure 11</ref> shows comparison results against the method of <ref type="bibr" target="#b8">[9]</ref>. For each subject, we show the final reconstruction, estimated diffuse, specular and light for each method. The first two subjects are from the authors of <ref type="bibr" target="#b8">[9]</ref>. Quite logically, the iterative optimization-based method of <ref type="bibr" target="#b8">[9]</ref> achieves slightly better reconstruction results and captures more details in the estimated albedos. This is because <ref type="bibr" target="#b8">[9]</ref> estimates and fine-tunes the facial and scene parameters specifically for each subject, while our method infers them directly without fine-tuning. Nevertheless, our method is almost on par with <ref type="bibr" target="#b8">[9]</ref> and can successfully handle some cases where <ref type="bibr" target="#b8">[9]</ref> falters. For instance, with the last two subjects of the <ref type="figure">Figure 11</ref>, in presence of shadows and strong expression, landmarks detector deliver less accurate initial starting points for the method of <ref type="bibr" target="#b8">[9]</ref> which consequently gets trapped in wrong local minima. This yields poor shape and artefacts in the estimated albedos (highlighted in red boxes). Our method does not suffer from this limitation, proves to be more robust and produces visually more plausible reconstruction. We note that <ref type="bibr" target="#b8">[9]</ref> estimates a roughness map, a parameter that we do not estimate. However, as reported by the authors, the missing statistical prior of the estimated roughness may sometimes yield to an over-fitting on this parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More comparison results</head><p>We show more qualitative comparison against <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b12">[13]</ref>, and <ref type="bibr" target="#b18">[19]</ref> in <ref type="figure" target="#fig_0">Figure 12</ref>. In <ref type="figure" target="#fig_1">Figure 13</ref>, we show more quantitative comparison against <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b12">[13]</ref>, and <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Face catalog</head><p>In <ref type="figure" target="#fig_2">Figure 14</ref>, we show more reconstruction results from in-the-wild images. For each subject we show the final reconstruction and the estimated diffuse, specular albedos and illumination. More results are in the accompanied video.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. More relighting examples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Method overview: A network E regresses semantic face attributes (shape, expression, light, statistical diffuse and specular albedos and camera). Two additional networks D1, D2 are used to estimate increments ? d , ?s on top of the statistical albedos to capture personalized reflectance (diffuse and specular) outside of the statistical prior space. A differentiable ray tracer is used for image formation. diffuse ? d and specular ? s increments to be added on top of the previously estimated textures, D and S, respectively. The resultant textures,D and?, are used to generate a new image I S 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Yellow box: Base reconstruction (Stage I section 3) with the estimated statistical albedo priors. Red box: Final reconstruction (Stage II section 3) with the final albedos and light.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Row I: Comparison of the estimated light using 3 and 9 SH bands. Row II: Comparison of a single network vs. our dual-network approach to estimate albedo increments. Row III: Comparison of results obtained with and without the symmetry and consistency regularizers. Row IV: Comparison of base vs. final geometry reconstruction. Row V: comparison of vertex-based renderer and ray tracing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Comparison against<ref type="bibr" target="#b10">[11]</ref> (subjects from authors paper).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Comparison against<ref type="bibr" target="#b8">[9]</ref> (right subject: authors paper).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 (</head><label>4</label><figDesc>row III) shows the importance of the regularizers used in equation 3. In fact, because our light model may not always perfectly capture the real shadows, these regularizers prevent from baking residual shadows in the albedo. Joint training Training E jointly with D 1 and D 2 (refer to section 3) improves the final geometry as shown in Figure 4 (row IV). The 'final' mesh better fits the input image compared to the 'base' mesh obtained from the base reconstruction (when training E only).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Ours Position error ? (cm) 0.236 0.184 0.176 0.174 0.181 Position error ? (cm) 0.114 0.072 0.065 0.064 0.069 Normal error ? (rad) 0.152 0.156 0.159 0.139 0.148 Normal error ? (rad) 0.051 0.043 0.046 0.046 0.048 not estimate the specular component. [36] produces convincing shadow-free diffuse albedo. However, they do not estimate light and head pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Our final, diffuse, and specular render images (Ours box) compared to GT (GT box). All GTs are rendered by Maya.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Limitations of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15</head><label>15</label><figDesc>and 16 show more relighting examples where the estimated illumination is replaced with an environmentmap. More relighting results are in the accompanied video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Comparison against<ref type="bibr" target="#b8">[9]</ref> More visual comparisons against state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 .</head><label>13</label><figDesc>More geometric comparisons against state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 .</head><label>14</label><figDesc>Face catalog of our reconstruction. For each subject, we show the input, final, diffuse, specular, and illumination. More results are in the accompanied video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 .</head><label>15</label><figDesc>Relighting examples (More relighting results are in the accompanied video).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 .</head><label>16</label><figDesc>Relighting examples (More relighting results are in the accompanied video).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Final render, diffuse and specular albedos in comparison with GT Maya renders. SSIM and PSNR (dB): higher is better.</figDesc><table><row><cell>vs GT</cell><cell>Final</cell><cell>Final</cell><cell cols="2">Diffuse Diffuse</cell><cell>Spec.</cell><cell>Spec.</cell></row><row><cell cols="7">Render (SSIM) (PSNR) (SSIM) (PSNR) (SSIM) (PSNR)</cell></row><row><cell>Ours</cell><cell>0.933</cell><cell>35.932</cell><cell>0.653</cell><cell>29.548</cell><cell>0.642</cell><cell>29.274</cell></row><row><cell>[9]</cell><cell>0.965</cell><cell>36.390</cell><cell>0.722</cell><cell>29.812</cell><cell>0.547</cell><cell>29.670</cell></row><row><cell>[19]</cell><cell>0.906</cell><cell>35.389</cell><cell>0.639</cell><cell>29.006</cell><cell>0.452</cell><cell>28.833</cell></row><row><cell>[36]</cell><cell>-</cell><cell>-</cell><cell>0.540</cell><cell>28.633</cell><cell>0.516</cell><cell>28.926</cell></row><row><cell>[35]</cell><cell>-</cell><cell>-</cell><cell>0.679</cell><cell>30.061</cell><cell>0.604</cell><cell>30.923</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the sake of conciseness, in the rest of the paper, we use E to denote the combination of this encoder with the fully connected layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">More details in supp. material section 3.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>This work has been supported by the ERC Consolidator Grant 4DReply (770784).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High-quality passive facial performance capture using anchor frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Gotsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">75</biblScope>
			<date type="published" when="2011" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shading-based dynamic shape refinement from multi-view video under general illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1108" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lightweight binocular facial performance capture under uncontrolled lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="187" to="188" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiview face capture using polarized spherical gradient illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borom</forename><surname>Tunwattanapong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">129</biblScope>
			<date type="published" when="2011" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Practical Dynamic Facial Appearance Modeling and Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gotardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIG-GRAPH Asia</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reconstructing detailed dynamic face geometry from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename><surname>Valgaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Total moving face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="796" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reconstruction of Personalized 3D Face Rigs from Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>28:1-28:15</idno>
	</analytic>
	<monogr>
		<title level="j">{ACM} Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Presented at SIG-GRAPH 2016</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Practical face reconstruction via differentiable ray tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Dib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Th?bault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe-Henri</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Chevallier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zoll?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theobalt</forename><surname>Christian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-supervised multi-level face model learning for monocular reconstruction at over 250 hz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2549" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards highfidelity nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>eeding of IEEE Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bharaj</surname></persName>
		</author>
		<title level="m">Face Model Learning from Videos. CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<title level="m">Matthias Nie?ner, and Christian Theobalt. State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning a model of facial shape and expression from 4d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">194</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d morphable face models-past, present, and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wuhrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romdhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A morphable face albedo model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alassane</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Seck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Dee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tiddeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Egger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An anatomically-constrained local deformation model for monocular face capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facelab: Scalable facial performance capture for visual effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Andrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Alessi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Dib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Th?bault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Chevallier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Romeo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Digital Production Symposium, DigiPro &apos;20</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Intrinsic face image decomposition with human face priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision -ECCV 2014</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="218" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cedric Thebault, Philippe-Henri Gosselin, and Louis Chevallier. Face reflectance and geometry modeling via differentiable ray tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Dib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH European Conference on Visual Media Production (CVMP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reparameterizing discontinuous integrands for differentiable rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Loubet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Holzschuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzel</forename><surname>Jakob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Graphics (Proceedings of SIG-GRAPH Asia)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Differentiable monte carlo ray tracing through edge sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Mao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno>222:1-222:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Large-scale celebfaces attributes (celeba) dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Look ma, no landmarks! -unsupervised, model-based dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuro</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sfsnet: Learning shape, reflectance and illuminance of facesin the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6296" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Personalized face modeling for improved face reconstruction and motion retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bindita</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noranart</forename><surname>Vesdapunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Photorealistic facial texture inference using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5144" to="5153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Production-level facial performance capture using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIG-GRAPH/Eurographics Symposium on Computer Animation</title>
		<meeting>the ACM SIG-GRAPH/Eurographics Symposium on Computer Animation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling facial geometry using compositional vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3877" to="3886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mesoscopic Facial Geometry Inference Using Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loc</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8407" to="8416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-fidelity Facial Reflectance and Geometry Inference from an Unconstrained Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Avatarme: Realistically renderable 3d facial reconstruction&quot; in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Lattas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baris</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Triantafyllou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Morphable face models-an open framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Luthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Sch?nborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A reflectance model for computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">E</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torrance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="24" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Microfacet models for refraction through rough surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Marschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Torrance</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The rendering equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Kajiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;86</title>
		<meeting>the 13th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;86<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno>De- cember 2015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A theory of frequency domain invariants: Spherical harmonic identities for brdf/lighting transfer and image consistency. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Efficient and differentiable shadow computation for inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00359</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to regress 3D face shape and expression from an image without 3D supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">High-fidelity monocular face reconstruction based on an unsupervised model-based face autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zoll?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The 2nd 3d face alignment in the wild challenge (3dfaw-video): Dense reconstruction from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laszlo</forename><forename type="middle">A</forename><surname>Rohith Krishnan Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyuan</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE International Conference on Computer Vision Workshops, October 2019</title>
		<meeting>the 2019 IEEE International Conference on Computer Vision Workshops, October 2019</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Face alignment in full pose range: A 3d total solution. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<idno>2018. 7</idno>
		<ptr target="https://github.com/cleardusk/3DDFA" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards fast, accurate and stable 3d dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The wikihuman project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename></persName>
		</author>
		<idno>2017. Accessed: 2020-05-21. 7</idno>
		<ptr target="https://vgl.ict.usc" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning formation of physically-based face attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Bladin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinmay</forename><surname>Chinara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinglei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratusha</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bipin</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="3410" to="3419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Analysis of human faces using a measurement-based skin reflectance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Weyrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Mcandless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Addy</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><forename type="middle">Wann</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1013" to="1024" />
			<date type="published" when="2006" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Portrait shadow manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuaner</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Ta</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fast, minimum storage ray-triangle intersection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Trumbore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Graph. Tools</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="28" />
			<date type="published" when="1997-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
