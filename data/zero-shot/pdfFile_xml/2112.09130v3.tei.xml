<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ensembling Off-the-shelf Models for GAN Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ensembling Off-the-shelf Models for GAN Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The advent of large-scale training has produced a cornucopia of powerful visual recognition models. However, generative models, such as GANs, have traditionally been trained from scratch in an unsupervised manner. Can the collective "knowledge" from a large bank of pretrained vision models be leveraged to improve GAN training? If so, with so many models to choose from, which one(s) should be selected, and in what manner are they most effective? We find that pretrained computer vision models can significantly improve performance when used in an ensemble of discriminators. Notably, the particular subset of selected models greatly affects performance. We propose an effective selection mechanism, by probing the linear separability between real and fake samples in pretrained model embeddings, choosing the most accurate model, and progressively adding it to the discriminator ensemble. Interestingly, our method can improve GAN training in both limited data and large-scale settings. Given only 10k training samples, our FID on LSUN CAT matches the StyleGAN2 trained on 1.6M images. On the full dataset, our method improves FID by 1.5 to 2? on cat, church, and horse categories of LSUN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image generation inherently requires being able to capture and model complex statistics in real-world visual phenomena. Computer vision models, driven by the success of supervised and self-supervised learning techniques <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b82">83]</ref>, have proven effective at capturing useful representations when trained on large-scale data <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b110">111]</ref>. What potential implications does this have on generative modeling? If one day, perfect computer vision systems could answer any question about any image, could this capability be leveraged to improve image synthesis models? Surprisingly, despite the aforementioned connection between synthesis and analysis, state-of-the-art generative adversarial networks (GANs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b108">109]</ref> are trained in an unsupervised manner without the aid of such pretrained networks. With a plethora of useful models easily available in the research ecosystem, this presents a missed opportunity to explore. Can the knowledge of pretrained visual represen-z D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swin-T (Detection)</head><p>: Off-the-shelf Model Bank</p><formula xml:id="formula_0">FFk ? F F D 1 F 1? 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VGG-16 (Classification)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swin-T (Segmentation)</head><p>ViT (CLIP)</p><p>ViT (DINO) Swin-T(MoBY)D K? K F K ? ? ? z ??z G <ref type="figure">Figure 1</ref>. Vision-aided GAN training. The model bank F consists of widely used and state-of-the-art pretrained networks. We automatically select a subset {F } K k=1 from F, which can best distinguish between real and fake distribution. Our training procedure consists of creating an ensemble of the original discriminator D and discriminatorsD k =? k ?F k based on the feature space of selected off-the-shelf models.? k is a shallow trainable network over the frozen pretrained features. tations actually benefit GAN training? If so, with so many models, tasks, and datasets to choose from, which models should be used, and in what manner are they most effective?</p><p>In this work, we study the use of a "bank" of pretrained deep feature extractors to aid in generative model training. Specifically, GANs are trained with a discriminator, aimed at continuously learning the relevant statistics differentiating real and generated samples, and a generator, which aims to reduce this gap. Na?vely using such strong, pretrained networks as a discriminator leads to the overfitting and overwhelming the generator, especially in limited data settings. We show that freezing the pretrained network (with a small, lightweight learned classifier on top, as shown in <ref type="figure">Figure 1</ref>) provides stable training when used with the original, learned discriminator. In addition, ensembling multiple pretrained networks encourages the generator to match the real distribution in different, complementary feature spaces.</p><p>To choose which networks work best, we propose to use an automatic model selection strategy based on the linear separability of real and fake images in the feature space, and progressively add supervision from a set of available pretrained networks. In addition, we use label smoothing <ref type="bibr" target="#b75">[76]</ref> and differentiable augmentation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b108">109]</ref> to stabilize the model training further and reduce overfitting. We experiment on several datasets in both limited and large-scale sample setting to show the effectiveness of our method. We improve the state-of-the-art on FFHQ <ref type="bibr" target="#b42">[43]</ref> and LSUN <ref type="bibr" target="#b97">[98]</ref> datasets given 1k training samples by 2-3? on the FID metric <ref type="bibr" target="#b35">[36]</ref>. For LSUN CATs, we match the FID of StyleGAN2 trained on the full dataset (1.6M images) with only 10k samples, as shown in <ref type="figure">Figure 2</ref>. In the full-scale data setting, our method improves FID for LSUN CATs from 6.86 to 3.98, LSUN CHURCH from 4.28 to 1.72, and LSUN HORSE from 4.09 to 2.11. Finally, we visualize the internal representation of our learned models as well as training dynamics. Our code is available on our website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Improving GAN training. Since the introduction of GANs <ref type="bibr" target="#b30">[31]</ref>, significant advances have been induced by architectural changes <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b70">71]</ref>, training schemes <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b101">102]</ref>, as well as objective functions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>. In previous works, the learning objectives often aim to minimize different types of divergences between real and fake distribution. The discriminators are typically trained from scratch and do not use external pretrained networks. As a result, the discriminator is prone to overfit the training set, especially for the limited data setting <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b108">109]</ref>. Use of pretrained models in image synthesis. Pretrained models have been widely used as perceptual loss functions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref> to measure the distance between an output image and a target image in deep feature space. The loss has proven effective for conditional image synthesis tasks such as super-resolution <ref type="bibr" target="#b49">[50]</ref>, image-to-image translation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b90">91]</ref>, and neural style transfer <ref type="bibr" target="#b27">[28]</ref>. Zhang et al. <ref type="bibr" target="#b105">[106]</ref> show that deep features can indeed match the human perception of image similarity better than classic metrics. Sungatullina et al. <ref type="bibr" target="#b84">[85]</ref> propose a perceptual discriminator to combine perceptual loss and adversarial loss for unpaired image-to-image translation. This idea was recently used by a concurrent work on CG2real <ref type="bibr" target="#b71">[72]</ref>. Another recent work <ref type="bibr" target="#b26">[27]</ref> proposes the use of pretrained objects detectors to detect regions in the image and train object-specific discriminators during GAN training. Our work is inspired by the idea of perceptual discriminators <ref type="bibr" target="#b84">[85]</ref> but differs in three ways. First, we focus on a different application of unconditional GAN training rather than image-to-image translation. Second, instead of using a single VGG model, we ensemble a diverse set of feature representations that complement each other. Finally, we propose an automatic model selection method to find models useful for a given domain. A concurrent work <ref type="bibr" target="#b77">[78]</ref> propose to reduce overfitting of perceptual discriminators <ref type="bibr" target="#b84">[85]</ref> using random projection and achieve better and faster GAN training.</p><p>Loosely related to our work, other works have used pretrained models for clustering, encoding, and nearest neighbor search during their model training. Logo-GAN <ref type="bibr" target="#b74">[75]</ref> uses deep features to get synthetic clustering labels for conditional GAN training. InclusiveGAN <ref type="bibr" target="#b98">[99]</ref> improves the recall of generated samples by enforcing each real image to be close to a generated image in deep feature space. Shocher et al. <ref type="bibr" target="#b80">[81]</ref> uses an encoder-decoder based generative model with pretrained encoder for image-to-image translation tasks. Pretrained features have also been used to condition the generator in GANs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b55">56]</ref>. Different from the above work, our method empowers the discriminator with pretrained models and requires no changes to the backbone generator. Use of pretrained models in image editing. Pretrained models have also been used in image editing once the generative model has been trained. Notable examples include image projection with a perceptual distance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b112">113]</ref>, text-driven image editing with CLIP <ref type="bibr" target="#b67">[68]</ref>, finding editable directions using attribute classifier models <ref type="bibr" target="#b79">[80]</ref>, and extracting semantic editing regions with pretrained segmentation networks <ref type="bibr" target="#b113">[114]</ref>. In our work, we focus on using the rich knowledge of computer vision models to improve model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer learning.</head><p>Large-scale supervised and selfsupervised models learn useful feature representations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b94">95]</ref> that can transfer well to unseen tasks, datasets, and domains <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b100">101]</ref>. In generative modeling, recent works propose transferring the weights of pretrained generators and discriminators from a source domain (e.g., faces) to a new domain (e.g., portraits of one person) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b106">107]</ref>. Together with differentiable data augmentation techniques <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b109">110]</ref>, they have shown faster convergence speed and better sampling quality for limited-data settings. Different from them, we transfer the knowledge of learned feature representations of computer vision models. This enables us to leverage the knowledge from a diverse set of sources at scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Generative Adversarial Networks (GANs) aim to approximate the distribution of real samples from a finite training set x ? P X . The generator network G, maps latent vectors z ? P(z) (e.g., a normal distribution) to samples G(z) ? P ? . The discriminator network D is trained adversarially to distinguish between the continuously changing generated distribution P ? and target real distribution P X . GANs perform the minimax optimization min</p><formula xml:id="formula_1">G max D V (D, G), where V (D, G) = E x?P X [log D(x)] + E z?P(z) [log(1 ? D(G(z)))].</formula><p>(1) Ideally, the discriminator should measure the gap be-   <ref type="bibr" target="#b10">[11]</ref> based discriminator vs. baseline StyleGAN2-ADA discriminator on FFHQ 1k dataset. Our discriminator based on pretrained features has higher accuracy on validation real images and thus shows better generalization. In the above training, visionaided adversarial loss is added at the 2M iteration.</p><p>tween P X and P ? and guide the generator towards P X . However, in practice, large capacity discriminators can easily overfit on a given training set, especially in the limiteddata regime <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b108">109]</ref>. Unfortunately, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>, even when we adopt the latest differentiable data augmentation <ref type="bibr" target="#b40">[41]</ref> to reduce overfitting, the discriminator still tends to overfit, failing to perform well on a validation set. In addition, the discriminator can potentially focus on artifacts that are indiscernible to humans but obvious for machines <ref type="bibr" target="#b89">[90]</ref>. To address the above issues, we propose ensembling a diverse set of deep feature representations as our discriminator. This new source of supervision can benefit us in two manners. First, training a shallow classifier over pretrained features is a common way to adapt deep networks to a smallscale dataset, while reducing overfitting <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, our method reduces the discriminator overfitting significantly. Second, recent studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b100">101]</ref> have shown that deep networks can capture meaningful visual concepts from low-level visual cues (edges and textures) to high-level concepts (objects and object parts). A discriminator built on these features may better match human perception <ref type="bibr" target="#b105">[106]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Given a set of pretrained feature extractors F = {F n } N n=1 , which learns to tackle different vision tasks, we train corresponding discriminators {D n } N n=1 . We add small classifier heads {C n } N n=1 to measure the gap between P X and P ? in the pretrained models' feature spaces. During discriminator training, the feature extractor F n is frozen, and only the classifier head is updated. The generator G is updated with the gradients from D and the discriminators {D n } based on pretrained feature extractors. In this manner, we propose to leverage pretrained models in an adversarial fashion for GAN training, which we refer to as Vision-aided Adversarial training:</p><formula xml:id="formula_2">min G max D,{Cn} N n=1 V (D, G) + vision-aided adversarial loss N n=1 V (D n , G) , where D n = C n ? F n .<label>(2)</label></formula><p>Here, C n is a small trainable head over the pretrained features. The above training objective involves the sum of discriminator losses based on all available pretrained models {F n }. Solving for this at each training iteration would be computationally and memory-intensive. Using all pretrained models would force a significant reduction in batch size to fit all models into memory, potentially hurting performance <ref type="bibr" target="#b8">[9]</ref>. To bypass the computational bottleneck, we automatically select a small subset of K models, where K &lt; N :</p><formula xml:id="formula_3">min G max D,{? k } K k=1 V (D, G) + K k=1 V (D k , G),<label>(3)</label></formula><p>whereD k =? k ?F k denotes the discriminator corresponding to k th selected model, and k ? {1, . . . , K}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Selection</head><p>We choose the models whose off-the-shelf feature spaces best distinguish samples from real and fake distributions. Given the pretrained model's features of real and fake images, the strongest adversary from the set of models isF k , where Select best modelF k ? F using Eqn.</p><formula xml:id="formula_4">k = arg max n { max C n V (D n , G)}, where D n = C n ? F n .<label>(4)</label></formula><formula xml:id="formula_5">4 4:F =F ? {F k } 5:D k =? k ?F k ? k is a shallow trainable network 6: F = F \F k 7:</formula><p>for t = 1 to T k do 8:</p><formula xml:id="formula_6">Sample x ? {xi} 9:</formula><p>Sample z ? P(z) <ref type="bibr">10:</ref> Update D,Dj ?j = 1, ? ? ? , k using Eqn. 3 <ref type="bibr">11:</ref> Sample z ? P(z)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Update G using Eqn. 3 <ref type="bibr">13:</ref> end for 14: end for Output: G with best training set FID Here F n is frozen, and C n is a linear trainable head over the pretrained features. In the case of limited real samples available and for computational efficiency, we use linear probing to measure the separability of real and fake images in the feature space of F n .</p><p>We split the union of real training samples {x i } and generated images {G(z i )} into training and validation sets. For each pretrained model F n , we train a logistic linear discriminator head to classify whether a sample comes from P X or P ? and measure V (D n , G) on the validation split. The above term measures the negative binary cross-entropy loss and returns the model with the lowest error. A low validation error correlates with higher accuracy of the linear probe, indicating that the features are useful for distinguishing real from generated samples and using these features will provide more useful feedback to the generator. We empirically validate this on GAN training with 1k training samples of FFHQ and LSUN CAT datasets. <ref type="figure" target="#fig_1">Figure 4</ref> shows that the GANs trained with the pretrained model F n with higher linear probe accuracy in general achieve better FID metrics.</p><p>To incorporate feedback from multiple off-the-shelf models, we explore two variants of model selection and ensembling strategies -(1) K-fixed model selection strategy chooses the K best off-the-shelf models at the start of training and trains until convergence and (2) K-progressive model selection strategy iteratively selects and adds the best, unused off-the-shelf model after a fixed number of iterations. K-progressive model selection. We find including multiple models in a progressive manner has lower computational complexity compared to the K-fixed strategy. This also helps in the selection of pretrained models, which captures differ-  <ref type="figure" target="#fig_9">Figure 12</ref> in the appendix. ent aspects of the data distribution. For example, the first two models selected through the progressive strategy are usually a pair of self-supervised and supervised models. For these reasons, we primarily perform all of our experiments using the progressive strategy. We also show a comparison between the two strategies in Section 4.4.</p><p>Discussion. The idea of linear separability as a metric has been previously used for evaluating GAN via classifier two-sample tests <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b107">108]</ref>. We adopt this in our work to evaluate the usefulness of available off-the-shelf discriminators, rather than evaluating generators. "Linear probing" is also a common technique for measuring the effectiveness of intermediate features spaces in both self-supervised <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b104">105]</ref> and supervised <ref type="bibr" target="#b2">[3]</ref> contexts, and model selection has been explored in previous works to predict expert models for transfer learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b68">69]</ref>. We explore this in context of generative modeling and propose a progressive addition of next best model to create an ensemble <ref type="bibr" target="#b11">[12]</ref> of discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Algorithm</head><p>As shown in Algorithm 1, our final algorithm consists of first training a GAN with standard adversarial loss <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b43">44]</ref>. Given this baseline generator, we search for the best off-theshelf models using linear probing and introduce our proposed loss objective during training. In the K-progressive strategy, we add the next vision-aided discriminator after training for a fixed number of iterations proportional to the number of available real training samples. The new vision-aided discriminator is added to the snapshot with the best training set FID in the previous stage. During training, we perform data augmentation through horizontal flipping and use differentiable augmentation techniques <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b108">109]</ref> and one-sided label smoothing <ref type="bibr" target="#b75">[76]</ref> as a regularization. We also observe that only using off-the-shelf models as the discriminator leads to divergence. Thus, the benefit is brought by ensembling the original discriminator and the newly added off-the-shelf models. We show results with the use of three pretrained models and observe minimal benefit with the progressive addition of next model if the linear probe accuracy is low and worse than the models already in the selected set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Here we conduct extensive experiments on multiple datasets of different resolutions with the StyleGAN2 architecture. We show results on FFHQ <ref type="bibr" target="#b42">[43]</ref>, LSUN CAT, and LSUN CHURCH datasets <ref type="bibr" target="#b97">[98]</ref> while varying training sample size from 1k to 10k, as well as with the full dataset. For real-world limited sample datasets, we perform experiments on the cat, dog, and wild categories of AFHQ <ref type="bibr" target="#b18">[19]</ref> dataset at 512 resolution and METFACES <ref type="bibr" target="#b40">[41]</ref> at 1024 resolution. In 100-400 low-shot settings, we perform experiments on AnimalFace cat and dog <ref type="bibr" target="#b81">[82]</ref>, and 100-shot Bridge-of-Sighs <ref type="bibr" target="#b108">[109]</ref> dataset. We also show results with BigGAN <ref type="bibr" target="#b8">[9]</ref> architecture on CIFAR <ref type="bibr" target="#b45">[46]</ref> datasets in Appendix B. Baseline and metrics. We compare with state-of-the-art methods for limited dataset GAN training, StyleGAN2-ADA <ref type="bibr" target="#b40">[41]</ref> and DiffAugment <ref type="bibr" target="#b108">[109]</ref>. We compute the commonly used Fr?chet Inception Distance (FID) metric <ref type="bibr" target="#b35">[36]</ref> using the clean-fid library <ref type="bibr" target="#b66">[67]</ref> to evaluate models. In low-shot settings we evaluate on KID <ref type="bibr" target="#b7">[8]</ref> metric as well. We report more evaluation metrics like precision and recall <ref type="bibr" target="#b48">[49]</ref>, and SwAV-FID <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b60">61]</ref> using feature space of SwAV <ref type="bibr" target="#b9">[10]</ref> model which was not used during our training in Appendix C. Off-the-shelf models. We include eight large-scale selfsupervised and supervised networks. Specifically, we perform experiments with CLIP <ref type="bibr" target="#b69">[70]</ref>, VGG-16 <ref type="bibr" target="#b82">[83]</ref> trained for ImageNet <ref type="bibr" target="#b19">[20]</ref> classification, and self-supervised models, DINO <ref type="bibr" target="#b10">[11]</ref> and MoBY <ref type="bibr" target="#b94">[95]</ref>. We also include face parsing <ref type="bibr" target="#b50">[51]</ref> and face normals prediction networks <ref type="bibr" target="#b1">[2]</ref>. Finally, we have Swin-Transformer <ref type="bibr" target="#b53">[54]</ref> based segmentation model trained on ADE-20K <ref type="bibr" target="#b111">[112]</ref> and object detection model trained on MS-COCO <ref type="bibr" target="#b51">[52]</ref>. Full details of all models is given in <ref type="table" target="#tab_2">Table 16</ref> in Appendix D. We exclude the Inception model <ref type="bibr" target="#b85">[86]</ref> trained on ImageNet since Inception features have already been used to calculate the FID metric. Vision-aided discriminator's architecture.</p><p>For dis-criminatorD k based on pretrained model features, we StyleGAN2 ADA extract spatial features from the last layer and use a small Conv-LeakyReLU-Linear-LeakyReLU-Linear architecture for binary classification. In the case of big transformer networks, such as CLIP and DINO, we explore a multi-scale architecture that works better. For all experiments, we use three pretrained models selected by the model selection strategy during training. Details about the architecture, model training, memory requirements, and hyperparameters are provided in Appendix D. <ref type="table" target="#tab_2">Table 1</ref> shows the results of our method when the training sample is varied from 1k to 10k for FFHQ, LSUN CAT, and LSUN CHURCH datasets. The considerable gain in FID for all settings shows the effectiveness of our method in the limited data scenario. To qualitatively analyze the difference between our method and StyleGAN2-ADA, we show randomly generated samples from both models given the same latent code in <ref type="figure" target="#fig_2">Figure 5</ref>. Our method improves the quality of the worst samples, especially for FFHQ and LSUN CAT (also see <ref type="figure" target="#fig_0">Figure 13</ref>, 14 in Appendix A). <ref type="figure" target="#fig_3">Figure 6</ref> shows the accuracy of linear probe over the pretrained models's features as we progressively add the next discriminator.</p><formula xml:id="formula_7">+ 1 st D + 2 nd D + 3 rd D 50</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">FFHQ and LSUN datasets</head><p>To analyze the overfitting behavior of discriminators, we evaluate its training and validation accuracy across iterations. Compared to the baseline StyleGAN2-ADA discriminator, our vision-aided discriminator shows better generalization on the validation set specifically for limited-data regime as shown in <ref type="figure" target="#fig_0">Figure 3</ref> for FFHQ 1k setting. Full-dataset training. In the full-dataset setting, we finetune the trained StyleGAN2 (config-F) <ref type="bibr" target="#b43">[44]</ref> model with our method. <ref type="table" target="#tab_3">Table 2</ref> shows the comparison of StyleGAN2 and ADM <ref type="bibr" target="#b20">[21]</ref> with our method trained using three visionaided discriminators. We report both FID and Perceptual Path Length (PPL) <ref type="bibr" target="#b42">[43]</ref> (W space) metric. On LSUN CAT, our method improves FID from 6.86 to 3.98, on LSUN   CHURCH from 4.28 to 1.72, and on LSUN HORSE from 4.09 to 2.11. For FFHQ dataset, our method improves the PPL metric from 144.62 to 127.58 and has similar performance on FID metric. Perceptual path length has been shown to correlate with image quality and indicates a smooth mapping in generator latent space <ref type="bibr" target="#b43">[44]</ref>. Random generated samples for all models are shown in <ref type="figure" target="#fig_6">Figure 18</ref> in Appendix A. GAN Dissection analysis on LSUN CHURCH. How does the generator change with the use of off-the-shelf models as discriminators? To analyze this, we use the existing technique of GAN Dissection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, which calculates the correlation between convolutional feature maps of the generator and scene parts obtained through a semantic segmentation network <ref type="bibr" target="#b93">[94]</ref>. Specifically, we select the convolutional layer with 32 resolution in the generator trained with our method and StyleGAN2 on the full LSUN CHURCH dataset. The total number of interpretable units <ref type="bibr" target="#b6">[7]</ref> increases from 254 to 297 by our method, suggesting that our model may learn a richer representation of semantic concepts. <ref type="figure" target="#fig_5">Figure 7</ref> shows  <ref type="table">Table 3</ref>. Results on AFHQ and METFACES. Our method, in general, results in lower FID and higher Recall. In transfer setup we fine-tune from a FFHQ trained model of similar resolution with D updated according to FreezeD technique <ref type="bibr" target="#b59">[60]</ref> similar to <ref type="bibr" target="#b40">[41]</ref>. We select the snapshot with the best FID and show an average of three evaluations. KID is shown in ?10 3 units following <ref type="bibr" target="#b40">[41]</ref>. . Qualitative comparison of our method with StyleGAN2-ADA on AFHQ. Left: randomly generated samples for both methods. Right: For both our model and StyleGAN2-ADA, we independently generate 5k samples and find the worst-case samples compared to real image distribution. We first fit a Gaussian model using the Inception <ref type="bibr" target="#b85">[86]</ref> feature space of real images. We then calculate the log-likelihood of each sample given this Gaussian prior and show the images with minimum log-likelihood (maximum Mahalanobis distance). We show more samples in <ref type="figure">Figure 19</ref> and <ref type="figure" target="#fig_12">Figure 20</ref> in Appendix A.</p><formula xml:id="formula_8">Dataset StyleGAN2 (F) Ours (w/ ADA) ADM FID ? PPL ? FID ? PPL ? FID ? FFHQ-</formula><formula xml:id="formula_9">Dataset Transfer StyleGAN2 StyleGAN2-ADA Ours (w/ ADA) FID ? KID ? Recall ? FID ? KID ? Recall ? FID ? KID ? Recall ? AFHQ</formula><p>the complete statistics of detected units corresponding to each semantic category and some of the example images of improved units by our method. We observe an overall  Human preference study. As suggested by <ref type="bibr" target="#b47">[48]</ref> we perform a human preference study on Amazon Mechanical Turk (AMT) to verify that our results agree with the human judgment regarding the improved sample quality. We compare StyleGAN2-ADA and our method trained on 1k samples of LSUN CAT, LSUN CHURCH, and FFHQ datasets. Since we fine-tune StyleGAN2-ADA with our method, the same latent code corresponds to similar images for the two models, as also shown in <ref type="figure" target="#fig_2">Figure 5</ref>. For randomly sampled latent codes, we show the two images generated by our method and StyleGAN2-ADA for six seconds to the test subject and ask to select the more realistic image. We perform this study for 50 test subjects per dataset, and each subject is shown a total of 55 images. On the FFHQ dataset, human preference for our method is 53.8% ? 1.3. For the LSUN CHURCH dataset, our method is preferred over StyleGAN2-ADA with 60.5% ? 1.7, and for the LSUN CAT dataset 63.5% ? 1.6.</p><p>These results correlate with the improved FID metric. Example images from our study are shown in <ref type="figure" target="#fig_2">Figure 15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">AFHQ and METFACES</head><p>To further evaluate our method on real-world limited sample datasets, we perform experiments on METFACES (1336 images) and AFHQ dog, cat, wild categories with ? 5k images per category. We compare with StyleGAN2-ADA under 0 5 10 15 20 <ref type="bibr" target="#b24">25</ref>   <ref type="figure">Figure 9</ref>. FID? w.r.t. training time comparison between StyleGAN2-ADA and our method (w/ ADA and one pretrained model) when applied from the start for FFHQ, LSUN CAT 1k, and LSUN CAT full-dataset setting. There is a warm-up of 0.5M images, and then our loss is added. Our method results in similar FID at more than twice the speedup. We show training time in hours measured on one RTX 3090.</p><p>two settings, (1) Fine-tuning StyleGAN2-ADA model with our loss (2) Fine-tuning from a StyleGAN2 model trained on FFHQ dataset of same resolution (transfer setup) using FreezeD <ref type="bibr" target="#b59">[60]</ref>. The second setting evaluates the transfer learning capability when fine-tuned from a generator trained on a different domain. <ref type="table">Table 3</ref> shows the comparison of our method with StyleGAN2 and StyleGAN2-ADA on multiple metrics. We outperform or perform on-par compared to the existing methods in general. <ref type="figure" target="#fig_6">Figure 8</ref> shows the qualitative comparison between our method and StyleGAN2-ADA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Low-shot Generation</head><p>To test our method to the limit of low-shot samples, we evaluate our method when only 100-400 samples are available. We finetune StyleGAN2 model with our method on AnimalFace cat (169 images) and dog (389 images) <ref type="bibr" target="#b81">[82]</ref>, and 100-shot Bridge-of-Sighs <ref type="bibr" target="#b108">[109]</ref> datasets. For differentiable augmentation, we use ADA except for the 100-shot dataset where we find that the DiffAugment <ref type="bibr" target="#b108">[109]</ref> works better than ADA <ref type="bibr" target="#b40">[41]</ref>, and therefore employ that. Our method leads to considerable improvement over existing methods on both FID and KID metrics as shown in <ref type="table" target="#tab_6">Table 4</ref>. We show nearest neighbour test and latent space interpolations in <ref type="figure" target="#fig_0">Figure 23</ref> and <ref type="figure" target="#fig_1">Figure 24</ref> of Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Fine-tuning vs. training from scratch. In all our experiments, we fine-tuned a well-trained StyleGAN2 model (both generator and discriminator) with our additional loss. We show here that our method works similarly well when training from scratch. <ref type="figure">Figure 9</ref> shows the plot of FID with training time for StyleGAN2-ADA and our method with a single vision-aided discriminator on FFHQ and LSUN CAT trained with 1k samples, and LSUN CAT full-dataset setting. Our method results in better FID and converges more than 2? faster. During training from scratch, we train with the standard adversarial loss for the first 0.5M images and then introduce the discriminator selected by the model selection strategy. Training with three vision-aided discriminators for same number of iterations as <ref type="table" target="#tab_2">Table 1</ref> we get similar FID of   10.60 and 12.24 for FFHQ and LSUN CAT 1k respectively. K-progressive vs. K-fixed model selection. We compare the K-progressive and K-fixed model selection strategies in this section. <ref type="figure" target="#fig_7">Figure 10</ref> shows the comparison for FFHQ 1k and LSUN CAT 1k trained for the same number of iterations with two models from our model bank. We observe that training with two fixed pretrained models from the start results in a similar or slightly worse FID at the cost of extra training time compared to the progressive addition.</p><p>Our model selection vs. random selection. We showed in <ref type="figure" target="#fig_1">Figure 4</ref> that FID correlates with model selection ranking in vision-aided GAN training with a single pretrained model. To show the effectiveness of model selection in K-progressive strategy, we compare it with (1) random selection of models during progressive addition and <ref type="formula" target="#formula_2">(2)</ref>    <ref type="table">Table 6</ref>. Additional ablation studies evaluated on FID? metric.</p><p>Having two discriminators during training (frozen with random weights or trainable) or standard adversarial training for more iterations leads to only marginal benefits in FID. Thus the improvement is through an ensemble of original and vision-aided discriminators. means FID increased to twice the baseline, and therefore, we stop the training run.</p><p>least linear probe accuracy. The results are as shown in Table <ref type="bibr" target="#b4">5</ref>. We observe that random selection of pretrained models from the model bank already provides benefit in FID, but with our model selection, it can be improved further. Details of selected models are given in Appendix D. Role of data augmentation and label smoothing. Here, we investigate the role of differentiable augmentation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b109">110]</ref> which is one of the important factors that enable the effective use of pretrained features. Label smoothing <ref type="bibr" target="#b75">[76]</ref> further improves the training dynamics, especially in a limited sample setting. We ablate each of these component and show its contribution in <ref type="figure" target="#fig_8">Figure 11</ref> on FFHQ and LSUN CAT dataset in 1k sample setting, and LSUN CAT full-dataset setting. <ref type="figure" target="#fig_8">Figure 11</ref> shows that replacing ADA <ref type="bibr" target="#b40">[41]</ref> augmentation strategy with DiffAugment <ref type="bibr" target="#b108">[109]</ref> in our method also performs comparably. Moreover, in the limited sample setting, without data augmentation, model collapses very early in training, and FID diverges. The role of label smoothing is more prominent in limited data setting e.g. LSUN CAT 1k. Additional ablation study. Here we further analyze the im-portance of our design choice. All the experiments are done on LSUN CAT and FFHQ. We compare our method with the following settings: (1) Fine-tuning ViT (CLIP) network as well in our vision-aided adversarial loss; (2) Randomly initializing the feature extractor network ViT (CLIP); (3) Training with two discriminators, where the 2 nd discriminator is of same architecture as StyleGAN2 original discriminator; (4) Training the StyleGAN2-ADA model longer for the same number of iterations as ours with standard adversarial loss. The results are as shown in <ref type="table">Table 6</ref>. We observe that the baseline methods provide marginal improvement, whereas our method offers significant improvement over StyleGAN2-ADA, as measured by FID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Discussion</head><p>In this work, we propose to use available off-the-shelf models to help in the unconditional GAN training. Our method significantly improves the quality of generated images, especially in the limited-data setting. While the use of multiple pretrained models as discriminators improves the generator, it has a few limitations. First, this increases memory requirement for training. Exploring the use of efficient computer vision models <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b86">87]</ref> will potentially make our method more accessible. Second, our model selection strategy is not ideal in the low-shot settings when only a dozen samples are available. We observe increased variance in the linear probe accuracy with sample size ? 100 which can lead to ineffective model selection. We plan to adopt few-shot learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b83">84]</ref> methods for these settings in future.</p><p>Nonetheless, as more and more self-supervised and supervised computer vision models are readily available, they should be used to good advantage for generative modeling. This paper serves as a small step towards improving generative modeling by transferring the knowledge from large-scale representation learning.</p><p>We show more visualizations and quantitative results to show the efficacy of our method. Namely, Section A shows qualitative comparisons between our method and leading methods for GAN training i.e. StyleGAN2-ADA <ref type="bibr" target="#b40">[41]</ref> and DiffAugment <ref type="bibr" target="#b108">[109]</ref>. In Section B we show results of our vision-aided adversarial training with BigGAN architecture on CIFAR-10 and CIFAR-100 datasets. In Section C, we show more evaluation of our model on other metrics. Section D details our training hyperparameters, discriminator architectures, and selected models in each experiment. In Section E, we discuss the societal impact of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Qualitative Image Analysis</head><p>In <ref type="figure" target="#fig_0">Figure 13</ref>, we show more randomly generated images by StyleGAN2-ADA and our method with the same latent code for FFHQ, LSUN CAT, and LSUN CHURCH 1k training sample setting similar to <ref type="figure" target="#fig_2">Figure 5</ref> of the main paper. <ref type="figure" target="#fig_1">Figure 14</ref> shows similar comparison between DiffAugment and our method. In many cases, our method improves the visual quality of samples compared to the baseline.</p><p>For the human preference study conducted on the 1k sample setting, <ref type="figure" target="#fig_2">Figure 15</ref> shows the sample images for the cases where users preferred our generated images or StyleGAN2-ADA generated images. <ref type="figure" target="#fig_3">Figure 16</ref> and <ref type="figure" target="#fig_5">Figure 17</ref> show randomly generated images by our method, StyleGAN2-ADA, and DiffAugment for varying training sample settings of FFHQ, LSUN CAT, and LSUN CHURCH.</p><p>For AFHQ and METFACES, <ref type="figure">Figure 19</ref> and <ref type="figure" target="#fig_12">Figure 20</ref> show the qualitative comparison between StyleGAN2-ADA and our method (similar to <ref type="figure" target="#fig_6">Figure 8</ref> in the main paper). <ref type="figure" target="#fig_17">Figure 21</ref> and <ref type="figure">Figure 22</ref> show similar comparison for FFHQ, LSUN CAT, and LSUN CHURCH 1k training sample setting. We also qualitatively evaluate our low-shot trained models on nearest neighbour test from training images in <ref type="figure" target="#fig_0">Figure 23</ref>. <ref type="figure" target="#fig_1">Figure 24</ref> shows the latent interpolation of models trained by our method in the low-shot setting with 100 ? 400 real samples. The smooth interpolation shows that the model is probably not overfitting on the few real samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vision-aided BigGAN</head><p>Here, we perform experiments with BigGAN architecture <ref type="bibr" target="#b8">[9]</ref> on CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b45">[46]</ref>. <ref type="table">Table 7</ref> shows the comparison of vision-aided adversarial training with the current leading method DiffAugment <ref type="bibr" target="#b108">[109]</ref> in both unconditional and conditional settings, with varying training dataset sizes. We outperform DiffAugment across all settings according to the FID metric. In the case of unconditional training with BigGAN, we use self-modulation in the generator layers <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b78">79]</ref>. During conditional training, we use projection discriminator <ref type="bibr" target="#b58">[59]</ref> in the vision-aided discriminator as well. The training is done for the same number of iterations as DiffAugment <ref type="bibr" target="#b108">[109]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation</head><p>We measure FID using clean-fid library <ref type="bibr" target="#b66">[67]</ref> with 50k generated samples and complete training dataset as the reference distribution in all our experiments similar to StyleGAN2-ADA <ref type="bibr" target="#b40">[41]</ref> except in low-shot setting. For lowshot 100-400 sample setting we compute FID and KID with 5k generated samples and full available real dataset as the reference following DiffAugment <ref type="bibr" target="#b108">[109]</ref>. In addition to the FID metric reported in the main paper, we report in <ref type="table">Table 9</ref> and <ref type="table" target="#tab_2">Table 10</ref>, precision and recall metrics <ref type="bibr" target="#b48">[49]</ref> for FFHQ and LSUN experiments with varying training sample size and full-dataset. We observe that our method improves the recall metric in all cases and has similar or better precision, particularly in the limited sample settings. Recent studies <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b60">61]</ref> suggest reporting FID metrics in different feature spaces, in order to avoid "attacking" the evaluation metric. As such, we also report FID, but using the feature space of a selfsupervised model trained on ImageNet via SwAV <ref type="bibr" target="#b60">[61]</ref>, in <ref type="table" target="#tab_2">Table 11</ref> to <ref type="table" target="#tab_2">Table 14</ref>, and observe consistent improvements. Moreover, as shown in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table">Table 8</ref>, when using CLIP (which is not trained on ImageNet) or using DINO (a self-supervised method that does not require ImageNet labels) in our vision-aided training, we also improve FID scores. <ref type="table" target="#tab_2">Table 15</ref> shows the results on progressive addition of vision-aided discriminator on METFACES and AFHQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training and Hyperparameter details</head><p>Off-the-shelf models and discriminator head architecture. We provide network details of off-the-shelf mod-  els we used in our experiments in <ref type="table" target="#tab_2">Table 16</ref>. For extracting features, we resize both real and fake images to the resolution that the pretrained network was trained on. For the trainable discriminator head, we use a Conv-LeakyReLU-Linear-LeakyReLU-Linear architecture over the spatial features after 2? downsampling for all pretrained models except CLIP and DINO. In the case of CLIP and DINO with ViT-B architecture, we observed that a multi-scale architecture leads to marginally better results (as shown in <ref type="table">Table 8</ref>). We extract the spatial features at 4 and 8 layers and the final classifier token feature. For each spatial feature, we use a Conv-LeakyReLU-Conv with downsampling to predict a 3 ? 3 real vs fake logits similar to PatchGAN <ref type="bibr" target="#b37">[38]</ref>, and the loss is averaged over the 3 ? 3 spatial grid. On the classifier token, we use Linear-LeakyReLU-Linear discriminator head for a global real vs. fake prediction. The final loss is the sum of losses at the three scales. Extracted feature size for each model and exact architecture of the trainable discriminator head is detailed in <ref type="table" target="#tab_2">Table 16</ref>. <ref type="figure" target="#fig_2">Figure 25</ref> - <ref type="figure">Figure 29</ref> show the linear probe accuracy of the pretrained models and the selected model based on that. We calculate linear probe accuracy on the average of 3 runs (variance is always less than 1.% except in 100-400 low-sample setting where it increases to ? 5 ? 8% ). For the limited sample setting, we use the complete set of real training samples and Training hyperparameters and memory requirement. We keep similar architecture and training hyperparameters as StyleGAN2-ADA <ref type="bibr" target="#b40">[41]</ref>. For experiments on FFHQ, LSUN CAT, and LSUN CHURCH with varying sample size, number of feature maps at shallow layers is halved <ref type="bibr" target="#b40">[41]</ref>. For 256 resolution datasets, the weight of R 1 regularization (?) in the original discriminator is 1, learning rate is 0.002 and path length regularization is 2. For datasets with 512 resolution, ? is 0.5 and learning rate is 0.0025. When using ADA in our vision-aided adversarial loss, we employ cutout + bgc <ref type="bibr" target="#b40">[41]</ref> policy for augmentation, and if the linear probe accuracy of the selected model is above 90% one-sided label smoothing <ref type="bibr" target="#b75">[76]</ref> is used as a regularization. The ADA target value for the original discriminator in StyleGAN2-ADA is kept the same at 0.6. For vision-aided discriminators, we use 0.3 as the target probability for ADA in all limited data experiments. In case of finetuning from the StyleGAN2 model   <ref type="table">Table 9</ref>. Precision (P) and Recall (R) metrics for experiments on FFHQ and LSUN datasets with varying training samples from 1k to 10k. Complete training dataset is used as the reference distribution for calculating the above metrics and average of 3 evaluation runs is reported. Our method results in higher recall and precision in all settings. In addition, we observe that as we add vision-aided discriminators, recall increases at the cost of slight decrease in precision.  <ref type="table" target="#tab_2">Table 10</ref>. FID, Preicison (P), and Recall (R) metrics on full-dataset setting. Our method results in improved recall for most cases and has similar precision compared to StyleGAN2. A higher recall is usually preferred as with truncation precision can be recovered <ref type="bibr" target="#b43">[44]</ref>.    <ref type="table" target="#tab_2">Table 13</ref>. SwAV-FID <ref type="bibr" target="#b60">[61]</ref> of models trained on full dataset of FFHQ and LSUN categories. FID? is measured in SwAV ResNet-50 feature space with complete dataset as reference distribution. We select the best snapshot according to training set FID, and report mean of 3 FID evaluations.  <ref type="table" target="#tab_2">Table 14</ref>. SwAV-FID <ref type="bibr" target="#b60">[61]</ref> and KID of models trained on lowshot datasets. FID and KID are measured in SwAV ResNet-50 feature space with complete dataset as reference distribution and 5k generated images. We select the best snapshot according to training set FID, and report mean of 3 FID and KID evaluations. KID is shown in ?10 3 units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear accuracy analysis of all experiments</head><formula xml:id="formula_10">+1 st D +2 nd D +3 rd D P ? R ? P ? R ? P ? R ? P ? R ? P ? R ? P ? R ?</formula><formula xml:id="formula_11">Dataset Resolution StyleGAN2 (F) Ours (w/ ADA) +1 st D +2 nd D +3 rd D FID ? P ? R ? FID ? P? R ? FID ? P? R ? FID ? P? R ? FFHQ</formula><formula xml:id="formula_12">Method Bridge AnimalFace Cat AnimalFace Dog FID ? KID ? FID ? KID ? FID ? KID ?<label>DiffAugment</label></formula><p>? value of 100 following <ref type="bibr" target="#b43">[44]</ref>.</p><p>In all experiments, we train with an ensemble of three vision-aided discriminators. The number of training iterations after which we add the second model is 1 million (1M) in low-shot generation settings, 4M for 1k training sample setting, and 8M for the rest. With the second and third pretrained model, we train for 1M training iterations on &lt; 1k training sample setting and 2M otherwise. The GPU memory requirement of our method is maximum when using VGG-16 model at ? 2.5GB. Next, CLIP and DINO model with ViT-B architecture have a memory requirement of ? 2GB. Pretrained models based on tiny Swin-T architecture and face normals and parsing model lead to &lt; 1GB of overhead in GPU memory during training. Compared to training the StyleGAN2 (config F) model architecture on 256 resolution images which required ? 10.5GB of GPU memory on a single RTX 3090, the overhead in memory with a single pretrained model is 10 ? 25% approximately. The maximum overhead in memory is when CLIP, DINO, and Swin-T based models are selected by model selection strategy. This results in ? 4.5GB of additional memory requirement as measured on one RTX 3090 while training with our default batch-size of 16 on 256 resolution dataset. In the future, we hope to explore the use of efficient <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b86">87]</ref> computer vision models to reduce the increased memory requirement of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Societal Impact</head><p>Our proposed method is towards improving the image quality of GANs, specifically in the limited sample setting. This can help users in novel content creation where usually only few relevant samples are available for inspiration. Also, the faster convergence of our method when used in training from scratch makes it accessible to a broad set of people as the model can be trained at a lower computational cost. This can lead to negative societal impact as well through the creation of fake data and disinformation. One of the possible solutions to mitigate this can be to ensure reliable detection of fake generated data <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b89">90]</ref>.</p><p>F. Change log v1: Original draft. v2: We included additional visualization and revised text in experiments and appendix section. Specifically, we added <ref type="figure" target="#fig_1">Figure 14</ref> to show qualitative comparison between our method and DiffAugment. We also added <ref type="table" target="#tab_2">Table 15</ref> in Appendix to show intermediate results with progressive addition of pretrained models for METFACES and AFHQ categories. <ref type="figure" target="#fig_2">Figure 25</ref> - <ref type="figure">Figure 29</ref> in Appendix is updated to include linear probe accuracy plots corresponding to experiments with DiffAugment. We also updated relevant citations. v3: We included additional results on CIFAR datasets with BigGAN in <ref type="table">Table 7</ref>. We also added the FID evaluation using SwAV model in <ref type="table" target="#tab_2">Table 11</ref> to <ref type="table" target="#tab_2">Table 14</ref> and nearest neighbour test for low-shot models in <ref type="figure" target="#fig_0">Figure 23</ref>.  <ref type="table" target="#tab_2">Table 15</ref>. Results on AFHQ and METFACES with progressive addition of vision-aided discriminators. In transfer setup we fine-tune from a FFHQ trained model of similar resolution with D updated according to FreezeD technique <ref type="bibr" target="#b59">[60]</ref> similar to <ref type="bibr" target="#b40">[41]</ref>. We select the snapshot with the best FID and show an average of three evaluations. KID is shown in ?10 3 units following <ref type="bibr" target="#b40">[41]</ref>.  <ref type="table" target="#tab_2">Table 16</ref>. Off-the-shelf Model Bank. We select state-of-the-art feature extractors and task specific networks to use as an ensemble of off-the-shelf discriminators during GAN training. We keep the discriminator head architecture small and fairly similar across different models. In the multi-scale architecture of CLIP and DINO, we extract the spatial features from 4 and 8 layers and final classification token feature. In case of conditional training for CIFAR-10 and CIFAR-100 we use an additional embedding layer for number of classes and employ projection discriminator <ref type="bibr" target="#b58">[59]</ref>.  <ref type="figure" target="#fig_2">Figure 15</ref>. Example images shown to users in the human preference study between our method (w/ ADA) and StyleGAN2-ADA on LSUN CAT, FFHQ, and LSUN CHURCH 1k training sample setting. Left: example instances where images generated by our method is preferred by users. Right: where images generated by StyleGAN2-ADA is preferred. For each dataset, top and bottom row show the two images generated by StyleGAN2-ADA and our method from the same random latent code and shown to the user. For LSUN CAT, FFHQ, and LSUN CHURCH our method is preferred with 63.5%, 53.8%, and 60.5% as mentioned in the main paper.</p><formula xml:id="formula_13">Dataset Transfer Ours (w/ ADA) +1 st D +2 nd D +3 rd D FID ? KID ? Recall ? FID ? KID ? Recall ? FID ? KID ? Recall ? AFHQ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FFHQ 10k</head><p>LSUN CAT 2k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSUN CAT 10k</head><p>DiffAugment Ours</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FFHQ 2k</head><p>SG2-ADA <ref type="figure" target="#fig_3">Figure 16</ref>. Randomly generated samples by DiffAugment <ref type="bibr" target="#b108">[109]</ref>, StyleGAN2-ADA <ref type="bibr" target="#b40">[41]</ref> and Our method (w/ ADA) on 2k and 10k sample setting of FFHQ and LSUN CAT.   <ref type="figure">Figure 19</ref>. Qualitative comparison of our method (w/ ADA) with StyleGAN2-ADA on AFHQ DOG and AFHQ CAT. Left: randomly generated samples for both methods. Right: Worst FID samples. For both our model and StyleGAN2-ADA, we independently generate 5k samples and find the worst-case samples compared to real image distribution. We first fit a Gaussian model using the Inception <ref type="bibr" target="#b85">[86]</ref> feature space of real images. We then calculate the log-likelihood of each sample given this Gaussian prior and show the images with minimum log-likelihood (maximum Mahalanobis distance). Our method shows better image quality on average compared to StyleGAN2-ADA.   <ref type="figure">Figure 19</ref>. Our method to a large extent prevents generation of rotated images with extreme artifacts in case of FFHQ 1k training sample setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SG2-ADA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours LSUN CHURCH 1k</head><p>Worst Random <ref type="figure">Figure 22</ref>. Qualitative comparison of our method (w/ ADA) with StyleGAN2-ADA on LSUN CHURCH 1k Left: randomly generated samples for both methods. Right: samples with maximum Mahalanobis distance as described in <ref type="figure">Figure 19</ref>. Our method has relatively better worst case samples compared to StyleGAN2-ADA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel-wise L 1 nearest neighbors LPIPS nearest neighbors Query</head><formula xml:id="formula_14">LSUN CHURCH 1k LSUN CAT 1k FFHQ 1k</formula><p>Bridge-of-sighs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AnimalFace-Cat</head><p>AnimalFace-Dog <ref type="figure" target="#fig_0">Figure 23</ref>. Nearest neighbor test on low-shot data settings. Left column: generated images by our model. Middle column: LPIPS based nearest neighbors from the training set. Right column: pixel wise L1 distance based nearest neighbors. We observe that the generated images are different from the training set. Thus our model is not simply memorizing the training set.   <ref type="figure" target="#fig_6">Figure 28</ref>. Linear probe accuracy of off-the-shelf models during our K-progressive ensemble training on full-dataset of FFHQ, LSUN categories and AFHQ, METFACES (transfer from FFHQ trained generator). In case of transfer from FFHQ, linear probe accuracy is 100% at the start as human faces and AFHQ categories have a significant domain gap. The selected model at each stage is annotated at the top of the bar-plot. As we include more vision-aided discriminators during GAN training, linear probe accuracy of the pretrained models decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Progressive Ensemble</head><p>StyleGAN2 ADA + 1 st D + 2 nd D + 3 rd D 50  <ref type="figure">Figure 29</ref>. Linear probe accuracy of off-the-shelf models during our K-progressive ensemble training on AnimalFace Cat, Dog <ref type="bibr" target="#b81">[82]</ref> and 100-shot Bridge-of-Sighs <ref type="bibr" target="#b108">[109]</ref> low-shot datasets, and AFHQ categories. The selected model at each stage is annotated at the top of the bar-plot. As we include more vision-aided discriminators during GAN training, linear probe accuracy of the pretrained models decreases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Training and validation accuracy w.r.t. training iterations for our DINO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIDFigure 4 .</head><label>4</label><figDesc>Model selection using linear probing of pretrained features. We show correlation of FID with the accuracy of a logistic linear model trained for real vs fake classification over the features of off-the-shelf models. Top dotted line is the FID of StyleGAN2-ADA generator used in model selection and from which we finetune with our proposed vision-aided adversarial loss. Similar analysis for LSUN CAT is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>LSUN CAT, FFHQ, and LSUN CHURCH paired sample comparison in 1k training dataset setting. For each dataset, the top row shows the baseline StyleGAN2-ADA samples, and the bottom row shows the samples by Our method for the same randomly sample latent code. We fine-tune the StyleGAN2-ADA model with our vision-aided adversarial loss. For the same latent code image quality improves with our method on average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Linear probe accuracy of off-the-shelf models during our K-progressive ensemble training on FFHQ 1k. For the StyleGAN2-ADA, ViT (DINO) model has the highest accuracy and is selected first, then ViT (CLIP) and then Swin-T (MoBY). As we train with vision-aided discriminators, linear probe accuracy decreases for most of the pretrained models. Similar trend for all our experiments are shown in the Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>s s tr e e s k y r o a d w in d o w d o o r p e r s o n m o h o u s e b u il d in g -t s k y -b r o o f s k y -l s k y -t d o m e b u il d in g -l s k y -r tr e e -b g r a s s -t c lo u d b u il d in g -r g r a s s -b p e r s o n -b tr e e -l p e r s o n -r p e r s o n -t tr e e -r h o u s e -t p a p e r s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>GAN Dissection visualization of improved units. We analyze StyleGAN2 and our model trained on LSUN CHURCH using GAN Dissection [7] and show here qualitative examples of units with improved IoU to a semantic category. The total number of detected units also increases from 254 to 297 for our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8</head><label>8</label><figDesc>Figure 8. Qualitative comparison of our method with StyleGAN2-ADA on AFHQ. Left: randomly generated samples for both methods. Right: For both our model and StyleGAN2-ADA, we independently generate 5k samples and find the worst-case samples compared to real image distribution. We first fit a Gaussian model using the Inception [86] feature space of real images. We then calculate the log-likelihood of each sample given this Gaussian prior and show the images with minimum log-likelihood (maximum Mahalanobis distance). We show more samples in Figure 19 and Figure 20 in Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>K-progressive vs K-fixed comparison on FFHQ and LSUN CAT 1k setting. Progressive addition of the next best model is computationally efficient and results in similar FID at lower run time. We show training time in hours measured on one RTX 3090.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>selection of models with Ablation of augmentation and label smoothing on FFHQ and LSUN CAT with 1k training samples and LSUN CAT fulldataset setting. We show the plot of FID w.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Model selection using linear probing of pretrained features on LSUN CAT 1k training sample setting. We show correlation of FID with the accuracy of a logistic linear model trained for real vs fake classification over the features of off-the-shelf models. Top dotted line is the FID of StyleGAN2-ADA generator used in model selection and from which we finetune with our proposed vision-aided adversarial loss. Thus, selecting models with higher linear probe accuracy in general results in better generative model with respect to FID metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>the same amount of generated samples. For the full-dataset setting, we randomly sample a subset of 10k real and generated samples during linear probe accuracy calculation. We observe diminished variance in the linear classifier validation accuracy with the increase in sample size. The computational cost of calculating linear probe accuracy for a model varies with the sample size and dataset resolution but is always in the order of 5 ? 10 minutes, including the time for fake image generation for training linear classifier as measured on one RTX 3090.Details of our model selection vs random selection experiment in Section 4.4 In FFHQ 1k, our method selects DINO, CLIP, and Swin-T (MoBY) during training. Random selection consists of VGG-16, Swin-T (MoBY), and U-Net (Face Parsing) networks and worst selection consists of VGG-16, U-Net (Face Parsing), and U-Net (Face Normals) networks. For LSUN CAT 1k setting, CLIP, DINO, and Swin-T (Segmentation) networks are selected by our method during training. In random selection VGG-16, CLIP, and Swin-T (Segmentation) networks are selected and worst selection consists of VGG-16, Swin-T (Segmentation), and Swin-T (Detection) networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>FFHQ 1k 0</head><label>0</label><figDesc>.580 0.000 0.681 0.034 0.675 0.088 0.719 0.139 0.740 0.139 0.694 0.173 2k 0.586 0.025 0.709 0.099 0.676 0.137 0.701 0.242 0.719 0.251 0.719 0.251 10k 0.669 0.191 0.704 0.256 0.700 0.255 0.683 0.334 0.687 0.342 0.697 0.351 LSUN CAT 1k 0.290 0.000 0.539 0.015 0.468 0.012 0.653 0.033 0.627 0.053 0.624 0.058 2k 0.527 0.001 0.607 0.032 0.617 0.066 0.667 0.084 .645 0.105 0.652 0.120 10k 0.632 0.099 0.628 0.188 0.598 0.111 0.639 0.152 0.615 0.181 0.599 0.203 LSUN Church 1k --0.593 0.015 0.554 0.038 0.652 0.047 0.609 0.065 0.645 0.063 2k 0.613 0.042 0.604 0.075 0.637 0.100 0.626 0.107 0.649 0.114 10k --0.567 0.256 0.617 0.108 0.662 0.132 0.645 0.112 0.643 0.133</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 .Figure 14 .</head><label>1314</label><figDesc>LSUN CAT, FFHQ, and LSUN CHURCH paired sample comparison in 1k training dataset setting with ADA. For each dataset, the top row shows random samples of the baseline StyleGAN2-ADA, and the bottom row shows the samples by our method for the same latent code. We fine-tune StyleGAN2-ADA model with our vision-aided adversarial loss. On average we observe improved image quality with our method for the same latent code. LSUN CAT, FFHQ, and LSUN CHURCH paired sample comparison in 1k training dataset setting with DiffAugment. For each dataset, the top row shows random samples of the baseline DiffAugment model with StyleGAN2 architecture, and the bottom row shows the samples by our method for the same latent code. We fine-tune StyleGAN2-DiffAugment model with our vision-aided adversarial loss. On average we observe improved image quality with our method for the same latent code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 .Figure 18 .</head><label>1718</label><figDesc>Randomly generated samples by DiffAugment<ref type="bibr" target="#b108">[109]</ref>, StyleGAN2-ADA<ref type="bibr" target="#b40">[41]</ref> and Our method (w/ ADA) on LSUN CHURCH 2k and 10k training sample setting. Uncurated samples generated by StyleGAN2<ref type="bibr" target="#b43">[44]</ref> and Our method (w/ ADA) trained on full-dataset of FFHQ, LSUN CAT, LSUN CHURCH, and LSUN HORSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 20 .</head><label>20</label><figDesc>Qualitative comparison of our method (w/ ADA) with StyleGAN2-ADA on AFHQ WILD and METFACES Left: randomly generated samples for both methods. Right: samples with maximum Mahalanobis distance as described inFigure 19.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 21 .</head><label>21</label><figDesc>Qualitative comparison of our method (w/ ADA) with StyleGAN2-ADA on FFHQ and LSUN CAT 1k training sample setting Left: randomly generated samples for both methods. Right: samples with maximum Mahalanobis distance as described in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 24 .</head><label>24</label><figDesc>Latent interpolation results of models trained with our method on AnimalFace Cat (169 images), AnimalFace Dog (389 images)<ref type="bibr" target="#b81">[82]</ref> and 100-shot Bridge-of-Sighs<ref type="bibr" target="#b108">[109]</ref> datasets. The smooth interpolation suggests that there is probably little overfitting in the trained generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>FFHQFigure 25 .Figure 26 .Figure 27 .</head><label>252627</label><figDesc>Linear probe accuracy of off-the-shelf models during our K-progressive ensemble training on FFHQ with different training sample setting for both ADA and DiffAugment. The selected model at each stage is annotated at the top of the bar-plot. As we include more vision-aided discriminators during GAN training, linear probe accuracy of the pretrained models decreases. Linear probe accuracy of off-the-shelf models during our K-progressive ensemble training on LSUN CAT with different training sample setting for both ADA and DiffAugment. The selected model at each stage is annotated at the top of the bar-plot. As we include more vision-aided discriminators during GAN training, linear probe accuracy of the pretrained models decreases.LSUN CHURCH (w/ DiffAugment) LSUN CHURCH (w/ ADA) Linear probe accuracy of off-the-shelf models during our K-progressive ensemble training on LSUN CHURCH with different training sample setting for both ADA and DiffAugment. The selected model at each stage is annotated at the top of the bar-plot. As we include more vision-aided discriminators during GAN training, linear probe accuracy of the pretrained models decreases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Performance on LSUN CAT and LSUN CHURCH. We compare with the leading methods StyleGAN2-ADA<ref type="bibr" target="#b40">[41]</ref> and DiffAugment<ref type="bibr" target="#b108">[109]</ref> on different sizes of training samples and full-dataset. Our method outperforms them by a large margin, especially in limited sample setting. For LSUN CAT we achieve similar FID as StyleGAN2<ref type="bibr" target="#b43">[44]</ref> trained on full-dataset using only 0.7% of the dataset.</figDesc><table><row><cell>10 20 30 40 FID?</cell><cell>41.1</cell><cell>43.3</cell><cell>12.2</cell><cell>17.5</cell><cell cols="4">13.2 LSUN Cat 16.7 12.6 7.9 similar FID with 6.9 6.9 4.0 0.7% data</cell><cell>5 10 15 20 FID?</cell><cell>19.7</cell><cell>19.4</cell><cell>9.6</cell><cell>11.2</cell><cell>LSUN Church 6.1 5.1 6.7 13.5</cell><cell>4.5</cell><cell>1.7 StyleGAN2 (ADA) 4.3 Di Augment Ours</cell></row><row><cell>0</cell><cell></cell><cell>1k</cell><cell></cell><cell></cell><cell cols="2">5k Training Samples 10k</cell><cell>1.6M</cell><cell></cell><cell>0</cell><cell></cell><cell>1k</cell><cell></cell><cell></cell><cell>2k Training Samples 10k</cell><cell>126k</cell></row><row><cell cols="6">1.0 Figure 2. 2M 1.0 StyleGAN2-ADA 0.0 0.2 0.4 0.6 0.8 Accuracy 0M 1M 2M 3M 4M 5M 6M Iterations 0.0 0.2 0.4 0.6 0.8 Accuracy training validation training validation</cell><cell>Ours 4M Iterations training 3M validation training validation</cell><cell>5M</cell><cell>6M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 GAN training with Vision-aided Adversarial loss. Input: G, D trained with standard GAN loss for baseline number of iterations. Off-the-shelf model bank F = {Fn} N n=1 . Training data {xi}.</figDesc><table><row><cell>Hyperparameters: K: maximum number of pretrained models</cell></row><row><cell>to use. {T k : k = 1 ? ? ? K}: training intervals before adding</cell></row><row><cell>next pretrained model.</cell></row><row><cell>1: Selected model setF =?</cell></row><row><cell>2: for k = 1 to K do</cell></row><row><cell>3:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>FFHQ and LSUN results with varying training samples from 1k to 10k. FID? is measured with complete dataset as reference distribution. We select the best snapshot according to training set FID, and report mean of 3 FID evaluations. In Ours (w/ ADA) we finetune the StyleGAN2-ADA model, and in Ours (w/ DiffAugment) we finetune the model trained with DiffAgument while using the corresponding policy for augmentation. Our method works with both ADA and DiffAugment strategy for augmenting images input to the discriminators.</figDesc><table><row><cell cols="5">Dataset StyleGAN2 DiffAugment ADA</cell><cell cols="3">Ours (w/ ADA)</cell><cell cols="3">Ours (w/ DiffAugment)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">+1 st D +2 nd D +3 rd D +1 st D +2 nd D +3 rd D</cell></row><row><cell>FFHQ</cell><cell></cell><cell>1k 2k 10k</cell><cell>62.16 42.62 16.07</cell><cell cols="3">27.20 19.57 11.43 10.39 16.63 16.06 10.17 8.73 8.15 8.38 6.90 6.39</cell><cell cols="3">10.58 12.33 13.39 8.18 10.01 9.24 5.90 6.94 6.26</cell><cell>12.76 10.99 6.43</cell></row><row><cell>LSUN</cell><cell>CAT</cell><cell>1k 2k 10k</cell><cell>185.75 68.03 18.59</cell><cell cols="3">43.32 41.14 15.49 12.90 25.70 23.32 13.44 13.35 12.56 13.25 8.37 7.13</cell><cell cols="3">12.19 13.52 12.52 11.51 12.20 11.79 6.86 8.19 7.90</cell><cell>11.01 11.33 7.79</cell></row><row><cell>LSUN</cell><cell>CHURCH</cell><cell>1k 2k 10k</cell><cell>---</cell><cell cols="2">19.38 19.66 11.39 13.46 11.17 5.25 6.69 6.12 4.80</cell><cell>9.78 5.06 4.82</cell><cell>9.56 5.26 4.47</cell><cell>10.15 6.09 3.42</cell><cell>9.87 6.37 3.41</cell><cell>9.94 5.56 3.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Results on full-dataset setting. we improve the FID metric on LSUN categories by a significant margin. On the FFHQ dataset we improve the PPL metric. * means directly reported from the ADM paper<ref type="bibr" target="#b20">[21]</ref>.</figDesc><table><row><cell>1024</cell><cell></cell><cell>2.98</cell><cell cols="2">144.62</cell><cell>3.01</cell><cell>127.58</cell><cell>-</cell></row><row><cell>LSUN CAT-256</cell><cell></cell><cell>6.86</cell><cell cols="2">437.13</cell><cell>3.98</cell><cell>420.15</cell><cell>5.57  *</cell></row><row><cell cols="2">LSUN CHURCH-256</cell><cell>4.28</cell><cell cols="2">343.02</cell><cell>1.72</cell><cell>388.94</cell><cell>-</cell></row><row><cell cols="7">LSUN HORSE-256 2.57  StyleGAN2 4.09 337.98 2.11 307.12 Unit #386 mountain: iou=0.06 Unit #354 tree: iou=0.39</cell></row><row><cell>Unit #386</cell><cell cols="3">mountain: iou=0.17</cell><cell>Unit #354</cell><cell></cell><cell>tree: iou=0.49</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Low-shot generation results on 100-shot Bridge dataset<ref type="bibr" target="#b108">[109]</ref>, AnimalFace cat and dog<ref type="bibr" target="#b81">[82]</ref> categories. Our method significantly improves FID and KID compared to leading methods for few-shot GAN training. KID is shown in ?10 3 units.</figDesc><table /><note>increase in the number of detected units as well as units corresponding to new semantic categories.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>FID? metric for models trained with different model selection strategies in K-progressive vision-aided training. 1 st Row: model selection with best linear probe accuracy. 2 nd Row: randomly selecting from the bank of off-the-shelf models. 3 rd Row: model selection with least linear probe accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 .Table 8</head><label>78</label><figDesc>Vision-aided GAN training on CIFAR-10 and CIFAR-100<ref type="bibr" target="#b45">[46]</ref> with the BigGAN architecture. We improve FID on both conditional and unconditional training setups. FID is calculated using clean-fid with 10k generated samples and test set as the reference distribution. The three off-the-shelf models selected by model selection are CLIP, DINO, and MoBY (Swin-T), respectively, in all settings.</figDesc><table><row><cell>Discriminator Architecture</cell><cell>Dataset</cell><cell></cell></row><row><cell></cell><cell cols="2">LSUN CAT 1k FFHQ 1k</cell></row><row><cell>Single-scale (MLP)</cell><cell>17.21</cell><cell>13.31</cell></row><row><cell>Multi-scale</cell><cell>15.49</cell><cell>11.63</cell></row></table><note>. FID? on Single-scale vs Multi-scale discriminator head for ViT (CLIP). We observe slightly better FID with a multi-scale discriminator head compared to a 2-layer MLP head on final classi- fication token feature, without any significant increase in training time. Therefore, we select the multi-scale discriminator head for all our experiments on both CLIP and DINO with ViT-B architecture.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 .</head><label>11</label><figDesc>SwAV-FID<ref type="bibr" target="#b60">[61]</ref> of models trained on FFHQ, LSUN datasets with varying training samples. FID? is measured in SwAV ResNet-50 feature space with complete dataset as reference distribution. We select the best snapshot according to training set FID, and report mean of 3 FID evaluations.</figDesc><table><row><cell>Dataset</cell><cell>Transfer</cell><cell>StyleGAN2-ADA</cell><cell>Ours (w/ ADA)</cell></row><row><cell>AFHQ DOG</cell><cell></cell><cell>2.02 1.89</cell><cell>1.04 1.03</cell></row><row><cell>AFHQ CAT</cell><cell></cell><cell>1.17 0.98</cell><cell>0.62 0.70</cell></row><row><cell>AFHQ WILD</cell><cell></cell><cell>1.89 1.23</cell><cell>1.10 0.97</cell></row><row><cell>METFACES</cell><cell></cell><cell>2.14</cell><cell>1.72</cell></row><row><cell>In Ours (w/ ADA) we</cell><cell></cell><cell></cell><cell></cell></row><row><cell>finetune the pretrained StyleGAN2-ADA model, and in Ours (w/</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DiffAugment) we finetune the model trained with DiffAgument</cell><cell></cell><cell></cell><cell></cell></row><row><cell>while using the corresponding policy for augmentation.</cell><cell></cell><cell></cell><cell></cell></row></table><note>in the full-dataset setting on FFHQ and LSUN categories, the original discriminator has non-augmented real and fake images as input and ADA target for additional discrimina- tors is 0.1. In case of training with DiffAugment, we always</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 .</head><label>12</label><figDesc>SwAV-FID<ref type="bibr" target="#b60">[61]</ref> of models trained on AFHQ categories and METFACES. FID? is measured in SwAV ResNet-50 feature space with complete dataset as reference distribution. We select the best snapshot according to training set FID, and report mean of 3 FID evaluations. In transfer setup we fine-tune from a FFHQ trained model of similar resolution with D updated according to FreezeD technique<ref type="bibr" target="#b59">[60]</ref> similar to<ref type="bibr" target="#b40">[41]</ref>. use one-sided label smoothing and all three augmentations color, translation, and cutout. All experiments are done with</figDesc><table><row><cell>Dataset</cell><cell>StyleGAN2 (F)</cell><cell>Ours (w/ ADA)</cell></row><row><cell>FFHQ-1024</cell><cell>0.57</cell><cell>0.38</cell></row><row><cell>LSUN CAT-256</cell><cell>2.65</cell><cell>1.03</cell></row><row><cell>LSUN CHURCH-256</cell><cell>1.81</cell><cell>0.58</cell></row><row><cell>LSUN HORSE-256</cell><cell>1.65</cell><cell>0.71</cell></row></table><note>a batch size of 16 (mini-batch std 4) on a single RTX 3090 GPU for 256 resolution, and 4 GPUs for 512, 1024 resolu- tion datasets. In the case of LSUN HORSE, we fine-tuned StyleGAN2 (config F) model with a batch size of 64 and use</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We thank Muyang Li, Sheng-Yu Wang, Chonghyuk (Andrew) Song for proofreading the draft. We are also grateful to Alexei A. Efros, Sheng-Yu Wang, Taesung Park, and William Peebles for helpful comments and discussion. The work is partly supported by Adobe Inc., Kwai Inc, Sony Corporation, and Naver Corporation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im-age2stylegan: How to embed images into the stylegan latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-modal deep face normals with deactivable skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adnane</forename><surname>Victoria Fern?ndez Abrevaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boukhayma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Understanding intermediate layers using linear classifier probes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01644</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiobjective training of generative adversarial networks with multiple discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Isabela Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breandan</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Considine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitliagkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the role of individual units in a deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gan dissection: Visualizing and understanding generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Demystifying mmd gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miko?aj</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Danica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ensemble selection from libraries of models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Crew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ksikes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Instance-conditioned gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marl?ne</forename><surname>Careil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What makes fake images detectable? understanding properties that generalize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On-line adaptative curriculum learning for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabela</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrey</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative multi-adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Gemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridhar</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Representation similarity analysis for efficient task taxonomy &amp; transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Roig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detail me more: Improving gan&apos;s photo-realism of complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial networks. Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lofgan: Fusing local representations for few-shot image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Bernhard Nessler, and Sepp Hochreiter</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<title level="m">What makes imagenet good for transfer learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Do better imagenet models transfer better? In CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Kynk??nniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06026</idno>
		<title level="m">The role of imagenet classes in fr\&apos;echet inception distance</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Kynk??nniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kunpeng Song, and Ahmed Elgammal. Towards faster and stabilized gan training for high-fidelity few-shot image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Revisiting classifier two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04256</idno>
		<title level="m">Data instance prior (disp) in generative adversarial networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">cgans with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Freeze the discriminator: a simple baseline for fine-tuning gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On self-supervised image representations for gan evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Deep ensembles for low-data transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06866,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Image generation from small datasets via batch statistics adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fewshot image generation via cross-domain correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11222</idno>
		<title level="m">On buggy resizing libraries and surprising subtleties in fid calculation</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Styleclip: Text-driven manipulation of stylegan imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Renggli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>ICLR, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Abu Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04619</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Enhancing photorealism enhancement. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Logo synthesis and manipulation with clustered generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Projected gans converge faster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A u-net based discriminator for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Interfacegan: Interpreting the disentangled face representation learned by gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Semantic pyramid for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Gandelsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Yarom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning hybrid image templates (hit) by information projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangzhang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Image manipulation with perceptual discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Sungatullina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Towards good practices for data augmentation in gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-Hung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Bao</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung-Kien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05338</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Cnn-generated images are surprisingly easy to spot... for now</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Minegan: effective knowledge transfer from gans to target domains with few images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Transferring gans: generating images from limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenshen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raducanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Self-supervised learning with swin transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Data-efficient instance generation from instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Inclusive gan: Improving data and minority coverage in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>ICLR, 2020. 15</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Splitbrain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">On leveraging pretrained gans for generation with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaoyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulai</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Large scale image completion via co-modulated generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<idno>ICLR, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Image augmentations for gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02595</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Barbershop: Gan-based image compositing using segmentation masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Femiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
