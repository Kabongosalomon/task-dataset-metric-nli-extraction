<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">F2DNet: Fast Focal Detection Network for Pedestrian Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><surname>Hannan Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">German Research Center for Artificial Intelligence (DFKI GmbH)</orgName>
								<address>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsin</forename><surname>Munir</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">German Research Center for Artificial Intelligence (DFKI GmbH)</orgName>
								<address>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludger</forename><surname>Van Elst</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">German Research Center for Artificial Intelligence (DFKI GmbH)</orgName>
								<address>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">German Research Center for Artificial Intelligence (DFKI GmbH)</orgName>
								<address>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Fachbereich Informatik</orgName>
								<orgName type="institution">Technische Universit?t Kaiserslautern</orgName>
								<address>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">F2DNet: Fast Focal Detection Network for Pedestrian Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Two-stage detectors are state-of-the-art in object detection as well as pedestrian detection. However, the current two-stage detectors are inefficient as they do bounding box regression in multiple steps i.e. in region proposal networks and bounding box heads. Also, the anchor-based region proposal networks are computationally expensive to train. We propose F2DNet, a novel two-stage detection architecture which eliminates redundancy of current two-stage detectors by replacing the region proposal network with our focal detection network and bounding box head with our fast suppression head. We benchmark F2DNet on top pedestrian detection datasets, thoroughly compare it against the existing state-of-the-art detectors and conduct cross dataset evaluation to test the generalizability of our model to unseen data. Our F2DNet achieves 8.7%, 2.2%, and 6.1% M R ?2 on City Persons, Caltech Pedestrian, and Euro City Person datasets respectively when trained on a single dataset and reaches 20.4% and 26.2% M R ?2 in heavy occlusion setting of Caltech Pedestrian and City Persons datasets when using progressive fine-tunning. Furthermore, F2DNet have significantly lesser inference time compared to the current state-of-the-art. Code and trained models will be available at https://github.com/ AbdulHannanKhan/F2DNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Pedestrian detection is a sub-domain of object detection where the target class is pedestrian and the rest is considered background. Pedestrian detection plays a vital role in autonomous driving as well as surveillance. In autonomous driving, one of the most important objectives is to avoid collision with pedestrians by detecting and tracking them. This objective is to be carried out in a limited resource scenario as limited computational power is available inside an autonomous vehicle due to compactness and power efficiency constraints. This requires the pedestrian detection model to be light and efficient. Also, the lesser the time model takes to process a single frame more frame per second it can process which yields better awareness of surroundings.</p><p>Region Proposal Networks were first proposed by Ross Girshick et al. <ref type="bibr" target="#b0">[1]</ref> to replace, slow, selective search-based region proposal generation with a faster, CNN-based network that can be trained end-to-end along with detection head. In the last decade, researchers have focused on improving two-stage detectors by proposing new detection heads <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> with little focus on region proposal network architecture. However, the role of region proposal networks in two-stage detectors MR -2 <ref type="figure">Fig. 1</ref>: Evolution of pedestrian detectors over the years and their corresponding M R ?2 on Caltech Pedestrian <ref type="bibr" target="#b4">[5]</ref> (orange), City Persons <ref type="bibr" target="#b5">[6]</ref> (green) and Euro City Persons <ref type="bibr" target="#b6">[7]</ref> (pink) datasets in reasonable settings. is limited to proposing candidate regions with the purpose of objectness score produced by region proposal networks limited to proposal filtering. Also, proposed bounding boxes from region proposal network need rigorous refinement in their coordinates, for example, Cascade RCNN <ref type="bibr" target="#b3">[4]</ref> applies three cascading heads to get refined detections.</p><p>Compared to two-stage detectors, single-stage detectors are efficient as they split the image into a grid and perform detection per patch eliminating region proposal network <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. However, single-stage detectors do not perform as good as two-stage detectors in terms of accuracy, this can be attributed to a class imbalance between positive and negative samples <ref type="bibr" target="#b9">[10]</ref>. Other than class imbalance, since each patch does not necessarily contain a full object, classifying if a patch contains enough parts of an object is sub-optimal, as a part may belong to multiple object classes. A common attribute of both single and two-stage detectors explained above is anchors. Both kinds of detectors rely on anchors with predefined aspect ratios.</p><p>In the last few years, anchor-free object detectors were proposed <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Motivated from anchor-free approaches in object detection, anchor-free approaches were proposed for pedestrian detection as well <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. These pedestrianspecific approaches, take the idea of single-stage detector to another level by predicting classes per pixel instead of per patch. However, this is done on a downscaled feature map to be efficient and robust <ref type="bibr" target="#b13">[14]</ref>. Unlike anchor-based approaches, center and scale-based approaches classify if each pixel is a center pixel of an object and regress the possible scale of that object. In this way, center and scale-based approaches eliminate the idea of predicting rough bounding boxes and refining them later on. Further, center and scale-based approaches use the focal loss as classification loss to deal with class imbalance <ref type="bibr" target="#b13">[14]</ref>.</p><p>Although center and scale-based approaches have optimal design and better convergence they produce more false positives due to penalty reduced focal loss, which does not punish much the false predictions in the neighborhood of positive pixels. This problem intensifies in the case of small and heavily occluded pedestrians.</p><p>Our method is different in nature from existing single and two-stage detectors. The closest single-stage detector is anchor-free, center and scale prediction <ref type="bibr" target="#b13">[14]</ref>. However, we only use the head from CSP <ref type="bibr" target="#b13">[14]</ref> with different loss settings as the CSP head <ref type="bibr" target="#b13">[14]</ref> is stronger and efficient compared to region proposal networks, also we use fast suppression head to further refine detections. Compared to two-stage detectors, we replace the region proposal network with a stronger detection network, we do not call it another region proposal network because focal detection network produces strong detection candidates compared to proposals that need further bounding box refinement and classification. Also, we replace computationally expensive traditional second stage, which predicts bounding boxes as well as classifies them, with a simple and efficient suppression head to only suppress false positives without altering bounding boxes.</p><p>Contribution of this paper is three fold;</p><p>? First, we redesign a two-stage detection architecture to remove redundant and inefficient bounding box prediction and replace region proposal network with a strong detection network, followed by a light-weight suppression head instead of multiple bounding box heads. ? Second, we propose focal detection network as our classification and bounding box regression head, which can independently produce satisfactory results. ? Third, we propose fast suppression head to handle false positives produced by focal detection head in small and heavily occluded settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Significant improvements have been made in recent years in the field of pedestrian detection using deep learning models <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref> as shown in <ref type="figure">Fig. 1</ref>. Most of the recent techniques follow general object detection workflow including a strong pre-trained backbone to extract features, an optional feature pyramid network (FPN) <ref type="bibr" target="#b17">[18]</ref> based feature enrichment layer, a region proposal network (RPN) <ref type="bibr" target="#b0">[1]</ref> in case of two-stage detectors and at the end, bounding box heads for bounding box regression and classification. Such pipelines are supported in modern object detection frameworks like mmdetection <ref type="bibr" target="#b18">[19]</ref>. Different types of pedestrian detectors have emerged in recent years which can be differentiated from each other based on how they use region proposal network and choice of bounding box heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Anchor Based Pedestrian Detectors</head><p>Region-based convolutional neural networks are two-staged object detectors, which were first proposed by <ref type="bibr">Girshick et al.</ref> in <ref type="bibr" target="#b19">[20]</ref> for object detection. Fast-RCNN and Faster-RCNN were proposed to improve the processing time of RCNN by using ROI pooling on features maps instead of raw image and CNN-based region proposal network respectively <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b0">[1]</ref>. Mask Guided Attention Network incorporates additional visibility information of the object to handle occlusions better <ref type="bibr" target="#b1">[2]</ref>. Cascade R-CNN proposed by Cai and Vasconcelos in <ref type="bibr" target="#b3">[4]</ref> uses multiple bounding box heads to refine detections in cascading manner. Another anchor-based but single-stage pedestrian detector is ALFNet, which is based on Single Shot Multibox Detector (SSD) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b7">[8]</ref>. RetinaNet is yet another single-stage detector, which is similar to SSD [8] but introduces focal loss to handle foreground and background class imbalance <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Anchor Free Pedestrian Detectors</head><p>Anchor-free pedestrian detectors are pedestrian-specific object detectors that do not use anchors or region proposal networks. Instead, they predict bounding box and class per pixel on down-scaled feature maps because performing detection per pixel on original resolution is costly. CornerNet <ref type="bibr" target="#b12">[13]</ref> uses CNN based approach to predict paired keypoint heatmaps i.e. one heatmap for each top-left and bottom-right corner. Fully convolutional single-stage object detection network, FCOS was proposed in <ref type="bibr" target="#b11">[12]</ref>, which adopts classification and bounding box prediction of R-CNN heads to pixel-wise fashion with bounding box predictions being pixel distances from object center, which is calculated using centeredness predicted per pixel by FCOS. Center and Scale Prediction CSP <ref type="bibr" target="#b13">[14]</ref> proposed for pedestrian detection uses a similar approach but instead predicts center heatmap, scale map and reconstructs the bounding boxes using the center and scale <ref type="bibr" target="#b13">[14]</ref>. ACSP <ref type="bibr" target="#b14">[15]</ref> uses switchable normalization for better convergence on different batch sizes and uses full resolution for training to improve recall. APD <ref type="bibr" target="#b15">[16]</ref> tries to handle crowded pedestrians by additionally predicting density and diversity. BGCNet replaces normal convolutions with box-guided convolution for center heatmap subnet to incorporate predicted scale and offset information in center heatmap prediction <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ViT Based Methods</head><p>Vision transformer (ViT) is the adaptation of transformers in the domain of computer vision. DETR <ref type="bibr" target="#b23">[24]</ref> and DETR for  pedestrian detection <ref type="bibr" target="#b24">[25]</ref> use ViT based bounding box heads to provide a wider receptive field. Recently proposed soft teacher based approach for object detection <ref type="bibr" target="#b25">[26]</ref> uses SWin transformer <ref type="bibr" target="#b26">[27]</ref> based backbone and semi-supervised training to produce promising results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. F2DNET: FAST FOCAL DETECTION NETWORK</head><p>Current two-stage object detection architectures employ a weak region proposal network followed by strong bounding box heads. We take a different approach and use a strong detection head succeeded by a light suppression head. In this way, the detection head focuses on precise localization and high classification recall while the suppression head takes care of false positives. In short, our two-stage detection architecture attains high efficiency by eliminating the repetition contained in current two-stage architectures.</p><p>In this section, we explain the architecture of our fast focal detection network in detail and argue about our design choices. First, we elaborate on the feature extraction process followed by the detailed architecture of F2DNet and conclude the section by explaining the detection formation strategy. <ref type="figure" target="#fig_1">Fig.  2</ref> shows complete architecture of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Extraction</head><p>To predict precise location and size high-resolution features are required which contain semantic and position information. Aggressive down and upscaling can result in loss of this vital information <ref type="bibr" target="#b16">[17]</ref>. Therefore, we use the HRNetW32v2 backbone <ref type="bibr" target="#b27">[28]</ref> for feature extraction as it extracts high-resolution features from images. To obtain feature maps of a single scale, we take feature maps from different stages of backbone, upscale them to (h/4, w/4) using bilinear interpolation and apply convolution operations. In this way, the model stays light on memory as interpolation operation has no memory cost but is effective as succeeding convolution operations provide necessary learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Focal Detection Network</head><p>The architecture of the focal detection network is based on the idea of center and scale map prediction which eliminates explicit modeling of bounding boxes for detection <ref type="bibr" target="#b13">[14]</ref>. Our approach is somewhat similar to that proposed in <ref type="bibr" target="#b13">[14]</ref> however, we use different loss settings to fine-tune the architecture for better convergence and precise localization.</p><p>Center loss for focal detection network can be formulated as:</p><formula xml:id="formula_0">L center = 1 K i j ? ij CE(p ij , y ij ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">CE(p ij , y ij ) = ? log(p ij ) if y ij = 1 ? log(1 ? p ij ) otherwise, ? ij = (1 ? p ij ) ? if y ij = 1 p ? ij (1 ? M ij ) ? otherwise.<label>(2)</label></formula><p>In equation above, p ij and y ij are predicted center probability and ground truth label respectively. CE(p ij , y ij ) represents cross entropy loss with ? ij being weight at each location (i, j). M ij represents gaussian based penalty reduction for surrounding pixels of true centers as designation of exact center brings difficulty in training <ref type="bibr" target="#b13">[14]</ref>. The p ? ij and (1 ? p ij ) ? terms define focus weight based on prediction confidence i.e. it reduces contribution of easy examples to the loss and helps optimizer to focus on hard examples. The (1 ? M ij ) ? term reduces loss for false positives closer to true centers. We found ? = 2 and ? = 4 to work best in our experiments.</p><p>In <ref type="bibr" target="#b20">[21]</ref> Smooth L1 loss is recommended for regression as it is robust to outliers. The Smooth L1 loss reduces penalty when the distance between predicted and actual height is small, which helps in better convergence. However, since we use log of height instead of actual height value it can cause smaller detections and ultimately result in false positives due to insufficient IoU. Therefore, we use Vanilla L1 Loss as regression loss to make height predictions more accurate. We define loss for the focal detection head as:</p><formula xml:id="formula_2">L F DN = ? r L reg + ? c L cls + ? o L of f<label>(3)</label></formula><p>Where ? r , ? c and ? o represent weights for regression, classification and offset loss respectively. We experimentally found ? r = 0.05, ? c = 0.01 and ? o = 0.1 help model converge better than other weight settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fast Suppression Head</head><p>Since, the focal detection network uses penalty-reduced focal loss as a center loss, false positives in the neighborhood of positive centers are not punished sufficiently. While most of these false positives are suppressed by Non-Maximum Suppression (NMS), it still needs another suppression step to suppress the rests i.e. where IoU with positive predictions is lower than 0.5. Therefore, we propose a simple and fast suppression head to further refine the detections. The fast suppression head comprises of ROI Align layer followed by convolutional and dense layers as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We train the fast suppression head in detached settings, i.e. the gradients from the fast suppression head do not flow back to feature maps or detection head. In this way, a simple, light yet effective suppression head is achieved. We use binary cross entropy as loss for our fast suppression head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Pedestrian Detection</head><p>Each prediction gets one score from the focal detection network and another from the fast suppression head. We eliminate thresholding hyperparameter by combining both scores using the generative model shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We are particularly interested in an event where pedestrian is detected an not suppressed i.e. P (?s, d|c, h). The detection model is derived from joint probability distribution of P (s, d, c, h) and represented by following relation:  </p><p>where c and h are the center and height of pedestrian respectively. P (d|c) is the probability of a position detected as pedestrian center by focal detection head and P (s|d, c, h) is the probability that given a bounding box detection it is suppressed by fast suppression head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>In this section, our experimental setup is detailed, which we follow in the rest of the paper unless stated otherwise. First, we briefly go through datasets, followed by the evaluation settings in which we evaluate results on these datasets, and finally, we explain the evaluation criteria we used to compare our models with the existing state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To benchmark, our model we present results on three commonly used pedestrian detection datasets i.e. City Persons <ref type="bibr" target="#b5">[6]</ref>, Euro City Persons <ref type="bibr" target="#b6">[7]</ref> and Caltech Pedestrian dataset <ref type="bibr" target="#b4">[5]</ref>. Detailed statistics of these datasets can be seen in <ref type="table" target="#tab_0">Table I</ref>.</p><p>All the results presented in this paper for Caltech pedestrian dataset <ref type="bibr" target="#b4">[5]</ref> are based on its test set, while for City Persons <ref type="bibr" target="#b5">[6]</ref> and Euro City Persons <ref type="bibr" target="#b6">[7]</ref> results are based on their respective validation sets, unless stated otherwise. Also, new annotations for Caltech pedestrian dataset <ref type="bibr" target="#b4">[5]</ref> proposed in <ref type="bibr" target="#b28">[29]</ref> were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Settings</head><p>In pedestrian detection, evaluation settings define different subsets of a dataset which are used to better judge the performance of a model in different scenarios. We use evaluation settings proposed in Caltech Pedestrian <ref type="bibr" target="#b4">[5]</ref> and Euro City Persons <ref type="bibr" target="#b6">[7]</ref> datasets. Based on visibility and height of annotations these evaluation settings form four groups where each annotation can belong to more than one group. Settings followed across the paper can be seen in the <ref type="table" target="#tab_0">Table II</ref>. It is important to note that evaluation settings are different for Euro City Persons dataset <ref type="bibr" target="#b6">[7]</ref> while City Persons <ref type="bibr" target="#b5">[6]</ref> and Caltech Pedestrian datasets <ref type="bibr" target="#b4">[5]</ref> share identical evaluation settings.  <ref type="bibr" target="#b5">[6]</ref> and Caltech Pedestrian <ref type="bibr" target="#b4">[5]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Criteria</head><p>We use Log-average miss rate over false positive per image or M R ?2 to compare our model against recent models as it has been suggested in pedestrian detection datasets <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref> as well as followed by the state of the art <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>. M R?2 is calculated by taking geometric mean of miss rates at 9 equally spaced f f pi thresholds in log space i.e. f ppi ? {10 ?2 , 10 ?1.75 , ..., 10 0 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Weighted Averaging</head><p>We used the mean teacher strategy of weighted averaging for better convergence and performance, as the model obtained after the weighted averaging performs better <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b13">[14]</ref>. All results of our models provided in this paper are based on the evaluation of the averaged model unless stated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training Details</head><p>We used the Nvidia RTXA6000 GPU cluster to train our models. We used Distributed Data-Parallel to achieve parallel training on multiple GPUs with a manual seed. We used 2 GPUs with 32 and 4 images per GPU for training model on Caltech Pedestrian <ref type="bibr" target="#b4">[5]</ref> and City Persons <ref type="bibr" target="#b5">[6]</ref> datasets respectively. However, for training the model on Euro City Persons dataset <ref type="bibr" target="#b6">[7]</ref> we used 4 GPUs with 4 images per GPU. We used a constant learning rate throughout the training after warm-up iterations with a maximum of 80 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS AND RESULTS</head><p>In this section, we present the comparison of F2DNet to the current state-of-the-art and top-performing detectors based on M R ?2 and inference time, along with the performance gains achieved by the suppression head. A. Qualitative Comparison <ref type="figure" target="#fig_3">Fig. 4</ref> shows qualitative comparison of current state-of-theart Cascade R-CNN <ref type="bibr" target="#b3">[4]</ref> with our F2DNet. It shows that our F2DNet can detect pedestrians even where Cascade R-CNN fails. For results shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, we took models trained on multiple datasets; for Cascade R-CNN we took model weights from Pedestron <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with the State-of-The-Art</head><p>To compare the performance of F2DNet with the current state-of-the-art and top-performing methods, we took models trained on a single dataset without using any extra data except for pre-trained backbones. F2DNet outperforms the existing state-of-the-art in Caltech pedestrian <ref type="bibr" target="#b4">[5]</ref> and Euro City Persons <ref type="bibr" target="#b6">[7]</ref> datasets as well as in heavy occlusion settings of City Persons dataset <ref type="bibr" target="#b5">[6]</ref> with a clear margin and achieves slightly better M R ?2 in reasonable and small settings of City Persons dataset <ref type="bibr" target="#b5">[6]</ref>. in heavy occlusion setting of Caltech pedestrian dataset <ref type="bibr" target="#b4">[5]</ref>. This performance drop can be attributed to the sparseness of the Caltech pedestrian dataset <ref type="bibr" target="#b4">[5]</ref> compared to other pedestrian datasets with less heavy occlusion samples to train suppression head well. <ref type="table" target="#tab_0">Table III</ref> shows the detailed results our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Efficacy of Suppression Head</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CROSS DATASET EVALUATION</head><p>We conduct cross dataset evaluation to test how well F2DNet generalizes to unseen data. We compare the generalizability of F2DNet, with two other models, namely CSP <ref type="bibr" target="#b13">[14]</ref> and Cascade RCNN <ref type="bibr" target="#b3">[4]</ref>. Both of these models are state of the art in the context of pedestrian detection. We used scores for Cascade RCNN <ref type="bibr" target="#b3">[4]</ref> and CSP <ref type="bibr" target="#b13">[14]</ref> provided in Pedestron <ref type="bibr" target="#b16">[17]</ref>. We train F2DNet only on training sets and conduct tests on the validation set for City Persons <ref type="bibr" target="#b5">[6]</ref> and Euro City Persons <ref type="bibr" target="#b6">[7]</ref> and on the test set of Caltech Pedestrian dataset <ref type="bibr" target="#b4">[5]</ref>. F2DNet generalizes better than CSP <ref type="bibr" target="#b13">[14]</ref> and Cascade RCNN <ref type="bibr" target="#b3">[4]</ref>, in most cases, for City Persons <ref type="bibr" target="#b5">[6]</ref> or Euro City Persons <ref type="bibr" target="#b6">[7]</ref> (refer <ref type="table" target="#tab_0">to Table IV</ref>). However, for Caltech dataset <ref type="bibr" target="#b4">[5]</ref> F2DNet generalizes slightly worse than other models. F2DNet beats CSP <ref type="bibr" target="#b13">[14]</ref> and Cascade RCNN <ref type="bibr" target="#b3">[4]</ref> with a large margin when trained on City Persons <ref type="bibr" target="#b5">[6]</ref> and tested on Euro City Persons <ref type="bibr" target="#b6">[7]</ref>, this shows that F2DNet performs well even when trained on a smaller dataset. Cross dataset evaluation scores can be seen in <ref type="table" target="#tab_0">Table IV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. PROGRESSIVE FINE TUNING</head><p>To further improve the performance of F2DNet we perform progressive fine-tuning. We initially train our model on a bigger and diverse dataset and fine-tune it towards the target dataset in cascading manner. For City Persons dataset <ref type="bibr" target="#b5">[6]</ref>, we train the model on Euro City Persons <ref type="bibr" target="#b6">[7]</ref> and fine-tune on City Persons dataset <ref type="bibr" target="#b5">[6]</ref>. For Caltech pedestrian dataset <ref type="bibr" target="#b4">[5]</ref> we take the fine-tuned model on City Persons dataset <ref type="bibr" target="#b5">[6]</ref> and fine-tune it on the Caltech pedestrian dataset <ref type="bibr" target="#b4">[5]</ref>. Through progressive  fine-tuning, we were able to achieve new all times low M R ?2 in heavy occlusion settings for Caltech Pedestrian <ref type="bibr" target="#b4">[5]</ref> and City Persons datasets <ref type="bibr" target="#b5">[6]</ref> as shown in <ref type="table" target="#tab_4">Table V</ref>. For both training and fine-tuning only train sets of respective datasets were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>Two-stage detectors perform well in pedestrian detection. However, the region proposal network-based two-stage detectors are inefficient as the region proposal networks predict weak proposals, dependent on further refinement. We replaced the region proposal network with a per-pixel center and scale regression based focal detection network to produce highquality, standalone detection candidates except for few false positives in small and occluded settings. We pass these strong detection candidates through a light yet fast suppression network, which with barely noticeable computational cost further refines the detections to produce promising results. Our model beats the state-of-the-art in most visibility and height settings while being on par in rest. Also, by using Euro City Persons <ref type="bibr" target="#b6">[7]</ref> and City Persons <ref type="bibr" target="#b5">[6]</ref> datasets as extra training data, our model achieves the lowest M R ?2 in a heavy occluded setting, in a multi-dataset setup.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The network architecture of our F2DNet. The input image is passed through backbone and FPN to extract feature maps which are then passed to the focal detection network to obtain initial detections. The detected bounding boxes are then passed to the fast suppressed head along with feature maps to suppress false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>(Left): Representation of pedestrian showing: the center c, height h, prediction from focal detection network P (d|c) and prediction from fast suppression head P (s|d, c, h). (Right): graphical representation of our pedestrian detection generative model where P represents pedestrian.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative comparison of F2DNet and Cascade R-CNN results. (a, b, c) Results of Cascade R-CNN on City Perons [6] and Caltech Pedestrians [5] datasets. Bounding Boxes marked red indicate false negatives. (d, e, f) Results of F2DNet on City Persons</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 Fig. 5 :</head><label>55</label><figDesc>first row shows that F2DNet without suppression head produces false positives most of which are suppressed by employing fast suppression head as shown in Fig. 5 second row. However, F2DNet with suppression head performs slightly worse compared to F2DNet without suppression head Results of F2DNet before and after suppression. (a, b, c) Results without suppression; it can be seen that there are few false positives (marked red) in small and heavy occlusion cases. (d, e, f) Results with suppression; the false positives have been successfully suppressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Pedestrian detection datasets summary.</figDesc><table><row><cell>Dataset</cell><cell>Images</cell><cell>Pedestrians</cell><cell>Density</cell><cell>Resolution</cell></row><row><cell>Caltech Pedestrians</cell><cell>42,782</cell><cell>13,674</cell><cell>0.32</cell><cell>640 ? 480</cell></row><row><cell>City Persons</cell><cell>2,975</cell><cell>19,238</cell><cell>6.47</cell><cell>2048 ? 1024</cell></row><row><cell>Euro City Persons</cell><cell>21,795</cell><cell>201,323</cell><cell>9.2</cell><cell>1920 ? 1024</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Evaluation settings for pedestrian datasets based on height and visibility.</figDesc><table><row><cell>Setting</cell><cell cols="2">City Persons, Caltech Visibility Height</cell><cell cols="2">Euro City Persons Visibility Height</cell></row><row><cell>Reasonable</cell><cell>[0.65, ?]</cell><cell>[50, ?]</cell><cell>[0.6, ?]</cell><cell>[40, ?]</cell></row><row><cell>Small</cell><cell>[0.65, ?]</cell><cell>[50, 75]</cell><cell>[0.6, ?]</cell><cell>[30, 60]</cell></row><row><cell>Heavy Occlusion</cell><cell>[0.2, 0.65]</cell><cell>[50. ?]</cell><cell>[0.2, 0.6]</cell><cell>[40. ?]</cell></row><row><cell>All</cell><cell>[0.2, ?]</cell><cell>[20. ?]</cell><cell>[0.2, ?]</cell><cell>[20. ?]</cell></row><row><cell cols="4">P (?s, d|c, h) =P (?s|d, c, h)P (d|c)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Comparison of F2DNet with the current state-ofthe-art detectors based on M R ?2 and inference Time.</figDesc><table><row><cell>Method\M R ?2</cell><cell cols="3">Reasonable Small Heavy Occ.</cell><cell>Time</cell></row><row><cell></cell><cell cols="2">City Persons [6]</cell><cell></cell><cell></cell></row><row><cell>ALFNet [11]</cell><cell>12.0</cell><cell>19.0</cell><cell>51.9</cell><cell>0.27s</cell></row><row><cell>Cascade R-CNN [17]</cell><cell>11.2</cell><cell>14.0</cell><cell>37.0</cell><cell>0.73s</cell></row><row><cell>CSP [14]</cell><cell>11.0</cell><cell>16.0</cell><cell>49.3</cell><cell>0.33s</cell></row><row><cell>PRNet [31]</cell><cell>10.8</cell><cell>-</cell><cell>42.0</cell><cell>0.22s</cell></row><row><cell>Beta R-CNN [3]</cell><cell>10.6</cell><cell>-</cell><cell>47.1</cell><cell>-</cell></row><row><cell>MGAN [2]</cell><cell>10.5</cell><cell>-</cell><cell>39.4</cell><cell>-</cell></row><row><cell>Adaptive CSP [15]</cell><cell>10.0</cell><cell>-</cell><cell>46.1</cell><cell>-</cell></row><row><cell>F2DNet no sup. (ours)</cell><cell>9.0</cell><cell>11.5</cell><cell>33.8</cell><cell>0.43s</cell></row><row><cell>BGCNet [23]</cell><cell>8.8</cell><cell>11.6</cell><cell>43.9</cell><cell>-</cell></row><row><cell>F2DNet (ours)</cell><cell>8.7</cell><cell>11.3</cell><cell>32.6</cell><cell>0.44s</cell></row><row><cell></cell><cell>Caltech [5]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cascade R-CNN [17]</cell><cell>6.2</cell><cell>7.4</cell><cell>55.3</cell><cell>0.20s</cell></row><row><cell>ALFNet [11]</cell><cell>6.1</cell><cell>7.9</cell><cell>51.0</cell><cell>0.05s</cell></row><row><cell>CSP [14]</cell><cell>5.0</cell><cell>6.8</cell><cell>46.6</cell><cell>-</cell></row><row><cell>Rep Loss [32]</cell><cell>5.0</cell><cell>5.2</cell><cell>47.9</cell><cell>-</cell></row><row><cell>F2DNet no sup. (ours)</cell><cell>2.3</cell><cell>2.7</cell><cell>38.2</cell><cell>0.13s</cell></row><row><cell>F2DNet (ours)</cell><cell>2.2</cell><cell>2.5</cell><cell>38.7</cell><cell>0.14s</cell></row><row><cell></cell><cell cols="2">Euro City Persons [7]</cell><cell></cell><cell></cell></row><row><cell>SSD [7]</cell><cell>10.5</cell><cell>20.5</cell><cell>42.0</cell><cell>-</cell></row><row><cell>YOLOv3 [7]</cell><cell>8.5</cell><cell>17.8</cell><cell>37.0</cell><cell>-</cell></row><row><cell>Faster R-CNN [7]</cell><cell>7.3</cell><cell>16.6</cell><cell>52.0</cell><cell>-</cell></row><row><cell>F2DNet no sup. (ours)</cell><cell>7.2</cell><cell>12.8</cell><cell>31.6</cell><cell>0.40s</cell></row><row><cell>Cascade RCNN [17]</cell><cell>6.6</cell><cell>13.6</cell><cell>33.3</cell><cell>0.44s</cell></row><row><cell>F2DNet (ours)</cell><cell>6.1</cell><cell>10.7</cell><cell>28.2</cell><cell>0.41s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Cross dataset evaluation results. Our model is more generaliable compared to CSP and Cascade RCNN is most cases specially when trained on City Persons<ref type="bibr" target="#b5">[6]</ref> and tested on Euro City Persons<ref type="bibr" target="#b6">[7]</ref>.Method\M R ?2Training Reasonable Small Heavy Occ.</figDesc><table><row><cell></cell><cell cols="2">City Persons [6]</cell><cell></cell><cell></cell></row><row><cell>CSP [17], [14]</cell><cell>ECP</cell><cell>11.5</cell><cell>16.6</cell><cell>38.2</cell></row><row><cell>Cascade RCNN [17]</cell><cell>ECP</cell><cell>10.9</cell><cell>11.4</cell><cell>40.9</cell></row><row><cell>F2DNet (ours)</cell><cell>ECP</cell><cell>10.1</cell><cell>12.1</cell><cell>36.4</cell></row><row><cell></cell><cell cols="2">Caltech [5]</cell><cell></cell><cell></cell></row><row><cell>F2DNet (ours)</cell><cell>CP</cell><cell>11.3</cell><cell>13.7</cell><cell>32.6</cell></row><row><cell>CSP [17], [14]</cell><cell>CP</cell><cell>10.1</cell><cell>13.3</cell><cell>34.4</cell></row><row><cell>Cascade R-CNN [17]</cell><cell>CP</cell><cell>8.8</cell><cell>9.8</cell><cell>28.8</cell></row><row><cell></cell><cell cols="2">Euro City Persons [7]</cell><cell></cell><cell></cell></row><row><cell>CSP [17], [14]</cell><cell>CP</cell><cell>19.6</cell><cell>51.0</cell><cell>56.4</cell></row><row><cell>Cascade RCNN [17]</cell><cell>CP</cell><cell>17.4</cell><cell>40.5</cell><cell>49.3</cell></row><row><cell>F2DNet (ours)</cell><cell>CP</cell><cell>11.6</cell><cell>14.7</cell><cell>40.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Results of F2DNet trained on multiple datasets in progression fine-tuning fashion.</figDesc><table><row><cell>Training</cell><cell>Testing</cell><cell cols="3">Reasonable Small Heavy Occ.</cell></row><row><cell>ECP ? CP</cell><cell>CP</cell><cell>7.80</cell><cell>9.43</cell><cell>26.23</cell></row><row><cell>ECP ? CP ? Caltech</cell><cell>Caltech</cell><cell>1.71</cell><cell>2.10</cell><cell>20.42</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by the German Ministry for Economic Affairs and Climate Action (BMWK) project KI Wissen under Grant 19A20020G.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mask-guided attention network for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4967" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beta r-cnn: Looking into pedestrian detection from another perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19" to="953" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eurocity persons: A novel benchmark for person detection in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1844" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient single-stage pedestrian detector by asymptotic localization fitting and multi-scale context encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1413" to="1425" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High-level semantic feature detection: A new perspective for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5187" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adapted center and scale prediction: more stable and more accurate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09053</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attribute-aware pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3085" to="3097" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalizable pedestrian detection: The elephant in the room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Box guided convolution for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1615" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Detr for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end semi-supervised object detection with soft teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3060" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the iEEE conference on computer vision and pattern recognition</title>
		<meeting>the iEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1259" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weight-averaged, consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">abs/1703</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page">1780</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressive refinement network for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="32" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7774" to="7783" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
