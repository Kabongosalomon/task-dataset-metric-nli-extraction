<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MonoScene: Monocular 3D Semantic Scene Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh-Quan</forename><surname>Cao</surname></persName>
							<email>anh-quan.cao@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Raoul de Charette Inria</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MonoScene: Monocular 3D Semantic Scene Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>MonoScene proposes a 3D Semantic Scene Completion (SSC) framework, where the dense geometry and semantics of a scene are inferred from a single monocular RGB image. Different from the SSC literature, relying on 2.5 or 3D input, we solve the complex problem of 2D to 3D scene reconstruction while jointly inferring its semantics. Our framework relies on successive 2D and 3D UNets, bridged by a novel 2D-3D features projection inspired by optics, and introduces a 3D context relation prior to enforce spatio-semantic consistency. Along with architectural contributions, we introduce novel global scene and local frustums losses. Experiments show we outperform the literature on all metrics and datasets while hallucinating plausible scenery even beyond the camera field of view. Our code and trained models are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating 3D from an image is a problem that goes back to the roots of computer vision <ref type="bibr" target="#b53">[54]</ref>. While we, humans, naturally understand a scene from a single image, reasoning all at once about geometry and semantics, this was shown remarkably complex by decades of research <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b79">80]</ref>. Subsequently, many algorithms use dedicated depth sensors such as Lidar <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62]</ref> or depth cameras <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>, easing the 3D estimation problem. These sensors are often more expensive, less compact and more intrusive than cameras which are widely spread and shipped in smartphones, drones, cars, etc. Thus, being able to estimate a 3D scene from an image would pave the way for new applications.</p><p>3D Semantic Scene Completion (SSC) addresses scene understanding as it seeks to jointly infer its geometry and semantics. While the task gained popularity recently <ref type="bibr" target="#b55">[56]</ref>, the existing methods still rely on depth data (i.e. occupancy grids, point cloud, depth maps, etc.) and are custom designed for either indoor or outdoor scenes.</p><p>Here, we present MonoScene which -unlike the literature -relies on a single RGB image to infer the dense 3D voxelized semantic scene working indifferently for indoor and outdoor scenes. To solve this challenging problem, we project 2D features along their line of sight, inspired by optics, bridging 2D and 3D networks while letting the 3D network self-discover relevant 2D features. The SSC literature mainly relies on cross-entropy loss which considers each voxel independently, lacking context awareness. We instead propose novel SSC losses that optimize the semantic distribution of group of voxels, both globally and in local frustums. Finally, to further boost context understanding, we design a 3D context layer to provide the network with a global receptive field and insights about the voxels semantic relations. We extensively tested MonoScene on indoor and outdoor, see <ref type="figure" target="#fig_0">Fig. 1</ref>, where it outperformed all comparable baselines and even some 3D input baselines. Our main contributions are summarized as follows.</p><p>? MonoScene: the first SSC method tackling both outdoor and indoor scenes from a single RGB image. ? A mechanism for 2D Features Line of Sight Projection bridging 2D and 3D networks (FLoSP, Sec. 3.1). ? A 3D Context Relation Prior (3D CRP, Sec. 3.2) layer that boosts context awareness in the network. ? New SSC losses to optimize scene-class affinity (Sec. 3.3.1) and local frustum proportions (Sec. 3.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>3D from a single image. Despite early researches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b79">80]</ref>, in the deep learning era the first works focused on single 3D object reconstruction with explicit <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr">Figure 2</ref>. MonoScene framework. We infer 3D SSC from a single RGB image, leveraging 2D and 3D UNets, bridged by our Features Line of Sight Projection (FLoSP Sec. 3.1), and a 3D Context Relation Prior (3D CRP, Sec. 3.2) to enhance spatio-semantic awareness. On top of standard cross-entropy (Lce), our Scene-Class Affinity loss (Lscal, Sec. 3.3.1) improves the global semantics (L sem scal ) and geometry (L geo scal ), and our Frustums Proportion loss (Lfp, Sec. 3.3.2) enforces class distribution in local frustums, providing supervision beyond occlusions. <ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b64">[65]</ref><ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b69">70]</ref> or implicit representations <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b67">68]</ref>. A comprehensive survey on the matter is <ref type="bibr" target="#b29">[30]</ref>. For multiple objects, a common practice is to couple reconstruction with object detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b77">78]</ref>. Closer to our work, holistic 3D understanding seeks to predict the scene and objects layout <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b82">83]</ref>, reaching a sparse scene representation. Only <ref type="bibr" target="#b17">[18]</ref> recently addressed indoor dense visible panoptic reconstruction by backprojecting individual 2D task features to 3D. We instead densely estimate semantics and geometry for both indoor and outdoor scenarios.</p><p>3D semantic scene completion (SSC). SSCNet <ref type="bibr" target="#b58">[59]</ref> first defined the 'SSC' task where geometry and semantics are jointly inferred. The task gained attention lately, and is thoroughly reviewed in a survey <ref type="bibr" target="#b55">[56]</ref>. Existing works all use geometrical inputs like depth <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b44">45]</ref>, occupancy grids <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b68">69]</ref> or point cloud <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b80">81]</ref>. Truncated Signed Distance Function (TSDF) were also proved informative <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79]</ref>. Among others originalities, some SSC works use adversarial training to guide realism <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b63">64]</ref>, exploit multi-task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38]</ref>, or use lightweight networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b54">55]</ref>. Of interest for us, while others have used RGB as input <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b80">81]</ref> it is always along other geometrical input (e.g. depth, TSDF, etc.). A remarkable point in <ref type="bibr" target="#b55">[56]</ref> is that existing methods are designed for either indoor or outdoor, performing suboptimally in the other setting. The same survey highlights the poor diversity of losses for SSC. Instead, we address SSC only using a single RGB image, with novel SSC losses, and are robust to various types of scenes.</p><p>Contextual awareness. Contextual features are crucial for semantics <ref type="bibr" target="#b70">[71]</ref> and SSC <ref type="bibr" target="#b55">[56]</ref> tasks. A simple strategy is to concatenate multiscale features with skip connections <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b76">77]</ref> or use dilated convolutions for large receptive fields <ref type="bibr" target="#b72">[73]</ref>, for example with the popular Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b6">[7]</ref> also used in SSC <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55]</ref>. Long-range information is gathered by self-attention in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33]</ref> and global pooling in <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b75">76]</ref>. Explicit contextual learning is shown to be beneficial in <ref type="bibr" target="#b70">[71]</ref>. We propose a 3D contextual component that leverages multiple relation priors and provides a global receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>3D Semantic Scene Completion (SSC) aims to jointly infer geometry and semantics of a 3D scene? by predicting labels C = {c 0 , c 1 , . . . , c M }, being free class c 0 and M semantic classes. This has been almost exclusively addressed with 2.5D or 3D inputs <ref type="bibr" target="#b55">[56]</ref>, such as point cloud, depth or else, which act as strong geometrical cues.</p><p>Instead, MonoScene solves voxel-wise SSC from a single RGB image x rgb , learning? = f (x rgb ). This is significantly harder due to the complexity of recovering 3D from 2D. Our pipeline in <ref type="figure">Fig. 2</ref> uses 2D and 3D UNets bridged by our Features Line of Sight Projection module (FLoSP, Sec. 3.1), lifting 2D features to plausible 3D locations, that boosts information flow and enables 2D-3D disentanglement. Inspired by <ref type="bibr" target="#b70">[71]</ref>, we capture long-range semantic context with our 3D Context Relation Prior component (3D CRP, Sec. 3.2) inserted between the 3D encoder and decoder. To guide the SSC training, we introduce new complementary losses. First, a Scene-Class Affinity Loss (Sec. 3.3.1) optimizes the intra-class and inter-class scene-wise metrics. Second, a Frustum Proportion Loss (Sec. 3.3.2) aligns the classes distribution in local frustums, which provides supervision beyond scene occlusions.</p><p>2D-3D backbones. We rely on consecutive 2D and 3D UNets with standard skip connections. The 2D UNet bases on a pre-trained EfficientNetB7 <ref type="bibr" target="#b60">[61]</ref> taking as input the image x rgb . The 3D UNet is a custom shallow encoderdecoder with 2 layers. The SSC output? is obtained by processing the 3D UNet output features with our completion head holding a 3D ASPP <ref type="bibr" target="#b6">[7]</ref> block and a softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Features Line of Sight Projection (FLoSP)</head><p>Lifting 2D to 3D is notoriously ill-posed due to the scale ambiguity of single view point <ref type="bibr" target="#b21">[22]</ref>. We rather reason from optics and backproject multiscale 2D features to all possible 3D correspondences, that is along their optical ray, aggregated in a unique 3D representation. Our intuition here is that processing the latter with a 3D network will provide  guidance from the ensemble of 2D features. Our projection mechanism is akin to <ref type="bibr" target="#b51">[52]</ref> but the latter projects each 2D map to a given 3D map -acting as 2D-3D skip connections. In contrast, our component bridges the 2D and 3D networks by lifting multiscale 2D features to a single 3D feature map. We argue this enables 2D-3D disentangled representations, providing the 3D network with the freedom to use high-level 2D features for fine-grained 3D disambiguation. Compared to <ref type="bibr" target="#b51">[52]</ref>, ablation in Sec. 4.3 shows our strategy is significantly better.</p><p>Our process is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. In practice, assuming known camera intrinsics, we project 3D voxels centroids (x c ) to 2D and sample corresponding features from the 2D decoder feature map F 1:s 2D of scale 1:s. Repeating the process at all scales S, the final 3D feature map F 3D writes</p><formula xml:id="formula_0">F 3D = s?S ? ?(x c ) (F 1:s 2D ) ,<label>(1)</label></formula><p>where ? a (b) is the sampling of b at coordinates a, and ?(?) is the perspective projection. In practice, we backproject from scales S={1, 2, 4, 8}, and apply a 1x1 conv on 2D maps before sampling to allow summation. Voxels projected outside the image have their feature vector set to 0. The output map F 3D is used as 3D UNet input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Context Relation Prior (3D CRP)</head><p>Because SSC is highly dependent on the context <ref type="bibr" target="#b55">[56]</ref>, we inspire from CPNet <ref type="bibr" target="#b70">[71]</ref> that demonstrates the benefit of binary context prior for 2D segmentation. Here, we propose a 3D Context Relation Prior (3D CRP) layer, inserted at the 3D UNet bottleneck, which learns n-way voxel?voxel semantic scene-wise relation maps. This provides the network with a global receptive field, and increases spatio-semantic awareness due to the relations discovery mechanism.</p><p>Because SSC is a highly imbalanced task, learning binary (i.e. n=2) relations as in <ref type="bibr" target="#b70">[71]</ref> is suboptimal.  We instead consider n=4 bilateral voxel?voxel relations, grouped into free and occupied corresponding to 'at least one voxel is free' and 'both voxels are occupied', respectively. For each group, we encode whether the voxels semantic classes are similar or different, leading to the 4 nonoverlapping relations: <ref type="figure" target="#fig_4">Fig. 4a</ref> illustrates the relations in 2D (see caption for colors meaning).</p><formula xml:id="formula_1">M={f s , f d , o s , o d }.</formula><p>As voxels relations are greedy with N 2 relations for N voxels, we present the lighter supervoxel?voxel relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervoxel?Voxel relation.</head><p>We define supervoxels as non-overlapping groups of s 3 neighboring voxels each, and learn the smaller supervoxel?voxel relation matrices of size N 2 s 3 . Considering a supervoxel V having voxels {? 1 , . . . , ? s 3 } and a voxel ?, there are s 3 pairwise relations {? 1 ??, . . . , ? s 3 ??}. Instead of regressing the complex count of M relations in V??, we predict which of the M relations exist, as depicted in <ref type="figure" target="#fig_4">Fig. 4b</ref>. This writes,</p><formula xml:id="formula_2">V?? = {? 1 ??, . . . , ? s 3 ??} = ,<label>(2)</label></formula><p>where {?} = returns distinct elements of a set.</p><p>3D Context Relation Prior Layer. <ref type="figure">Fig. 5</ref> illustrates the architecture of our layer. It takes as input a 3D map of spatial dimension HxW xD, on which is applied a serie of ASPP convolutions <ref type="bibr" target="#b6">[7]</ref> to gather a large receptive field, then split into n=|M| matrices of size HW D? HW D s 3 . Each matrix? m encodes a relation m?M , supervised by its ground truth A m . We then optimize a weighted multilabel binary cross entropy loss:</p><formula xml:id="formula_3">L rel =? m?M,i [(1-A m i ) log(1-? m i )+w m A m i log? m i ],<label>(3)</label></formula><p>where i loops through all elements of the relation matrix and</p><formula xml:id="formula_4">w m = i (1?A m i ) i A m i .</formula><p>The relation matrices are multiplied with reshaped supervoxels features to gather global context.</p><p>Alternatively, relations in A m can be self-discovered (w/o M) by removing L rel , i.e. behaving as attention matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervoxels features</head><p>1x1 conv, sigmoid reshape ASPP strided conv</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation matrices</head><p>Combine reshape <ref type="figure">Figure 5</ref>. 3D Context Relation Prior (3D CRP). We infer relation matrices? m (here, 4), where each encodes a unique relation m ? M -optionally supervised with a relation loss (Lrel). The matrices are multiplied with the supervoxels features to gather context, and later combined (concate, conv, DDR <ref type="bibr" target="#b39">[40]</ref>) with input features. The feature dimension is omitted for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Losses</head><p>We now introduce new losses pursuing distinct global (Sec. 3.3.1) or local (Sec. 3.3.2) optimization objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Scene-Class Affinity Loss</head><p>We seek to explicitly let the network be aware of the global SSC performance. To do so, we build upon the 2D binary affinity loss in <ref type="bibr" target="#b70">[71]</ref> and introduce a multi-class version directly optimizing the scene-and class-wise metrics.</p><p>Specifically, we optimize the class-wise derivable (P)recision, (R)ecall and (S)pecificity where P c and R c measure the performance of similar class c voxels, and S c measures the performance of dissimilar voxels (i.e. not of class c). Considering p i the ground truth class of voxel i, andp i,c its predicted probability to be of class c, we define:</p><formula xml:id="formula_5">P c (p, p) = log ip i,c p i = c ip i,c ,<label>(4)</label></formula><formula xml:id="formula_6">R c (p, p) = log ip i,c p i = c i p i = c ,<label>(5)</label></formula><formula xml:id="formula_7">S c (p, p) = log i (1 ?p i,c )(1 ? p i = c ) i (1 ? p i = c ) ,<label>(6)</label></formula><p>with . the Iverson brackets. For more generality, our loss L scal maximizes the above class-wise metrics with:</p><formula xml:id="formula_8">L scal (p, p) = ? 1 C C c=1 (P c (p, p)+R c (p, p)+S c (p, p)). (7)</formula><p>In practice, we optimize semantics L sem scal =L scal (?, y) and geometry L geo scal =L scal (? geo , y geo ), where {y, y geo } are semantic and geometric labels with respective predictions {?,? geo }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Frustum Proportion Loss</head><p>Disambiguation of occlusions is impossible from a single viewpoint and we observe that occluded voxels tend to be Image 2x2 patches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head><p>Ground-truth soft labels labels <ref type="figure">Figure 6</ref>. Frustum Proportion Loss. Considering an image divided into same-size 2D patches (here, 2?2), each corresponds to a 3D frustum in the scene, we align the predicted frustum class probabilities (e.g.P k ) with the corresponding ground truth (P k ). This provides cues to the network for occlusions disambiguation.</p><p>predicted as part of the object that shadows them. To mitigate this effect, we propose a Frustum Proportion Loss that explicitly optimizes the class distribution in a frustum. As illustrated in <ref type="figure">Fig. 6</ref>, rather than optimizing the camera frustum distribution, we divide the input image into ? local patches of equal size and apply our loss on each local frustum (defined as the union of the individual pixels frustum in the patch). Intuitively, aligning the frustums distributions provide additional cues to the network on the scene visible and occluded structure, giving a sense of what is likely to be occluded (eg. cars are likely to occlude road).</p><p>Given a frustum k, we compute P k the ground truth class distribution of voxels in k, and P k,c the proportion of class c in k. LetP k andP k,c be their soft predicted counterparts, obtained from summing per-class predicted probabilities. To enforce consistency, we compute L fp as the sum of local frustums Kullback-Leibler (KL) divergence:</p><formula xml:id="formula_9">L fp = 2 k=1 D KL (P k ||P k ) = 2 k=1 c?C k P k (c) log P k (c) P k (c) .<label>(8)</label></formula><p>Note the use of C k instead of C. Indeed, frustums include small scene portions where some classes may be missing, making KL locally undefined. Instead, we compute the KL on C k , the ground truth classes that exist in the frustum k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training strategy</head><p>MonoScene is trained end-to-end from scratch by optimizing our 4 losses and the standard cross-entropy (L ce ):</p><formula xml:id="formula_10">L total = L ce + L rel + L sem scal + L geo scal + L fp .<label>(9)</label></formula><p>Because real-world data comes with sparse ground truth y due to occlusions, the losses are computed only where y is defined <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59]</ref>. Ground truths y geo and A m , for L geo scal and L rel , respectively, are simply obtained from y. We employ class weighting for L ce following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b54">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate MonoScene on popular real-world SSC datasets being, indoor NYUv2 <ref type="bibr" target="#b57">[58]</ref> and outdoor Se-manticKitti <ref type="bibr" target="#b2">[3]</ref>. Because we first address 3D SSC from a 2D image, we detail our non-trivial adaptation of recent SSC baselines <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b68">69]</ref> (Sec. 4.1) and then detail our performance (Sec. 4.2) and ablations (Sec. 4.3).</p><p>Datasets. NYUv2 <ref type="bibr" target="#b57">[58]</ref> has 1449 Kinect captured indoor scenes, encoded as 240x144x240 voxel grids labeled with 13 classes (11 semantics, 1 free, 1 unknown). The input RGBD is 640x480. Similar to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45]</ref> we use 795/654 train/test splits and evaluate on the test set at the scale 1:4.</p><p>SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> holds outdoor Lidar scans voxelized as 256x256x32 grid of 0.2m voxels, labeled with 21 classes (19 semantics, 1 free, 1 unknown). We use RGB image of cam2 of size 1226x370, left cropped to 1220?370. We use the official 3834/815 train/val splits and always evaluate at full scale (i.e. 1:1). Main results are from the hidden test set (online server), and ablations are from the validation set.</p><p>Training setup. Unless otherwise mentioned, we use FLoSP at scales <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8)</ref>, 4 supervised relations for 3D CRP (i.e. n=4, with L rel ), and ? =8?8 frustums for L fp . The 3D UNet input is 60x36x60 (1:4) for NYUv2 and 128x128x16 (1:2) for Sem.KITTI due to memory reason. The output of Sem.KITTI is upscaled to 1:1 with a deconv layer in the completion head. Details in Appendix A. We train 30 epochs with an AdamW <ref type="bibr" target="#b45">[46]</ref> optimizer, a batch size of 4 and a weight decay of 1e-4. The learning rate is 1e-4, divided by 10 at epoch 20/25 for NYUv2/SemanticKITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics.</head><p>Following common practices, we report the intersection over union (IoU) of occupied voxels, regardless of their semantic class, for the scene completion (SC) task and the mean IoU (mIoU) of all semantic classes for the SSC task. Note the strong interaction between IoU and mIoU since better geometry estimation (i.e. high IoU) can be achieved by invalidating semantic labels (i.e. low mIoU).</p><p>As mentioned in <ref type="bibr" target="#b55">[56]</ref>, the training and evaluation practices differ for indoor (with metrics evaluated only on observed surfaces and occluded voxels) or outdoor settings (evaluated on all voxels) due to the different depth/Lidar sparsity. To cope with both settings, we use the harder metrics on all voxels. We subsequently retrained all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines</head><p>We consider 4 main SSC baselines among the best opensource ones available -selecting two indoor-designed methods, 3DSketch <ref type="bibr" target="#b8">[9]</ref> and AICNet <ref type="bibr" target="#b38">[39]</ref>, and two outdoordesigned, LMSCNet <ref type="bibr" target="#b54">[55]</ref> and JS3CNet <ref type="bibr" target="#b68">[69]</ref>. We also locally compare against S3CNet <ref type="bibr" target="#b11">[12]</ref>, Local-DIFs <ref type="bibr" target="#b52">[53]</ref>, CoReNet <ref type="bibr" target="#b51">[52]</ref>. We evaluate baselines in their 3D-input version and main baselines also in an RGB-inferred version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB-inferred baselines.</head><p>Unlike us, all baselines need a 3D input e.g. occupancy grid, point cloud or depth map, giving them an unfair geometric advantage. For fair comparison, we adapt main baselines to infer their 3D inputs directly from the 2D image (x rgb ) -relying on the best found methods -, coined as 'RGB-inferred', denoted with a superscript, e.g. AICNet rgb . Note that baselines are unchanged. Inferred 3D inputs are denoted with a hat, eg.x depth . We use the pretrained AdaBin <ref type="bibr" target="#b3">[4]</ref> to infer a depth map (x depth ) serving as input for AICNet rgb . Using intrinsic calibration, we further converted depth to TSDF (x TSDF ) with <ref type="bibr" target="#b73">[74]</ref> for 3DSketch rgb input, and unproject depth to get a point cloud (x pts ) directly used as input for JS3CNet rgb or discretized as occupancy grid (x occ ) input for LMSCNet rgb . For training only, JS3CNet rgb also requires a semantic point cloud (x sem pts ), obtained by augmentingx pts with 2D semantic labels from a pretrained network <ref type="bibr" target="#b81">[82]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Evaluation</head><p>Tab. 1 reports performance of MonoScene and RGBinferred baselines for NYUv2 (test set) and SemanticKITTI official benchmark (hidden test set). The low numbers for all methods advocate for task complexity.</p><p>On both datasets we outperform all methods by a significant mIoU margin of +4.03 on NYUv2 (Tab. 1a) and +2.11 on SemanticKITTI (Tab. 1b). Importantly, the IoU is improved or on par (+3.87 and +0.16) which demonstrates our network captures the scene geometry while avoiding naively increasing the mIoU by lowering the IoU. On individual classes, MonoScene performs either best or second, excelling on large structural classes for both datasets (e.g. floor, wall ; road, building). On SemanticKITTI we get outperformed mostly on small moving objects classes (car, motorcycle, person, etc.) which we ascribe to the aggregation of moving objects in the ground truth, highlighted in <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b55">56]</ref>. This forces to predict the individual object's motion which we argue is eased when using a 3D input.</p><p>Qualitative. We compare our SSC outputs in <ref type="figure">Fig. 7</ref> showing the input image (leftmost column) and its corresponding camera frustum in ground truth (rightmost). Notice the noisy labels in NYUv2 having missing objects (e.g. windows, rows 2; ceiling, row 3), and in SemanticKITTI having sparse geometry (e.g. holes in buildings, rows 1-3).</p><p>On indoor scenes (NYUv2, <ref type="figure">Fig. 7a</ref>), all methods correctly capture global scene layouts though only MonoScene recovers thin elements as table legs and cushions (row 1), or the painting frame and properly sized TV (row 2).</p><p>On complex cluttered outdoor scenes (SemanticKITTI, <ref type="figure">Fig. 7b</ref>), compared to baselines, MonoScene evidently captures better the scene layout, e.g. cross-roads (rows 1,3). It also infers finer occluded geometry which is apparent with  <ref type="bibr" target="#b8">[9]</ref> x rgb ,x <ref type="bibr">TSDF</ref>   <ref type="bibr" target="#b8">[9]</ref> x rgb ,x TSDF 26.85 37.70 19.80 0.00 0.00 12.10 17.10 0.00 0.00 0.00 0.00 12.10 0.00 16.10 0.00 0.00 0.00 3.40 0.00 0.00 6.23 AICNet rgb <ref type="bibr" target="#b38">[39]</ref> x rgb ,x depth 23.93 39. <ref type="bibr" target="#b29">30</ref>   <ref type="table">Table 1</ref>. Performance on (a) NYUv2 <ref type="bibr" target="#b57">[58]</ref> and (b) SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>. We report the performance on semantic scene completion (SSC -mIoU) and scene completion (SC -IoU) for RGB-inferred baselines and our method. Despite the various indoor and outdoor setups, we significantly outperform other RGB-inferred baselines, in both mIoU and IoU.    <ref type="bibr" target="#b54">[55]</ref> AICNet rgb <ref type="bibr" target="#b38">[39]</ref> 3DSketch rgb <ref type="bibr" target="#b8">[9]</ref> MonoScene (ours) Ground Truth ceiling floor wall window chair sofa table tvs furniture objects (a) NYUv2 <ref type="bibr" target="#b57">[58]</ref> (test set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>AICNet rgb <ref type="bibr" target="#b38">[39]</ref> LMSCNet rgb <ref type="bibr" target="#b54">[55]</ref> JS3CNet rgb <ref type="bibr" target="#b68">[69]</ref> MonoScene <ref type="formula">(</ref>  <ref type="figure">Figure 7</ref>. Outputs on (a) NYUv2 <ref type="bibr" target="#b57">[58]</ref> and (b) SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>. In both, the input is shown left and the camera viewing frustum is shown in the ground truth (rightmost) with darker colors being parts of scenes unseen by the image in (b). MonoScene better captures the scene layout on both datasets. On indoor scene (a), it reconstructs thin objects like table legs (row 1), painting and tv (row 2), while in outdoor (b), it better estimates occluded geometry e.g. car (row 1-3) and better hallucinates the scenery beyond the field of view (row 1-4   (cf. text) shows in (b) we get significantly better results at comparable (1,2,4) scales or even with only one (1).</p><p>We further compare FLoSP to the 'Ray-traced skip connections' of CoReNet <ref type="bibr" target="#b51">[52]</ref> being close in nature, putting our best effort to push CoReNet performance. To properly evaluate only the effect of features projection, we remove our other components, producing a light version ('Ours-light') with the same 2D encoder (E), 3D decoder (D), and projection scales <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4)</ref>, corresponding to all possible scales in the 3D decoder, as in CoReNet, shown in <ref type="figure" target="#fig_6">Fig. 8a</ref>. On NYUv2, <ref type="figure" target="#fig_6">Fig. 8b</ref> shows FLoSP is very significantly better (+10.2 IoU, +8.56 mIoU). We conjecture this relates to the fact that CoReNet applies same-scale 2D-3D connections, while FLoSP disentangles 2D-3D scales, letting the network relies on self-learned relevant features, confirmed by the good performance of the (1) scale in <ref type="figure" target="#fig_6">Fig. 8b</ref>. Effect of relations in 3D CRP. While 3D CRP (Sec. 3.2) is shown beneficial in Tab. 3, we evaluate the effect of different numbers of relations (i.e. n). Tab. 4b shows the benefit of our 4 relations M={f s , f d , o s , o d } instead of only 2 (i.e. M={s, d}), which matches our expectation due to the overwhelming imbalance of free/occupied voxels (? 9:1 in NYUv2). Our supervision of relation matrices? with the relation loss L rel from Eq. (3) also shows an increase of all metrics. Without supervision, our 3D CRP acts as a selfattention layer that learns the context information implicitly. Effect of local frustums loss. Tab. 5 shows the effect of varying number of ? frustums (Eq. (8), Sec. 3.3.2) on both datasets. Higher numbers result in smaller frustums, i.e. finer local supervision. As ? increases, all metrics increase accordingly, showing the loss benefit, especially  <ref type="table">Table 5</ref>. Frustums Proportion loss ablation. Varying the number of local frustums ( ? ) in our loss shows more frustums (i.e. smaller) lead to finer guidance and better results on both datasets.</p><p>Cityscapes <ref type="bibr" target="#b16">[17]</ref> Nuscenes <ref type="bibr" target="#b4">[5]</ref> SemanticKITTI <ref type="bibr" target="#b2">[3]</ref>  when compared to applied image-wise (i.e. 1?1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>MonoScene tackles monocular SSC originally using successive 2D-3D UNets, bridged by a new features projection, with increased contextual awareness and new losses. Limitations. Despite good results, our framework still struggles to infer fine-grained geometry, or to separate semantically-similar classes, e.g. car/truck or chair/sofa. It also performs poorly on small objects partly due to their scarcity (&lt;0.3% in Sem.KITTI <ref type="bibr" target="#b2">[3]</ref>). Due to the single viewpoint, occlusion artefacts such as distortions are visible along the line of sight in outdoor scenes. Additionally, as we exploit 2D-3D projection with the FLoSP module (Sec. 3.1), we evaluate the effect of inferring from datasets having various camera setups, showing in <ref type="figure">Fig. 9</ref> that results -though consistent -have increasingly greater distortion when departing from the camera settings of the training set. Broader impact, Ethics. Jointly understanding the 3D geometry and semantics from image paves ways for better mixed reality, photo editing or mobile robotics applications. But the inevitable errors in the scene understanding could have fatal issues (e.g. autonomous driving) and such algorithms should always be seconded by other means.</p><p>We provide details about the main baselines and MonoScene in Appendix A, and include additional qualitative and quantitative results in Appendix B.</p><p>Results on image sequences are in the supplementary video: https://youtu.be/qh7La1tRJmE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architectures details</head><p>A.1. Baselines AICNet <ref type="bibr" target="#b38">[39]</ref>. We use the official implementation of AIC-Net 1 . For the RGB-inferred version, i.e. AICNet rgb , we infer depth with the pre-trained AdaBins <ref type="bibr" target="#b3">[4]</ref> on NYUv2 <ref type="bibr" target="#b57">[58]</ref> and SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> from the official repository 2 .</p><p>3DSketch <ref type="bibr" target="#b8">[9]</ref>. We use 3DSketch official code <ref type="bibr" target="#b2">3</ref> . For 3DSketch rgb , we again use AdaBins (cf . above) and convert depth to TSDF with 'tsdf-fusion' 4 from the 3DMatch Toolbox <ref type="bibr" target="#b73">[74]</ref>.</p><p>JS3C-Net <ref type="bibr" target="#b68">[69]</ref>. We use the official code of JS3C-Net 5 . For JS3C-Net rgb , we generate the input point cloud by unprojecting the predicted depth (using AdaBins) to 3D using the camera intrinsics. The semantic point clouds, required to train JS3C-Net, are obtained by augmenting the unprojected point clouds with the 2D semantics obtained using the code 6 of <ref type="bibr" target="#b81">[82]</ref>.</p><p>LMSCNet <ref type="bibr" target="#b54">[55]</ref>. We use the official implementation of LMSCNet 7 . For LMSCNet rgb , the input occupancy grid is obtained by discretizing the unprojected point cloud. <ref type="figure" target="#fig_0">Fig. 10</ref> details our 3D UNet. Similar to 3DSketch <ref type="bibr" target="#b8">[9]</ref>, we adopt DDR <ref type="bibr" target="#b39">[40]</ref> as the basic building block for large receptive field and low memory cost. The 3D encoder has 2 layers, each downscales by half and has 4 DDR blocks. The 3D decoder has two deconv layers, each doubles the scale. Similar to others <ref type="bibr" target="#b54">[55]</ref> the completion head has an ASPP with dilations <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3)</ref> to gather multi-scale features and an optional deconv to reach output size -used in Se-manticKITTI only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. MonoScene</head><p>For training, MonoScene took 7 hours using 2 V100 32g GPUs (2 items per GPU) on NYUv2 <ref type="bibr" target="#b57">[58]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. NYUv2</head><p>We show additional qualitative results in <ref type="figure" target="#fig_0">Fig. 12</ref>. In overall, MonoScene predicts better scene layouts and better objects geometry, evidently in rows 1-4, 6, 9, 10. Still, MonoScene mispredicts complex (e.g. bookshelfs, row 1, 4, 6), or rare objects (running machine, row 8). Sometimes, it confuses semantically-similar classes (e.g. window/objects, row 6, 8; beds/objects, row 1, 5; furniture/table, row 1, 2) due to the high variance of indoor scene i.e. wide range of camera poses, objects have completely different appearances, poses and positions even in the same category e.g. beds (rows 1, 5-7, 9); sofa (row 2-4).  <ref type="bibr" target="#b8">[9]</ref> x rgb ,x TSDF 33.30 41.32 21.63 0.00 0.00 14.81 18.59 0.00 0.00 0.00 0.00 19.09 0.00 26.40 0.00 0.00 0.00 0.73 0.00 0.00 7.50 AICNet rgb <ref type="bibr" target="#b38">[39]</ref> x rgb ,x depth 29. <ref type="bibr" target="#b58">59</ref>   <ref type="table">Table 6</ref>. Performance on SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> (validation set). We report the performance on semantic scene completion (SSC -mIoU) and scene completion (SC -IoU) for RGB-inferred baselines and our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Input</head><p>IoU mIoU</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D</head><p>SSCNet <ref type="bibr" target="#b58">[59]</ref> x TSDF 29.8 9.5 TS3D <ref type="bibr" target="#b24">[25]</ref> x TSDF +x rgb 29.8 9.5 TS3D+DNet <ref type="bibr" target="#b2">[3]</ref> x TSDF +x rgb 25.0 10.2 ESSCNet <ref type="bibr" target="#b76">[77]</ref> x pts 41.8 17.5 LMSCNet <ref type="bibr" target="#b54">[55]</ref> x occ 56.7 17.6 TS3D+DNet+SATNet <ref type="bibr" target="#b2">[3]</ref> x occ 50.6 17.7 Local-DIFs <ref type="bibr" target="#b52">[53]</ref> x occ 57.7 22.7 JS3C-Net <ref type="bibr" target="#b68">[69]</ref> x pts 56.6 23.8 S3CNet <ref type="bibr" target="#b11">[12]</ref> x occ 45.6 29.5</p><p>2D MonoScene x rgb 34.2 11.1 <ref type="table">Table 7</ref>. Complete SemanticKITTI official benchmark (hidden test set). Results are taken from <ref type="bibr" target="#b55">[56]</ref>. Despite using only single RGB image as input, MonoScene still surpasses some of the SSC baselines with 3D input. <ref type="figure" target="#fig_0">Fig. 13</ref> illustrates the predictions of MonoScene, trained on SemanticKITTI, on datasets with different camera setups. We can see the increase in distortion as the camera setups depart from the ones used during training. Further-  <ref type="table">Table 8</ref>. SemanticKITTI performance (validation set) on in-/out-FOV and the Whole Scene. We report the performance on the scenery inside (in-FOV), outside (out-FOV) camera FOV, and considering all voxels (Whole Scene). MonoScene is best in most cases, with in-FOV performance logically higher. more, the domain gap (i.e. city, country, etc.) also plays an important role. As MonoScene is trained on the mid-size German city of Karlsruhe, with residential scenes and narrow roads, the gap is smaller with KITTI-360 having similar scenes. The results on nuScenes and Cityscapes suffer both from the camera setup changes and the large metropolitan scenes (i.e. Stuttgart -Cityscapes; Singapore, Boston -nuScenes) having wider streets. Input AICNet rgb <ref type="bibr" target="#b38">[39]</ref> LMSCNet rgb <ref type="bibr" target="#b54">[55]</ref> JS3CNet rgb <ref type="bibr" target="#b68">[69]</ref> MonoScene (ours) Ground Truth bicycle car motorcycle truck other vehicle person bicyclist motorcyclist road parking sidewalk other ground building fence vegetation trunk terrain pole traffic sign <ref type="figure" target="#fig_0">Figure 11</ref>. Results on SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> (validation set). The input is shown left. Darker voxels represent the scenery outside the viewing frustum (i.e. unseen by the image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Generalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>LMSCNet rgb <ref type="bibr" target="#b54">[55]</ref> AICNet rgb <ref type="bibr" target="#b38">[39]</ref> 3DSketch rgb <ref type="bibr" target="#b8">[9]</ref> MonoScene (ours) Ground Truth ceiling floor wall window chair sofa table tvs furniture objects <ref type="figure" target="#fig_0">Figure 12</ref>. Results on NYUv2 <ref type="bibr" target="#b57">[58]</ref> (test set). The input is shown leftmost and the camera viewing frustum is shown in the ground truth (rightmost).</p><p>Cityscapes <ref type="bibr" target="#b16">[17]</ref> nuScenes <ref type="bibr" target="#b4">[5]</ref> SemanticKITTI <ref type="bibr" target="#b2">[3]</ref> KITTI-360 <ref type="bibr">[</ref>  <ref type="figure" target="#fig_0">Figure 13</ref>. Domain gap and Camera effects. Outputs of MonoScene when trained on SemanticKITTI having horizontal FOV of 82 ? , and tested on datasets with decreasing (left) or increasing (right) FOV. SemanticKITTI and KITTI-360 are recored in mid-size German city of Karlsruhe while nuScenes and Cityscapes are from large metropolitan areas (e.g. Stuttgart -Cityscapes; Singapore, Boston -nuScenes) whose streets are much wider, denser and have different landscapes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>RGB Semantic Scene Completion with MonoScene.Our framework infers dense semantic scenes, hallucinating scenery outside the field of view of the image (dark voxels, right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Features Line of Sight Projection (FLoSP). We project multi-scale 2D features F 1:s 2D (here, s ? {1, 2, 4, 8}) along their line of sight by sampling (?(?)) them where the 3D voxels centroids (x c ) project (?(?)). This boosts the 2D-3D information flow, and lets the 3D network discover which 2D features are relevant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>2D illustration of 4-way relations. (a) We consider voxel?voxel relations whether one is free or both are occupied, and if their semantics is similar or different. (b) For memory reason, we encode Supervoxel?Voxel relations framed as multi-label classification. ( free, occupied -colors denote semantics)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ours) Ground Truth bicycle car motorcycle truck other vehicle person bicyclist motorcyclist road parking sidewalk other ground building fence vegetation trunk terrain pole traffic sign (b) SemanticKITTI [3] (val set).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Type of 2D-3D features projections. (a) Comparing our FLoSP and 'Ray-traced skip connections' from CoReNet<ref type="bibr" target="#b51">[52]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.49 88.41 4.63 0.25 3.94 32.03 15.44 6.57 0.02 14.51 4.39 15.88 AICNet rgb [39] x rgb ,x depth 30.03 7.58 82.97 9.15 0.05 6.93 35.87 22.92 11.11 0.71 15.90 6.45 18.15 3DSketch rgb</figDesc><table><row><cell></cell><cell></cell><cell>SC</cell><cell>ceiling (1.37%)</cell><cell>floor (17.58%)</cell><cell>wall (15.26%)</cell><cell>window (1.99%)</cell><cell>chair (3.01%)</cell><cell>bed (7.08%)</cell><cell>SSC sofa (4.70%)</cell><cell>table (4.31%)</cell><cell>tvs (0.47%)</cell><cell>furniture (30.04%)</cell><cell>objects (14.19%)</cell></row><row><cell>Method</cell><cell>Input</cell><cell>IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mIoU</cell></row><row><cell cols="2">LMSCNet rgb [55]x occ</cell><cell>33.93 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>38.64 8.53 90.45 9.94 5.67 10.64 42.29 29.21 13.88 9.38 23.83 8.19 22.91 MonoScene (ours) x rgb 42.51 8.89 93.50 12.06 12.57 13.72 48.19 36.11 15.13 15.22 27.96 12.94 26.94</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) NYUv2 (test set)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>SC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>road (15.30%)</cell><cell>sidewalk (11.13%)</cell><cell>parking (1.12%)</cell><cell>other-grnd (0.56%)</cell><cell>building (14.1%)</cell><cell>car (3.92%)</cell><cell>truck (0.16%)</cell><cell>bicycle (0.03%)</cell><cell>motorcycle (0.03%)</cell><cell>other-veh. (0.20%)</cell><cell>vegetation (39.3%)</cell><cell>trunk (0.51%)</cell><cell>terrain (9.17%)</cell><cell>person (0.07%)</cell><cell>bicyclist (0.07%)</cell><cell>motorcyclist. (0.05%)</cell><cell>fence (3.90%)</cell><cell>pole (0.29%)</cell><cell>traf.-sign (0.08%)</cell></row><row><cell>Method</cell><cell cols="2">SSC Input IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mIoU</cell></row><row><cell cols="2">LMSCNet rgb [55]x occ</cell><cell cols="18">31.38 46.70 19.50 13.50 3.10 10.30 14.30 0.30 0.00 0.00 0.00 10.80 0.00 10.40 0.00 0.00 0.00 5.40 0.00 0.00 7.07</cell></row><row><cell>3DSketch rgb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>?0.<ref type="bibr" target="#b14">15</ref> 26.94 ?0.<ref type="bibr" target="#b9">10</ref> 37.12 ?0.<ref type="bibr" target="#b14">15</ref> 11.50 ?0.14 Ours w/o FLoSP 28.39 ?0.53 14.11 ?0.21 27.55 ?0.87 4.78?0.10 Ours w/o 3D CRP 41.39 ?0.08 26.27 ?0.15 36.20 ?0.19 10.96 ?0.21 Ours w/o L sem scal 42.82 ?0.22 25.33 ?0.26 36.78 ?0.34 9.89 ?0.11 Ours w/o L geo scal 40.96 ?0.28 26.34 ?0.23 34.92 ?0.34 11.35 ?0.22 Ours w/o L fp 41.90 ?0.26 26.37 ?0.16 36.74 ?0.33 11.11 ?0.24</figDesc><table><row><cell></cell><cell cols="2">NYUv2</cell><cell cols="2">SemanticKITTI</cell></row><row><cell></cell><cell>IoU ?</cell><cell>mIoU ?</cell><cell>IoU ?</cell><cell>mIoU ?</cell></row><row><cell>Ours</cell><cell>42.51</cell><cell></cell><cell></cell></row><row><cell>cars in rows 1-3 having better shapes. Interestingly, de-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>spite a narrow camera field of view (FOV) with respect to</cell><cell></cell><cell></cell><cell></cell></row><row><cell>the scene, MonoScene properly hallucinates scenery not im-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>aged, i.e. outside of the camera FOV (darker voxels). This</cell><cell></cell><cell></cell><cell></cell></row><row><cell>is striking in rows 3,4 where the bottom part of the scene</cell><cell></cell><cell></cell><cell></cell></row><row><cell>is reasonably guessed, though not in the viewing frustum.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Appendix B.1 provides in-/out-FOV performance details.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.2.2 Comparison against 2.5/3D-input baselines</cell><cell></cell><cell></cell><cell></cell></row></table><note>2.5/3D input baselines. Despite a single RGB, MonoScene still outperforms the mIoU of some indoor baselines.For completeness, we also compare with some original baselines (i.e. using real 3D input) in Tab. 2. Despite the unfair setup since we use only RGB, in NYUv2 (Tab. 2a) we still beat the recent LMSCNet and AICNet in mIoU by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Architecture ablation. Our components boost performance on NYUv2<ref type="bibr" target="#b57">[58]</ref> (test set) and SemanticKitti<ref type="bibr" target="#b2">[3]</ref> (val. set). a comfortable margin (+6.48 and +3.17), but with a lower IoU (-1.6 and -1.26). Of note AICNet also uses RGB in addition to depth, showing our method excels at recovering geometry from image only. 3DSketch, using RGB + TSDF, outperforms us on both mIoU and IoU showing the benefit of TSDF for SSC as mentioned in<ref type="bibr" target="#b55">[56]</ref>. In SemanticKITTI (Tab. 2b), the baselines clearly surpass us in all metrics which relates both to the lidar-originated 3D input having a much wider horizontal FOV than the camera (180 ? vs 82 ? ), and to the far more complex and cluttered outdoor scenesthus harder to reconstruct from a single image viewpoint.The large 2D -2.5/3D gaps in Tab. 2 partly result of the low depth accuracy. For example, on NYUv2/Sem.KITTI AdaBins<ref type="bibr" target="#b3">[4]</ref> gets 0.36/2.36m RMSE while voxels size is</figDesc><table /><note>Input LMSCNet rgb</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>). 0.08/0.2m. Furthermore, as we account for occluded voxels, our geometry is expectedly better. This is assessed using MonoScene geometry as input of LMSCNet, which improves IoU/mIoU on Sem.KITTI val. set from 28.61/6.70 to 35.94/9.44. Still, Tab. 3 shows that MonoScene predicts better geometry and semantics reaching 37.12/11.50.4.3. Ablation studiesWe ablate our MonoScene framework on both NYUv2 (test set) and SemanticKITTI (validation set), and report the average of 3 runs to account for training variability. Architectural components. Tab. 3 shows that all components contribute to the best results. For 'w/o FLoSP', we instead interpolate and convolve the 2D decoder features to the required 3D UNet input size. Specifically, FLoSP (Sec. 3.1) is shown to be the most crucial as it im-proves remarkably both semantics ([+12.83,+6.72] mIoU) and geometry ([+14.11,+9.56] IoU). 3D CRP (Sec. 3.2) contributes equally to IoU (in [+0.77,+1.12]) and mIoU (in [+0.54, +1.33]). Both SCAL losses (Sec. 3.3.1) contribute differently as expected, since L sem scal helps semantics (+1.61 mIoU in both), while L geo scal boosts geometry ([+1.55, +2.20] IoU). In NYUv2 only, L sem scal harms IoU (-0.31) but improves the same metric on SemanticKITTI (+0.34). Finally, the frustums proportion loss (Sec. 3.3.2) boosts both metrics on both datasets by at least +0.38 and up to +0.61. Effect of features projection. We now study in-depth the effect of FLoSP (Sec. 3.1) at the core our RGB-based task. In Tab. 4a, we use our FLoSP projecting only 2D features from given 2D scales by changing S in Eq. (1). More 2D scales boosts IoU and mIoU consistently and leans to lower variance -showing (1,2,4,8) projections are indeed best. 42.51 ?0.15 26.94 ?0.10 1, 2, 4 42.08 ?0.69 26.28 ?0.24 1, 2 41.56 ?0.18 25.66 ?0.21 1 41.57 ?0.11 25.61 ?0.43 w/o FLoSP 28.39 ?0.53 14.11 ?0.21 ?0.15 26.94 ?0.10 42.24 ?0.15 26.55 ?0.29 2 42.09 ?0.15 26.63 ?0.05 42.15 ?0.26 26.47 ?0.16 w/o 3D CRP 41.39 ?0.08 26.27 ?0.15</figDesc><table><row><cell>2D scales (S)</cell><cell>IoU ?</cell><cell>mIoU ?</cell><cell>n L rel</cell><cell>IoU ?</cell><cell>mIoU ?</cell></row><row><cell cols="3">1, 2, 4, 8 (a) Scales in FLoSP</cell><cell cols="3">4 42.51 (b) Relations and supervision in 3D CRP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Study of FLoSP and 3D CRP.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Projecting from differ-</cell></row><row><cell cols="5">ent 2D scales (S) in FLoSP (Sec. 3.1) show more scales is better.</cell></row><row><cell cols="5">(b) In our 3D CRP (Sec. 3.2) using more relations (n) and super-</cell></row><row><cell cols="5">vision (L rel ) lead to higher metrics. Results are on NYUv2.</cell></row><row><cell></cell><cell cols="3">Ray-traced skip conn.</cell><cell></cell></row><row><cell>CoReNet</cell><cell>E</cell><cell>Pool &amp; Deconv</cell><cell>D</cell><cell>IoU ? CoReNet (1,2,4) 30.60 ?0.46 17.34 ?0.37 mIoU ?</cell></row><row><cell>Ours-light</cell><cell>E</cell><cell>FLoSP</cell><cell>D</cell><cell>Ours-light (1,2,4) 40.80 ?0.43 25.90 ?0.63 Ours-light (1) 40.13 ?0.31 25.33 ?0.58</cell></row><row><cell cols="4">(a) Architectures (2D or 3D)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>? 8 42.51 ?0.15 26.94 ?0.10 37.12 ?0.15 11.50 ?0.14 4 ? 4 42.52 ?0.12 26.85 ?0.19 37.09 ?0.09 11.45 ?0.15 2 ? 2 42.41 ?0.13 26.85 ?0.22 36.88 ?0.11 11.27 ?0.25 1 ? 1 42.39 ?0.18 26.52 ?0.31 36.83 ?0.42 11.33 ?0.11 w/o L fp 41.90 ?0.26 26.37 ?0.16 36.74 ?0.33 11.11 ?0.24</figDesc><table><row><cell></cell><cell cols="2">NYUv2</cell><cell cols="2">SemanticKITTI</cell></row><row><cell>?</cell><cell>IoU ?</cell><cell>mIoU ?</cell><cell>IoU ?</cell><cell>mIoU ?</cell></row><row><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>MonoScene 3D network. The 3D UNet uses 2 downscale layers with DDR blocks<ref type="bibr" target="#b39">[40]</ref> and 2 upscale layers with deconv. The completion head uses ASPP and an optional deconv layer. Notations: DDR(dilation, downsample rate), Deconv(kernel size, dilation), ASPP(dilations). We report performance on validation set in Tab. 6. Comparing against the test set performance from the main paper Tab. 1b, we notice MonoScene generalizes better than JS3C-Net rgb and AICNet rgb since the validation and test set gap is smaller (?0.42 vs ?1.34 and ?1.22). We also report the complete SemanticKITTI official benchmark (i.e. hidden test set) in Tab.<ref type="bibr" target="#b6">7</ref> showing that while MonoScene uses only RGB, it still outperforms some of the 3D input SSC baselines.Evaluation scope. Tab. 8 reports the performance when considering either only the voxels inside FOV (in-FOV), outside FOV (out-FOV), or all voxels (Whole Scene) as reported in the main paper. Compared to the Whole Scene, the in-FOV performance is higher since it considers visible surfaces, whereas the out-FOV performance is significantly lower since the image does not observe it.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Skip connection</cell><cell></cell><cell></cell><cell cols="2">Optional</cell></row><row><cell>DDR(1, 2)</cell><cell>DDR(1, 1)</cell><cell>DDR(2, 1)</cell><cell>DDR(3, 1)</cell><cell>DDR(1, 2)</cell><cell>DDR(1, 1)</cell><cell>DDR(2, 1)</cell><cell>DDR(3, 1)</cell><cell>3D CRP</cell><cell>Deconv(3, 2)</cell><cell>Deconv(3, 2)</cell><cell>Deconv(3, 2)</cell><cell>ASPP(1, 2, 3)</cell></row><row><cell cols="2">3D encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3D decoder</cell><cell></cell><cell cols="2">Completion head</cell></row><row><cell cols="8">Figure 10. B. Additional results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">B.1. SemanticKITTI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">Quantitative performance. Qualitative performance. In Fig. 11 we also include</cell></row><row><cell cols="13">additional qualitative results. Compared to all baselines,</cell></row><row><cell cols="13">MonoScene captures better landscape and objects (e.g. cars,</cell></row><row><cell cols="13">rows 3-9; pedestrian, rows 6, 10; traffic-sign, rows 3, 5).</cell></row><row><cell cols="13">Still, it struggles to predict thin small objects (e.g. trunk,</cell></row><row><cell cols="13">row 1; pedestrian, row 3; traffic-sign, row 2, 6), separate</cell></row><row><cell cols="13">far away consecutive cars (e.g. row 5, 7, 8), and infer very</cell></row><row><cell cols="13">complex, highly cluttered scenes (e.g. rows 9, 10).</cell></row><row><cell>and 28 hours to</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>train using 4 V100 32g GPUs (1 item per GPU) on Se-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>manticKITTI [3].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 https://github.com/waterljwant/SSC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 https://github.com/shariqfarooq123/AdaBins</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3 https://github.com/charlesCXK/TorchSSC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 https://github.com/andyzeng/tsdf-fusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 https://github.com/yanx27/JS3C-Net</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6 https://github.com/YeLyuUT/SSeg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7 https://github.com/cv-rits/LMSCNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>43.55 20.55 11.97 0.07 12.94 14.71 4.53 0.00 0.00 0.00 15.37 2.90 28.71 0.00 0.00 0.00 2.52 0.06 0.00 8.31 *JS3C-Net rgb [69]x pts 38.98 50.49 23.74 11.94 0.07 15.03 24.65 4.41 0.00 0.00 6.15 18.11 4.33 26.86 0.67 0.27 0.00 3.94 3.77 1.45 10.31 MonoScene (ours) x rgb 37.12 57.47 27.05 15.72 0.87 14.24 23.55 7.83 0.20 0.77 3.59 18.12 2.57 30.76 1.79 1.03 0.00 6.39 4.11 2.48 11.50</figDesc><table /><note>* Uses pretrained semantic segmentation network.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>IoU ? mIoU ? IoU ? mIoU ? LMSCNet rgb [55] 37.62 8.87 25.36 5.48 34.41 8.17 3DSketch rgb [9] 32.24 7.82 26.50 5.83 33.30 7.50 AICNet rgb [39] 35.69 8.75 25.79 5.61 29.59 8.31 *JS3C-Net rgb [69] 42.22 11.29 28.27 6.31 38.98 10.31 MonoScene(ours) 39.13 12.78 31.60 7.45 37.12 11.50 * Uses pretrained semantic segmentation network.</figDesc><table><row><cell>in-FOV</cell><cell>out-FOV</cell><cell>Whole Scene</cell></row><row><cell>IoU ? mIoU ?</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>44](49 ? , 25 ? ) (65 ? , 39 ? ) (H=82 ? , V=29 ? ) (104 ? , 38 ? )</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment This work used HPC resources from GENCI-IDRIS (Grant 2021-AD011012808). It was done in the SAMBA collaborative project, co-funded by BpiFrance in the Investissement d'Avenir Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scan2cad: Learning cad model alignment in rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Avetisyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Dahnert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AdaBins: Depth Estimation using Adaptive Bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Shariq Farooq Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic scene completion via integrating instances and scene in-the-loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>T-PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Am2fnet: Attention-based multiscale &amp; multi-modality fused network. ROBIO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlong</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gang Zeng, and Hongsheng Li. 3d sketch-aware semantic scene completion via semi-supervised structure prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d semantic scene completion from a single depth image using adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Tung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bsp-net: Generating compact meshes via binary space partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">S3cnet: A sparse semantic scene completion network for lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Agia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Nassir Navab, and Federico Tombari. Scfusion: Real-time incremental scene reconstruction with semantic completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Shun Cheng Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tateno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning priors for semantic 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Cherabier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fran?ois Rameau, Minjun Kang, and In-So Kweon. Volumefusion: Deep depth fusion for 3d scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Panoptic 3d scene reconstruction from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Dahnert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sg-nn: Sparse generative neural networks for self-supervised scene completion of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Diller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">EdgeNet: Semantic scene completion from a single RGB-D image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aloisio</forename><surname>Dourado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Teofilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansung</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single 360-degree image and depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dourado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISAPP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single-View 3D reconstruction: A Survey of deep learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Fahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Zarif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two stream 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mesh r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Location field descriptors: Single image 3d model retrieval in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic scene completion combining colour and depth: preliminary experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre Bernardes Soares</forename><surname>Guedes</surname></persName>
		</author>
		<idno>abs/1802.04735</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Te?filo Em?dio de Campos, and Adrian Hilton</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Image-based 3d object reconstruction: State-of-the-art and trends in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Feng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Laga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>T-PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Holistic 3d scene parsing and reconstruction from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxue</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Im2cad</surname></persName>
		</author>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-scale volumetric scene reconstruction using lidar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilman</forename><surname>K?hner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>K?mmerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Roomnet: End-to-end room layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imenet: Joint 3d semantic scene completion and 2d semantic segmentation through iterative mutual enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laiyan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Anisotropic convolutional networks for 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rgbd based dimensional decomposition residual network for 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Depth based semantic scene completion with position importance aware loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>RA-L</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention-based multi-modal fusion network for semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep marching cubes: Learning explicit surface representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Donn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">KITTI-360: A novel dataset and benchmarks for urban scene understanding in 2d and 3d. arXiv.org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">See and Think: Disentangling Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shice</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y U</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beibei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Total3dunderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinyu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian Jun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DeepSDF: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">High-precision depth estimation with the 3d lidar and stereo fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Convolutional occupancy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyou</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Corenet: Coherent 3d scene reconstruction from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bauszat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Semantic scene completion using local deep implicit functions on lidar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Emmerichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dariu</forename><surname>Gavrila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Machine perception of threedimensional solids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Lmscnet: Lightweight multiscale 3d semantic completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Rold?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Rold?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<title level="m">3D Semantic Scene Completion: a Survey. IJCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Make3d: Learning 3d scene structure from a single still image. T-PAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<editor>Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Horizonnet: Learning room layout with 1d representation and pano stretch data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wei</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Poisson Surface Reconstruction for LiDAR Odometry and Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chebrolu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">ForkNet: Multi-branch volumetric semantic completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Joseph</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Pix2vox: Context-aware 3d reconstruction from single and multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Pix2vox++: Multi-scale contextaware 3d object reconstruction from single and multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radomir</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pointflow: 3d point cloud generation with continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">3dmatch: Learning local geometric descriptors from rgb-d reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Holistic 3d scene understanding from a single image with implicit representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Efficient semantic scene completion network with spatial group convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongen</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Perceiving 3d human-object spatial arrangements from a single image in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Pepose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Cascaded context pyramid for full-resolution 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Sing</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Edwin</forename><surname>Cryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<title level="m">Shape-from-shading: a survey. T-PAMI</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Semantic point completion network for 3D semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Layoutnet: Reconstructing the 3d room layout from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
