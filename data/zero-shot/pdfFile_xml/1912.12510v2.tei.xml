<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Out-of-Distribution Examples with In-distribution Examples and Gram Matrices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandramouli</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
						</author>
						<title level="a" type="main">Detecting Out-of-Distribution Examples with In-distribution Examples and Gram Matrices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Vector Institute, Toronto Dalhousie University, Halifax Sageev Oore Vector Institute, Toronto Dalhousie University, Halifax</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When presented with Out-of-Distribution (OOD) examples, deep neural networks yield confident, incorrect predictions. Detecting OOD examples is challenging, and the potential risks are high. In this paper, we propose to detect OOD examples by identifying inconsistencies between activity patterns and class predicted. We find that characterizing activity patterns by Gram matrices and identifying anomalies in gram matrix values can yield high OOD detection rates. We identify anomalies in the gram matrices by simply comparing each value with its respective range observed over the training data. Unlike many approaches, this can be used with any pre-trained softmax classifier and does not require access to OOD data for finetuning hyperparameters, nor does it require OOD access for inferring parameters. The method is applicable across a variety of architectures and vision datasets and, for the important and surprisingly hard task of detecting far-from-distribution out-of-distribution examples, it generally performs better than or equal to state-ofthe-art OOD detection methods (including those that do assume access to OOD examples). 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b16">Lee et al. [2018b]</ref> <p>and <ref type="bibr" target="#b17">Liang et al. [2018]</ref> add small input perturbations to achieve better results; the former do so to increase the confidence score, while the latter do so to increase the softmax score. <ref type="bibr" target="#b23">Quintanilha et al. [2019]</ref> achieve results comparable to that of <ref type="bibr" target="#b16">Lee et al. [2018b]</ref> by training a logistic regression detector that looks at the means and standard deviations of various channels activations. Unlike the previous two techniques, <ref type="bibr" target="#b23">Quintanilha et al. [2019]</ref> achieves comparable results even without the use of input perturbations, which allows it to be applicable to non-continuous domains. Our work, too, does not involve input perturbations.</p><p>Recently, <ref type="bibr" target="#b0">Abdelzad et al. [2019]</ref> propose to detect OOD examples by training a one-class detector over the representations of an intermediate layer, chosen for each OOD detection task.</p><p>All of these techniques depend on OOD examples for fine-tuning hyperparameters <ref type="bibr" target="#b17">[Liang et al., 2018</ref> or for training auxiliary OOD classifiers <ref type="bibr" target="#b16">(Lee et al. [2018b]</ref>, <ref type="bibr" target="#b23">Quintanilha et al. [2019]</ref>). Furthermore, these classifiers neither transfer between one non-training distribution and another, nor do they transfer between networks, so separate classifiers must be trained for each (In-Distribution, OOD, Architecture) triplet. In many real-world applications, we may not be able to assume advance access to all possible OOD distributions. Motivated by this observation, our work does not require access to OOD samples.</p><p>Alternative Training Strategies. <ref type="bibr" target="#b15">Lee et al. [2018a]</ref> jointly train a classifier, a generator and an adversarial discriminator such that the classifier produces a more uniform distribution on the boundary examples generated by the generator; they use OOD examples to fine-tune hyperparameters. DeVries and Taylor <ref type="bibr">[2018]</ref> train neural networks with a multi-task loss for jointly learning to classify and estimate confidence. <ref type="bibr" target="#b27">Shalev et al. [2018]</ref> use multiple semantic dense representations as the target instead of sparse one-hot vectors and use a cosine-similarity based measure for detecting OODs. Building on the idea proposed by <ref type="bibr" target="#b15">Lee et al. [2018a]</ref>, <ref type="bibr" target="#b11">Hendrycks et al. [2019a]</ref> propose an Outlier Exposure (OE) technique. They regularize a softmax classifier to predict uniform distribution on (any) OOD distribution and show the resulting model can identify examples from unseen OOD distributions; this differs significantly from previous works which used the same OOD distributions for both training and testing. Unlike other methods, they retain the architecture of the classifier and introduce just one additional hyperparameter-the regularization rate-and also demonstrate that their model is quite robust to the choice of OOD examples chosen for the regularization. However, while the OE method is able to generalize across different non-training distributions, it understandably does not achieve the rates of <ref type="bibr" target="#b16">Lee et al. [2018b]</ref> on most cases. In the same vein,  propose a strategy to generate boundary OOD examples to train a classifer with a reject option. Similarly, <ref type="bibr">Yu and Aizawa [2019]</ref> propose to train a two-head CNN on in-distribution data with different decision boundaries by encouraging a higher discrepancy in predictions on unlabeled OOD data. <ref type="bibr" target="#b8">Golan and El-Yaniv [2018]</ref> show how self-supervised classifiers trained to predict geometrical transformations in the input image can be used for one-class OOD detection. Recently, <ref type="bibr" target="#b12">Hendrycks et al. [2019b]</ref> make significant advances in detecting near-distribution outliers without having any knowledge of the exact out-of-distribution examples by using in-distribution examples in a selfsupervised training setting.</p><p>Generative Models. <ref type="bibr" target="#b24">Ren et al. [2019]</ref> hypothesize that stylistic factors might impact the likelihood assignment and propose to detect OOD examples by computing a likelihood ratio which depends on the semantic factors that remain after the dominant stylistic factors are cancelled out. On the other hand, <ref type="bibr" target="#b20">Nalisnick et al. [2019b]</ref> argue that samples generated by a generative model reside in the typical set, which might not necessarily coincide with areas of high density. They demonstrate empirically that OOD examples can be identified by checking if an input resides in the typical set of the generative model. Unlike the standard experimental setting, they aim to identify distributional shift, which predicts if a batch of examples are OOD. Several other recent works <ref type="bibr" target="#b14">[Huang et al., 2019</ref><ref type="bibr" target="#b26">, Serr? et al., 2019</ref><ref type="bibr" target="#b4">, Daxberger and Hern?ndez-Lobato, 2019</ref><ref type="bibr" target="#b28">, Song et al., 2019</ref> also aim to solve these problems.</p><p>that works across architectures. Gram matrices can be used to compute pairwise feature correlations, and are often used in DNNs to encode stylistic attributes like textures and patterns <ref type="bibr" target="#b7">[Gatys et al., 2016]</ref>. We extend these matrices as will be described below, and then use them to compute class-conditional bounds of feature correlations at multiple layers of the network. Starting with a pre-trained network, we compute these bounds over only the training set, and then use them at test time to effectively discriminate between in-distribution samples and out-of-distribution samples. Unlike other SOTA algorithms, we do not need to "look" at any out-of-distribution samples to tune any parameters; the only tuning required is that of a normalizing factor, which we compute using a randomly-selected validation partition of the (in-distribution) test set.</p><p>Notation If the considered deep convolutional network has L layers and the l th layer has n l channels, we consider feature co-occurrences between the 1&lt;=l&lt;=L n l * (n l +1) 2 pairs of featuremaps. (Note that by "layer" we refer to any set of values obtained immediately after applying convolution or activation functions.) We use the following notation:</p><formula xml:id="formula_0">F l (D)</formula><p>The feature map at the l-th layer for input image D; when referring to an arbitrary image D, we just write F l . It can be stored in a matrix of dimensions n l ? p l , where n l is the number of channels at the l-th layer and p l , the number of pixels per channel, is the height times the width of the feature map.</p><formula xml:id="formula_1">D c /f (D)</formula><p>The predicted class for input image D Train</p><p>The Gram Matrices and Higher order Gram Matrices We compute pairwise feature correlations between channels of the l-th layer using the Gram matrix:</p><formula xml:id="formula_2">G l = F l F l<label>(1)</label></formula><p>where F l is an n l ? p l matrix as defined above.</p><p>In order to compute feature correlations with more prominent activations of the feature maps, we define a higher-order gram matrix, which we write G p l , to be a matrix computed identically to the regular Gram matrix, but where, instead of using a raw channel activation a, we use a p , the p th power of each activation. G p l is therefore computed using F p l , where the power of F l is computed element-wise; in an effort to retain uniform scale across all orders of Gram matrices for a given layer, we compute the (element-wise) p-th root. The p-th order gram matrix is thus computed as:</p><formula xml:id="formula_3">G p l = F p l F p l 1 p<label>(2)</label></formula><p>We show in Section 5 that higher p values help significantly in improving the OOD detectability. In our experiments, we limit the value of p to 10, as exponents beyond 10 are not worth the extra computation that is needed to avoid overflow errors 2 .</p><p>The flattened upper (or lower) triangular matrix along with the diagonal entries is denoted as G p l . The set of all orders of gram matrices (in our case {1, . . . 10}) to be considered is denoted by P . The schematic diagram of the proposed algorithm is shown in <ref type="figure" target="#fig_1">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(in Appendix A).</head><p>Preprocessing If we compute G p l for every layer l and every order p ? P , we obtain a total of N S = p?P L l=1 1 2 n l (n l + 1) correlations for any image D. The preprocessing involves computing the class-specific minimum and maximum values for the correlations: for every class c, the minimum and maximum values for each of the N S correlations are computed over all training examples D classified as c. We keep track of the minimum and maximum values of the N S correlations for all the classes in 4-D arrays Mins and Maxs, each of the order C ? L ? |P | ? max 1?l?L n l (n l +1) 2 .</p><p>Since each layer has different number of channels, the 4-th dimension must be large enough to accommodate the layer with the highest number of channels.</p><p>Algorithm 1 Compute the minimum and maximum values of feature co-occurrences for each class, layer and order Computing Layerwise Deviations Given the class-specific minimum and maximum values of the N S feature correlations, we can compute the deviation of the test sample from the images seen at train time with respect to each of the layers. In order to account for the scale of values, we compute the deviation as the percentage change with respect to the maximum or minimum values of feature co-occurrences; the deviation of an observed correlation value from the minimum and maximum correlation values observed during train time can be computed as:</p><formula xml:id="formula_4">?(min,max,value) = ? ? ? ? ? 0 if min ? value ? max min?value |min| if value &lt; min value?max |max| if value &gt; max<label>(3)</label></formula><p>The deviation of a test image with respect to a given layer l is the sum total of the deviations with respect to each of the p?P 1 2 n l (n l + 1) correlation values:</p><formula xml:id="formula_5">? l (D) = P p=1 1 2 n l (n l +1) i=1 ? Mins[D c ][l][p][i], Maxs[D c ][l][p][i], G p l (D)[i]<label>(4)</label></formula><p>Total Deviation of a test image D (?(D)), is computed by taking the sum total of the layerwise deviations (? l (D)). However, the scale of layerwise deviations (? l ) varies with each layer depending on the number of channels in the layer, number of pixels per channel and semantic information contained in the layer. Therefore, we normalize the deviations by dividing it by E Va [? l ], the expected deviation at layer ? l , computed using the validation data. Note that we use the same normalizing factor irrespective of the class assigned.</p><formula xml:id="formula_6">?(D) = L l=1 ? l (D) E Va [? l ]<label>(5)</label></formula><p>Threshold As is standard <ref type="bibr" target="#b16">[Lee et al., 2018b]</ref>, a threshold, ? , for discriminating between out-ofdistribution data and in-distribution data is computed as the 95th percentile of the total deviations Can work with pre-trained Net?</p><p>Can work without knowledge of OOD test examples? DPN <ref type="bibr" target="#b18">[Malinin and Gales, 2018]</ref> Semantic <ref type="bibr" target="#b27">[Shalev et al., 2018]</ref> Variational Dirichlet <ref type="bibr" target="#b1">[Chen et al., 2019]</ref> Mahalanobis <ref type="bibr" target="#b16">[Lee et al., 2018b]</ref> ODIN <ref type="bibr" target="#b17">[Liang et al., 2018]</ref> OE <ref type="bibr" target="#b11">[Hendrycks et al., 2019a]</ref> Baseline <ref type="bibr" target="#b10">[Hendrycks and Gimpel, 2017]</ref> Ours <ref type="table">Table 1</ref>: List of closely related methods. Note: OE uses OOD examples during training, but unrelated to test of test data (?(D)). In other words, the threshold is computed so that 95% of test examples have deviations lesser than the threshold ? ; the threshold-based discriminator can be formally written as:</p><formula xml:id="formula_7">isOOD(D) = True if ?(D) &gt; ?, False if ?(D) ? ?<label>(6)</label></formula><p>Computational Complexity. In order to reduce computational time, we can in fact compute deviations based on row-wise sums rather than individual elements. This would mean that the variable stat, defined in line 8 of Algorithm 1, would now contain row-wise sums of G p l instead of the flattened upper triangular matrix; the inner loop of Eq. 4 would loop over n l elements instead of 1 2 n l (n l + 1) elements while also reducing the storage required for Mins and Maxs. In practise, we found that computing the anomalies this way yields differences of less than 0.5%, and usually imperceptible, so the results described in the next section were computed in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments -Detecting OOD</head><p>In this section, we demonstrate the effectiveness of the proposed metric using competitive deep convolutional neural network architectures such as DenseNet and ResNet on various computer vision benchmark datasets such as: CIFAR-10, CIFAR-100, SVHN, TinyImageNet, LSUN and iSUN.</p><p>For fair comparison and to aid reproducibility, we use the pretrained ResNet <ref type="bibr" target="#b9">[He et al., 2016]</ref> and DenseNet <ref type="bibr" target="#b13">[Huang et al., 2017</ref>] models open-sourced by <ref type="bibr" target="#b16">Lee et al. [2018b]</ref>, i.e. ResNet34 and DenseNet3 models trained on CIFAR-10, CIFAR-100 and SVHN datasets. For each of these models, we considered the corresponding test partitions as the in-distribution (positive) examples. For CIFAR-10 and CIFAR-100, we considered the out-of-distribution datasets used by <ref type="bibr" target="#b16">Lee et al. [2018b]</ref>: TinyImagenet, LSUN and SVHN. Additionally, we also considered the iSUN dataset. For ResNet and DenseNet models trained on SVHN, we used considered CIFAR-10 dataset as the third OOD dataset. Details on these datasets are available in Appendix B.</p><p>We benchmark our algorithm with the works listed in <ref type="table">Table 1</ref> using the following metrics:</p><p>1. TNR@95TPR is the probability that an OOD (negative) example is correctly identified when the true positive rate (TPR) is as high as 95%. TPR can be computed as T P R = T P/(T P + F N ), where TP and FN denote True Positive and False Negative respectively.</p><p>2. Detection Accuracy measures the maximum possible classification accuracy over all possible thresholds in discriminating between in-distribution and out-of-distribution examples. For those methods which assign a higher-score to the in-distribution examples, it can be calculated as max ? {0.5P in (f (x) ? ? ) + 0.5P out (f (x) &lt; ? )}; for those methods which assign a lower score to in-distribution examples, it can be calculated as</p><formula xml:id="formula_8">max ? {0.5P in (f (x) ? ? ) + 0.5P out (f (x) &gt; ? )}.</formula><p>3. AUROC is the measure of the area under the plot of TPR vs FPR. For example, for those methods which assign a higher score to the in-distribution examples, this measures the probability that an OOD example is assigned a lower score than an in-distribution example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setup:</head><p>We use a pre-trained network to extract class-specific minimum and maximum correlation values for all pairs of features across all orders of gram matrices. Subsequently, the total deviation is computed for each example following Eq. 5. Since the total deviation values depend on the randomly selected validation examples, we repeat the experiment 10 times to get a reliable estimate of the performance. The OOD detection performance for several combinations of model architecture, in-distribution dataset and out-of-distribution dataset are shown in The results of <ref type="table" target="#tab_2">Table 2</ref> show that at a glance, over a total of 24 combinations of model architecture/indistribution-dataset/out-of-distribution-datasets, the proposed method outperforms the previous competing methods in 15 of them, is on par in 6 of them, and gives second highest results on 3 of them 3 . Furthermore, it does so without requiring access to samples from the OOD dataset. If the hyperparameters and/or parameters of Mahalanobis and ODIN algorithms are fine-tuned using FGSM adversarial examples instead of the real OOD examples, their performance decreases. We also observe that our performance is similar for both architectures.</p><p>We also performed experiments with fully-connected networks by using three different MLP architectures trained on MNIST; Fashion-MNIST <ref type="bibr" target="#b30">[Xiao et al., 2017]</ref> and KMNIST <ref type="bibr" target="#b3">[Clanuwat et al., 2018]</ref> were considered as the out-of-distribution datasets (Results are provided in Appendix F.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-dist</head><p>Mean  <ref type="bibr" target="#b11">[Hendrycks et al., 2019a]</ref>. Since OE uses a different model from ours, we also report the corresponding baseline accuracy. We extract the mean TNR @ TPR95 for our technique by considering both ResNet and DenseNet models. Some more results are available in Appendix F.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Near-distribution Outliers</head><p>The metric does not perform well for near-distribution outliers (for example, CIFAR10 vs CIFAR100) -the detailed results can be seen in <ref type="table" target="#tab_5">Table 4</ref>; a recent work which performs very well for near-distribution outliers and yet does not require knowledge of OOD distribution is <ref type="bibr" target="#b12">Hendrycks et al. [2019b]</ref>.</p><p>Source of Performance Gain The above results are obtained when we consider all elements of gram matrix (Algorithm 1), compute deviations from extrema (Eq 3) and finally, compute the total deviation with normalized layerwise deviations (Eq 5). In order to better understand the source of performance gain, we conduct a detailed ablation study that considers alternative choices for the three steps outlined before; the alternative choices are chosen in order to answer the following questions: Q1) What if strictly diagonal elements or strictly off-diagonal elements are considered instead of complete Gram Matrix?; Q2) What happens if the deviation is computed from the mean instead of the extrema?; Q3) What happens if we do not normalize the layerwise deviations? In all, we conduct 12 experiments: 3 choices for Q1 ? 2 choices for Q2 ? 2 choices for Q3. As a broad summary, we find that, while there is no single rule that is unbroken by an exception, our proposed combination-i.e. using the complete Gram matrix, using the min/max metric, and using normalization as in Eq 5-is generally more robust than any of the other combinations that we tried. More details on the ablation tests and discussions are available in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>Beyond explicit OOD detection, this line of work may ultimately help better interpret neural networks' responses to OOD examples. With this goal in mind, and at the same time to clarify the internal mechanism of our method, we perform tests to address the following two questions:</p><p>1. Which layer representations are most useful? In order to examine the role of the depth at which we compute G in detecting OODs, we construct detectors which make use of correlations derived from just one residual or dense block at a time; however, all orders of gram matrices are considered. Representative results are shown in <ref type="figure">Figure 1</ref>. For all combinations of model/in-distribution/out-of-distribution-dataset, we find that the lower level representations are much more informative in discriminating between in-distribution and out-of-distribution datasets. However, the difference in detective power depends on the in-distribution dataset considered: for example, the difference in detective power between higher-level representations and lower-level representations is bigger for Cifar-100 than for Cifar-10. More graphs are available in Appendix C.1.</p><p>2. Which orders of gram matrices are most useful? In order to understand which orders of gram matrices are most helpful in detecting OODs, we construct detectors which make use of only one order of gram matrix at a time; however, correlations are derived from the representations of all residual and dense blocks. Representative results are shown in <ref type="figure">Figure 2</ref>. For all combinations of model/in-distribution/out-of-distribution-dataset, we find that the higher order gram matrices are much more informative in discriminating between in-distribution and out-of-distribution datasets. Ignoring the variations at orders greater than 4, we find that the TNR @ 95TPR increases with higher orders and finally saturates. More graphs are available in Appendix C in <ref type="figure">Figure 4</ref> to 9.</p><p>Conclusion. Out-of-distribution detection is a challenging and important problem. We have proposed and reported on a relatively simple OOD detection method based on pairwise feature correlations that gives new state of the art detection results without requiring access to anything other than the training data itself.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Description of OOD Datasets</head><p>The following includes the description of the out-of-distribution datasets:</p><p>1. TinyImagenet, a subset of ImageNet <ref type="bibr" target="#b25">[Russakovsky et al., 2015]</ref> images, contains 10,000 test images from 200 different classes. Each image is downsampled to size 32 x 32 and all 10,000 images are used, as given in the opensourced version by <ref type="bibr" target="#b17">Liang et al. [2018]</ref>.</p><p>2. LSUN, the Large-scale Scene UNderstanding dataset <ref type="bibr" target="#b33">[Yu et al., 2015]</ref> has 10,000 test images from 10 different scenes. Each image is downsampled to size 32 x 32 and all 10,000 images are used, as given in the opensourced version by <ref type="bibr" target="#b17">Liang et al. [2018]</ref>.</p><p>3. iSUN, a subset of SUN images <ref type="bibr" target="#b31">[Xiao et al., 2010]</ref>, consists of 8925 images. Each image is downsampled to size 32 x 32 and is used; the downsampled version of the dataset has been opensourced by <ref type="bibr" target="#b17">Liang et al. [2018]</ref>.</p><p>4. SVHN, the Street View House Numbers dataset <ref type="bibr" target="#b21">[Netzer et al., 2011]</ref>, involves recognizing digits 0-9 in natural scene images. The test partition consisting of 26,032 images is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Detailed Ablation Results</head><p>The results in the main paper correspond to the performance obtained when considering:</p><p>1. Feature Set: all gram matrix entries 2. Metric: layerwise deviations computed with respect to the mins and maxs.</p><p>3. Aggregation Scheme: the total deviation is then computed using Eq 5.</p><p>In this section, detailed ablation results are reported by considering other options. Specifically:</p><p>1. Alternate Feature Set: In addition to considering all gram matrix entries, we consider a proper partition of the gram matrix: strictly diagonal elements, and strictly off-diagonal elements. The diagonal elements correspond to the unary features, while the off-diagonal elements correspond to pairwise features. This can be done by appropriately changing the definition of variable stat in Line 7 of Algorithm 1. In these experiments, we consider row-wise sums wherever the size of stat is O(n 2 ); in other words, we consider row-wise sums when considering off-diagonal elements and all gram matrix entries. 2. Alternate Metric: An alternative formulation for computing feature-wise deviations can be to compute the deviation from the means using the one-dimensional Mahalanobis distance.</p><p>In the preprocessing stage, this would be done by storing the Means and Variances of stat (feature-wise) instead of their Mins and Maxs. Under this new alternative, the function ? defined in Eq 3 would be redefined as:</p><formula xml:id="formula_9">?(mean,variance,value) = (value ? mean) 2 variance<label>(7)</label></formula><p>Accordingly, the layerwise deviation ? l can be defined as:</p><formula xml:id="formula_10">? l (D) = P p=1 |G p l (D)| i=1 ? Means[D c ][l][p][i], Variances[D c ][l][p][i], G p l (D)[i]<label>(8)</label></formula><p>where G p l (D) would correspond to the statistic chosen in the previous step: diagonal entries only, row-wise sums of off-diagonal entries only or row-wise sums of complete gram matrix. We thus consider 2 options for computing the deviations: the Min/Max method presented in the main paper and the Mean/Variance method (Gaussian) described above. 3. Alternate Aggregation Scheme: In order to compute the total deviation ? from the layerwise deviations ? l , we can compute it by following 5 or taking a simple sum as shown:</p><formula xml:id="formula_11">?(D) = L l=1 ? l<label>(9)</label></formula><p>We refer to Eq 5 as the normalized estimate and Eq 9 as the unnormalized estimate.</p><p>In all, the  We notice that using the unary features (diagonal entries) sometimes did well when pairwise features (off-diagonal entries) did not do well, and vice versa, so using both gives the kind of effect that we want in an ensemble: models that cover and work well over different parts of the space. Therefore, an overall message of our experiments is that it is worthwhile to consider all elements of the gram matrix. (a) The individual components of gram matrices do not follow normal distribution strictly and Mean/Var assigns lower probabilities to the in-distribution images as well. (b) The total deviation ? -computed by simply summing across the layerwise deviations, ? l -was not able to accurately summarize the information contained in the different ? l s. Specifically, information about the layer where the input example had a higher deviation was lost when a simple sum was taken.</p><p>The proposed Min/Max idea solves problem (a) by employing a weaker metric: deviation from extrema instead of the mean. It can also be said that the Min/Max metric considers a uniform probability density between the extrema. Problem (b), which exists even for this newer metric, is solved by the normalization scheme described in the main paper for computing the sum total deviation.</p><p>Higher Order Gram Matrices The Min/Max metric is a weak approximation to the true probability density. On conducting a thorough analysis of how the OOD examples were able to fool the metric, it appeared that the intermediate features had several tiny activations that could yield innocuous correlation values. Higher-order gram matrices as described in the main paper provide a natural way to mitigate these effects.</p><p>Notable observations from Figures 4 through 9 (all layers are considered but only one order of gram matrix is considered at a time):</p><p>? Ensemble effect: In 24/28 cases, higher order gram matrices improve detection rates. Higher order gram matrices help both the Min/Max and the Mean/Var metrics. In most cases, the even powers are more helpful than the odd powers; in some cases, the odd powers are more helpful (Ex: DenseNet/CIFAR-100 vs CIFAR-10). Despite these variations, it is possible to get an ensemble effect by considering all possible powers as demonstrated in the main paper.</p><p>? In ResNet:CIFAR-10 vs CIFAR-100 and DenseNet:CIFAR-10 vs CIFAR-100, the higher order gram matrices yield lower detection rates. We find these exceptions interesting, and would like to understand them better in future.</p><p>Summary The unambiguous message from this ablation study is that the Gram matrix contains useful information which can be used for detecting OOD examples. While the standard Mean/Variance metric does not always work well, the proposed Min/Max metric yields consistent performance competitive with state-of-the-art methods. The use of higher-order Gram matrices further boosts the overall performance. Although the Min/Max method can work very well for "far-from-distribution" examples, it does not work well when a fine grained estimate is needed (for example, CIFAR-10 vs CIFAR-100). We hope the strong empirical proof that Gram matrices contain useful information can motivate the development of OOD detectors with powerful density estimators; eventually, such estimators might not need higher-order Gram matrices to boost their performance. Here they allow us to propose a metric based on simple thresholding that, generally, works independently of the particular OOD examples and independently of the architecture, and we believe that the fact that this is the case-the fact that this approach scores relatively well-itself raises several interesting new questions. We hope that answering these questions will ultimately strengthen our understanding of deep neural networks and give rise to more robust neural network models in the future.             <ref type="table" target="#tab_18">Ours + MSP  TNR at  TPR95  AUROC DTACC  TNR at  TPR95  AUROC DTACC  TNR at  TPR95  AUROC DTACC  TNR at  TPR95  AUROC DTACC  TNR at  TPR95  AUROC DTACC  TNR at  TPR95  AUROC</ref>      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>DenseNet/CIFAR-100 vs Tiny ImageNet Figure 1: Significance of depth: The TNR@TPR95 is computed by constructing detectors which make use of all the gram matrices but consider only one residual or dense block at a time. ResNet32 has 4 residual blocks and DenseNet3 has 3 dense blocks. DenseNet/CIFAR-100 vs Tiny ImageNet Figure 2: The importance of higher order gram matrices: The TNR@TPR95 is computed by constructing detectors which make use of only one of the gram matrices but consider all layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The Schematic Diagram demonstrating the proposed algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>ResNet/CIFAR-10: The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied. ResNet/CIFAR-100: The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>DenseNet/CIFAR-10: The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied. DenseNet/CIFAR-100: The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>DenseNet/SVHN: The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied.C.1 Significance of Depth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>DenseNet/CIFAR-10: The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :Figure 12 :Figure 13 :Figure 14 :</head><label>11121314</label><figDesc>DenseNet/CIFAR-100: The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network. ResNet/CIFAR-10: The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network. ResNet/CIFAR-100: The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network. DenseNet/SVHN: The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The results for Outlier Exposure (OE) are available in Table 3; some more results for OE and the results for DPN, Variational Dirichlet and Semantic are available in Appendix F.1 and Appendix F.2 respectively. TinyImgNet (R) 41.0 / 67.9 / 97.1 / 98.7 91.0 / 94.0 / 99.5 / 99.7 85.1 / 86.5 / 96.3 / 97.8 SVHN 50.5 / 70.3 / 87.8 / 97.6 89.9 / 96.7 / 99.1 / 99.5 85.1 / 91.1 / 95.8 / 96.7 TinyImgNet (R) 79.8 / 84.1 / 99.9 / 99.1 94.8 / 95.1 / 99.9 / 99.7 90.2 / 90.4 / 98.9 / 97.9 TinyImgNet (R) 79.0 / 82.0 / 99.9 / 99.3 93.5 / 92.0 / 99.9 / 99.7 90.4 / 89.4 / 99.1 / 97.9 CIFAR-10 78.3 / 79.8 / 98.4 / 85.8 92.9 / 92.1 / 99.3 / 97.3 90.0 / 89.4 / 96.9 / 92.0 Table 2: Comparison of OOD Detection Performance for all combinations of model architecture and training dataset are shown. The hyperparameters of ODIN and the hyperparameters and parameters of Mahalanobis are tuned using a random sample of the OOD dataset. More results are available inTable 5.</figDesc><table><row><cell>In-dist (model)</cell><cell>OOD</cell><cell>TNR at TPR 95% Baseline / ODIN / Mahalanobis / Ours AUROC Detection Acc.</cell></row><row><cell></cell><cell>iSUN</cell><cell>44.6 / 73.2 / 97.8 / 99.3 91.0 / 94.0 / 99.5 / 99.8 85.0 / 86.5 / 96.7 / 98.1</cell></row><row><cell>CIFAR-10</cell><cell>LSUN (R)</cell><cell>49.8 / 82.1 / 98.8 / 99.6 91.0 / 94.1 / 99.7 / 99.9 85.3 / 86.7 / 97.7 / 98.6</cell></row><row><cell>(ResNet)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>iSUN</cell><cell>16.9 / 45.2 / 89.9 / 94.8 75.8 / 85.5 / 97.9 / 98.8 70.1 / 78.5 / 93.1 / 95.6</cell></row><row><cell>CIFAR-100</cell><cell>LSUN (R)</cell><cell>18.8 / 23.2 / 90.9 / 96.6 75.8 / 85.6 / 98.2 / 99.2 69.9 / 78.3 / 93.5 / 96.7</cell></row><row><cell>(ResNet)</cell><cell cols="2">TinyImgNet (R) 20.4 / 36.1 / 90.9 / 94.8 77.2 / 87.6 / 98.2 / 98.9 70.8 / 80.1 / 93.3 / 95.0</cell></row><row><cell></cell><cell>SVHN</cell><cell>20.3 / 62.7 / 91.9 / 80.8 79.5 / 93.9 / 98.4 / 96.0 73.2 / 88.0 / 93.7 / 89.6</cell></row><row><cell></cell><cell>iSUN</cell><cell>62.5 / 93.2 / 95.3 / 99.0 94.7 / 98.7 / 98.9 / 99.8 89.2 / 94.3 / 95.2 / 97.9</cell></row><row><cell>CIFAR-10</cell><cell>LSUN (R)</cell><cell>66.6 / 96.2 / 97.2 / 99.5 95.4 / 99.2 / 99.3 / 99.9 90.3 / 95.7 / 96.3 / 98.6</cell></row><row><cell>(DenseNet)</cell><cell cols="2">TinyImgNet (R) 58.9 / 92.4 / 95.0 / 98.8 94.1 / 98.5 / 98.8 / 99.7 88.5 / 93.9 / 95.0 / 97.9</cell></row><row><cell></cell><cell>SVHN</cell><cell>40.2 / 86.2 / 90.8 / 96.1 89.9 / 95.5 / 98.1 / 99.1 83.2 / 91.4 / 93.9 / 95.9</cell></row><row><cell></cell><cell>iSUN</cell><cell>14.9 / 37.4 / 87.0 / 95.9 69.5 / 84.5 / 97.4 / 99.0 63.8 / 76.4 / 92.4 / 95.6</cell></row><row><cell>CIFAR-100</cell><cell>LSUN (R)</cell><cell>17.6 / 41.2 / 91.4 / 97.2 70.8 / 85.5 / 98.0 / 99.3 64.9 / 77.1 / 93.9 / 96.4</cell></row><row><cell>(DenseNet)</cell><cell cols="2">TinyImgNet (R) 17.6 / 42.6 / 86.6 / 95.7 71.7 / 85.2 / 97.4 / 99.0 65.7 / 77.0 / 92.2 / 95.5</cell></row><row><cell></cell><cell>SVHN</cell><cell>26.7 / 70.6 / 82.5 / 89.3 82.7 / 93.8 / 97.2 / 97.3 75.6 / 86.6 / 91.5 / 92.4</cell></row><row><cell></cell><cell>iSUN</cell><cell>78.3 / 82.</cell></row><row><cell>SVHN</cell><cell></cell><cell></cell></row><row><cell>(DenseNet)</cell><cell></cell><cell></cell></row></table><note>2 / 99.9 / 99.4 94.4 / 94.7 / 99.9 / 99.8 89.6 / 89.7 / 99.2 / 98.3 LSUN (R) 77.1 / 81.1 / 99.9 / 99.5 94.1 / 94.5 / 99.9 / 99.8 89.1 / 89.2 / 99.3 / 98.6CIFAR-10 69.3 / 71.7 / 96.8 / 80.4 91.9 / 91.4 / 98.9 / 95.5 86.6 / 85.8 / 95.9 / 89.1 SVHN (ResNet) iSUN 77.1 / 79.1 / 99.7 / 99.4 92.2 / 91.4 / 99.8 / 99.8 89.7 / 89.2 / 98.3 / 98.1 LSUN (R) 74.3 / 77.3 / 99.9 / 99.6 91.6 / 89.4 / 99.9 / 99.8 89.0 / 87.2 / 99.5 / 98.5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Q. Yu and K. Aizawa. Unsupervised out-of-distribution detection by maximum classifier discrepancy.CoRR, abs/1908.04951, 2019. URL http://arxiv.org/abs/1908.04951.</figDesc><table><row><cell cols="4">A Schematic Diagram 13/09/2019</cell><cell></cell><cell></cell><cell cols="4">schematic diagram.drawio</cell></row><row><cell>Test image</cell><cell>( )</cell><cell>Conv</cell><cell>Activation Func</cell><cell>Conv</cell><cell>Activation Func</cell><cell>Fully-Connected</cell><cell>Softmax</cell><cell>Predicted class</cell><cell>( )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Pairwise Correlations between the feature-maps of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">every layer are computed using Gram matrices of various</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">orders. In the preprocessing stage, the class-specific</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">element-wise minimum and maximum values are noted</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">for each of the gram-matrices.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Layerwise Deviation (</cell><cell cols="2">) is the sum total deviation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">of the entries in all gram matrices</cell><cell>from their</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">corresponding minimum and maximum values extracted</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">from training data points classified as</cell><cell>. In other words,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">for all channel-pairs, if any of the computed correlation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">values are greater (or lesser) than corresponding the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">maximum (or minimum) value extracted for training data</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">points classified as</cell><cell cols="3">,the extent of deviation is noted.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Total deviation ( ) is computed by summing across the</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">deviations of all the layers. However, since the scale of</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">deviations of each layer are different, we normalize by</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>dividing it with</cell><cell cols="3">, the expected deviation at layer ,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">computed using the Validation Data.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TNR at TPR95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AUROC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DTACC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>In-dist</cell><cell>OOD</cell><cell></cell><cell cols="2">Diagonal Elements</cell><cell></cell><cell cols="4">Off Diagonal Elements (Row-wise Sums)</cell><cell cols="4">Complete Gram Matrix (Row-wise Sums)</cell><cell></cell><cell cols="3">Diagonal Elements</cell><cell cols="4">Off Diagonal Elements (Row-wise Sums)</cell><cell cols="4">Complete Gram Matrix (Row-wise Sums)</cell><cell></cell><cell cols="3">Diagonal Elements</cell><cell cols="4">Off-Diagonal Elements (Row-wise Sums)</cell><cell cols="4">Complete Gram Matrix (Row-wise Sums)</cell></row><row><cell>(model)</cell><cell></cell><cell>Min/ Max</cell><cell>Mean/ Var</cell><cell>Min/ Max (U)</cell><cell>Mean/ Var (U)</cell><cell>Min/ Max</cell><cell>Mean/ Var</cell><cell>Min/ Max (U)</cell><cell>Mean/ Var (U)</cell><cell>Min/ Max</cell><cell>Mean/ Var</cell><cell>Min/ Max (U)</cell><cell>Mean/ Var (U)</cell><cell>Min/ Max</cell><cell>Mean/ Var</cell><cell>Min/ Max (U)</cell><cell>Mean/ Var (U)</cell><cell>Min/ Max</cell><cell>Mean/ Var</cell><cell>Min/ Max (U)</cell><cell>Mean/ Var (U)</cell><cell>Min/ Max</cell><cell>Mean/ Var</cell><cell>Min/ Max (U)</cell><cell>Mean/ Var (U)</cell><cell>Min/ Max</cell><cell>Mean/ Var</cell><cell>Min/ Max (U)</cell><cell>Mean/ Var (U)</cell><cell>Min/ Max</cell><cell>Mean/ Var</cell><cell>Min/ Max (U)</cell><cell>Mean/ Var (U)</cell><cell>Min/ Max</cell><cell>Mean/ Var</cell><cell>Min/ Max (U)</cell><cell>Mean/ Var (U)</cell></row><row><cell></cell><cell>iSUN</cell><cell>99.1</cell><cell>96.3</cell><cell>95.7</cell><cell cols="2">73.8 99.3</cell><cell cols="2">97.1 97.0</cell><cell cols="2">90.2 99.3</cell><cell cols="2">97.1 97.1</cell><cell cols="2">89.3 99.7</cell><cell cols="2">99.2 98.7</cell><cell cols="2">96.2 99.7</cell><cell cols="2">99.4 99.0</cell><cell cols="2">98.1 99.7</cell><cell cols="2">99.4 99.0</cell><cell cols="2">98.0 97.9</cell><cell cols="2">95.7 95.5</cell><cell cols="2">92.1 98.0</cell><cell cols="2">96.2 96.0</cell><cell cols="2">93.5 98.1</cell><cell cols="2">96.2 96.0</cell><cell>93.4</cell></row><row><cell>CIFAR-10 (ResNet)</cell><cell cols="2">LSUN TinyImgNet 98.6 99.5 SVHN 97.8</cell><cell>98.3 93.8 70.7</cell><cell>97.0 96.2 94.8</cell><cell cols="2">77.7 99.6 68.2 98.8 19.9 97.6</cell><cell cols="2">98.8 98.1 97.2 95.7 81.1 94.8</cell><cell cols="2">93.6 99.6 88.5 98.7 40.7 97.6</cell><cell cols="2">98.8 98.0 97.2 95.7 80.2 94.9</cell><cell cols="2">94.1 99.8 87.5 99.6 38.2 99.5</cell><cell cols="2">99.5 98.7 99.1 98.3 95.2 98.9</cell><cell cols="2">96.7 99.8 95.6 99.6 88.4 99.4</cell><cell cols="2">99.7 99.1 99.4 98.8 96.2 98.8</cell><cell cols="2">98.5 99.8 97.8 99.6 92.0 99.4</cell><cell cols="2">99.7 99.1 99.4 98.8 96.2 98.9</cell><cell cols="2">98.4 98.5 97.7 97.5 91.8 97.0</cell><cell cols="2">97.0 96.0 95.6 94.7 90.1 95.0</cell><cell cols="2">93.0 98.6 91.2 97.7 84.7 96.6</cell><cell cols="2">97.6 96.6 96.3 95.3 90.9 94.9</cell><cell cols="2">94.6 98.6 93.1 97.8 87.0 96.7</cell><cell cols="2">97.6 96.6 96.4 95.4 90.8 95.1</cell><cell>94.5 92.9 86.7</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>33.3</cell><cell>29.4</cell><cell>42.5</cell><cell cols="2">32.9 32.9</cell><cell cols="2">27.2 41.8</cell><cell cols="2">29.3 32.8</cell><cell cols="2">27.4 42.2</cell><cell cols="2">29.2 79.7</cell><cell cols="2">78.4 84.9</cell><cell cols="2">83.3 78.8</cell><cell cols="2">75.8 84.1</cell><cell cols="2">79.0 79.0</cell><cell cols="2">76.1 84.2</cell><cell cols="2">79.2 72.4</cell><cell cols="2">71.7 78.2</cell><cell cols="2">76.8 71.5</cell><cell cols="2">69.1 77.4</cell><cell cols="2">72.1 71.7</cell><cell cols="2">69.4 77.4</cell><cell>72.2</cell></row><row><cell></cell><cell>iSUN</cell><cell>93.8</cell><cell>71.8</cell><cell>50.0</cell><cell cols="2">33.8 95.4</cell><cell cols="2">85.9 67.2</cell><cell cols="2">55.0 95.1</cell><cell cols="2">85.3 65.4</cell><cell cols="2">52.8 98.7</cell><cell cols="2">95.5 92.1</cell><cell cols="2">87.8 98.9</cell><cell cols="2">97.4 94.5</cell><cell cols="2">92.7 98.8</cell><cell cols="2">97.3 94.2</cell><cell cols="2">92.3 94.5</cell><cell cols="2">89.3 85.4</cell><cell cols="2">81.1 95.3</cell><cell cols="2">92.2 87.9</cell><cell cols="2">86.1 95.1</cell><cell cols="2">92.0 87.6</cell><cell>85.6</cell></row><row><cell>CIFAR-100 (ResNet)</cell><cell cols="2">LSUN TinyImgNet 94.1 95.6 SVHN 83.1</cell><cell>70.8 68.0 29.8</cell><cell>45.6 51.4 53.2</cell><cell cols="2">32.9 97.2 34.6 95.3 26.0 79.1</cell><cell cols="2">87.5 64.2 84.2 68.1 34.4 51.8</cell><cell cols="2">52.0 97.0 52.8 95.1 29.6 81.4</cell><cell cols="2">86.8 62.4 83.5 66.6 33.9 55.6</cell><cell cols="2">49.7 99.1 50.8 98.8 29.2 96.5</cell><cell cols="2">95.9 91.8 95.1 92.5 84.7 92.3</cell><cell cols="2">87.8 99.3 87.1 99.0 80.8 95.7</cell><cell cols="2">97.8 94.6 97.2 94.8 86.8 91.6</cell><cell cols="2">92.9 99.2 92.5 98.9 83.5 96.1</cell><cell cols="2">97.7 94.3 97.1 94.6 86.7 92.2</cell><cell cols="2">92.4 95.4 92.1 94.6 83.1 90.2</cell><cell cols="2">90.1 85.7 88.5 85.9 77.5 85.1</cell><cell cols="2">81.4 96.3 80.1 95.2 73.6 89.2</cell><cell cols="2">93.1 88.4 91.8 88.4 80.3 84.3</cell><cell cols="2">86.6 96.1 85.9 95.1 75.3 89.7</cell><cell cols="2">93.0 88.1 91.6 88.2 80.3 84.8</cell><cell>86.1 85.4 74.9</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>12.9</cell><cell>18.1</cell><cell>19.2</cell><cell cols="2">18.2 11.4</cell><cell cols="2">17.5 17.5</cell><cell cols="2">17.9 12.1</cell><cell cols="2">17.6 18.1</cell><cell cols="2">18.0 69.3</cell><cell cols="2">71.2 76.6</cell><cell cols="2">74.6 67.3</cell><cell cols="2">70.0 75.3</cell><cell cols="2">71.9 67.8</cell><cell cols="2">70.1 75.5</cell><cell cols="2">72.0 64.6</cell><cell cols="2">66.0 71.1</cell><cell cols="2">69.1 63.0</cell><cell cols="2">64.8 69.8</cell><cell cols="2">66.5 63.3</cell><cell cols="2">65.0 70.1</cell><cell>66.6</cell></row><row><cell></cell><cell>iSUN</cell><cell>98.9</cell><cell>97.1</cell><cell>98.8</cell><cell cols="2">96.3 99.1</cell><cell cols="2">97.8 99.1</cell><cell cols="2">97.5 99.1</cell><cell cols="2">97.8 99.0</cell><cell cols="2">97.5 99.8</cell><cell cols="2">99.5 99.8</cell><cell cols="2">99.3 99.8</cell><cell cols="2">99.6 99.8</cell><cell cols="2">99.6 99.8</cell><cell cols="2">99.6 99.8</cell><cell cols="2">99.5 97.8</cell><cell cols="2">96.5 97.8</cell><cell cols="2">95.9 97.9</cell><cell cols="2">96.9 97.8</cell><cell cols="2">96.8 98.0</cell><cell cols="2">96.8 97.8</cell><cell>96.7</cell></row><row><cell>CIFAR-10 (DenseNet)</cell><cell cols="2">LSUN TinyImgNet 98.7 99.4 SVHN 96.6</cell><cell>98.8 97.5 87.8</cell><cell>99.4 98.6 96.9</cell><cell cols="2">98.4 99.5 97.0 98.8 88.2 95.9</cell><cell cols="2">99.2 99.5 98.0 98.8 84.7 96.4</cell><cell cols="2">99.0 99.5 97.7 98.8 86.9 96.0</cell><cell cols="2">99.1 99.4 97.9 98.7 84.0 96.5</cell><cell cols="2">98.9 99.9 97.7 99.7 87.1 99.2</cell><cell cols="2">99.8 99.9 99.5 99.7 97.6 99.3</cell><cell cols="2">99.7 99.9 99.3 99.7 97.6 99.1</cell><cell cols="2">99.8 99.9 99.6 99.7 96.9 99.2</cell><cell cols="2">99.8 99.9 99.5 99.7 97.4 99.1</cell><cell cols="2">99.8 99.9 99.6 99.7 96.8 99.2</cell><cell cols="2">99.8 98.7 99.5 97.9 97.4 96.3</cell><cell cols="2">98.0 98.6 96.8 97.8 92.3 96.5</cell><cell cols="2">97.4 98.6 96.3 97.8 92.3 95.8</cell><cell cols="2">98.2 98.5 97.1 97.7 91.1 96.0</cell><cell cols="2">98.1 98.7 97.0 97.9 92.0 95.8</cell><cell cols="2">98.1 98.5 97.0 97.7 90.9 96.1</cell><cell>98.1 97.0 92.0</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>26.4</cell><cell>28.9</cell><cell>29.0</cell><cell cols="2">28.2 27.0</cell><cell cols="2">25.2 30.1</cell><cell cols="2">24.8 26.8</cell><cell cols="2">25.5 30.1</cell><cell cols="2">25.1 68.5</cell><cell cols="2">75.1 68.9</cell><cell cols="2">74.1 72.1</cell><cell cols="2">72.7 73.3</cell><cell cols="2">72.4 72.2</cell><cell cols="2">72.9 73.4</cell><cell cols="2">72.5 66.5</cell><cell cols="2">68.8 66.6</cell><cell cols="2">67.8 67.6</cell><cell cols="2">67.2 68.9</cell><cell cols="2">66.6 67.3</cell><cell cols="2">67.3 68.6</cell><cell>66.6</cell></row><row><cell></cell><cell>iSUN</cell><cell>96.0</cell><cell>84.2</cell><cell>95.5</cell><cell cols="2">91.9 96.1</cell><cell cols="2">88.8 95.5</cell><cell cols="2">95.7 95.9</cell><cell cols="2">88.5 95.3</cell><cell cols="2">95.8 99.1</cell><cell cols="2">97.2 98.9</cell><cell cols="2">98.2 99.0</cell><cell cols="2">97.9 98.9</cell><cell cols="2">99.0 99.1</cell><cell cols="2">97.8 98.9</cell><cell cols="2">99.0 95.7</cell><cell cols="2">91.4 95.4</cell><cell cols="2">93.6 95.7</cell><cell cols="2">92.6 95.3</cell><cell cols="2">95.5 95.7</cell><cell cols="2">92.6 95.3</cell><cell>95.5</cell></row><row><cell>CIFAR-100 (DenseNet)</cell><cell cols="2">LSUN TinyImgNet 95.8 97.4 SVHN 89.4</cell><cell>87.5 81.4 59.7</cell><cell>96.9 95.4 88.8</cell><cell cols="2">95.5 97.5 90.2 95.9 64.5 87.3</cell><cell cols="2">91.4 97.0 86.9 95.3 63.2 86.4</cell><cell cols="2">97.8 97.3 94.2 95.8 67.4 89.4</cell><cell cols="2">91.2 96.8 86.4 95.2 62.9 87.9</cell><cell cols="2">97.8 99.4 94.3 99.0 67.3 97.4</cell><cell cols="2">97.8 99.3 96.6 98.9 92.5 97.3</cell><cell cols="2">99.0 99.4 97.8 99.0 92.6 97.0</cell><cell cols="2">98.3 99.3 97.5 98.9 92.7 96.9</cell><cell cols="2">99.4 99.4 98.7 99.0 93.4 97.4</cell><cell cols="2">98.3 99.3 97.4 98.9 92.7 97.1</cell><cell cols="2">99.4 96.4 98.7 95.5 93.4 92.4</cell><cell cols="2">92.7 96.1 90.4 95.2 85.7 92.0</cell><cell cols="2">95.3 96.5 92.9 95.5 86.0 91.7</cell><cell cols="2">93.7 96.2 91.8 95.2 86.0 91.4</cell><cell cols="2">96.7 96.4 94.7 95.6 87.1 92.4</cell><cell cols="2">93.7 96.2 91.7 95.2 86.2 91.9</cell><cell>96.7 94.7 87.1</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>10.5</cell><cell>16.4</cell><cell>11.1</cell><cell cols="2">13.7 10.6</cell><cell cols="2">15.6 10.2</cell><cell cols="2">13.9 10.5</cell><cell cols="2">15.6 10.2</cell><cell cols="2">13.8 64.4</cell><cell cols="2">70.1 65.0</cell><cell cols="2">66.7 63.7</cell><cell cols="2">68.3 64.2</cell><cell cols="2">66.2 64.2</cell><cell cols="2">68.7 64.6</cell><cell cols="2">66.2 60.6</cell><cell cols="2">64.9 61.3</cell><cell cols="2">62.2 59.7</cell><cell cols="2">63.4 60.5</cell><cell cols="2">61.6 60.4</cell><cell cols="2">63.8 61.0</cell><cell>61.5</cell></row><row><cell></cell><cell>iSUN</cell><cell>99.3</cell><cell>99.8</cell><cell>98.1</cell><cell cols="2">96.9 99.5</cell><cell cols="2">99.9 97.8</cell><cell cols="2">98.5 99.5</cell><cell cols="2">99.9 97.9</cell><cell cols="2">98.6 99.8</cell><cell cols="2">99.9 98.8</cell><cell cols="2">98.3 99.8</cell><cell cols="2">99.9 99.0</cell><cell cols="2">99.0 99.8</cell><cell cols="2">99.9 99.0</cell><cell cols="2">99.0 98.0</cell><cell cols="2">98.4 96.6</cell><cell cols="2">96.3 98.1</cell><cell cols="2">98.5 96.4</cell><cell cols="2">96.8 98.1</cell><cell cols="2">98.5 96.5</cell><cell>96.8</cell></row><row><cell>SVHN</cell><cell>LSUN</cell><cell>99.6</cell><cell>99.9</cell><cell>98.5</cell><cell cols="2">97.4 99.6</cell><cell cols="2">99.9 98.3</cell><cell cols="2">99.0 99.6</cell><cell cols="2">99.9 98.4</cell><cell cols="2">99.0 99.9</cell><cell cols="2">99.9 98.9</cell><cell cols="2">98.4 99.8</cell><cell cols="2">99.9 99.1</cell><cell cols="2">99.1 99.8</cell><cell cols="2">99.9 99.1</cell><cell cols="2">99.1 98.5</cell><cell cols="2">98.9 96.8</cell><cell cols="2">96.5 98.5</cell><cell cols="2">98.9 96.7</cell><cell cols="2">97.0 98.5</cell><cell cols="2">98.9 96.7</cell><cell>97.0</cell></row><row><cell>(ResNet)</cell><cell cols="2">TinyImgNet 99.0</cell><cell>99.6</cell><cell>97.8</cell><cell cols="2">96.5 99.3</cell><cell cols="2">99.6 97.8</cell><cell cols="2">98.1 99.3</cell><cell cols="2">99.6 98.4</cell><cell cols="2">99.0 99.7</cell><cell cols="2">99.8 98.8</cell><cell cols="2">98.3 99.7</cell><cell cols="2">99.8 99.0</cell><cell cols="2">99.0 99.7</cell><cell cols="2">99.8 99.0</cell><cell cols="2">99.0 97.6</cell><cell cols="2">98.1 96.4</cell><cell cols="2">96.2 97.9</cell><cell cols="2">98.2 96.4</cell><cell cols="2">96.6 97.9</cell><cell cols="2">98.2 96.5</cell><cell>96.6</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>82.6</cell><cell>93.7</cell><cell>86.1</cell><cell cols="2">86.8 85.9</cell><cell cols="2">94.9 86.5</cell><cell cols="2">90.5 85.5</cell><cell cols="2">94.9 86.6</cell><cell cols="2">90.5 96.8</cell><cell cols="2">98.6 97.1</cell><cell cols="2">97.3 97.3</cell><cell cols="2">98.8 97.3</cell><cell cols="2">97.8 97.3</cell><cell cols="2">98.8 97.3</cell><cell cols="2">97.8 91.1</cell><cell cols="2">94.9 92.5</cell><cell cols="2">94.0 92.0</cell><cell cols="2">95.2 92.8</cell><cell cols="2">94.3 92.0</cell><cell cols="2">95.2 92.8</cell><cell>94.2</cell></row><row><cell></cell><cell>iSUN</cell><cell>99.3</cell><cell>99.6</cell><cell>99.4</cell><cell cols="2">99.1 99.3</cell><cell cols="2">99.4 99.4</cell><cell cols="2">97.9 99.3</cell><cell cols="2">99.4 99.4</cell><cell cols="2">98.0 99.8</cell><cell cols="2">99.9 99.9</cell><cell cols="2">99.8 99.8</cell><cell cols="2">99.9 99.8</cell><cell cols="2">99.5 99.8</cell><cell cols="2">99.9 99.8</cell><cell cols="2">99.5 98.3</cell><cell cols="2">98.8 98.4</cell><cell cols="2">98.3 98.4</cell><cell cols="2">98.5 98.4</cell><cell cols="2">97.3 98.3</cell><cell cols="2">98.6 98.4</cell><cell>97.3</cell></row><row><cell>SVHN</cell><cell>LSUN</cell><cell>99.5</cell><cell>99.7</cell><cell>99.7</cell><cell cols="2">99.4 99.5</cell><cell cols="2">99.4 99.6</cell><cell cols="2">98.2 99.5</cell><cell cols="2">99.4 99.6</cell><cell cols="2">98.3 99.9</cell><cell cols="2">99.9 99.9</cell><cell cols="2">99.9 99.8</cell><cell cols="2">99.9 99.9</cell><cell cols="2">99.6 99.8</cell><cell cols="2">99.9 99.9</cell><cell cols="2">99.6 98.6</cell><cell cols="2">98.9 98.7</cell><cell cols="2">98.5 98.6</cell><cell cols="2">98.6 98.7</cell><cell cols="2">97.6 98.5</cell><cell cols="2">98.7 98.7</cell><cell>97.7</cell></row><row><cell>(DenseNet)</cell><cell cols="2">TinyImgNet 99.2</cell><cell>99.5</cell><cell>99.3</cell><cell cols="2">99.2 99.1</cell><cell cols="2">99.2 99.2</cell><cell cols="2">98.0 99.0</cell><cell cols="2">99.2 99.2</cell><cell cols="2">98.1 99.7</cell><cell cols="2">99.9 99.8</cell><cell cols="2">99.8 99.7</cell><cell cols="2">99.8 99.8</cell><cell cols="2">99.6 99.7</cell><cell cols="2">99.8 99.8</cell><cell cols="2">99.6 97.9</cell><cell cols="2">98.5 98.1</cell><cell cols="2">98.2 98.0</cell><cell cols="2">98.3 98.1</cell><cell cols="2">97.3 97.9</cell><cell cols="2">98.3 98.1</cell><cell>97.3</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>76.6</cell><cell>93.5</cell><cell>76.9</cell><cell cols="2">91.8 81.2</cell><cell cols="2">94.3 85.6</cell><cell cols="2">90.1 80.2</cell><cell cols="2">94.2 84.7</cell><cell cols="2">90.1 94.5</cell><cell cols="2">98.5 94.9</cell><cell cols="2">98.1 95.6</cell><cell cols="2">98.6 96.5</cell><cell cols="2">97.6 95.5</cell><cell cols="2">98.6 96.4</cell><cell cols="2">97.7 88.1</cell><cell cols="2">94.3 88.6</cell><cell cols="2">93.5 89.2</cell><cell cols="2">94.7 90.7</cell><cell cols="2">92.6 89.0</cell><cell cols="2">94.7 90.6</cell><cell>92.6</cell></row><row><cell>Summary</cell><cell>MEAN STD-DEV</cell><cell>95.4 6.2</cell><cell>86.6 17.2</cell><cell>87.9 18.1</cell><cell cols="2">77.3 95.6 27.0 6.0</cell><cell cols="2">90.1 90.4 14.7 13.5</cell><cell cols="2">83.7 95.7 21.3 5.7</cell><cell cols="2">89.9 90.4 14.9 13.5</cell><cell cols="2">83.3 99.0 22.0 1.3</cell><cell cols="2">97.5 97.7 3.4 2.7</cell><cell cols="2">95.6 99.0 5.2 1.3</cell><cell cols="2">98.0 98.1 2.9 2.2</cell><cell cols="2">96.9 99.0 3.9 1.2</cell><cell cols="2">98.0 98.1 2.9 2.2</cell><cell cols="2">96.8 96.0 4.0 2.9</cell><cell cols="2">93.7 94.1 5.2 4.5</cell><cell cols="2">91.6 96.1 6.8 2.8</cell><cell cols="2">94.4 94.5 4.5 3.9</cell><cell cols="2">92.9 96.2 5.6 2.8</cell><cell cols="2">94.4 94.5 4.5 3.9</cell><cell>92.8 5.7</cell></row></table><note>reports detection rates in 12 settings: 3 choices for stat (only the diagonal entries of gram matrix G, only the off-diagonal entries of G, or all of G) ? 2 metrics for computing deviation (Min/Max or Mean/Variance) ? 2 choices for computing total deviation (Normalized sum or Unnormalized sum). All layers and all orders of gram matrix are considered in Table 4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Detailed Ablation Results demonstrating the detection rates under 12 different settings. The MEAN and STD-DEV are computed by using all elements in the table excepting the CIFAR-10 vs CIFAR-100 and CIFAR-100 vs CIFAR-10 entries. cases, the unary features are marginally better (Ex: ResNet/CIFAR-10 vs SVHN) and in some cases, the pairwise features are marginally better (Ex: ResNet/CIFAR-100 vs iSUN/LSUN/TinyImgNet). Interestingly, the behavior of the Mean/Var metric is different: the performance with pairwise features are significantly higher than with unary features in 19 out of 28 tested cases. For example, the TNR at TPR95 for ResNet/CIFAR-100 vs TinyImgNet is 68.0 with unary features and 84.2 with pairwise features.</figDesc><table /><note>By analysing the ablation results, we attempt to answer the following questions: 1. Are pairwise features more useful than unary features? We observe that the Min/Max metric can work equally well with both unary and pairwise features; in some</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>The TNR at TPR95 trends for Min/Max and Mean/Var as the order of Gram Matrix is varied.</figDesc><table><row><cell></cell><cell></cell><cell>0.99</cell><cell>Min/Max Mean/Var</cell><cell>DenseNet: CIFAR10 vs iSUN</cell><cell>0.995</cell><cell></cell><cell>DenseNet: CIFAR10 vs LSUN</cell><cell></cell><cell>Min/Max Mean/Var</cell><cell>0.99</cell><cell>Min/Max Mean/Var</cell><cell></cell><cell cols="2">DenseNet: CIFAR10 vs TinyImgNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DenseNet: CIFAR10 vs SVHN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.300</cell><cell>DenseNet: CIFAR10 vs CIFAR100</cell><cell>Min/Max Mean/Var</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.98</cell><cell></cell><cell></cell><cell>0.990</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.275</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.250</cell></row><row><cell cols="2">TNR at TPR95</cell><cell>0.97</cell><cell></cell><cell>TNR at TPR95</cell><cell>0.985 0.980</cell><cell></cell><cell></cell><cell></cell><cell>TNR at TPR95</cell><cell>0.96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TNR at TPR95</cell><cell>0.7 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TNR at TPR95</cell><cell>0.200 0.225</cell></row><row><cell></cell><cell></cell><cell>0.96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.975</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.175</cell></row><row><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.150</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.970</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell>Min/Max Mean/Var</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.125</cell></row><row><cell>TNR at TPR95</cell><cell cols="2">0.970 0.975 0.980 0.985 0.990 0.995 1.000</cell><cell></cell><cell cols="2">2 Figure 6: ResNet/SVHN: 2 4 6 8 10 Order of Gram Matrix ResNet: SVHN vs iSUN Min/Max Mean/Var 0.970 0.975 0.980 0.985 0.990 0.995 1.000 TNR at TPR95 4 6 8 10 Order of Gram Matrix 2</cell><cell>4</cell><cell>2 Order of Gram Matrix 6</cell><cell>4 8</cell><cell>6 Order of Gram Matrix ResNet: SVHN vs LSUN 10</cell><cell></cell><cell>8</cell><cell>2</cell><cell>10 Min/Max Mean/Var 4</cell><cell>0.965 0.970 0.975 0.980 0.985 0.990 0.995 Order of Gram Matrix TNR at TPR95 6</cell><cell>8</cell><cell>2</cell><cell>10</cell><cell>4</cell><cell cols="3">6 Order of Gram Matrix ResNet: SVHN vs TinyImgNet 2</cell><cell>4</cell><cell>8</cell><cell>6 Order of Gram Matrix</cell><cell>10 Min/Max Mean/Var</cell><cell>8</cell><cell>TNR at TPR95</cell><cell>0.75 0.80 0.85 0.90 0.95</cell><cell>10</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>6 Order of Gram Matrix ResNet: SVHN vs CIFAR10 4 Order of Gram Matrix 6</cell><cell>8</cell><cell>8</cell><cell>10 Min/Max Mean/Var 10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>The TNR at TPR95 trends for Min/Max and Mean/Var as we go deeper in the network. / 73.2 / 97.8 / 99.3 91.0 / 94.0 / 99.5 / 99.8 85.0 / 86.5 / 96.7 / 98.1 LSUN (R) 49.8 / 82.1 / 98.8 / 99.6 91.0 / 94.1 / 99.7 / 99.9 85.3 / 86.7 / 97.7 / 98.6 65.5 80.2 / 91.4 / 81.7 / 91.4 72.7 / 83.3 / 74.0 / 83.6 TinyImgNet (R) 17.6 / 42.6 / 86.6 / 95.7 71.7 / 85.2 / 97.4 / 99.0 65.7 / 77.0 / 92.2 / 95.5 TinyImgNet (C) 24.6 / 51.0 / 60.1 / 89.0 76.2 / 88.3 / 88.8 / 97.7 69.0 / 80.2 / 81.6 / 92.5 Comparison of OOD Detection Performance for all combinations of model architecture and training dataset are shown. The hyperparameters of ODIN and the hyperparameters and parameters of Mahalanobis are tuned using a random sample of the OOD dataset.</figDesc><table><row><cell>TNR at TPR95</cell><cell>0.800 0.825 0.850 0.875 0.900 0.925 0.950 0.975 1.000</cell><cell>1.00</cell><cell>1.25</cell><cell>1.50</cell><cell>1.75 CIFAR-10 2.00 2.25 2.50 Block DenseNet: SVHN vs iSUN In-dist (model) (ResNet) CIFAR-100 (ResNet) CIFAR-10 (DenseNet) CIFAR-100 (DenseNet) SVHN (DenseNet) SVHN (ResNet) Table 5: E Combining OE + Ours 2.75 3.00 Min/Max Mean/Var 1.00 1.25 1.50 1.75 0.80 0.85 0.90 0.95 1.00 TNR at TPR95 DenseNet: SVHN vs LSUN 2.00 Block OOD TNR at TPR 95% 2.25 2.50 2.75 3.00 Min/Max Mean/Var Baseline / ODIN / Mahalanobis / Ours 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Block 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00 TNR at TPR95 DenseNet: SVHN vs TinyImgNet Min/Max Mean/Var 1.00 0.5 0.6 0.7 0.8 0.9 TNR at TPR95 AUROC Detection Acc. 1.25 1.50 1.75 2.00 Block DenseNet: SVHN vs CIFAR10 2.25 iSUN 44.6 LSUN (C) 48.6 / 62.0 / 81.3 / 89.8 91.9 / 91.2 / 96.7 / 97.8 86.3 / 82.4 / 90.5 / 92.6 2.50 TinyImgNet (R) 41.0 / 67.9 / 97.1 / 98.7 91.0 / 94.0 / 99.5 / 99.7 85.1 / 86.5 / 96.3 / 97.8 TinyImgNet (C) 46.4 / 68.7 / 92.0 / 96.7 91.4 / 93.1 / 98.6 / 99.2 85.4 / 85.2 / 93.9 / 96.1 SVHN 50.5 / 70.3 / 87.8 / 97.6 89.9 / 96.7 / 99.1 / 99.5 85.1 / 91.1 / 95.8 / 96.7 CIFAR-100 33.3 / 42.0 / 41.6 / 32.9 86.4 / 85.8 / 88.2 / 79.0 80.4 / 78.6 / 81.2 / 71.7 iSUN 16.9 / 45.2 / 89.9 / 94.8 75.8 / 85.5 / 97.9 / 98.8 70.1 / 78.5 / 93.1 / 95.6 LSUN (R) 18.8 / 23.2 / 90.9 / 96.6 75.8 / 85.6 / 98.2 / 99.2 69.9 / 78.3 / 93.5 / 96.7 LSUN (C) 18.7 / 44.1 / 64.8 / 64.8 75.5 / 82.7 / 92.0 / 92.1 69.2 / 75.9 / 84.0 / 84.2 TinyImgNet (R) 20.4 / 36.1 / 90.9 / 94.8 77.2 / 87.6 / 98.2 / 98.9 70.8 / 80.1 / 93.3 / 95.0 TinyImgNet (C) 24.3 / 44.3 / 80.9 / 88.5 79.7 / 85.4 / 96.3 / 97.7 72.5 / 78.3 / 89.9 / 92.2 SVHN 20.3 / 62.7 / 91.9 / 80.8 79.5 / 93.9 / 98.4 / 96.0 73.2 / 88.0 / 93.7 / 89.6 CIFAR-10 19.1 / 18.7 / 20.2 / 12.2 77.1 / 77.2 / 77.5 / 67.9 71.0 / 71.2 / 72.1 / 63.4 iSUN 62.5 / 93.2 / 95.3 / 99.0 94.7 / 98.7 / 98.9 / 99.8 89.2 / 94.3 / 95.2 / 97.9 LSUN (R) 66.6 / 96.2 / 97.2 / 99.5 95.4 / 99.2 / 99.3 / 99.9 90.3 / 95.7 / 96.3 / 98.6 LSUN (C) 51.8 / 70.6 / 48.2 / 88.4 92.9 / 93.6 / 80.2 / 97.5 86.9 / 86.4 / 75.6 / 92.0 TinyImgNet (R) 58.9 / 92.4 / 95.0 / 98.8 94.1 / 98.5 / 98.8 / 99.7 88.5 / 93.9 / 95.0 / 97.9 TinyImgNet (C) 56.7 / 87.0 / 84.2 / 96.7 93.8 / 97.6 / 95.3 / 99.3 88.1 / 92.3 / 89.9 / 96.1 SVHN 40.2 / 86.2 / 90.8 / 96.1 89.9 / 95.5 / 98.1 / 99.1 83.2 / 91.4 / 93.9 / 95.9 CIFAR-100 40.3 / 53.1 / 14.5 / 26.7 89.3 / 90.2 / 58.5 / 72.0 82.9 / 82.7 / 57.2 / 67.3 iSUN 14.9 / 37.4 / 87.0 / 95.9 69.5 / 84.5 / 97.4 / 99.0 63.8 / 76.4 / 92.4 / 95.6 LSUN (R) 17.6 / 41.2 / 91.4 / 97.2 70.8 / 85.5 / 98.0 / 99.3 64.9 / 77.1 / 93.9 / 96.4 LSUN (C) 28.6 / 57.8 / 42.1 / SVHN 26.7 / 70.6 / 82.5 / 89.3 82.7 / 93.8 / 97.2 / 97.3 75.6 / 86.6 / 91.5 / 92.4 CIFAR-10 18.9 / 16.8 / 7.7 / 10.6 75.9 / 74.2 / 60.1 / 64.2 69.7 / 68.6 / 57.8 / 60.4 iSUN 78.3 / 82.2 / 99.9 / 99.4 94.4 / 94.7 / 99.9 / 99.8 89.6 / 89.7 / 99.2 / 98.3 LSUN (R) 77.1 / 81.1 / 99.9 / 99.5 94.1 / 94.5 / 99.9 / 99.8 89.1 / 89.2 / 99.3 / 98.6 TinyImgNet (R) 79.8 / 84.1 / 99.9 / 99.1 94.8 / 95.1 / 99.9 / 99.7 90.2 / 90.4 / 98.9 / 97.9 CIFAR-10 69.3 / 71.7 / 96.8 / 80.4 91.9 / 91.4 / 98.9 / 95.5 86.6 / 85.8 / 95.9 / 89.1 iSUN 77.1 / 79.1 / 99.7 / 99.4 92.2 / 91.4 / 99.8 / 99.8 89.7 / 89.2 / 98.3 / 98.1 LSUN (R) 74.3 / 77.3 / 99.9 / 99.6 91.6 / 89.4 / 99.9 / 99.8 89.0 / 87.2 / 99.5 / 98.5 TinyImgNet (R) 79.0 / 82.0 / 99.9 / 99.3 93.5 / 92.0 / 99.9 / 99.7 90.4 / 89.4 / 99.1 / 97.9 CIFAR-10 78.3 / 79.8 / 98.4 / 85.8 92.9 / 92.1 / 99.3 / 97.3 90.0 / 89.4 / 96.9 / 92.0 In-dist (WRN 40-2) OOD MSP Ours Ours + MSP TNR at TPR 95% AUROC DTACC TNR at TPR 95% AUROC DTACC TNR at TPR 95% AUROC DTACC CIFAR-10 iSUN 98.3 99.3 96.9 98.9 99.8 97.8 99.8 99.9 99.0 LSUN (R) 98.5 99.4 97.0 99.4 99.9 98.4 99.8 99.9 99.1 LSUN (C) 98.0 99.4 96.9 89.5 97.8 92.5 98.6 99.6 97.3 TinyImgNet (R) 93.9 98.5 94.6 98.5 99.7 97.6 99.5 99.9 98.5 TinyImgNet (C) 95.2 98.7 95.2 95.9 99.1 95.7 99.1 99.8 97.8 SVHN 98.0 99.5 96.9 97.6 99.4 96.8 99.3 99.8 98.2 CIFAR-100 73.9 94.8 87.9 38.9 80.1 73.3 72.9 93.9 87.0 CIFAR-100 iSUN 50.9 89.8 82.3 96.3 99.1 95.9 95.6 98.9 96.0 LSUN (R) 58.3 92.0 84.7 98.4 99.6 97.3 97.4 99.3 97.4 LSUN (C) 69.5 94.0 86.6 69.7 92.6 85.3 83.1 96.3 89.7 TinyImgNet (R) 36.1 85.1 77.5 96.3 99.1 95.9 92.8 98.2 94.6 TinyImgNet (C) 41.6 86.3 78.6 90.1 97.7 92.8 87.1 96.9 91.1 SVHN 56.2 92.5 85.6 84.8 96.5 90.8 85.6 96.8 90.4 Figure 15: DenseNet/SVHN: D Expanded Table CIFAR-10 17.4 78.4 71.7 7.5 59.3 57.3 16.5 77.7 71.6</cell><cell>2.75</cell><cell>3.00 Min/Max Mean/Var</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>?(x) max x?Va ?(x) ? MSP.</cell></row></table><note>Table shows results when our method is combined with OE. The experiment was conducted with pretrained WideResNet open-sourced by Hendrycks et al. [2019a]. MSP uses Maximum Softmax Probability; "Ours" refers to the metric ? (Eq. 5); "Ours+MSP" is obtained by using ? (x) =</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 7 :</head><label>7</label><figDesc>Table shows results from preliminary experiments on combining OE with our method. The experiment was conducted with pretrained WideResNet open-sourced by<ref type="bibr" target="#b11">Hendrycks et al. [2019a]</ref>. MSP uses Maximum Softmax Probability; "Ours" refers to the metric ? (Eq. 5); "Ours+MSP" is obtained by dividing ? with MSP.</figDesc><table><row><cell>F Few more OOD results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F.1 Comparing with OE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">In-distribution OOD</cell><cell>OE (Base)</cell><cell>OE</cell><cell>Ours (Base)</cell><cell>Ours</cell></row><row><cell>CIFAR-10</cell><cell>Gaussian</cell><cell cols="2">85.6 99.3</cell><cell cols="2">43.5 100.</cell></row><row><cell></cell><cell>Rademacher</cell><cell cols="2">52.4 99.5</cell><cell cols="2">48.3 100.</cell></row><row><cell></cell><cell>Blob</cell><cell cols="2">83.8 99.4</cell><cell cols="2">52.9 99.8</cell></row><row><cell></cell><cell>Texture</cell><cell cols="2">57.2 87.8</cell><cell cols="2">37.0 85.3</cell></row><row><cell></cell><cell>SVHN</cell><cell cols="2">71.2 95.2</cell><cell cols="2">45.4 96.1</cell></row><row><cell></cell><cell>LSUN</cell><cell cols="2">61.3 87.9</cell><cell cols="2">58.2 99.5</cell></row><row><cell>CIFAR-100</cell><cell>Gaussian</cell><cell cols="2">45.7 87.9</cell><cell cols="2">18.2 100.</cell></row><row><cell></cell><cell>Rademacher</cell><cell cols="2">61.0 82.9</cell><cell cols="2">15.6 100.</cell></row><row><cell></cell><cell>Blob</cell><cell cols="2">62.0 87.9</cell><cell cols="2">38.4 98.6</cell></row><row><cell></cell><cell>Texture</cell><cell cols="2">28.5 45.6</cell><cell cols="2">19.9 68.5</cell></row><row><cell></cell><cell>SVHN</cell><cell cols="2">30.7 57.1</cell><cell cols="2">23.5 85.4</cell></row><row><cell></cell><cell>LSUN</cell><cell cols="2">26.0 42.5</cell><cell cols="2">18.2 97.2</cell></row><row><cell>SVHN</cell><cell>Gaussian</cell><cell cols="2">94.6 100.</cell><cell cols="2">87.65 100.</cell></row><row><cell></cell><cell>Bernoulli</cell><cell cols="2">95.6 100.</cell><cell cols="2">92.25 100.</cell></row><row><cell></cell><cell>Blob</cell><cell cols="2">96.3 100.</cell><cell cols="2">93.35 100.</cell></row><row><cell></cell><cell>Texture</cell><cell cols="2">92.8 99.8</cell><cell cols="2">72.6 94.9</cell></row><row><cell></cell><cell>Cifar-10</cell><cell cols="2">94.0 99.9</cell><cell cols="2">73.8 83.0</cell></row><row><cell></cell><cell>LSUN</cell><cell cols="2">93.6 99.9</cell><cell cols="2">75.7 99.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 8 :</head><label>8</label><figDesc>Comparison of Mean TNR@TPR95 values.Following<ref type="bibr" target="#b11">Hendrycks et al. [2019a]</ref>, we created the gaussian, rademacher, blob and bernoulli synthetic datasets. Their descriptions are as follows: Gaussian anomalies have each dimension i.i.d. sampled from an isotropic Gaussian distribution. Rademacher anomalies are images where each dimension is -1 or 1 with equal probability, so each dimension is sampled from a symmetric Rademacher distribution. Bernoulli images have each pixel sampled from a Bernoulli distribution if the input range is [0, 1]. Blobs data consist of algorithmically generated amorphous shapes with definite edges.Textures is a dataset of describable textural images<ref type="bibr" target="#b2">[Cimpoi et al., 2014]</ref>.F.2 Comparing with DPN, VD and Semantic.</figDesc><table><row><cell>OOD</cell><cell>Method</cell><cell>TNR @ TPR95</cell><cell>AUROC</cell><cell>Detection Accuracy</cell><cell>OOD</cell><cell>Method</cell><cell>TNR @ TPR95</cell><cell>AUROC</cell><cell>Detection Accuracy</cell></row><row><cell></cell><cell>DPN</cell><cell>42.60</cell><cell>90.20</cell><cell>79.50</cell><cell></cell><cell>Semantic</cell><cell>41.60</cell><cell>85.20</cell><cell>88.40</cell></row><row><cell></cell><cell>VD</cell><cell>92.30</cell><cell>98.30</cell><cell>94.10</cell><cell></cell><cell>VD</cell><cell>80.20</cell><cell>94.20</cell><cell>87.80</cell></row><row><cell>LSUN</cell><cell>Baseline ODIN</cell><cell>49.80 82.10</cell><cell>91.00 94.10</cell><cell>85.30 86.70</cell><cell>iSUN</cell><cell>Baseline ODIN</cell><cell>16.89 45.21</cell><cell>75.80 85.48</cell><cell>70.11 78.47</cell></row><row><cell></cell><cell>Mahalanobis</cell><cell>98.80</cell><cell>99.70</cell><cell>97.70</cell><cell></cell><cell>Mahalanobis</cell><cell>89.91</cell><cell>97.91</cell><cell>93.05</cell></row><row><cell></cell><cell>Ours</cell><cell>99.85</cell><cell>99.89</cell><cell>98.66</cell><cell></cell><cell>Ours</cell><cell>95.12</cell><cell>98.9</cell><cell>95.18</cell></row><row><cell></cell><cell>DPN</cell><cell>71.60</cell><cell>93.00</cell><cell>86.40</cell><cell></cell><cell>Semantic</cell><cell>20.50</cell><cell>79.00</cell><cell>57.80</cell></row><row><cell></cell><cell>VD</cell><cell>82.90</cell><cell>96.80</cell><cell>91.30</cell><cell></cell><cell>VD</cell><cell>85.50</cell><cell>95.90</cell><cell>90.40</cell></row><row><cell>Tiny ImgNet</cell><cell>Baseline ODIN</cell><cell>41.00 67.90</cell><cell>91.00 94.00</cell><cell>85.10 86.50</cell><cell>LSUN</cell><cell>Baseline ODIN</cell><cell>18.80 23.20</cell><cell>75.80 85.60</cell><cell>69.90 78.30</cell></row><row><cell></cell><cell>Mahalanobis</cell><cell>97.10</cell><cell>99.50</cell><cell>96.30</cell><cell></cell><cell>Mahalanobis</cell><cell>90.89</cell><cell>98.2</cell><cell>93.5</cell></row><row><cell></cell><cell>Ours</cell><cell>99.48</cell><cell>99.72</cell><cell>97.82</cell><cell></cell><cell>Ours</cell><cell>97.14</cell><cell>99.28</cell><cell>96.19</cell></row><row><cell></cell><cell>DPN</cell><cell>79.90</cell><cell>95.90</cell><cell>87.30</cell><cell></cell><cell>Semantic</cell><cell>37.60</cell><cell>83.10</cell><cell>75.60</cell></row><row><cell></cell><cell>VD</cell><cell>71.30</cell><cell>93.20</cell><cell>86.40</cell><cell></cell><cell>VD</cell><cell>83.70</cell><cell>95.30</cell><cell>89.70</cell></row><row><cell>SVHN</cell><cell>Baseline ODIN</cell><cell>50.50 70.30</cell><cell>89.90 96.70</cell><cell>85.10 91.10</cell><cell>Tiny ImgNet</cell><cell>Baseline ODIN</cell><cell>20.40 36.1</cell><cell>77.20 87.6</cell><cell>70.80 80.1</cell></row><row><cell></cell><cell>Mahalanobis</cell><cell>87.80</cell><cell>99.10</cell><cell>95.80</cell><cell></cell><cell>Mahalanobis</cell><cell>90.92</cell><cell>98.20</cell><cell>93.30</cell></row><row><cell></cell><cell>Ours</cell><cell>98.14</cell><cell>99.50</cell><cell>96.71</cell><cell></cell><cell>Ours</cell><cell>95.12</cell><cell>98.97</cell><cell>95.13</cell></row><row><cell></cell><cell cols="3">(a) ResNet/CIFAR-10</cell><cell></cell><cell></cell><cell cols="3">(b) ResNet/CIFAR-100</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 9 :</head><label>9</label><figDesc>We compare our method with DPN, VD and Semantic by reporting results where available.</figDesc><table><row><cell cols="3">F.3 Results for Fully-connected Networks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Architecture</cell><cell>OOD</cell><cell cols="4">Method TNR @ TPR95 AUROC Detection Accuracy</cell></row><row><cell>300</cell><cell>KMNIST Fashion-MNIST</cell><cell>Baseline Ours Baseline Ours</cell><cell>47.66 98.57 44.93 93.51</cell><cell>73.96 99.66 66.93 98.64</cell><cell>73.91 97.37 71.07 94.36</cell></row><row><cell>300-150</cell><cell>KMNIST Fashion-MNIST</cell><cell>Baseline Ours Baseline Ours</cell><cell>59.79 97.8 70.73 95.2</cell><cell>75.17 99.4 77.10 99.00</cell><cell>79.49 96.55 83.00 95.17</cell></row><row><cell>300-150-50</cell><cell>KMNIST Fashion-MNIST</cell><cell>Baseline Ours Baseline Ours</cell><cell>70.4 97.5 73.92 95.7</cell><cell>79.75 99.11 76.54 98.94</cell><cell>83.38 96.4 84.67 95.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>The method even works quite well with a fully-connected neural network trained on MNIST. The results are shown for 300-unit single layer MLP, 300-150 two-layer MLP and 300-150-50 MLP.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Extending Gram Matrices for Out-of-Distribution DetectionOverview In light of the above considerations, we are interested in proposing a method that does not require access to any OOD examples, that does not introduce hyperparameters that need tuning, and</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The maximum activation values observed in the convolution layers of a ResNet trained on Cifar-10 (opensourced by<ref type="bibr" target="#b16">Lee et al. [2018b]</ref>) are 6.5 and 6.3 on train and test partitions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This is based on the TNR at TPR 95% value; AUROC and Detection Accuracy results are comparable.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Scott Lowe, Jackson Wang, Rich Zemel and the anonymous reviewers for their insightful comments and discussion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Detecting outof-distribution inputs in deep neural networks using an early-layer output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdelzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denounden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vernekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10307</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A variational dirichlet framework for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ByxmXnA9FQ" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.461</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.461" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep learning for classical japanese literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bayesian variational autoencoders for unsupervised out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Daxberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05651</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning confidence for out-of-distribution detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04865</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dropout As a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045390.3045502" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
	<note>ICML&apos;16</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.265</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.265" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8183-deep-anomaly-detection-using-geometric-transformations" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="9781" to="9791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hkg4TI9xl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyxCxhRcY7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.243" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Out-of-distribution detection using neural rendering generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04572</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training confidence-calibrated classifiers for detecting out-ofdistribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryiAv2xAZ" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-atta" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1VGkIxRZ" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7936-predictive-uncertainty-estimation-via-prior-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="7047" to="7058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>G?r?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1xwNhCcYm" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Detecting out-of-distribution inputs to deep generative models using a test for typicality. CoRR, abs/1906.02994</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.02994" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf" />
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298640</idno>
		<idno>doi: 10.1109/ CVPR.2015.7298640</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298640" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Detecting out-ofdistribution samples using low-order deep features statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Quintanilha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Filho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Nunes</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgpCoRctm" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Likelihood ratios for out-of-distribution detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.02845" />
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Input complexity and out-ofdistribution detection with likelihood-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>?lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Slizovskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>N??ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luque</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11480</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection using multiple semantic label representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7967-out-of-distribution-detection-using-multiple-semantic-label-representations" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="7386" to="7396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised out-of-distribution detection with batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09115</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Out-of-distribution detection in classifiers via generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vernekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaurav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Abdelzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Denouden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04241</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2010.5539970</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2010" />
	</analytic>
	<monogr>
		<title level="m">The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2010</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Calibration of confidence measures in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASL.2011.2141988</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2461" to="2473" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1506.03365" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
