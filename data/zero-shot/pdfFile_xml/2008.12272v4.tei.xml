<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monocular, One-stage, Regression of Multiple 3D People</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<email>yusun@stu.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
							<email>baoqian@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
							<email>liuwu@live.cn</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@live.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Monocular, One-stage, Regression of Multiple 3D People</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our onestage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-ofthe-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The released code 1 is the first real-time implementation of monocular multi-person 3D mesh regression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure" target="#fig_4">Figure 1</ref><p>. Given a challenging multi-person image like (a), the state-of-the-art approaches, e.g., VIBE <ref type="bibr" target="#b21">[22]</ref> (left), fail to deal with truncation, scene occlusion, and person-person occlusion. The reason lies in the multi-stage design (b), where the bounding-boxlevel features are often implicit, ambiguous, and inseparable in multi-person cases. We propose to regress all meshes in one single stage for multiple 3D people. Specifically, we develop an explicit pixel-level representation (c) for fine-grained one-stage estimation that increases robustness to truncation and occlusion while significantly reducing computational complexity.</p><p>2D person detector to handle multi-person scenes. Generally, they first detect regions with people and then extract the bounding-box-level features, which are used to regress each single 3D human mesh <ref type="bibr">[11,</ref><ref type="bibr">20,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b59">60]</ref>. However, as shown in <ref type="figure" target="#fig_4">Fig. 1</ref>, this strategy is prone to fail in cases of multi-person occlusion and truncation.</p><p>Specifically, as shown in <ref type="figure" target="#fig_4">Fig. 1(b)</ref>, when two people overlap, it is hard for the multi-stage method to estimate diverse body meshes from similar image patches. The ambiguity of the implicit bounding-box-level representation results in failure for such inseparable multi-person cases.</p><p>For multi-person 2D pose estimation, this problem has been tackled via a subtle and effective bottom-up framework. The paradigm first detects all body joints and then assigns them to different people by grouping body joints. This pixel-level body-joint representation enables their impressive performance in crowded scenes <ref type="bibr">[7,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b39">40]</ref>. However, it is non-trivial to extend this bottom-up one-stage process beyond joints <ref type="bibr">[15]</ref>. Unlike 2D pose estimation, which predicts dozens of body joints, we need to regress a human body mesh with thousands of vertices, making it hard to follow the paradigm of body joint detection and grouping.</p><p>Instead, we introduce ROMP, a one-stage network for regressing multiple 3D people in a per-pixel prediction fashion. It directly estimates multiple differentiable maps from the whole image, from which we can easily parse out the 3D meshes of all people. Specifically, as shown in <ref type="figure" target="#fig_4">Fig. 1(c)</ref>, ROMP predicts a Body Center heatmap and a Mesh Parameter map, representing the 2D position of the body center and the parameter vectors of the corresponding 3D body mesh, respectively. Via a simple parameter sampling process, we extract 3D body mesh parameter vectors of all people from the Mesh Parameter map at the body center locations described by the heatmap. Then we put the sampled mesh parameter vectors into the SMPL body model <ref type="bibr" target="#b32">[33]</ref> to derive multi-person 3D meshes.</p><p>Following the guidance of body centers during training, the ambiguity of the regression target is greatly alleviated in crowded multi-person scenes. Additionally, in contrast to the local, bounding-box-level, features learned by traditional methods, end-to-end learning from the whole image forces the model to learn appropriate features from the holistic scene to predict bodies with occlusion. This holistic approach captures the complexity of real-world scenes, enabling the generalization and robustness to complex multiperson cases.</p><p>Moreover, since the body centers of severely overlapping people may collide at the same 2D position, we further develop an advanced collision-aware representation (CAR). The key idea is to construct a repulsion field of body centers, where close body centers are analogous to positive charges that are pushed apart by mutual repulsion. In this way, the body centers of the overlapping people are more distinguishable. Especially in the case of severe overlap, most part of the human body is invisible. Mutual repulsion will push the center to the visible body area, meaning that the model tends to sample 3D mesh parameters estimated from the position centered on the visible body parts. It improves the robustness under heavy person-person occlusion.</p><p>Compared with previous state-of-the-art methods for multi-person <ref type="bibr">[15,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> and single-person <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> 3D mesh regression, ROMP achieves superior performance on challenging benchmarks, including 3DPW <ref type="bibr" target="#b48">[49]</ref> and CMU Panoptic <ref type="bibr">[18]</ref>. Experiments on person-person occlusion datasets (Crowdpose <ref type="bibr" target="#b27">[28]</ref> and 3DPW-PC, a personoccluded subset of 3DPW <ref type="bibr" target="#b48">[49]</ref>) demonstrate the effectiveness of the proposed CAR under person-person occlusion. To further evaluate it in general cases, we test ROMP on images from the Internet and web camera videos. With the same backbone as the multi-stage counterparts, ROMP runs in real-time (over 30 FPS) on a 1070Ti GPU.</p><p>In summary, the contributions are: (1) To the best of our knowledge, ROMP is the first one-stage method for monocular multi-person 3D mesh regression, along with an open-source real-time implementation. Its simple yet effective framework leads to superior accuracy and efficiency.</p><p>(2) The proposed explicit body-center-guided representation facilitates the pixel-level human mesh regression in an end-to-end manner. <ref type="formula" target="#formula_4">(3)</ref> We develop a collision-aware representation to deal with cases of severe overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single-person 3D mesh regression. Parametric human body models, e.g., SMPL <ref type="bibr" target="#b32">[33]</ref>, have been widely adopted to encode the complex 3D human mesh into a lowdimensional parameter vector, which can be regressed from images <ref type="bibr" target="#b31">[32]</ref>. Impressive performance has been achieved using various weak supervision signals, such as 2D pose <ref type="bibr">[9,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b24">25]</ref>, semantic segmentation <ref type="bibr" target="#b50">[51]</ref>, geometric priors and discriminative training <ref type="bibr">[20]</ref>, motion dynamics <ref type="bibr">[21]</ref>, temporal coherence <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">47]</ref>, texture consistency <ref type="bibr" target="#b38">[39]</ref>, optimization <ref type="bibr">[6]</ref> in the loop <ref type="bibr" target="#b22">[23]</ref>, etc. However, all these methods adopt a bounding-box-level representation, which is implicit and ambiguous for multi-person/occlusion cases. To handle occluding objects, Zhang et al. <ref type="bibr" target="#b54">[55]</ref> use a 2D UV map to represent a 3D human mesh. Considering the objectoccluded body parts as blank areas in the partial UV map, they in-paint the partial UV map to make up the occluded information. However, in the case of person-person occlusion where one person's body parts are occluded by those of another, it is hard to generate the partial UV map.</p><p>Multi-person 3D pose estimation. Mainstream methods can be divided into two categories: the one-stage and the multi-stage. Many multi-stage methods follow the top-down design of Faster R-CNN <ref type="bibr" target="#b41">[42]</ref>, such as LCR-Net++ <ref type="bibr" target="#b42">[43]</ref> and 3DMPPE <ref type="bibr" target="#b35">[36]</ref>. From anchor-based feature proposals, they estimate the target via regression. Other works explore a one-stage solution that reasons about all people in a single forward pass. They estimate all the body joint positions and then group them into individuals. Mehta et al. <ref type="bibr" target="#b34">[35]</ref> propose occlusion-robust pose-maps and exploit the body part association to avoid the bounding box predic- <ref type="figure" target="#fig_5">Figure 2</ref>. An overview of ROMP. Given an input image, ROMP predicts multiple maps: 1) the Body Center heatmap predicts the probability of each position being a body center, 2) the Camera map and 3) SMPL map contain the camera and SMPL parameters <ref type="bibr" target="#b32">[33]</ref> of the person at each center, respectively. As the concatenation of the Camera map and SMPL map, the Mesh Parameter map contains the information of the predicted 3D body mesh and its location. Via the designed parameter sampling process, we can obtain the final 3D mesh results by parsing the Body Center heatmap and sampling the Mesh Parameter map. tion. PandaNet <ref type="bibr">[5]</ref> is an anchor-based one-stage model that relies on a huge number of pre-defined anchor predictions and the positive anchor selection. To handle person-person occlusion, Zhen et al. <ref type="bibr" target="#b55">[56]</ref> extend part affinity fields <ref type="bibr">[7]</ref> to be depth-aware. ROMP extends the end-to-end one-stage process beyond joints through a concise body-center-guided pixel-level representation and does not require the part association or an enormous number of anchor predictions.</p><p>Multi-person 3D mesh regression. There are only a few approaches for multi-person 3D mesh regression. Zanfir et al. <ref type="bibr" target="#b52">[53]</ref> estimate the 3D mesh of each person from its intermediate 3D pose estimation. Zanfir et al. <ref type="bibr" target="#b51">[52]</ref> further employ multiple scene constraints to optimize the multiperson 3D mesh results. Jiang et al. <ref type="bibr">[15]</ref> propose a network for Coherent Reconstruction of Multiple Humans (CRMH). Built on Faster-RCNN <ref type="bibr" target="#b41">[42]</ref>, they use the RoI-aligned feature of each person to predict the SMPL parameters. Additionally, they learn the relative positions between multiple people via an interpenetration and depth ordering loss. All these methods follow a multi-stage design. The complex multi-step process requires a repeated feature extraction, which is computationally expensive. Moreover, since they rely on detected bounding boxes, the ambiguity and the limited local view of the bounding-box-level features make it hard to effectively learn from person-person occlusion and truncation. Instead, our proposed one-stage method learns an explicit pixel-level representation with a holistic view, which significantly improves both accuracy and efficiency in multi-person in-the-wild scenes.</p><p>Pixel-level representations have proven useful in anchor-free detection, such as CornerNet <ref type="bibr" target="#b26">[27]</ref>, Center-Net <ref type="bibr">[10,</ref><ref type="bibr" target="#b56">57]</ref>, and ExtremeNet <ref type="bibr" target="#b57">[58]</ref>. They directly estimate the corner or center point of the bounding box in a heatmap manner, which avoids the dense anchor-based proposal. Inspired by these, we develop a pixel-level fine-grained representation for multi-person 3D meshes. Different from the bounding box center used in <ref type="bibr" target="#b56">[57]</ref>, our body center is deter-mined by the body joints, as introduced in Sec. 3.3. A recent work, BMP <ref type="bibr" target="#b53">[54]</ref>, uses a multi-scale grid-level representation for multi-person 3D mesh recovery, which locates a target person at the center of the grid cell. <ref type="bibr">2</ref> In contrast to these methods, ROMP adopts a concise body-center-based representation and further evolves it to a collision-aware version to deal with the inherent center collision problem.</p><p>Disambiguation is a key goal of ROMP, enabling it to deal with the crowded multi-person scenes. Related techniques have been studied in many other fields. For instance segmentation, Adaptis <ref type="bibr" target="#b44">[45]</ref> separately learns the segmentation mask of each instance selected by the guide point. To alleviate the ambiguity between embeddings of similar samples, associate embedding <ref type="bibr" target="#b37">[38]</ref>, triplet loss <ref type="bibr" target="#b43">[44]</ref>, and poseguided association <ref type="bibr">[4]</ref> are developed for pose estimation, face recognition, and tracking respectively. In this paper, a robust and distinguishable representation is developed to help the model explicitly learn from the crowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The overall framework is illustrated in <ref type="figure" target="#fig_5">Fig. 2</ref>. It adopts a simple multi-head design with a backbone and three head networks. Given a single RGB image as input, it outputs a Body Center heatmap, Camera map, and SMPL map, describing the detailed information of the estimated 3D human meshes. In the Body Center heatmap, we predict the probability of each position being a human body center. At each position of the Camera/SMPL map, we predict the camera/SMPL parameters of the person that takes the position as the center. For simplicity, we combine the Camera map and SMPL map into the Mesh Parameter map. During inference, we sample the 3D body mesh parameter results from the Mesh Parameter map at the 2D body center locations parsed from the Body Center heatmap. Finally, we put the sampled parameters into the SMPL model to generate the 3D body meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Basic Representations</head><p>In this section, we introduce the detailed representation of each map. Each output map is of size n ? H ? W , where n is the number of channels. Here, we set H = W = 64.</p><p>Body Center heatmap: C m ? R 1?H?W is a heatmap representing the 2D human body center in the image. Each body center is represented as a Gaussian distribution in the Body Center heatmap. For better representation learning, the Body Center heatmap also integrates the scale information of the body in the 2D image. Specifically, we calculate the Gaussian kernel size k of each person center in terms of its 2D body scale in the image. Given the diagonal length d bb of the person bounding box and the width W of the Body Center heatmap, k is derived as</p><formula xml:id="formula_0">k = k l + ( d bb ? 2W ) 2 k r ,<label>(1)</label></formula><p>where k l is the minimum kernel size and k r is the variation range of k. We set k l = 2 and k r = 5 by default. Mesh Parameter map: P m ? R 145?H?W consists of two parts, the Camera map and SMPL map. Assuming that each location of these maps is the center of a human body, we estimate its corresponding 3D body mesh parameters. Following <ref type="bibr">[20,</ref><ref type="bibr" target="#b46">47]</ref>, we employ a weak-perspective camera model to project K 3D body joints J = (x k , y k , z k ), k = 1 ? ? ? K of the estimated 3D mesh back to the 2D joints J = ( x k , y k ) on the image plane. This facilitates training the model with in-the-wild 2D pose datasets, which helps with robustness and generalization.</p><p>Camera map: A m ? R 3?H?W contains the 3-dim camera parameters (s, t x , t y ) that describe the 2D scale s and translation t = (t x , t y ) of the person in the image. The scale s reflects the body size and the depth to some extent. t x and t y , ranging in (?1, 1), reflect the normalized translation of the human body relative to the image center on the x and y axis, respectively. The 2D projection J of 3D body joints J can be derived as</p><formula xml:id="formula_1">x k = sx k + t x , y k = sy k + t y .</formula><p>The translation parameters allow more accurate position estimates than the Body Center heatmap.</p><p>SMPL map: S m ? R 142?H?W contains the 142-dim SMPL parameters, which describe the 3D pose and shape of the body mesh. SMPL establishes an efficient mapping from the pose ? and shape ? parameters to the human 3D body mesh M ? R 6890?3 . The shape parameter ? ? R 10 is the top-10 PCA coefficients of the SMPL statistical shape space. The pose parameters ? ? R 6?22 contain the 3D rotation of the 22 body joints in a 6D representation <ref type="bibr" target="#b58">[59]</ref>. Instead of using the full 24 joints of the original SMPL model, we drop the last two hand joints. The 3D rotation of the first joint denotes the body 3D orientation in the camera coordinate system, while the remainder are the relative 3D ori- entations of each body part with respect to its parent in a kinematic chain. 3D joints J are derived via P M , where P ? R K?6890 is a sparse weight matrix that describes the linear mapping from 6890 vertices of the body mesh M to the K body joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">CAR: Collision-Aware Representation</head><p>The entire framework is based on a concise body-centerguided representation. It is crucial to define an explicit and robust body center so that the model can easily estimate the center location in various cases. Here we introduce the basic definition of the body center for the general case and its advanced version for severe occlusion.</p><p>Basic definition of the body center. Existing centerbased methods <ref type="bibr">[10,</ref><ref type="bibr" target="#b56">57]</ref> define the center of the bounding box as the target center. This works well for general objects (e.g., a ball or bottle) that lack semantically meaningful keypoints. However, a bounding box center is not a meaningful point on the human body and often falls outside the body area. For stable parameter sampling, we need an explicit body center. Therefore, we calculate each body center from the ground truth 2D pose. Considering that any body joint may be occluded in general cases, we define the body center as the center of visible torso joints (neck, left/right shoulders, pelvis, and left/right hips). When all torso joints are invisible, the center is simply determined by the average of the visible joints. In this way, the model is encouraged to predict the body location from the visible parts.</p><p>However, in cases of severely overlapping people, the body center of the people might be very close or even at the same location on C m . This center collision problem makes the center ambiguous and hard to identify in crowded cases. To tackle this, we develop a more robust representation to deal with person-person occlusion. To alleviate the ambiguity, the center points of overlapping people should be kept at a minimum distance to ensure that they can be well distin-guished. Additionally, to avoid sampling multiple parameters for the same person, the network should assign a unique and explicit center for each person.</p><p>Based on these principles, we develop a novel Collision-Aware Representation (CAR). To ensure that the body centers are far enough from each other, we construct a repulsion field. In this field, each body center is treated as a positive charge, whose radius of repulsion is equal to its Gaussian kernel size derived by Eq. (1). In this way, the closer the body centers are, the greater the mutual repulsion and the further they will be pushed apart. <ref type="figure" target="#fig_0">Fig. 3</ref> illustrates the principle of CAR. Suppose that c 1 ? R 2 , c 2 ? R 2 are the body centers of two overlapping people. If their Euclidean distance d cm and Gaussian kernel sizes k 1 , k 2 satisfy d cm &lt; k 1 + k 2 + 1, the repulsion is triggered to push the close centers apart vi?</p><formula xml:id="formula_2">c 1 = c 1 + ?d p ,? 2 = c 2 ? ?d p , d p = k 1 + k 2 + 1 ? d cm d cm (c 1 ? c 2 ),<label>(2)</label></formula><p>where d p ? R 2 is the repulsion vector from c 2 to c 1 and ? is an intensity coefficient to adjust the strength. When there are multiple overlapped people, we take Eq. (2) to generate the mutual repulsion vectors d i p for the i-th pair of centers. For the center that is affected by N repulsive forces, we calculate the composition of these forces as the numerical summation</p><formula xml:id="formula_3">N i=1 d i p .</formula><p>During training, we use CAR to push apart close body centers and use them to supervise the Body Center heatmap. In this way, the model is encouraged to estimate the centers that maintain a distinguishable distance. For the Body Center heatmap, it helps the model to effectively locate the occluded person. For the Mesh Parameter map, sampling the parameters from these shifted locations enables the model to extract diverse and individual features for each person. The model trained with CAR is more suitable for crowded scenes with significant person-person occlusion such as train stations, canteens, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Parameter Sampling</head><p>To parse the 3D body meshes from the estimated maps, we need to first parse the 2D body center coordinates c ? R K?2 from C m , where K is the number of the detected people, and then use them to sample P m for the SMPL parameters. In this section, we introduce the process of the center parsing, matching and sampling.</p><p>C m is a probability map whose local maxima are regarded as the body centers. The local maxima are derived via M p(C m ) ? C m where M p is the max pooling operation and ? is the logical conjunction operation. Let c be the 2D coordinates of a local maximum with confidence score larger than a threshold t c . We rank the confidence score at each c and take the top N as the final centers. During inference, we directly sample the parameters from P m at c.</p><p>During training, the estimated c are matched with the nearest ground truth body center according to the L 2 distance.</p><p>Additionally, we approximate the depth order between multiple people by using the center confidence from C m and the 2D body scale s of the camera parameters from A m . For people of different s, we regard the one with the larger s as located in the front. For people of similar s, the person with a higher center confidence is considered to be in the front. Please refer to the SuppMat for the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Functions</head><p>To supervise ROMP, we develop individual loss functions for different maps. In total, ROMP is supervised by the weighted sum of the body center loss L c and mesh parameter loss L p .</p><p>Body Center loss. L c encourages a high confidence value at the body center c of the Body Center heatmap C m and low confidence elsewhere. To deal with the imbalance between the center location and the non-center locations in C m , we train the Body Center heatmap based on the focal loss <ref type="bibr" target="#b28">[29]</ref>. Given the predicted Body Center heatmap C p m and the ground truth C gt m , L c is defined as</p><formula xml:id="formula_4">L c = ? L pos + L neg I pos w c , L neg = log(1 ? C p m )(C p m ) 2 (1 ? C gt m ) 4 (1 ? I pos ), L pos = log(C p m )(1 ? C p m ) 2 I pos , I pos = C gt m ? 1,<label>(3)</label></formula><p>where I pos is a binary matrix with a positive value at the body center location, and w c is the loss weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesh Parameter loss.</head><p>As we introduced in Sec. 3.4, the parameter sampling process matches each ground truth body with a predicted parameter result for supervision. The mesh parameter loss is derived as</p><formula xml:id="formula_5">L p = w pose L pose + w shape L shape + w j3d L j3d +w paj3d L paj3d + w pj2d L pj2d + w prior L prior .<label>(4)</label></formula><p>L pose is the L 2 loss of the pose parameters in the 3 ? 3 rotation matrix format. L shape is the L 2 loss of the shape parameters. L j3d is the L 2 loss of the 3D joints J regressed from the body mesh M . L paj3d is the L 2 loss of the 3D joints J after Procrustes alignment. L pj2d is the L 2 loss of the projected 2D joints J . L prior is the Mixture Gaussian prior loss of the SMPL parameters adopted in <ref type="bibr">[6,</ref><ref type="bibr" target="#b32">33]</ref> for supervising the plausibility of 3D joint rotation and body shape. Lastly, w (.) denotes the corresponding loss weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Network Architecture. For a fair comparison with other approaches, we use ResNet-50 <ref type="bibr">[12]</ref> as the default backbone. Since our method is not limited to a specific backbone, we </p><formula xml:id="formula_6">b = W b = 128.</formula><p>The maximum number of detections, N = 64, which is set manually. The loss weights are set to w c = 160, w j3d = 360, w paj3d = 400, w pj2d = 420, w pose = 80, w shape = 1, and w prior = 1.6 to ensure that the weighted loss items are of the same magnitude. The threshold t c of the Body Center heatmap is 0.2. The repulsion coefficient ? of CAR is 0.2.</p><p>Training Datasets. For a fair comparison with previous methods <ref type="bibr">[15,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47]</ref>, the basic training datasets we used in the experiments include two 3D pose datasets (Hu-man3.6M <ref type="bibr">[14]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b33">[34]</ref>), one pseudo-label 3D dataset (UP <ref type="bibr" target="#b25">[26]</ref>) and four in-the-wild 2D pose datasets (MS COCO <ref type="bibr" target="#b29">[30]</ref>, MPII <ref type="bibr">[2]</ref>, LSP <ref type="bibr">[16,</ref><ref type="bibr">17]</ref> and AICH <ref type="bibr" target="#b49">[50]</ref>). We also use the pseudo 3D annotations from <ref type="bibr" target="#b22">[23]</ref>. To further explore the upper limit of performance, we also use additional training datasets, including two 3D pose datasets (MuCo-3DHP <ref type="bibr" target="#b33">[34]</ref> and OH <ref type="bibr" target="#b54">[55]</ref>), the pseudo 3D labels of 2D pose datasets provided by <ref type="bibr">[19]</ref>, and two 2D pose datasets (PoseTrack <ref type="bibr">[1]</ref> and Crowdpose <ref type="bibr" target="#b27">[28]</ref>), to train an advanced model.</p><p>Evaluation Benchmarks. 3DPW <ref type="bibr" target="#b48">[49]</ref> is employed as the main benchmark for evaluating 3D mesh/joint error since it contains in-the-wild multi-person videos with abundant 2D/3D annotations. Specially, we divide 3DPW into 3 subsets, including 3DPW-PC for person-person occlusion, 3DPW-OC for object occlusion, and 3DPW-NC for the non-occluded/truncated cases, to evaluate the performance in different scenarios. Additionally, we also evaluate on a indoor multi-person 3D pose benchmark, CMU Panoptic <ref type="bibr">[18]</ref>. Furthermore, we evaluate the stability under occlusion on Crowdpose <ref type="bibr" target="#b27">[28]</ref>, a crowded-person in-the-wild 2D pose benchmark.</p><p>Evaluation Metrics. We adopt per-vertex error (PVE) to evaluate the 3D surface error. To evaluate the 3D pose accuracy, we employ mean per joint position error (MPJPE), Procrustes-aligned MPJPE (PMPJPE), percentage of correct keypoints (PCK), and area under the PCK-threshold curve (AUC). We also adopt mean per joint angle error (MPJAE), and Procrustes-aligned MPJAE (PA-MPJAE) to evaluate the 3D joint rotation accuracy. Also, to evaluate the pose accuracy in crowded scenes, we calculate the average precision (AP 0.5 ) between the 2D-projection J and the ground truth 2D poses on Crowdpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons to the State-of-the-Art</head><p>3DPW. We adopt three evaluation protocols, which reveal different properties. To validate the performance in actual scenes, we follow Protocol 1 from the 3DPW Challenge to evaluate on the entire 3DPW dataset without using any ground truth, especially the bounding box. With the whole image as input, we equip each single-person method <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> with a human detector (OpenPose <ref type="bibr">[7]</ref> or YOLO <ref type="bibr" target="#b40">[41]</ref>  <ref type="table">Table 5</ref>. Comparisons to the state-of-the-art methods on the person-occluded (3DPW-PC), object-occluded (3DPW-OC) and non-occluded/truncated (3DPW-NC) subsets of 3DPW. We also ablate CAR and vary the repulsion coeff. ?. The evaluation metric is PMPJPE.</p><p>ing method <ref type="bibr" target="#b22">[23]</ref>. We obtain the results of OpenPose + SPIN from <ref type="bibr">[13]</ref>. The results of YOLO + VIBE are obtained using the officially released code, which already contains the YOLO part for human detection. The results of BMP, which adopts a multi-scale grid-level representation, are obtained from <ref type="bibr" target="#b53">[54]</ref>. In Tab. 1, ROMP significantly outperforms all these methods, particularly in MPJPE, PMPJPE, and MP-JAE. These results validate that learning a robust pixel-level representation with a holistic view is helpful for improving the robustness and generalization in actual scenes. Training with extra datasets ( * ) shows that the accuracy of ROMP can be further improved. As a sanity check, we also compare ROMP with the single-person approaches in the evaluation protocols that allow them to use the cropped single-person image as input,  <ref type="table">Table 7</ref>. Run-time comparisons on a 1070Ti GPU. while ROMP still takes the whole image as input. Following VIBE <ref type="bibr" target="#b21">[22]</ref>, Protocol 2 uses the 3DPW test set for evaluation without fine-tuning on the training set, while Protocol 3 fine-tunes the model on the 3DPW training set and uses the test set for evaluation. In Tab. 2, ROMP outperforms these multi-stage approaches on Protocol 2, further demonstrating the advantage of our one-stage design. In Tab. 3, ROMP achieves comparable results with the state-of-the-art methods. If we use HRNet-32 as the backbone, the accuracy improves significantly after fine-tuning.</p><p>CMU Panoptic. Following the evaluation protocol of CRMH <ref type="bibr">[15]</ref>, we evaluate ROMP on the multi-person benchmark, CMU Panoptic, without any fine-tuning. For a fair comparison, we use the same backbone and similar training set as CRMH. As shown in Tab. 4 , ROMP outperforms the existing multi-stage methods <ref type="bibr">[15,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> in all activities by a large margin. These results further demonstrate that learning pixel-level representation with a holistic view improves the performance on multi-person scenes.</p><p>Occlusion benchmarks. To validate the stability under occlusion, we evaluate ROMP on multiple occlusion benchmarks. Firstly, on the person-occluded 3DPW-PC and Crowdpose <ref type="bibr" target="#b27">[28]</ref>, results in Tab. 5 and 6 show that ROMP significantly outperforms previous state-of-the-art methods <ref type="bibr">[15,</ref><ref type="bibr" target="#b21">22]</ref>. Additionally, in <ref type="figure" target="#fig_2">Fig. 5</ref>, some qualitative comparisons to CRMH also demonstrate ROMP's robustness to person-person occlusion. These results suggest that the pixel-level representation is important for improving the performance under person-person occlusion. Finally, on the object-occluded 3DPW-OC, ROMP also achieves promising performance. These results demonstrate that the finegrained pixel-level representation is beneficial for dealing with various occlusion cases.</p><p>Runtime comparisons. We compare ROMP with the state-of-the-art methods in processing videos captured by a web camera. All runtime comparisons are performed on a desktop with a GTX 1070Ti GPU, Intel i7-8700K CPU, and 8 GB RAM. As shown in Tab. 7, ROMP achieves realtime performance, significantly faster than the competing methods. Additionally, as shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, compared with the multi-stage methods <ref type="bibr">[15,</ref><ref type="bibr" target="#b21">22]</ref>, ROMP's processing time is roughly constant regardless of the number of people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study of the CAR</head><p>As shown in Tab. 5 and 6, CAR improve the PMPJPE metric on the 3DPW-PC and the Crowdpose datasets by 4.8% and 10.3%, respectively. Additionally, <ref type="figure" target="#fig_3">Fig. 6</ref> shows, qualitatively, the impact of ablating CAR. Adding CAR improves performance in crowded scenes, which demonstrates that CAR effectively alleviates the center collision problem.</p><p>Intensity coefficient ? of the CAR. To set ?, we conduct an ablation study on 3DPW-PC. In Tab. 5, setting ? = 0.2 performs much better on the crowded scenes (3DPW-PC) and its performance on the normal cases (3DPW-NC/3DPW-OC) is comparable to the best. For general inthe-wild cases of Crowdpose, setting ? = 0.2 improves the performance by 10% over ? = 0 in Tab. 6. Therefore, we suggest training the model with ? = 0.2 for all cases. The reason performance degrades in the normal cases is probably that pushing apart the body centers affects the consistency of the body-center-guided representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>To understand the source of our performance gains, we conduct an ablation study on different subsets of 3DPW subsets. Results in Tab. 5 show that our main gains come from the person-occluded and the non-occluded/truncated cases. It demonstrates the effectiveness of the proposed pixel-level representation in improving the disambiguation in the crowded scenes. Our experiments suggest that the difference between ROMP and the state-of-the-arts <ref type="bibr">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> is the method of representation learning. ROMP learns the pixel-level representation from a holistic view, while the multi-stage methods learn a bounding-box-level representation in a local view. Our one-stage framework enables ROMP to learn more discriminative features that are robust to rich disturbances outside the bounding box, helping generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce a novel one-stage network, ROMP, for monocular multi-person 3D mesh regression. For pixellevel estimation, we propose an explicit body-center-guided representation and further develop it as a collision-aware version, CAR, enabling robust prediction under personperson occlusion. ROMP is the first open-source one-stage method that achieves state-of-the-art performance on multiple benchmarks as well as real-time inference speed. ROMP can serve as a simple yet effective foundation for related multi-person 3D tasks, such as depth estimation, tracking, and interaction modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This supplementary material first provides details about the network architecture design in Sec. 2, followed by hyper-parameter configurations and implementation details in Sec. 3. Finally, we present more qualitative results in Sec. 4, including an analysis of failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Ablation study on the architecture</head><p>The common architecture underlying most existing methods uses a ResNet-50 backbone followed by a single head that produces SMPL parameters. Most existing methods use an iterative approach, first introduced by HMR <ref type="bibr">[9]</ref>. To arrive at the ROMP architecture in <ref type="figure" target="#fig_4">Fig. 1</ref>, we progressively add/change elements of this basic design and evaluate the effects in an ablation study. As shown in <ref type="figure" target="#fig_5">Fig. 2</ref>, two main design choices of the head architecture are explored, single head (SH) vs. separated multiple heads (MH), and the number of convolution blocks (NB) in each head. In this section, we first introduce the experiment settings of the ablation study and then report the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Experiment settings</head><p>Architecture details. 1) SH v.s. MH: The architecture of the single head model is presented as (b) in <ref type="figure" target="#fig_5">Fig. 2</ref>. It is a straightforward and lightweight design, which uses a single branch to jointly estimate the Camera and SMPL maps. In contrast, the separated multi-head design is adopted in <ref type="figure" target="#fig_4">Fig. 1. 2</ref>) NB: We adopt ResNet blocks (shown in <ref type="figure" target="#fig_4">Fig. 1</ref>) as the basic unit of the head networks. We conduct an ablation study to determine the proper number of blocks in each head. The architecture is presented as (c) in <ref type="figure" target="#fig_5">Fig. 2</ref>.</p><p>Datasets. For pretraining, we take three 2D pose datasets, COCO <ref type="bibr">[15]</ref>, CrowdPose <ref type="bibr">[14]</ref>, AICH <ref type="bibr">[20]</ref> and one detection dataset, CrowdHuman <ref type="bibr">[18]</ref>. For the formal training, we take four 3D pose datasets, Human3.6M <ref type="bibr">[3]</ref>, MPI-INF-3DHP <ref type="bibr">[16]</ref>, MuCo-3DHP <ref type="bibr">[16]</ref>, UP <ref type="bibr">[13]</ref>, and three 2D pose datasets, COCO <ref type="bibr">[15]</ref>, MPII <ref type="bibr">[1]</ref>,LSP <ref type="bibr">[5]</ref>. Especially, we only use the 7 subjects <ref type="figure" target="#fig_0">(Subject 1, 2, 3, 4</ref> Training. Firstly, we pretrain the ResNet-50/HRNet-32 model on the pretraining 2D pose datasets for 120 epochs. The network architecture for pretraining is presented as (a) in <ref type="figure" target="#fig_5">Fig. 2</ref>. We follow the hyper-parameter settings of Higher-HRNet <ref type="bibr">[2]</ref> during this process.</p><p>During formal training, we set the learning rate to 5e ?5 , weight decay to 1e ?6 , batch size to 64. To achieve the best performance of each architecture as much as possible, the loss weights are adjusted, according to the visualization results on the validation set, to avoid the model falling into a local minimum. Each model has been trained at least 100 epochs. We observe that the model with fewer head layers often achieves its best performance at about 120 epochs after several rounds of adjusting hyperparameters, while the model with more head layers often achieves its best performance at about 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Result analysis</head><p>In Tab. 1, we present the results of different architecture designs. The experiments are performed with two kinds of backbone, HRNet-32 and ResNet-50. In this settings, HRNet-32 performs better than ResNet-50. Just like its superior performance of fine-tunning on 3DPW (Tab. 3 in the main paper), HRNet-32 once again proves its excellent ability to fit a specific data domain on MPI-INF-3DHP.  Using the same backbone, the performance gap between different head architecture designs is relatively small. Among these designs, we find out that 1) compared with the SH, the disentangled multi-head design performs better in the most cases and is prone to train; 2) regarding the NB, setting n = 2 is a better choice to balance accuracy and training time. <ref type="figure" target="#fig_4">Fig. 1</ref> shows the details of the architecture. ROMP adopts a fully convolutional multi-head design. Compared with previous methods (like <ref type="bibr">[9,</ref><ref type="bibr">11,</ref><ref type="bibr">12]</ref>), we do not need to use iterative regression. It is interesting because previous methods rely on this iterative updating to achieve good pose accuracy. ROMP has a harder task than these previous methods that are given a cropped image of the person. ROMP needs to detect people, sort out what image features belong to which person, and estimate the pose. By learning from a holistic view of the whole image, ROMP is forced to learn more about people and how they appear in images. For example, people at the edge of the image usually tend to be truncated. In addition, the holistic view provides the opportunity of learning the interaction between multiple people, which helps handle the crowded scenes. Since ROMP has to solve the pose-estimation problem given an image of the whole scene, it must learn more distinguishable features to solve the task. We posit that these more powerful features enable it to estimate the body shape and pose without iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training Strategy</head><p>The basic settings of pretraining and formal training are introduced in Sec. 2.1. Here, we introduce the detailed strategy we used for training ROMP. Our training uses 4 NVIDIA P40 GPUs with a batch size of 128. We adopt the Adam optimizer <ref type="bibr">[10]</ref> for training. To avoid the multi-step training and adapt to people of diverse scales, we take both the cropped single-person images and the whole images as input. The ratio of loading the entire images is first set to 10% in the first 60 epochs, and then adjusted to 60% in the remaining 60 epochs. Especially, to accelerate the training and reduce the GPU memory usage, we use the automatic mixed precision (AMP) training of Pytorch <ref type="bibr">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Effect of hyper-parameter configurations during inference</head><p>ROMP has several hyper-parameters that can be adjusted during inference to adapt to different scenes, particularly the confidence threshold t c of the Body Center map and the maximum number of people in an image, N . The confidence threshold t c is used to filter out the detected people with the confidence value lower than t c . Similarly, we set the max person number N to take the top N detected people (sorted by their confidence value on the Body Center map). Changing these parameters only affects the number of output bodies and does not require retraining the model.</p><p>We observe that a higher t c filters out inaccurate/untrusted predictions, while a lower t c leaves more detection results. The setting of the max person number N follows the same rule. The qualitative ablation study in <ref type="figure" target="#fig_0">Fig. 3</ref> illustrates this conclusion. For evaluation on all benchmarks, we have set t c = 0.2 and N = 64 for a fair comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Name</head><p>Frame Ranges downtown bus 00 1620-1900 courtyard hug 00 100-500 courtyard dancing 00 60-370 courtyard dancing 01 60-270 courtyard basketball 00 200-280 courtyard captureSelfies 00 500-600 <ref type="table">Table 2</ref>. Video sequences of the person-occluded subset, 3DPW-PC, in 3DPW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Depth Ordering</head><p>For better visualization, we attempt to approximate the depth ordering between the estimated multi-person body meshes to render the meshes onto the original 2D images. In detail, we construct a depth ordering map to determine the visible meshes in front, using 2D body scale and center confidence as the cue. First, we sort by the body center confidence value from largest to smallest. Second, for each body mesh, we compute the 2D area of the body projected onto the image; this gives an approximate measure of its scale. Finally, we adjust the visible mesh at each position according to their 2D areas. Specifically, at a certain position, if the area value of the invisible body mesh is greater than the currently visible body mesh by a threshold, we swap their positions. In this way, we bring to the front, the body mesh that occupies a larger area on the image plane. This is an approximate solution and future work should explore an integrated solution for depth ordering during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Datasets</head><p>3DPW-PC and 3DPW-OC are the subsets of the 3DPW <ref type="bibr">[19]</ref> dataset that are used to evaluate the performance under person or object occlusion respectively. 3DPW-NC is simply the rest images in the 3DPW. 3DPW-PC contains 1314 frames of 6 person-occluded video sequences. They contain severe person-person occlusion cases with at least two-people overlapping. Details are provided in Tab. 2. Following Zhang et al. <ref type="bibr">[21]</ref>, 3DPW-OC contains 23 object-occluded video sequences. Please refer to <ref type="bibr">[21]</ref> for the details.</p><p>Crowdpose <ref type="bibr">[14]</ref> is a crowded dataset with 2D pose annotations. It contains an abundant variety of person-person and person-object occlusion. Specifically, it contains 20,000 images with about 80,000 persons. We employ their default splits with <ref type="bibr">9</ref> COCO <ref type="bibr">[15]</ref>, MPII <ref type="bibr">[1]</ref>, LSP <ref type="bibr">[5]</ref>, LSP Extended <ref type="bibr">[6]</ref>, and AICH <ref type="bibr">[20]</ref> are in-the-wild 2D pose datasets. We use them for training. Especailly, we use the pseudo SMPL annotations of part images generated by <ref type="bibr">[8,</ref><ref type="bibr">12]</ref> for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Evaluation Metrics</head><p>Per-vertex error (PVE) measures the average Euclidean distance from the 3D body mesh predictions to the ground truth after aligning the pelvis keypoints in millimeters.</p><p>The mean per joint position error (MPJPE) measures the average Euclidean distance from the 3D pose predictions to the ground truth in millimeters. The predictions are first translated to match the ground truth. Generally, they are aligned by the pelvis keypoint for comparison.</p><p>Procrustes-aligned MPJPE (PMPJPE) is MPJPE after rigid alignment of the predicted pose with ground truth in millimeters. It is also called as the reconstruction error. Through Procrustes alignment, the effects of translation, rotation, and scale are eliminated, thus PMPJPE focuses on evaluating the accuracy of the reconstructed 3D skeleton.</p><p>3D PCK &amp; AUC. 3D PCK is the 3D version of the 2D PCK metric, which computes the Percentage of Correct Keypoints. Following the ECCV 2020 3DPW Challenge, the threshold of successful prediction is set to 50mm. Correspondingly, the AUC, which is the total area under the PCK-threshold curve, is calculated by computing PCKs by varying the threshold from 0 to 200mm.</p><p>The mean per joint angle error (MPJAE) measures <ref type="figure" target="#fig_1">Figure 4</ref>. Qualitative results on 3DPW <ref type="bibr">[19]</ref>, 3DOH50K <ref type="bibr">[21]</ref>, and CMU Panoptic <ref type="bibr">[7]</ref> from top to down.</p><p>the angle between the predicted joint orientation and the ground truth orientation in degrees. The orientation difference is measured as the geodesic distance in SO(3). Specifically, only the angles of four limbs and the root are used for evaluation. Procrustes-aligned MPJAE (PMPJAE) measures the MPJAE after applying the rotation matrix, obtained from the Procrustes alignment, on all predicted orientations. As above, it neglects the global mismatch.</p><p>Average Precision (AP) measures multi-person 2D pose accuracy. We employ it to measure the accuracy of the back-projected 2D body keypoints for evaluating the performance on crowded scenes. A detected keypoint candidate is considered to be correct (true positive) if it lies within a threshold of the ground-truth. Each keypoint separately calculates its correspondence with the ground-truth poses. AP correctly penalizes both missed detections and false positives. In <ref type="bibr">[15]</ref>, for multi-person pose estimation, AP is further designed by defining the object keypoint similarity (OKS) that is a similarity measure between the predictions and the ground truth. Analog to IoU in object detection, OKS is defined as</p><formula xml:id="formula_7">OKS = i exp(?d 2 i /2s 2 k 2 i )?(v i &gt; 0) i ?(v i &gt; 0) ,<label>(1)</label></formula><p>where d i is the Euclidean distance between the detected keypoint and the corresponding ground truth, v i is the ground-truth visibility flag, s is the person scale, and k i is a per-keypoint constant that controls falloff. For each keypoint the OKS ranges between 0 and 1. Given the OKS over all labeled keypoints, average precision (AP) and average recall (AR) can be computed. By tuning OKS values, the precision-recall curve can be calculated, and AP and AR at different OKS can throughly reflect the performance of the testing algorithms. Here, we adopt AP 0.5 (AP at OKS = 0.50) and AR 0.5 for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Qualitative Results</head><p>First, in <ref type="figure" target="#fig_1">Fig. 4</ref>, we present some qualitative results on evaluation benchmarks that are representative of our results. Next, we present more results on in-the-wild images in <ref type="figure" target="#fig_2">Fig. 5</ref>. Finally, in <ref type="figure" target="#fig_3">Fig. 6</ref>, we present some failure cases in estimating depth ordering, detection, and 3D pose. Additionally, we also present results of CRMH <ref type="bibr">[4]</ref> on these failure cases for comparison.</p><p>Discussion of failure cases. <ref type="figure" target="#fig_3">Fig. 6</ref> shows the performance of ROMP in estimating the depth order for complex scenes of overlapping people. It illustrates that our body-level depth ordering is limited in cases of extreme crowding with complex depth relationships. CRMH <ref type="bibr">[4]</ref> is a Faster-RCNN-based multi-person state-of-the-art method that supervises mesh interpenetration and depth ordering at the vertex level. It has advantages for determining the multi-person depth ordering in crowded scenes. In contrast, ROMP produces more robust and accurate pose estimation in crowded scenes. In future work, we intend to develop a fine-grained depth estimation approach to tackle this problem. <ref type="figure" target="#fig_3">Fig. 6</ref> also shows some extremely challenging images in terms of pose and occlusion. Images like them clearly challenge the state of the art but may also be challenging for humans to perceive.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Collision-Aware Representation. The body centers of overlapping people are treated as positive charges, which are pushed apart if they are too close in the repulsion field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The FPS variations of ROMP and YOLO+VIBE<ref type="bibr" target="#b21">[22]</ref> when processing images with different number of people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparisons to CRMH [15] on the Crowdpose and the internet images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative ablation study of the CAR on the Crowdpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 .</head><label>1</label><figDesc>Architecture of the proposed ROMP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of the pre-train model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative ablation study of the confidence threshold tc, on a crowded image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons to the state-of-the-art methods on 3DPW following Protocol 1 (without using any ground truth during inference). ? means using extra datasets for training. The input images are resized to 512 ? 512, keeping the same aspect ratio and padding with zeros. The size of the backbone feature is H</figDesc><table><row><cell cols="6">Method OpenPose + SPIN [23] 95.8 MPJPE? PMPJPE? PCK? AUC? MPJAE? PMPJAE? 66.4 33.3 55.0 23.9 24.4 YOLO + VIBE [22] ? 94.7 66.1 33.9 56.6 25.2 20.46 CRMH [15] 105.9 71.8 28.5 51.4 26.4 22.0 BMP [54] ? 104.1 63.8 32.1 54.5 --</cell></row><row><cell>ROMP (ResNet-50) ROMP (ResNet-50) ? ROMP (HRNet-32) ?</cell><cell>87.0 80.1 82.7</cell><cell>62.0 56.8 60.5</cell><cell>34.4 57.6 36.4 60.1 36.5 59.7</cell><cell>21.9 20.8 20.5</cell><cell>20.1 19.1 18.9</cell></row><row><cell cols="2">Method HMR [20] Kanazawa et al. [21] 116.5 MPJPE? PMPJPE? PVE? 130.0 76.7 -72.6 139.3 Arnab et al. [3] -72.2 -GCMR [24] -70.2 -DSD-SATN [47] -69.5 -SPIN [23] 96.9 59.2 116.4 ROMP (ResNet-50) 91.3 54.9 108.3 I2L-MeshNet [37] ? 93.2 58.6 -EFT [19] ? -54.2 -VIBE [22] ? 93.5 56.5 113.4 ROMP (ResNet-50) ? 89.3 53.5 105.6 ROMP (HRNet-32) ? 85.5 53.3 103.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Table 2. Comparisons to the state-of-the-art methods on 3DPW following VIBE [22], using Protocol 2 (on the test set only).? means using extra datasets (compared with SPIN) for training.also test HRNet-32 [8] in the experiments. Through the backbone, a feature vector f b ? R 32?H b ?W b is extracted from a single RGB image. Also, we adopt the Coord- Conv [31] to enhance the spatial information. Therefore, the backbone feature f ? R 34?H b ?W b is the combination of a coordinate index map ci ? R 2?H b ?W b and f b . Next, from f , three head networks are developed to estimate the Body Center, Camera, and SMPL maps. More details of the architecture are in the SuppMat. Setting Details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparisons to the state-of-the-art methods on 3DPW following Protocol 3 (fine-tuned on the training set). ? means using extra datasets (compared with EFT) for training. ? 107.8 125.3 135.4 141.8 127.6 ROMP (HRNet-32) ? 110.8 122.8 141.6 137.6 128.2 Comparisons to the state-of-the-art methods on CMU Panoptic [18] benchmark. The evaluation metric is MPJPE after centering the root joint. All methods are directly evaluated without any fine-tuning. ? means using extra datasets for training.</figDesc><table><row><cell>). For a fair comparison, ROMP uses the same backbone (ResNet-50) and training datasets as the compet-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6 .</head><label>6</label><figDesc>Comparisons to the state-of-the-art methods on the Crowdpose<ref type="bibr" target="#b27">[28]</ref> benchmark. The evaluation metric is AP 0.5 .</figDesc><table><row><cell>Split Test Validation</cell><cell>CRMH [15] ROMP ROMP+CAR 33.9 54.1 59.7 32.9 55.6 58.6</cell></row><row><cell cols="2">Method VIBE [22] CRMH [15] ROMP FPS ? 10.9 14.1 20.8 Backbone ResNet-50 ResNet-50 HRNet-32 ResNet-50 ROMP 30.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Ablation study on the architecture design. SH denotes the single head design and MH is the separated multi-head design. NB is the number of blocks used in each head.</figDesc><table><row><cell>, 5, 6, and 7) of MPI-INF-3DHP for training. Subject 8 is used for val-idation. The test set of MPI-INF-3DHP is employed for evaluation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>,963 samples for training, 7,991 test samples, and 1,997 validation samples. 3DOH50K [21] is a 3D human occlusion dataset. In the image, the human body is occluded by various objects, such as a laptop computer, box, chair, etc. It contains 50,310 images for training and 1,290 images for testing. It is used to evaluate the performance under object occlusion. MPI-INF-3DHP [16] is a single-person multi-view 3D pose dataset. It contains 8 actors performing 8 activities. Over 1.3M frames are captured from all 14 cameras. Except for the indoor RGB videos of a single person, they also provide MATLAB code to generate a multi-person dataset, MuCo-3DHP, via mixing up segmented foreground human appearance.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The arXiv version of ROMP, called CenterHMR<ref type="bibr" target="#b45">[46]</ref>, predates BMP.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Monocular, One-stage, Regression of Multiple 3D People **Supplementary Material**</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pose-guided tracking-by-detection: Robust multiperson pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PandaNet: Anchor-based single-shot multi-person 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Benzine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Luvison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Achard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HigherHRNet: Scaleaware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">HoloPose: Holistic 3D human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Beyond weak perspective for monocular 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kissos</forename><surname>Imry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fritz</forename><surname>Lior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goldman</forename><surname>Matan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meir</forename><surname>Omer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oks</forename><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kliger</forename><surname>Mark</surname></persName>
		</author>
		<idno>ECCVW, 2020. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exemplar fine-tuning for 3D human pose fitting towards in-thewild 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning 3D human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Appearance consensus driven self-supervised human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">Mysore</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CrowdPose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Recent advances in monocular 2d and 3d human pose estimation: A deep learning perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3D pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3D multiperson pose estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-lixel prediction network for accurate 3D human pose and mesh estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">TexturePose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2D and 3D pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adaptis: Adaptive instance selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Centerhmr: a bottom-up single-shot method for multi-person 3d mesh recovery from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Tao</surname></persName>
		</author>
		<idno>arxiv, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Ai challenger: A large-scale dataset for going deeper in image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DenseRaC: Joint 3D pose and shape estimation by dense render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3D sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Body meshes as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Objectoccluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SMAP: Single-shot multiperson absolute 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jingwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Jimei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Detailed human shape estimation from a single image by hierarchical mesh deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Exemplar fine-tuning for 3D human pose fitting towards in-thewild 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">CrowdPose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Ai challenger: A large-scale dataset for going deeper in image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Objectoccluded human shape and pose estimation from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
				<title level="m">Figure 5. Qualitative results on in-the-wild images</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Failure cases on estimating depth ordering, 3D pose, and detection results in extremely challenging scenes</title>
		<imprint/>
	</monogr>
	<note>Figure 6</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
