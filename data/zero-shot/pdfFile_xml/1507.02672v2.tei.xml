<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Learning with Ladder Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Curious AI Company</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Curious AI Company</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
							<affiliation key="aff2">
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Aalto University &amp; The Curious AI Company</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Aalto University &amp; The Curious AI Company</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Learning with Ladder Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on the Ladder network proposed by Valpola <ref type="formula">(2015)</ref>, which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification, in addition to permutationinvariant MNIST classification with all labels. arXiv:1507.02672v2 [cs.NE] 24 Nov 2015 decoder can recover any details discarded by the encoder. Previously, the Ladder network has only been demonstrated in unsupervised learning <ref type="bibr" target="#b33">(Valpola, 2015;</ref><ref type="bibr" target="#b23">Rasmus et al., 2015a)</ref> but we now combine it with supervised learning.</p><p>The key aspects of the approach are as follows:</p><p>Compatibility with supervised methods. The unsupervised part focuses on relevant details found by supervised learning. Furthermore, it can be added to existing feedforward neural networks, for example multi-layer perceptrons (MLPs) or convolutional neural networks (CNNs) (Section 3). We show that we can take a state-of-the-art supervised learning method as a starting point and improve the network further by adding simultaneous unsupervised learning (Section 4).</p><p>Scalability resulting from local learning. In addition to a supervised learning target on the top layer, the model has local unsupervised learning targets on every layer, making it suitable for very deep neural networks. We demonstrate this with two deep supervised network architectures.</p><p>Computational efficiency. The encoder part of the model corresponds to normal supervised learning. Adding a decoder, as proposed in this paper, approximately triples the computation during training but not necessarily the training time since the same result can be achieved faster through the better utilization of the available information. Overall, computation per update scales similarly to whichever supervised learning approach is used, with a small multiplicative factor.</p><p>As explained in Section 2, the skip connections and layer-wise unsupervised targets effectively turn autoencoders into hierarchical latent variable models which are known to be well suited for semisupervised learning. Indeed, we obtain state-of-the-art results in semi-supervised learning in the MNIST, permutation invariant MNIST and CIFAR-10 classification tasks (Section 4). However, the improvements are not limited to semi-supervised settings: for the permutation invariant MNIST task, we also achieve a new record with the normal full-labeled setting. 1 2 Derivation and justification Latent variable models are an attractive approach to semi-supervised learning because they can combine supervised and unsupervised learning in a principled way. The only difference is whether the class labels are observed or not. This approach was taken, for instance, by Goodfellow et al.</p><p>(2013a) with their multi-prediction deep Boltzmann machine. A particularly attractive property of hierarchical latent variable models is that they can, in general, leave the details for the lower levels to represent, allowing higher levels to focus on more invariant, abstract features that turn out to be relevant for the task at hand.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we introduce an unsupervised learning method that fits well with supervised learning. The idea of using unsupervised learning to complement supervision is not new. Combining an auxiliary task to help train a neural network was proposed by <ref type="bibr" target="#b30">Suddarth and Kergosien (1990)</ref>. By sharing the hidden representations among more than one task, the network generalizes better. There are multiple choices for the unsupervised task, for example, reconstruction of the inputs at every level of the model (e.g., <ref type="bibr" target="#b22">Ranzato and Szummer, 2008)</ref> or classification of each input sample into its own class . Although some methods have been able to simultaneously apply both supervised and unsupervised learning <ref type="bibr" target="#b22">(Ranzato and Szummer, 2008;</ref><ref type="bibr" target="#b8">Goodfellow et al., 2013a)</ref>, often these unsupervised auxiliary tasks are only applied as pre-training, followed by normal supervised learning (e.g., <ref type="bibr" target="#b12">Hinton and Salakhutdinov, 2006)</ref>. In complex tasks there is often much more structure in the inputs than can be represented, and unsupervised learning cannot, by definition, know what will be useful for the task at hand. Consider, for instance, the autoencoder approach applied to natural images: an auxiliary decoder network tries to reconstruct the original input from the internal representation. The autoencoder will try to preserve all the details needed for reconstructing the image at pixel level, even though classification is typically invariant to all kinds of transformations which do not preserve pixel values. Most of the information required for pixel-level reconstruction is irrelevant and takes space from the more relevant invariant features which, almost by definition, cannot alone be used for reconstruction.</p><p>Our approach follows <ref type="bibr" target="#b33">Valpola (2015)</ref>, who proposed a Ladder network where the auxiliary task is to denoise representations at every level of the model. The model structure is an autoencoder with skip connections from the encoder to decoder and the learning task is similar to that in denoising autoencoders but applied to every layer, not just the inputs. The skip connections relieve the pressure to represent details in the higher layers of the model because, through the skip connections, the The training process of latent variable models can typically be split into inference and learning, that is, finding the posterior probability of the unobserved latent variables and then updating the underlying probability model to fit the observations better. For instance, in the expectation-maximization (EM) algorithm, the E-step corresponds to finding the expectation of the latent variables over the posterior distribution assuming the model fixed and the M-step then maximizes the underlying probability model assuming the expectation fixed.</p><p>The main problem with latent variable models is how to make inference and learning efficient. Suppose there are layers l of latent variables z <ref type="bibr">(l)</ref> . Latent variable models often represent the probability distribution of all the variables explicitly as a product of terms, such as p(z (l) | z (l+1) ) in directed graphical models. The inference process and model updates are then derived from Bayes' rule, typically as some kind of approximation. The inference is often iterative as it is generally impossible to solve the resulting equations in a closed form as a function of the observed variables.</p><p>There is a close connection between denoising and probabilistic modeling. On the one hand, given a probabilistic model, you can compute the optimal denoising. Say you want to reconstruct a latent z using a prior p(z) and an observationz = z + noise. We first compute the posterior distribution p(z |z), and use its center of gravity as the reconstruction?. One can show that this minimizes the expected denoising cost (? ? z) 2 . On the other hand, given a denoising function, one can draw samples from the corresponding distribution by creating a Markov chain that alternates between corruption and denoising . <ref type="bibr" target="#b33">Valpola (2015)</ref> proposed the Ladder network, where the inference process itself can be learned by using the principle of denoising, which has been used in supervised learning <ref type="bibr" target="#b27">(Sietsma and Dow, 1991)</ref>, denoising autoencoders (dAE) <ref type="bibr" target="#b35">(Vincent et al., 2010)</ref>, and denoising source separation (DSS) <ref type="bibr" target="#b26">(S?rel? and Valpola, 2005)</ref> for complementary tasks. In dAE, an autoencoder is trained to reconstruct the original observation x from a corrupted versionx. Learning is based simply on minimizing the norm of the difference of the original x and its reconstructionx from the corruptedx; that is the cost is x ? x 2 .</p><p>While dAEs are normally only trained to denoise the observations, the DSS framework is based on the idea of using denoising functions? = g(z) of the latent variables z to train a mapping z = f (x) which models the likelihood of the latent variables as a function of the observations. The cost function is identical to that used in a dAE except that the latent variables z replace the observations x; that is, the cost is ? ? z 2 . The only thing to keep in mind is that z needs to be normalized somehow as otherwise the model has a trivial solution at z =? = constant. In a dAE, this cannot happen as the model cannot change the input x. <ref type="figure">Figure 1</ref> depicts the optimal denoising function? = g(z) for a one-dimensional bimodal distribution, which could be the distribution of a latent variable inside a larger model. The shape of the denoising function depends on the distribution of z and the properties of the corruption noise. With no noise at all, the optimal denoising function would be the identity function. In general, the denoising function pushes the values towards higher probabilities, as shown by the green arrows. <ref type="figure" target="#fig_1">Figure 2</ref> shows the structure of the Ladder network. Every layer contributes to the cost function a term C (l) d = z (l) ?? (l) 2 which trains the layers above (both encoder and decoder) to learn the denoising function? (l) = g (l) (z (l) ,? (l+1) ) which maps the corruptedz (l) onto the denoised estimat? z <ref type="bibr">(l)</ref> . As the estimate? <ref type="bibr">(l)</ref> incorporates all prior knowledge about z, the same cost function term also trains the encoder layers below to find cleaner features which better match the prior expectation.</p><p>Since the cost function needs both the clean z (l) and corruptedz <ref type="bibr">(l)</ref> , during training the encoder is run twice: a clean pass for z (l) and a corrupted pass forz <ref type="bibr">(l)</ref> . Another feature which differentiates the Ladder network from regular dAEs is that each layer has a skip connection between the encoder and decoder. This feature mimics the inference structure of latent variable models and makes it possible for the higher levels of the network to leave some of the details for lower levels to represent. <ref type="bibr" target="#b23">Rasmus et al. (2015a)</ref> showed that such skip connections allow dAEs to focus on abstract invariant features on the higher levels, making the Ladder network a good fit with supervised learning that can select which information is relevant for the task at hand.</p><p>One way to picture the Ladder network is to consider it as a collection of nested denoising autoencoders which share parts of the denoising machinery with each other. From the viewpoint of the autoencoder on layer l, the representations on the higher layers can be treated as hidden neurons. In other words, there is no particular reason why? <ref type="bibr">(l+i)</ref> as produced by the decoder should resemble the corresponding representations z (l+i) as produced by the encoder. It is only the cost function C (l+i) d that ties these together and forces the inference to proceed in reverse order in the decoder. This sharing helps a deep denoising autoencoder to learn the denoising process as it splits the task into meaningful sub-tasks of denoising intermediate representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation of the Model</head><p>The steps involved in implementing the Ladder network (Section 3.1) are typically as follows: 1) take a feedforward model which serves supervised learning as the encoder (Section 3.2); 2) add a decoder which can invert the mappings on each layer of the encoder and supports unsupervised learning (Section 3.3); and 3) train the whole Ladder network by minimizing the sum of all the cost function terms.</p><p>In this section, we will go through these steps in detail for a fully connected MLP network and briefly outline the modifications required for convolutional networks, both of which are used in our experiments (Section 4). Corrupted Clean <ref type="figure">Figure 1</ref>: A depiction of an optimal denoising function for a bimodal distribution. The input for the function is the corrupted value (x axis) and the target is the clean value (y axis). The denoising function moves values towards higher probabilities as show by the green arrows. ? y The feedforward path (x ? z (1) ? z (2) ? y) shares the mappings f (l) with the corrupted feedforward path, or encoder (x ?z (1) ?z (2) ??). The decoder (z (l) ?? (l) ?x) consists of the denoising functions g <ref type="bibr">(l)</ref> and has cost functions C (l) d on each layer trying to minimize the difference between? (l) and z <ref type="bibr">(l)</ref> . The output? of the encoder can also be trained to match available labels t(n).</p><formula xml:id="formula_0">g (1) (?, ?) g (0) (?, ?) f (1) (?) f (1) (?) f (2) (?) f (2) (?) N (0, 2 ) N (0, 2 ) N (0, 2 ) C (2) d C (1) d C (0) d z (1) z (2)?(2) z (1) z (1) z (2) xx x x x g (2) (?, ?)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Calculation of the output y and cost function C of the Ladder network</head><p>Require: x(n) # Corrupted encoder and classifier</p><formula xml:id="formula_1">h (0) ?z (0) ? x(n) + noise for l = 1 to L d? z (l) ? batchnorm(W (l)h(l?1) ) + nois? h (l) ? activation(? (l) (z (l) + ? (l) )) end for P (? | x) ?h (L) # Clean encoder (for denoising targets) h (0) ? z (0) ? x(n) for l = 1 to L do z (l) pre ? W (l) h (l?1) ? (l) ? batchmean(z (l) pre ) ? (l) ? batchstd(z (l) pre ) z (l) ? batchnorm(z (l) pre ) h (l) ? activation(? (l) (z (l) + ? (l) )) end for # Final classification: P (y | x) ? h (L) # Decoder and denoising for l = L to 0 do if l = L then u (L) ? batchnorm(h (L) ) else u (l) ? batchnorm(V (l+1)?(l+1) ) end if ?i :? (l) i ? g(z (l) i , u (l) i ) # Eq. (2) ?i :? (l) i,BN ?? (l) i ?? (l) i ? (l) i end for # Cost function C for training: C ? 0 if t(n) then C ? ? log P (? = t(n) | x(n)) end if C ? C + L l=0 ? l z (l) ?? (l) BN 2 # Eq. (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Steps for Implementing the Ladder Network</head><p>Consider training a classifier, 2 , or a mapping from input x to output y with targets t, from a training set of pairs {x(n), t(n) | 1 ? n ? N }. Semi-supervised learning <ref type="bibr" target="#b5">(Chapelle et al., 2006)</ref> studies how auxiliary unlabeled data {x(n) | N + 1 ? n ? M } can help in training a classifier. It is often the case that labeled data are scarce whereas unlabeled data are plentiful, that is N M .</p><p>The Ladder network can improve results even without auxiliary unlabeled data but the original motivation was to make it possible to take well-performing feedforward classifiers and augment them with an auxiliary decoder as follows:</p><p>1. Train any standard feedforward neural network. The network type is not limited to standard MLPs, but the approach can be applied, for example, to convolutional or recurrent networks. This will be the encoder part of the Ladder network.</p><p>2. For each layer, analyze the conditional distribution of representations given the layer above, p(z (l) | z (l+1) ). The observed distributions could resemble for example Gaussian distributions where the mean and variance depend on the values z (l+1) , bimodal distributions where the relative probability masses of the modes depend on the values z (l+1) , and so on.</p><p>3. Define a function? (l) = g(z (l) ,? (l+1) ) which can approximate the optimal denoising function for the family of observed distributions. The function g is therefore expected to form a reconstruction? (l) that resembles the clean z (l) given the corruptedz (l) and the higher-level reconstruction? (l+1) .</p><p>4. Train the whole network in a fully-labeled or semi-supervised setting using standard optimization techniques such as stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fully Connected MLP as Encoder</head><p>As a starting point we use a fully connected MLP network with rectified linear units. We follow <ref type="bibr" target="#b13">Ioffe and Szegedy (2015)</ref> and apply batch normalization to each preactivation including the topmost layer in the L-layer network. This serves two purposes. First, it improves convergence as a result of reduced covariate shift as originally proposed by <ref type="bibr" target="#b13">Ioffe and Szegedy (2015)</ref>. Second, as explained in Section 2, DSS-type cost functions for all but the input layer require some type of normalization to prevent the denoising cost from encouraging the trivial solution where the encoder outputs just constant values as these are the easiest to denoise. Batch normalization conveniently serves this purpose, too.</p><p>Formally, batch normalization for the layers l = 1 . . . L is implemented as</p><formula xml:id="formula_2">z (l) = N B (W (l) h (l?1) ) h (l) = ? ? (l) (z (l) + ? (l) ) , where h (0) = x, N B is a component-wise batch normalization N B (x i ) = (x i ?? xi )/? xi ,</formula><p>where? xi and? xi are estimates calculated from the minibatch, ? (l) and ? (l) are trainable parameters, and ?(?) is the activation function such as the rectified linear unit (ReLU) for which ?(?) = max(0, ?). For outputs y = h (L) we always use the softmax activation. For some activation functions the scaling parameter ? (l) or the bias ? (l) are redundant and we only apply them in non-redundant cases. For example, the rectified linear unit does not need scaling, the linear activation function needs neither scaling nor bias, but softmax requires both.</p><p>As explained in Section 2 and shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the Ladder network requires two forward passes, one clean and one corrupted, which produce clean z (l) and h (l) and corruptedz (l) andh (l) , respectively. We implemented corruption by adding isotropic Gaussian noise n to inputs and after each batch normalization:x</p><formula xml:id="formula_3">=h (0) = x + n (0) z (l) pre = W (l)h(l?1) z (l) = N B (z (l) pre ) + n (l) h (l) = ? ? (l) (z (l) + ? (l) ) .</formula><p>Note that we collect the valuez (l) pre here because it will be needed in the decoder cost function in Section 3.3.</p><p>The supervised cost C c is the average negative log probability of the noisy output? matching the target t(n) given the inputs x(n)</p><formula xml:id="formula_4">C c = ? 1 N N n=1 log P (? = t(n) | x(n)).</formula><p>In other words, we also use the noise to regularize supervised learning.</p><p>We saw networks with this structure reach close to state-of-the-art results in purely supervised learning (see e.g. <ref type="table" target="#tab_0">Table 1</ref>), which makes them good starting points for improvement via semi-supervised learning by adding an auxiliary unsupervised task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder for Unsupervised Learning</head><p>When designing a suitable decoder to support unsupervised learning, we had to make a choice as to what kinds of distributions of the latent variables the decoder would optimally be able to denoise. We ultimately ended up choosing a parametrization that supports the optimal denoising of Gaussian latent variables. We also experimented with alternative denoising functions, more details of which can be found in Appendix B. Further analysis of different denoising functions was recently published by <ref type="bibr" target="#b19">Pezeshki et al. (2015)</ref>.</p><p>In order to derive the chosen parametrization and justify why it supports Gaussian latent variables, let us begin with the assumption that the noisy value of one latent variablez that we want to denoise has the formz = z + n, where zis the clean latent variable value that has a Gaussian distribution with variance ? 2 z , and n is the Gaussian noise with variance ? 2 n . We now want to estimate?, a denoised version ofz, so that the estimate minimizes the squared error of the difference to the clean latent variable values z. It can be shown that the functional form of? = g(z) has to be linear in order to minimize the denoising cost, with the assumption being that both the noise and the latent variable have a Gaussian distribution <ref type="bibr">(Valpola, 2015, Section 4.1)</ref>. Specifically, the result will be a weighted sum of the corruptedz and a prior ?. The weight ? of the corruptedz will be a function of the variance of z and n according to:</p><formula xml:id="formula_5">? = ? 2 z ? 2 z + ? 2 n</formula><p>The denoising function will therefore have the form:</p><formula xml:id="formula_6">z = g(?) = ? * z + (1 ? ?) * ? = (z ? ?) * ? + ?<label>(1)</label></formula><p>We could let ? and ? be trainable parameters of the model, where the model would learn some estimate of the optimal weighting ? and prior ?. The problem with this formulation is that it only supports the optimal denoising of latent variables with a Gaussian distribution, as the function g is linear wrt.z.</p><p>We relax this assumption by making the model only require the distribution of z of a layer to be Gaussian conditional on the values of the latent variables of the layer above. In a similar vein, in a layer of multiple latent variables we can assume that the latent variables are independent conditional on the latent variables of the layer above. The distribution of the latent variables z (l) is therefore assumed to follow the distribution</p><formula xml:id="formula_7">p(z (l) | z (l+1) ) = i p(z (l) i | z (l+1) ) where p(z (l) i | z (l+1)</formula><p>) are conditionally independent Gaussian distributions. One interpretation of this formulation is that we are modeling the distribution of z (l) as a mixture of Gaussians with diagonal covariance matrices, where the value of the above layer z (l+1) modulates the form of the Gaussian that z (l) is distributed as. In practice, we will implement the dependence of v and ? on? (l+1) with a batch normalized projection from? (l+1) followed by an expressive nonlinearity with trainable parameters. The final formulation of the denoising function is therefor?</p><formula xml:id="formula_8">z (l) i = g i (z (l) i , u (l) i ) = z (l) i ? ? i (u (l) i ) ? i (u (l) i ) + ? i (u (l) i )<label>(2)</label></formula><p>where u (l) i propagates information from? (l+1) by a batch normalized projection:</p><formula xml:id="formula_9">u (l) = N B (V (l+1)?(l+1) ) ,</formula><p>where the matrix V (l) has the same dimension as the transpose of W (l) on the encoder side. The projection vector u (l) therefore has the same dimensionality as z <ref type="bibr">(l)</ref> . Furthermore, the functions</p><formula xml:id="formula_10">? i (u (l) i ) and ? i (u (l)</formula><p>i ) are modeled as expressive nonlinearities:</p><formula xml:id="formula_11">? i (u (l) i ) = a (l) 1,i sigmoid(a (l) 2,i u (l) i + a (l) 3,i ) + a (l) 4,i u (l) i + a (l) 5,i ? i (u (l) i ) = a (l) 6,i sigmoid(a (l) 7,i u (l) i + a (l) 8,i ) + a (l) 9,i u (l) i + a (l) 10,i , where a (l) 1,i . . . a<label>(l)</label></formula><p>10,i are the trainable parameters of the nonlinearity for each neuron i in each layer l. It is worth noting that in this parametrization, each denoised value? <ref type="bibr">(l)</ref> i only depends onz (l) i and not the fullz <ref type="bibr">(l)</ref> . This means that the model can only optimally denoise conditionally independent distributions. While this nonlinearity makes the number of parameters in the decoder slightly higher than in the encoder, the difference is insignificant as most of the parameters are in the vertical projection mappings W (l) and V (l) , which have the same dimensions (apart from transposition). Note the slight abuse of the notation here since g <ref type="bibr">(l)</ref> i is now a function of the scalarsz <ref type="bibr">(l)</ref> i and u (l) i rather than the full vectorsz (l) and? (l+1) . Given u (l) , this parametrization is linear with respect t? z (l) , and both the slope and the bias depend nonlinearly on u (l) , as we hoped.</p><p>For the lowest layer,x =? (0) andx =z (0) by definition, and for the highest layer we chose u (L) =?. This allows the highest-layer denoising function to utilize prior information about the classes being mutually exclusive, which seems to improve convergence in cases where there are very few labeled samples.</p><p>As a side note, if the values of z (l) are truly independently distributed Gaussians, there is nothing left for the layer above,? (l+1) , to model. In that case, a mixture of Gaussians is not needed to model z (l) , but a diagonal Gaussian which can be modeled with a linear denoising function with constant values for ? and ? as in Equation 1, would suffice. In this parametrization all correlations, nonlinearities, and non-Gaussianities in the latent variables z (l) have to be represented by modulations from the layers above for optimal denoising. As the parametrization allows the distribution of z (l) to be modulated by z (l+1) through u (l) , it encourages the decoder to find representations z (l) that have high mutual information with z (l+1) . This is crucial as it allows supervised learning to have an indirect influence on the representations learned by the unsupervised decoder: any abstractions selected by supervised learning will bias the lower levels to find more representations which carry information about the same abstractions.</p><p>The cost function for the unsupervised path is the mean squared reconstruction error per neuron, but there is a slight twist which we found to be important. Batch normalization has useful properties, as noted in Section 3.2, but it also introduces noise which affects both the clean and corrupted encoder pass. This noise is highly correlated between z (l) andz (l) because the noise derives from the statistics of the samples that happen to be in the same minibatch. This highly correlated noise in z (l) andz (l) biases the denoising functions to be simple copies 3?(l) ?z (l) .</p><p>The solution we found was to implicitly use the projections z (l) pre as the target for denoising and scale the cost function in such a way that the term appearing in the error term is the batch normalized z (l) instead. For the moment, let us see how that works for a scalar case:</p><formula xml:id="formula_12">1 ? 2 z pre ?? 2 = z pre ? ? ? ?? ? ? ? 2 = z ?? BN 2 z = N B (z pre ) = z pre ? ? ? z BN =? ? ? ? ,</formula><p>where ? and ? are the batch mean and batch std of z pre , respectively, that were used in batch normalizing z pre into z. The unsupervised denoising cost function C d is thus</p><formula xml:id="formula_13">C d = L l=0 ? l C (l) d = L l=0 ? l N m l N n=1 z (l) (n) ?? (l) BN (n) 2 ,<label>(3)</label></formula><p>where m l is the layer's width, N the number of training samples, and the hyperparameter ? l a layerwise multiplier determining the importance of the denoising cost.</p><formula xml:id="formula_14">The model parameters W (l) , ? (l) , ? (l) , V (l) , a (l) i , b (l) i , andc<label>(l)</label></formula><p>i can be trained simply by using the backpropagation algorithm to optimize the total cost C = C c + C d . The feedforward pass of the full Ladder network is listed in Algorithm 1. Classification results are read from the y in the clean feedforward path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Variations</head><p>Section 3.3 detailed how to build a decoder for the Ladder network to match the fully connected encoder described in Section 3.2. It is easy to extend the same approach to other encoders, for instance, convolutional neural networks (CNN). For the decoder of fully connected networks we used vertical mappings whose shape is a transpose of the encoder mapping. The same treatment works for the convolution operations: in the networks we have tested in this paper, the decoder has convolutions whose parametrization mirrors the encoder and effectively just reverses the flow of information. As the idea of convolution is to reduce the number of parameters by weight sharing, we applied this to the parameters of the denoising function g, too.</p><p>Many convolutional networks use pooling operations with stride; that is, they downsample the spatial feature maps. The decoder needs to compensate for this with a corresponding upsampling. There are several alternative ways to implement this and in this paper we chose the following options: 1) on the encoder side, pooling operations are treated as separate layers with their own batch normalization and linear activation function, and 2) the downsampling of the pooling on the encoder side is compensated for by upsampling with copying on the decoder side. This provides multiple targets for the decoder to match, helping the decoder to recover the information lost on the encoder side.</p><p>It is worth noting that a simple special case of the decoder is a model where ? l = 0 when l &lt; L. This corresponds to a denoising cost only on the top layer and means that most of the decoder can be omitted. This model, which we call the ?-model because of the shape of the graph, is useful as it can easily be plugged into any feedforward network without decoder implementation. In addition, the ?-model is the same for MLPs and convolutional neural networks. The encoder in the ?-model still includes both the clean and the corrupted paths as in the full ladder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>With the experiments with the MNIST and CIFAR-10 datasets, we wanted to compare our method to other semi-supervised methods but also show that we can attach the decoder both to a fully connected MLP network and to a convolutional neural network, both of which were described in Section 3. We also wanted to compare the performance of the simpler ?-model (Sec. 3.4) to the full Ladder network and experimented with only having a cost function on the input layer. With CIFAR-10, we only tested the ?-model.</p><p>We also measured the performance of the supervised baseline models which only included the encoder and the supervised cost function. In all cases where we compared these directly with Ladder networks, we did our best to optimize the hyperparameters and regularization of the baseline supervised learning models so that any improvements could not be explained, for example, by the lack of suitable regularization which would then have been provided by the denoising costs.</p><p>With convolutional networks, our focus was exclusively on semi-supervised learning. The supervised baselines for all labels only intend to show that the performance of the selected network architectures is in line with the ones reported in the literature. We make claims neither about the optimality nor the statistical significance of these baseline results.</p><p>We used the Adam optimization algorithm <ref type="bibr" target="#b14">(Kingma and Ba, 2015)</ref> for the weight updates. The learning rate was 0.002 for the first part of the learning, followed by an annealing phase during which the learning rate was linearly reduced to zero. The minibatch size was 100. The source code for all the experiments is available at https://github.com/arasmus/ladder unless explicitly noted in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MNIST dataset</head><p>For evaluating semi-supervised learning, we used the standard 10,000 test samples as a held-out test set and randomly split the standard 60,000 training samples into a 10,000-sample validation set and used M = 50, 000 samples as the training set. From the training set, we randomly chose N = 100, 1000, or all labels for the supervised cost. 4 All the samples were used for the decoder, which does not need the labels. The validation set was used for evaluating the model structure and hyperparameters. We also balanced the classes to ensure that no particular class was over-represented. We repeated each training 10 times, varying the random seed that was used for the splits.  <ref type="bibr" target="#b25">(Rifai et al., 2011)</ref> 12.03 3.64 0.81 Pseudo-label <ref type="bibr" target="#b16">(Lee, 2013)</ref> 10.49 3.46 AtlasRBF <ref type="bibr" target="#b20">(Pitelis et al., 2014)</ref> 8.10 (? 0.95) 3.68 (? 0.12) 1.31 DGN <ref type="bibr" target="#b15">(Kingma et al., 2014)</ref> 3.33 (? 0.14) 2.40 (? 0.02) 0.96 DBM, Dropout <ref type="bibr" target="#b29">(Srivastava et al., 2014)</ref> 0.79 Adversarial <ref type="bibr" target="#b9">(Goodfellow et al., 2015)</ref> 0.78 Virtual Adversarial <ref type="bibr" target="#b18">(Miyato et al., 2015)</ref> 2 After optimizing the hyperparameters, we performed the final test runs using all the M = 60, 000 training samples with 10 different random initializations of the weight matrices and data splits. We trained all the models for 100 epochs followed by 50 epochs of annealing. With minibatch size of 100, this amounts to 75,000 weight updates for the validation runs and 90,000 for the final test runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Fully connected MLP</head><p>A useful test for general learning algorithms is the permutation invariant MNIST classification task. Permutation invariance means that the results need to be invariant with respect to permutation of the elements of the input vector. In other words, one is not allowed to use prior information about the spatial arrangement of the input pixels. This excludes, among others, convolutional networks and geometric distortions of the input images.</p><p>We chose the layer sizes of the baseline model to be 784-1000-500-250-250-250-10. The network is deep enough to demonstrate the scalability of the method but does not yet represent overkill for MNIST.</p><p>The hyperparameters we tuned for each model are the noise level that is added to the inputs and to each layer, and the denoising cost multipliers ? <ref type="bibr">(l)</ref> . We also ran the supervised baseline model with various noise levels. For models with just one cost multiplier, we optimized them with a search grid {. . ., 0.1, 0.2, 0.5, 1, 2, 5, 10, . . .}. Ladder networks with a cost function on all their layers have a much larger search space and we explored it much more sparsely. For instance, the optimal model we found for N = 100 labels had ? (0) = 1000, ? (1) = 10, and ? (?2) = 0.1. A good value for the std of the Gaussian corruption noise n (l) was mostly 0.3 but with N = 1000 labels, 0.2 was a better value. For the complete set of selected denoising cost multipliers and other hyperparameters, please refer to the code.</p><p>The results presented in <ref type="table" target="#tab_0">Table 1</ref> show that the proposed method outperforms all the previously reported results. Encouraged by the good results, we also tested with N = 50 labels and got a test error of 1.62 % (? 0.65 %).</p><p>The simple ?-model also performed surprisingly well, particularly for N = 1000 labels. With N = 100 labels, all the models sometimes failed to converge properly. With bottom level or full costs in Ladder, around 5 % of runs result in a test error of over 2 %. In order to be able to estimate the average test error reliably in the presence of such random outliers, we ran 40 instead of 10 test runs with random initializations.  <ref type="bibr" target="#b36">(Weston et al., 2012)</ref> 7.75 SWWAE <ref type="bibr" target="#b39">(Zhao et al., 2015)</ref> 9.17 0.71 Baseline: Conv-Small, supervised only 6.43 (? 0.84) 0.36 Conv-FC 0.99 (? 0.15) Conv-Small, ?-model 0.89 (? 0.50)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Convolutional networks</head><p>We tested two convolutional networks for the general MNIST classification task but omitted data augmentation such as geometric distortions. We focused on the 100-label case since with more labels the results were already so good even in the more difficult permutation invariant task.</p><p>The first network was a straightforward extension of the fully connected network tested in the permutation invariant case. We turned the first fully connected layer into a convolution with 26-by-26 filters, resulting in a 3-by-3 spatial map of 1000 features. Each of the nine spatial locations was processed independently by a network with the same structure as in the previous section, finally resulting in a 3-by-3 spatial map of 10 features. These were pooled with a global mean-pooling layer.</p><p>Essentially we thus convolved the image with the complete fully connected network. Depooling on the topmost layer and deconvolutions on the layers below were implemented as described in Section 3.4. Since the internal structure of each of the nine almost independent processing paths was the same as in the permutation invariant task, we used the same hyperparameters that were optimal for the permutation invariant task. In <ref type="table" target="#tab_2">Table 2</ref>, this model is referred to as Conv-FC.</p><p>With the second network, which was inspired by ConvPool-CNN-C from <ref type="bibr" target="#b28">Springenberg et al. (2014)</ref>, we only tested the ?-model. The MNIST classification task can typically be solved with a smaller number of parameters than CIFAR-10, for which this topology was originally developed, so we modified the network by removing layers and reducing the number of parameters in the remaining layers. In addition, we observed that adding a small fully connected layer with 10 neurons on top of the global mean pooling layer improved the results in the semi-supervised task. We did not tune other parameters than the noise level, which was chosen from {0.3, 0.45, 0.6} using the validation set. The exact architecture of this network is detailed in <ref type="table">Table 4</ref> in Appendix A. It is referred to as Conv-Small since it is a smaller version of the network used forthe CIFAR-10 dataset.</p><p>The results in <ref type="table" target="#tab_2">Table 2</ref> confirm that even the single convolution on the bottom level improves the results over the fully connected network. More convolutions improve the ?-model significantly, although the high variance of the results suggests that the model still suffers from confirmation bias. The Ladder network with denoising targets on every level converges much more reliably. Taken together, these results suggest that combining the generalization ability of convolutional networks 5 and efficient unsupervised learning of the full Ladder network would have resulted in even better performance but this was left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convolutional networks on CIFAR-10</head><p>The CIFAR-10 dataset consists of small 32-by-32 RGB images from 10 classes. There are 50,000 labeled samples for training and 10,000 for testing. Like the MNIST dataset, it has been used for testing semi-supervised learning so we decided to test the simple ?-model with a convolutional network that has been reported to perform well in the standard supervised setting with all labels.</p><p>We tested a few model architectures and selected ConvPool-CNN-C by <ref type="bibr" target="#b28">Springenberg et al. (2014)</ref>. We also evaluated the strided convolutional version by <ref type="bibr" target="#b28">Springenberg et al. (2014)</ref>, and while it performed well with all labels, we found that the max-pooling version overfitted less with fewer labels, and thus used it. Test error % with # of used labels 4 000 All All-Convolutional ConvPool-CNN-C  9.31 Spike-and-Slab Sparse Coding  31.9 Baseline: Conv-Large, supervised only 23.33 (? 0.61) 9.27 Conv-Large, ?-model 20.40 (? 0.47)</p><p>The main differences to ConvPool-CNN-C are the use of Gaussian noise instead of dropout and the convolutional per-channel batch normalization following <ref type="bibr" target="#b13">Ioffe and Szegedy (2015)</ref>. While dropout was useful with all labels, it did not seem to offer any advantage over additive Gaussian noise with fewer labels. For a more detailed description of the model, please refer to model Conv-Large in <ref type="table">Table 4</ref>.</p><p>While testing the purely supervised model performance with a limited number of labeled samples (N = 4000), we found out that the model overfitted quite severely: the training error for most samples decreased so much that the network effectively learned nothing from them as the network was already very confident about their classification. The network was equally confident about validation samples even when they were misclassified. We noticed that we could regularize the network by stripping away the scaling parameter ? (L) from the last layer. This means that the variance of the input to the softmax is restricted to unity. We also used this setting with the corresponding ?-model although the denoising target already regularizes the network significantly and the improvement was not as pronounced.</p><p>The hyperparameters (noise level, denoising cost multipliers, and number of epochs) for all models were optimized using M = 40, 000 samples for training and the remaining 10, 000 samples for validation. After the best hyperparameters were selected, the final model was trained with these settings on all the M = 50, 000 samples. All experiments were run with with four different random initializations of the weight matrices and data splits. We applied global contrast normalization and whitening following <ref type="bibr" target="#b10">Goodfellow et al. (2013b)</ref>, but no data augmentation was used.</p><p>The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. The supervised reference was obtained with a model closer to the original ConvPool-CNN-C in the sense that dropout rather than additive Gaussian noise was used for regularization. <ref type="bibr">6</ref> We spent some time tuning the regularization of our fully supervised baseline model for N = 4000 labels and indeed, its results exceed the previous state of the art. This tuning was important to make sure that the improvement offered by the denoising target of the ?-model is not a sign of a poorly regularized baseline model. Although the improvement is not as dramatic as with the MNIST experiments, it came with a very simple addition to standard supervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Early works on semi-supervised learning <ref type="bibr" target="#b17">(McLachlan, 1975;</ref><ref type="bibr" target="#b32">Titterington et al., 1985)</ref> proposed an approach where inputs x are first assigned to clusters, and each cluster has its class label. Unlabeled data would affect the shapes and sizes of the clusters, and thus alter the classification result. This approach can be reinterpreted as input vectors being corrupted copiesx of the ideal input vectors x (the cluster centers), and the classification mapping being split into two parts: first denoisingx into x (possibly probabilistically), and then labeling x.</p><p>It is well known (see, e.g., <ref type="bibr" target="#b38">Zhang and Oles, 2000)</ref> that when a probabilistic model that directly estimates P (y | x) is being trained, unlabeled data cannot help. One way to study this is to assign probabilistic labels q(y(n)) = P (y(n) | x(n)) to unlabeled inputs x(n) and try to train P (y | x) using those labels: it can be shown (see, e.g., <ref type="bibr">Raiko et al., 2015, Eq. (31)</ref>) that the gradient will vanish. There are different ways of circumventing this phenomenon by adjusting the assigned labels q(y(n)). These are all related to the ?-model.</p><p>Label propagation methods <ref type="bibr" target="#b31">(Szummer and Jaakkola, 2003)</ref> estimate P (y | x), but adjust probabilistic labels q(y(n)) on the basis of the assumption that the nearest neighbors are likely to have the same label. The labels start to propagate through regions with high-density P (x). The ?-model implicitly assumes that the labels are uniform in the vicinity of a clean input since corrupted inputs need to produce the same label. This produces a similar effect: the labels start to propagate through regions with high density P (x). <ref type="bibr" target="#b36">Weston et al. (2012)</ref> explored deep versions of label propagation.</p><p>Co-training <ref type="bibr" target="#b4">(Blum and Mitchell, 1998)</ref> assumes we have multiple views on x, say x = (x (1) , x (2) ).</p><p>When we train classifiers for the different views, we know that even for the unlabeled data, the true label is the same for each view. Each view produces its own probabilistic labeling q (j) (y(n)) = P (y(n) | x(n) (j) ) and their combination q(y(n)) can be fed to train the individual classifiers. If we interpret having several corrupted copies of an input as different views on it, we see the relationship to the proposed method. <ref type="bibr" target="#b16">Lee (2013)</ref> adjusts the assigned labels q(y(n)) by rounding the probability of the most likely class to one and others to zero. The training starts by trusting only the true labels and then gradually increasing the weight of the so-called pseudo-labels. Similar scheduling could be tested with our ?-model as it seems to suffer from confirmation bias. It may well be that the denoising cost which is optimal at the beginning of the learning is smaller than the optimal one at later stages of learning. <ref type="bibr" target="#b6">Dosovitskiy et al. (2014)</ref> pre-train a convolutional network with unlabeled data by treating each clean image as its own class. During training, the image is corrupted by transforming its location, scaling, rotation, contrast, and color. This helps to find features that are invariant to the transformations that are used. Discarding the last classification layer and replacing it with a new classifier trained on real labeled data leads to surprisingly good experimental results.</p><p>There is an interesting connection between our ?-model and the contractive cost used by <ref type="bibr" target="#b25">Rifai et al. (2011)</ref>: a linear denoising function?</p><formula xml:id="formula_15">(L) i = a iz (L) i + b i ,</formula><p>where a i and b i are parameters, turns the denoising cost into a stochastic estimate of the contractive cost.</p><p>Recently <ref type="bibr" target="#b18">Miyato et al. (2015)</ref> achieved impressive results with a regularization method that is similar to the idea of contractive cost. They required the output of the network to change as little as possible close to the input samples. As this requires no labels, they were able to use unlabeled samples for regularization. While their semi-supervised results were not as good as ours with a denoising target on the input layer, their results with full labels come very close. Their cost function is on the last layer which suggests that the approaches are complementary and could be combined, potentially improving the results further.</p><p>So far we have reviewed semi-supervised methods which have an unsupervised cost function on the output layer only and therefore are related to our ?-model. We will now move to other semisupervised methods that concentrate on modeling the joint distribution of the inputs and the labels.</p><p>The Multi-prediction deep Boltzmann machine (MP-DBM) <ref type="bibr" target="#b8">(Goodfellow et al., 2013a)</ref> is a way to train a DBM with backpropagation through variational inference. The targets of the inference include both supervised targets (classification) and unsupervised targets (reconstruction of missing inputs) that are used in training simultaneously. The connections through the inference network are somewhat analogous to our lateral connections. Specifically, there are inference paths from observed inputs to reconstructed inputs that do not go all the way up to the highest layers. Compared to our approach, MP-DBM requires an iterative inference with some initialization for the hidden activations, whereas in our case, the inference is a simple single-pass feedforward procedure.</p><p>The Deep AutoRegressive Network <ref type="bibr" target="#b11">(Gregor et al., 2014)</ref> is an unsupervised method for learning representations that also uses lateral connections in the hidden representations. The connectivity within the layer is rather different from ours, though: each unit h i receives input from the preceding units h 1 . . . h i?1 , whereas in our case each unit? i receives input only from z i . Their learning algorithm is based on approximating a gradient of a description length measure, whereas we use a gradient of a simple loss function. <ref type="bibr" target="#b15">Kingma et al. (2014)</ref> proposed deep generative models for semi-supervised learning, based on variational autoencoders. Their models can be trained with the variational EM algorithm, stochastic gradient variational Bayes, or stochastic backpropagation. They also experimented on a stacked version (called M1+M2) where the bottom autoencoder M1 reconstructs the input data, and the top autoencoder M2 can concentrate on classification and on reconstructing only the hidden representation of M1. The stacked version performed the best, hinting that it might be important not to carry all the information up to the highest layers. Compared with the Ladder network, an interesting point is that the variational autoencoder computes the posterior estimate of the latent variables with the encoder alone while the Ladder network uses the decoder too to compute an implicit posterior approximate (the encoder provides the likelihood part, which gets combined with the prior). It will be interesting to see whether the approaches can be combined. A Ladder-style decoder might provide the posterior and another decoder could then act as the generative model of variational autoencoders. <ref type="bibr" target="#b37">Zeiler et al. (2011)</ref> train deep convolutional autoencoders in a manner comparable to ours. They define max-pooling operations in the encoder to feed the max function upwards to the next layer, while the argmax function is fed laterally to the decoder. The network is trained one layer at a time using a cost function that includes a pixel-level reconstruction error, and a regularization term to promote sparsity. <ref type="bibr" target="#b39">Zhao et al. (2015)</ref> use a similar structure and call it the stacked what-where autoencoder (SWWAE). Their network is trained simultaneously to minimize a combination of the supervised cost and reconstruction errors on each level, just like ours.</p><p>Recently <ref type="bibr" target="#b1">Bengio (2014)</ref> proposed target propagation as an alternative to backpropagation. The idea is to base learning not on errors and gradients but on expectations. This is very similar to the idea of denoising source separation and therefore resembles the propagation of expectations in the decoder of the Ladder network. In the Ladder network, the additional lateral connections between the encoder and the decoder play an important role and it remains to be seen whether the lateral connections are compatible with target propagation. Nevertheless, it is an interesting possibility that while the Ladder network includes two mechanisms for propagating information, backpropagation of gradients and forward propagation of expectations in the decoder, it may be possible to rely solely on the latter, thus avoiding problems related to the propagation of gradients through many layers, such as exploding gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We showed how a simultaneous unsupervised learning task improves CNN and MLP networks reaching the state of the art in various semi-supervised learning tasks. In particular, the performance obtained with very small numbers of labels is much better than previous published results, which shows that the method is capable of making good use of unsupervised learning. However, the same model also achieves state-of-the-art results and a significant improvement over the baseline model with full labels in permutation invariant MNIST classification, which suggests that the unsupervised task does not disturb supervised learning.</p><p>The proposed model is simple and easy to implement with many existing feedforward architectures, as the training is based on backpropagation from a simple cost function. It is quick to train and the convergence is fast, thanks to batch normalization.</p><p>Not surprisingly, the largest improvements in performance were observed in models which have a large number of parameters relative to the number of available labeled samples. With CIFAR-10, we started with a model which was originally developed for a fully supervised task. This has the benefit of building on existing experience but it may well be that the best results will be obtained with models which have far more parameters than fully supervised approaches could handle.</p><p>An obvious future line of research will therefore be to study what kind of encoders and decoders are best suited to the Ladder network. In this work, we made very small modifications to the encoders, whose structure has been optimized for supervised learning, and we designed the parametrization of the vertical mappings of the decoder to mirror the encoder: the flow of information is just reversed. There is nothing preventing the decoder from having a different structure than the encoder.</p><p>An interesting future line of research will be the extension of the Ladder networks to the temporal domain. While datasets with millions of labeled samples for still images exist, it is prohibitively costly to label thousands of hours of video streams. The Ladder networks can be scaled up easily and therefore offer an attractive approach for semi-supervised learning in such large-scale problems.</p><p>the results obtained with the original parametrization. We tuned the hyperparameters of each comparison model separately using a grid search over some of the relevant hyperparameters. However, the standard deviation of additive Gaussian corruption noise was set to 0.3. This means that the comparison does not include the best-performing models reported in <ref type="table" target="#tab_0">Table 1</ref> that achieved the best validation errors after more careful hyperparameter tuning.</p><p>As in the proposed function g, all comparison denoising functions mapped neuron-wise the corrupted hidden layer pre-activationz (l) to the reconstructed hidden layer activation given one projection from the reconstruction of the layer above:?</p><formula xml:id="formula_16">(l) i = g(z (l) i , u (l) i ).</formula><p>Test error % with # of used labels 100 1000 Proposed g: Gaussian z 1.06 (? 0.07) 1.03 (? 0.06) Comparison g 1 : miniature MLP withzu 1.11 (? 0.07) 1.11 (? 0.06) Comparison g 2 : No augmented termzu 2.03 (? 0.09) 1.70 (? 0.08) Comparison g 3 : Linear g but withzu 1.49 (? 0.10) 1.30 (? 0.08) Comparison g 4 : Only the mean depends on u 2.90 (? 1.19) 2.11 (? 0.45) <ref type="table">Table 5</ref>: Semi-supervised results from the MNIST dataset. The proposed function g is compared to alternative parametrizations. Note that the hyperparameter search was not as exhaustive as in the final results, which means that the results of the proposed model deviate slightly from the final results presented in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The comparison functions g 1...4 are parametrized as follows:</p><p>Comparison g 1 : Miniature MLP withz? z = g(z, u) = a? + bsigmoid(c?)</p><p>where ? = [1,z, u,zu] T is an augmented input, a and c are trainable weight vectors, b is a trainable scalar weight. This parametrization is capable of learning denoising of several different distributions including sub-and super-Gaussian and bimodal distributions.</p><p>Comparison g 2 : No augmented term g 2 (z, u) = a? + bsigmoid(c? )</p><p>where ? = [1,z, u] T . g 2 therefore differs from g 1 in that the input lacks the augmented termzu.</p><p>Comparison g 3 : Linear g g 3 (z, u) = a?. (7) g 3 differs from g in that it is linear and does not have a sigmoid term. As this formulation is linear, it only supports Gaussian distributions. Although the parametrization has the augmented term that lets u modulate the slope and shift of the distribution, the scope of possible denoising functions is still fairly limited.</p><p>Comparison g 4 : u affects only the mean of p(z | u) g 4 (z, u) = a 1 u + a 2 sigmoid(a 3 u + a 4 ) + a 5z + a 6 sigmoid(a 7z + a 8 ) + a 9 (8) g 4 differs from g 1 in that the inputs from u are not allowed to modulate the terms that depend onz, but that the effect is additive. This means that the parametrization only supports optimal denoising functions for a conditional distribution p(z | u) where u only shifts the mean of the distribution of z but otherwise leaves the shape of the distribution intact.</p><p>Results All models were tested in a similar setting as the semi-supervised fully connected MNIST task using N = 1000 labeled samples. We also reran the best comparison model on N = 100 labels. The results of the analyses are presented in <ref type="table">Table 5</ref>.</p><p>As can be seen from the table, the alternative parametrizations of g are inferior to the proposed parametrization, at least in the model structure we use.</p><p>These results support the finding by <ref type="bibr" target="#b23">Rasmus et al. (2015a)</ref> that modulation of the lateral connection fromz to? by u is critical for encouraging the development of invariant representations at the higher layers of the model. Comparison function g 4 lacked this modulation and it clearly performed worse than any other denoising function listed in <ref type="table">Table 5</ref>. Even the linear g 3 performed very well as long it had the termzu. Leaving the nonlinearity but removingzu in g 2 hurt the performance much more.</p><p>In addition to the alternative parametrizations for the g-function, we ran experiments using a more standard autoencoder structure. In that structure, we attached an additional decoder to the standard MLP by using one hidden layer as the input to the decoder and the reconstruction of the clean input as the target. The structure of the decoder was set to be the same as the encoder: that is, the number and size of the layers from the input to the hidden layer where the decoder was attached were the same as the number and size of the layers in the decoder. The final activation function in the decoder was set to be the sigmoid nonlinearity. During training, the target was the weighted sum of the reconstruction cost and the classification cost.</p><p>We tested the autoencoder structure with 100 and 1000 labeled samples. We ran experiments for all possible decoder lengths: that is, we tried attaching the decoder to all the hidden layers. However, we did not manage to get a significantly better performance than the standard supervised model without any decoder in any of the experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A conceptual illustration of the Ladder network when L = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A collection of previously reported MNIST test errors in the permutation invariant setting followed by the results with the Ladder network. * = SVM. Standard deviation in parentheses.</figDesc><table><row><cell>Test error % with # of used labels</cell><cell>100</cell><cell>1000</cell><cell>All</cell></row><row><cell>Semi-sup. Embedding (Weston et al., 2012)</cell><cell>16.86</cell><cell>5.73</cell><cell>1.5</cell></row><row><cell cols="2">Transductive SVM (from Weston et al., 2012) 16.81</cell><cell>5.38</cell><cell>1.40*</cell></row><row><cell>MTC</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>CNN results for MNIST Test error without data augmentation % with # of used labels 100 all EmbedCNN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test results for CNN on CIFAR-10 dataset without data augmentation</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Preliminary results on the full-labeled setting on a permutation invariant MNIST task were reported in a short early version of this paper<ref type="bibr" target="#b24">(Rasmus et al., 2015b)</ref>. Compared to that, we have added noise to all layers of the model and further simplified the denoising function g. This further improved the results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here we only consider the case where the output t(n) is a class label but it is trivial to apply the same approach to other regression tasks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The whole point of using denoising autoencoders rather than regular autoencoders is to prevent skip connections from short-circuiting the decoder and force the decoder to learn meaningful abstractions which help in denoising.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In all the experiments, we were careful not to optimize any parameters, hyperparameters, or model choices on the basis of the results on the held-out test samples. As is customary, we used 10,000 labeled validation samples even for those settings where we only used 100 labeled samples for training. Obviously, this is not something that could be done in a real case with just 100 labeled samples. However, MNIST classification is such an easy task, even in the permutation invariant case, that 100 labeled samples there correspond to a far greater number of labeled samples in many other datasets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In general, convolutional networks excel in the MNIST classification task. The performance of the fully supervised Conv-Small with all labels is in line with the literature and is provided as a rough reference only (only one run, no attempts to optimize, not available in the code package).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Same caveats hold for this fully supervised reference result for all labels as with MNIST: only one run, no attempts to optimize, not available in the code package.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We have received comments and help from a number of colleagues who all deserve to be mentioned but we wish to thank especially Yann LeCun, Diederik Kingma, Aaron Courville, Ian Goodfellow, S?ren S?nderby, Jim Fan, and Hugo Larochelle for their helpful comments and suggestions. The software for the simulations for this paper was based on Theano <ref type="bibr" target="#b0">(Bastien et al., 2012;</ref><ref type="bibr" target="#b3">Bergstra et al., 2010)</ref> and Blocks (van Merri?nboer et al., 2015). We also acknowledge the computational resources provided by the Aalto Science-IT project. The Academy of Finland has supported Tapani Raiko.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Specification of the convolutional models Here we describe two model structures, Conv-Small and Conv-Large, that were used for the MNIST and CIFAR-10 datasets, respectively. They were both inspired by ConvPool-CNN-C by <ref type="bibr" target="#b28">Springenberg et al. (2014)</ref>. <ref type="table">Table 4</ref> details the model architectures and differences between the models in this work and ConvPool-CNN-C. It is noteworthy that this architecture does not use any fully connected layers, but replaces them with a global mean pooling layer just before the softmax function. The main differences between our models and ConvPool-CNN-C are the use of Gaussian noise instead of dropout and the convolutional per-channel batch normalization following <ref type="bibr" target="#b13">Ioffe and Szegedy (2015)</ref>. We also used 2x2 stride 2 max-pooling instead of 3x3 stride 2 max-pooling. LeakyReLU was used to speed up training, as mentioned by <ref type="bibr" target="#b28">Springenberg et al. (2014)</ref>. We utilized batch normalization to all layers, including the pooling layers. Gaussian noise was also added to all layers, instead of applying dropout in only some of the layers as with ConvPool-CNN-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Formulation of the Denoising Function</head><p>The denoising function g tries to map the clean z (l) to the reconstructed? (l) , where? (l) = g(z (l) ,? (l+1) ). The reconstruction is therefore based on the corrupted value and the reconstruction of the layer above.</p><p>An optimal functional form of g depends on the conditional distribution p(z (l) | z (l+1) ) that we want the model to be able to denoise. For example, if the distribution p(z (l) | z (l+1) ) is Gaussian, the optimal function g, that is, the function that achieves the lowest reconstruction error, is going to be linear with respect toz (l) <ref type="bibr">(Valpola, 2015, Section 4.1)</ref>. This is the parametrization that we chose on the basis of preliminary comparisons of different denoising function parametrizations.</p><p>The proposed parametrization of the denoising function was therefore:</p><p>We modeled both ?(u) and ?(u) with an expressive nonlinearity 7 : ?(u) = a 1 sigmoid(a 2 u+a 3 )+ a 4 u + a 5 and ?(u) = a 6 sigmoid(a 7 u + a 8 ) + a 9 u + a 10 . We have left out the superscript (l) and subscript i in order not to clutter the equations. Given u, this parametrization is linear with respect toz, and both the slope and the bias depended nonlinearly on u.</p><p>In order to test whether the elements of the proposed function g were necessary, we systematically removed components from g or replaced g altogether and compared the resulting performance to <ref type="bibr">7</ref> The parametrization can also be interpreted as a miniature MLP network</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How auto-encoders could provide credit assignment in deep networks via target propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.7906</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalized denoising auto-encoders as generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS 2013)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference</title>
		<meeting>the Python for Scientific Computing Conference</meeting>
		<imprint>
			<publisher>Oral Presentation</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Eleventh Annual Conference on Computational Learning Theory (COLT &apos;98)</title>
		<meeting>of the Eleventh Annual Conference on Computational Learning Theory (COLT &apos;98)</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS 2014)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale feature learning with spikeand-slab sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML 2012</title>
		<meeting>of ICML 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1439" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-prediction deep Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS 2013)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
	</analytic>
	<monogr>
		<title level="m">the International Conference on Learning Representations (ICLR 2015)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep autoregressive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">the International Conference on Learning Representations (ICLR 2015)</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS 2014)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="365" to="369" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00677</idno>
		<title level="m">Distributional smoothing by virtual adversarial examples</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06430</idno>
		<title level="m">Deconstructing the ladder network architecture</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using an unsupervised atlas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pitelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agapito</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases (ECML PKDD 2014)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Techniques for learning binary stochastic feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>San Diego</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of compact document representations with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="792" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Denoising autoencoder with modulated lateral connections learns invariant representations of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7210</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08215</idno>
		<title level="m">Lateral connections in denoising autoencoders support supervised learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The manifold tangent classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24 (NIPS 2011)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2294" to="2302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Denoising source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?rel?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="233" to="272" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Creating artificial neural networks that generalize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sietsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Dow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>arxiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rule-injection hints as a means of improving network performance and learning time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Suddarth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kergosien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EURASIP Workshop 1990 on Neural Networks</title>
		<meeting>the EURASIP Workshop 1990 on Neural Networks</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Partially labeled classification with Markov random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="945" to="952" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Statistical analysis of finite mixture distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Titterington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Makov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley Series in Probability and Mathematical Statistics</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From neural PCA to deep unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7783</idno>
	</analytic>
	<monogr>
		<title level="m">Adv. in Independent Component Analysis and Learning Machines</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="143" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Blocks and fuel: Frameworks for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1506.00619</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The value of unlabeled data for classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Oles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1191" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02351</idno>
		<title level="m">Stacked what-where auto-encoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
