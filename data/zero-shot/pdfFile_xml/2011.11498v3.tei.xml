<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Sun</surname></persName>
							<email>chengsun@gapp.nthu.edu.tw</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
							<email>sunmin@ee.nthu.edu.tw</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
							<email>htchen@cs.nthu.edu.tw</email>
						</author>
						<title level="a" type="main">HoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present HoHoNet, a versatile and efficient framework for holistic understanding of an indoor 360-degree panorama using a Latent Horizontal Feature (LHFeat). The compact LHFeat flattens the features along the vertical direction and has shown success in modeling per-column modality for room layout reconstruction. HoHoNet advances in two important aspects. First, the deep architecture is redesigned to run faster with improved accuracy. Second, we propose a novel horizon-to-dense module, which relaxes the per-column output shape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is fast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones respectively, for modeling dense modalities from a high-resolution 512 ? 1024 panorama. HoHoNet is also accurate. On the tasks of layout estimation and semantic segmentation, HoHoNet achieves results on par with current state-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior arts by a large margin. Code is available at https:// github.com/ sunset1995/ HoHoNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Panoramic images can capture the complete 360?FOVs in one shot to provide a wide range of context that facilitates scene understanding <ref type="bibr" target="#b30">[29]</ref>. As omnidirectional cameras become more easily accessible and several large-scale panorama datasets have been released, a growing number of techniques are developed for tasks of panoramic scene modeling such as semantic segmentation <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b29">28</ref>], depth estimation <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27]</ref>, layout reconstruction <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b34">33]</ref>, and indoor real-time navigation <ref type="bibr" target="#b2">[3]</ref>.</p><p>This paper aims to address the problem of holistic scene modeling from a single high-resolution equirectangular projection (ERP) image that captures the 360?panorama. We present HoHoNet as an efficient, effective, and versatile framework to achieve this goal <ref type="figure">(Fig. 1)</ref>. The input ERP <ref type="bibr" target="#b0">1</ref> National Tsing Hua University 2 ASUS AICS Department 3 Joint Research Center for AI Technology and All Vista Healthcare 4 Aeolus Robotics <ref type="figure">Figure 1</ref>: One framework for all: HoHoNet is a novel deep learning framework for modeling layout structure, dense depth, and semantic segmentation through a Latent Horizontal Feature representation (LHFeat) whose height dimension is flattened. The proposed horizon-to-dense (h2d) module can produce dense predictions from the compact LHFeat. image is first passed through a CNN backbone for feature pyramid extraction, and then a proposed efficient height compression module encodes the feature pyramid into a Latent Horizontal Feature representation (LHFeat) whose height dimension is flattened. Finally, from LHFeat, the HoHoNet framework can yield both per-column and per-pixel modalities with state-of-the-art quality.</p><p>Our way of encoding ERP images into LHFeat is inspired by Sun et al. <ref type="bibr" target="#b21">[21]</ref>. However, their model is only applicable to tasks of predicting per-column modalities (e.g., corners or boundaries of layout), which constrains its feasibility in other scenarios requiring per-pixel predictions. We show that LHFeat can flexibly encode latent features for recovering the target 2D per-pixel modalities, based on our observation of the strong regularity between human-made structures and gravity aligned y-axis of ERP images <ref type="figure">(Fig. 2)</ref>.</p><p>In HoHoNet we introduce a new horizon-to-dense (h2d) module for recovering 2D per-pixel modalities while maintaining the efficiency of overall framework <ref type="figure">(Fig. 1)</ref>. A naive method is to treat the channel dimension of horizontal prediction as height and apply a linear interpolation if required. However, this requires the shallow Conv1D layers to disentangle the row-dependent information from the row-independent LHFeat. The spatial (the row) blended essence of LHFeat motivates us to model dense information in the frequency domain, and we resort to the discrete cosine transform (DCT) for its long-standing applications in data compression. By replacing linear interpolation with IDCT, we are able to improve the dense prediction results. With our horizon-to-dense module, the efficiently encoded LHFeat can now model dense modalities.</p><p>We summarize the key merits and contributions of Ho-HoNet for holistic scene modeling from a 360?image.</p><p>? Fast. HoHoNet can yield dense modalities for a highresolution 512 ? 1024 panorama at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 respectively.</p><p>? Versatile. Our method relaxes the final prediction space upon the compact LHFeat from O(W ) to the most common O(HW ), capable of modeling layout, dense depth, and semantic segmentation.</p><p>? Accurate. The performances of HoHoNet on semantic segmentation and layout reconstruction are on par with the recent state-of-the-art. On dense depth estimation, HoHoNet outperforms prior arts by a margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Indoor 360 datasets. Scene modeling on 360?images is a topic with a growing number of researches recently. Several 360 datasets are released to facilitate the learning-based methods. Stanford2D3D <ref type="bibr" target="#b0">[1]</ref> and Matterport3D <ref type="bibr" target="#b1">[2]</ref> datasets are currently the two largest real-world indoor 360 datasets with various modalities being provided. To model the higher level indoor structure, human-annotated layout datasets <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b34">33]</ref> are constructed with more data and topology. Structured3D <ref type="bibr" target="#b31">[30]</ref> is a recently published photorealistic 360 dataset with abundant data and structure annotations from virtual environments. In this work, we focus on real-world datasets to model depth, semantic, and layout modalities.</p><p>Input 360 format. Three standard 360 input formats are commonly used in the literature-i) equirectangular projection (ERP), ii) multiple perspective projections, and iii) icosahedron mesh. ERP preserves all captured information in one image, but it also introduces distortion that might degrade the performance of the conventional convolution layer designed for perspective imagery. A number of variants of convolution layers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22]</ref> have been proposed to address the issue of ERP distortion. Projecting the 360?s ignal to multiple planar images makes it applicable to use classical CNNs with plenty of pre-trained models available, but the FOV of each view is limited. Several padding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">24]</ref> and view sampling <ref type="bibr" target="#b9">[9]</ref> strategies are proposed to deliver context information between views. Recently, a few approaches </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>We show that the structure information of an image column can be better kept in compression when the y-axis of the image is gravity aligned. We sample 1000 depth maps from Structured3D <ref type="bibr" target="#b31">[30]</ref> dataset for the statistic. A 512?1024 depth map is compressed to 16 ? 1024 via discrete cosine transform with high frequency truncated, which is applied to each column separately. We measure the absolute error between the original depth and the inverse transformed one.</p><p>propose to represent the omnidirectional input via icosahedron mesh for scene modeling <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b29">28]</ref>. In this work, our model takes ERP as the 360?input format and apply classical convolution layers directly. Although we speculate that incorporating distortion-aware techniques into our model with extra computational overheads could potentially improve performance, for the sake of simplicity and efficiency, we do not digress to pursue in that direction as the proposed method already achieves state-of-the-art performance.</p><p>Depth estimation on 360 imagery. To model depth on omnidirectional imagery, OmniDepth <ref type="bibr" target="#b32">[31]</ref> designs encoder-decoder architectures considering the ERP distortion. PanoPopups <ref type="bibr" target="#b8">[8]</ref> shows that learning 360 depth with plane-aware loss is beneficial in the synthetic environment. Recent works on panorama dense depth estimation propose to jointly learn from different projections <ref type="bibr" target="#b24">[24]</ref> or different modalities <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b27">27]</ref>. In contrast to most recent methods <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27]</ref> that employ multiple backbones with cascaded training stages, HoHoNet consists of only one backbone and is trained in only one stage. Besides, HoHoNet models dense depth through the compact LHFeat while the prior arts estimate depth from conventional dense features.</p><p>Semantic segmentation on 360 imagery. Semantic segmentation is a fundamental task for scene modeling. Dist-Conv <ref type="bibr" target="#b22">[22]</ref> proposes a distortion-aware deformable convolu- <ref type="figure">Figure 3</ref>: An overview of the HoHoNet framework for dense depth estimation. (a) A high-resolution panorama is first processed by the backbone (e.g., ResNet). (b) The feature pyramid is then squeezed and fused by the proposed Efficient Height Compression (EHC) module, with a Multi-Head Self-Attention (MHSA) for refinement (detailed in Sec. 3.2). Note that the resulting LHFeat is compact (e.g., it is R 256?1024 if the input image is R 3?512?1024 ), enabling the overall network to run much faster than conventional encoder-decoder networks for dense features. (c) Finally, 1D convolution layers are employed to yield the final prediction. We find that predicting in DCT frequency domain brings about superior results, so we apply IDCT to the prediction of each column (detailed in Sec. 3.4). Sec. 3 and supplementary material contain more architectural details.</p><p>tion layer for dense depth and semantic prediction on ERP images. Most of the recent methods for 360 semantic segmentation design a trainable layer operating on representation related to icosahedral mesh <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b29">28]</ref>. However, all methods above run on a relatively low resolution for the panoramic signal. Tangent images <ref type="bibr" target="#b9">[9]</ref> project omnidirectional signals to multiple planar images tangent to a subdivided icosahedron, which allows to process high-resolution panoramas and to deploy the pre-trained weights on perspective images. Similar to <ref type="bibr" target="#b9">[9]</ref>, HoHoNet can also operate on a high-resolution image, which is shown to be an essential factor in achieving better semantic segmentation accuracy. In contrast to the recent methods, HoHoNet runs on ERP images directly, and the highly optimized deep-learning library can easily implement all our operations.</p><p>Latent horizontal features (LHFeat). HoHoNet is closely related to HorizonNet <ref type="bibr" target="#b21">[21]</ref> on the motivation of using 1D features. However, HorizonNet only tackles a specific layout reconstruction task and can only predict horizontal modalities. We design a new architecture for encoding the LHFeat with much better speed and accuracy, and, importantly, we relax the constraint on output space via the proposed horizon-to-dense module, which enables densemodality holistic scene modeling. We show that the compact LHFeat can be effectively applied to more tasks including dense depth estimation and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework overview</head><p>An overview of the proposed framework is depicted in <ref type="figure">Fig. 3</ref>. We describe the details below.</p><p>Input 360 image. We use the standard equirectangular projection (ERP) for 360?images. The resolution of input ERP images, H inp. ? W inp. , is a hyperparameter, and we set it according to the standard practice of each benchmark. We show in <ref type="figure">Fig. 2</ref> that the structure signals of an image column are preserved better after compression if the gravity direction is aligned with the image's y-axis, which is also a desirable property for our framework to encode a column into a latent vector. In this work, the 360 data provided by the benchmarks are mostly well-aligned, so we do not apply any pre-processing. Future applications could consider using the IMU sensor or 360 VP detection algorithm <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b33">32]</ref> to pre-process and align the input for better robustness.</p><p>Backbone. We adopt ResNet <ref type="bibr" target="#b10">[10]</ref>, and the intermediate features from the four ResNet stages form the fea-</p><formula xml:id="formula_0">ture pyramid-{R C ?H ?W } =1,2,3,4 where H = Hinp. 2 +1 , W = Winp.</formula><p>2 +1 and C is the latent dimension of ResNet. Extracting latent horizontal features (LHFeat). We propose an efficient height compression (EHC) module to extract the LHFeat R D?W1 from the backbone's feature pyramid. We detail the EHC module in Sec. 3.2.</p><p>Predicting modalities. We use N in this work to denote the number of target channels for a task (e.g., N is set to 1 for depth estimation and is set to the number of classes for semantic segmentation). Given the LHFeat R D?W1 , we show how HoHoNet predicts 1D output R N ?Winp. in Sec. 3.3. In Sec. 3.4, we propose the first method to yield 2D dense prediction R N ?Hinp.?Winp. from the compact LHFeat, which widely extends the potential applications of the proposed efficient framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">EHC module for LHFeat</head><p>The proposed efficient height compression (EHC) module is illustrated in <ref type="figure" target="#fig_1">Fig. 4</ref>. We first employ EHC blocks to squeeze the height of each 2D feature from the backbone's pyramid. The resulting 1D features are then simply fused by summation. Within the EHC block, the input 2D features are first processed by a Conv2D block for channel reduction, and then the spatial width is upsampled to W 1 if needed, and finally, another Conv2D block refines the upsampled features. To efficiently reduce the feature height to 1, we design the ConvSqueezeH layer, a depthwise convolution layer with kernel size set to (h, 1) to cover full feature height without padding. Note that the parameter h of each EHC block is automatically pre-computed given H inp. . Finally, a Conv2D layer converts the number of channels to LHFeat's latent size D, and the height dimension is simply discarded as it is already reduced to 1 by the ConvSqueezeH layer.</p><p>To further refine the initial LHFeat, the similar prior work <ref type="bibr" target="#b21">[21]</ref> adopts bidirectional LSTM <ref type="bibr" target="#b11">[11]</ref> for horizontal prediction. We find the recurrent layer accounts for 22% of our deep net processing time, so we employ multi-head self-attention <ref type="bibr" target="#b23">[23]</ref> (MHSA) instead. Our results show that MHSA runs faster and improves accuracy more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Predicting 1D per-column modalities</head><p>The target modality of some applications can be formulated into per-column prediction instead of the conventional per-pixel format. An example in this regard has been shown by Sun et al. <ref type="bibr" target="#b21">[21]</ref> for layout estimation. To predict the 1D modalities, we first upsample the horizontal features from R D?W1 to R D?Winp. and apply three Conv1D layers of kernel size 3, 3, and 1 respectively with BN, ReLU in between. The last layer yields the final prediction in R N ?Winp. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Predicting 2D per-pixel modalities</head><p>The strategy of shaping output space into per-column format does not apply to tasks that involve per-pixel modalities.</p><p>Here we present the horizon-to-dense module of HoHoNet to derive dense prediction R N ?Hinp.?Winp. from the compact LHFeat R D?W1 . This functionality opens the door to a more common scenario for various applications.</p><p>The trainable layers for 2D modality prediction are almost the same as the layers for 1D prediction introduced in Sec. 3.3 except that the number of channels in the output layer is augmented to E = N ? r where N is the number of target channels for a task and r is the number of components shared by a image column. The produced prediction is then reshaped from R E?Winp. to R N ?r?Winp. . We present two different operations to recover R r back to R Hinp. for each column depending on the physical meaning we assign to the r predicted values.</p><p>Interpolation. The simplest way is to view the latent dimension r as the output height and apply linear interpolation</p><formula xml:id="formula_1">to resize r to H inp. if r &lt; H inp. .</formula><p>Inverse discrete cosine transform (IDCT). Inspired by the application of the DCT in image compression for its energy compaction property, we view the r predicted values as if they are in the DCT frequency domain with higher frequencies being truncated. In this case, we can apply IDCT to recover the low-pass signal back to the original signal. Let x = [x n ] r?1 n=0 ? R r be the prediction; the final output X = [X m ] H?1 m=0 ? R H can be recovered by</p><formula xml:id="formula_2">X m = x 0 2 + r?1 n=1</formula><p>x n cos ? H n m + 1 2 .</p><p>(1)</p><p>A unified view. We can put the two aforementioned operations into a unified view of matrix multiplication as X = M x where x ? R r , X ? R H , and M ? R H?r consisting of r orthogonal column vectors. Depending on the choice of basis, this unified view can implement linear interpolation or IDCT, as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. We find that IDCT constantly outperforms linear interpolation. We elaborate the intuition as follows. The LHFeat blends the spatial-row information (as described in Sec. 3.2), so training the last layers to disentangle the row-dependent dense modality from the flattened row-less LHFeat would pose a challenge. Conversely, learning to predict in the frequency domain can benefit from the well defined basis functions with meaningful spatial frequencies that characterize each column's original row information as a whole, and therefore may alleviate the row-dependency problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In Sec. 4.1, we first conduct ablation studies for the proposed components in HoHoNet. We then compare the performance of HoHoNet with state-of-the-art methods on dense depth estimation (Sec. 4.2), semantic segmentation (Sec. 4.3), and layout estimation (Sec. <ref type="bibr">4.4)</ref>. Note that we train HoHoNet for each task separately and focus on showcasing the effectiveness of HoHoNet in learning a modality. In Sec. 4.5, we analyze the effect of non-gravity-aligned view. More quantitative and qualitative results are included in the supplementary material. <ref type="table" target="#tab_1">Table 1</ref> summarizes the results of ablation experiments, where we compare different settings of HoHoNet for dense depth estimation. Detailed descriptions are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation study</head><p>Ablation split for Matterport3D <ref type="bibr" target="#b1">[2]</ref>. Matterport3D is a large-scale real-world dataset of indoor panoramas. We prepare the ablation split by splitting the official 61 training houses into 41 and 20 houses (containing 4,921 and 2,908 panoramas) for training and validation during ablation study. We do not use the official validation split for ablation study as it will be used for state-of-the-art comparison later. The input ERPs are resized to 512 ? 1024.</p><p>Training and evaluation. We use Adam <ref type="bibr" target="#b14">[14]</ref> to optimize the L1 loss for 40 epochs with batch-size of 4. The learning rate is set to 1e-4, and we apply polynomial learning rate decay with factor 0.9. Standard depth evaluation metric-MAE, RMSE, and ? 1 -are used. We measure the average frame per second (FPS) for processing 50 individual 512 ? 1024 panoramas on a GeForce RTX 2080 Ti.</p><p>Architecture of LHFeat extraction. <ref type="table" target="#tab_1">Table 1a</ref> compares the proposed efficient height compression (EHC) module with the architecture used in the related work <ref type="bibr" target="#b21">[21]</ref>. In <ref type="bibr" target="#b21">[21]</ref>, a sequence of convolution layers gradually reduces the feature heights to form the initial LHFeat, which is then followed by a bidirectional LSTM (Bi-LSTM) for feature refinement. (Detailed architectures are in the supplementary material.) <ref type="table" target="#tab_1">Table 1a</ref> shows that employing the proposed EHC module for initial LHFeat extraction achieves better speed and accuracy under different refinement configurations. We also find that using multi-head self-attention for feature refinement provides a better speed-accuracy tradeoff. Finally, our overall architecture for extracting the LHFeat is considerably better than <ref type="bibr" target="#b21">[21]</ref>'s-the depth MAE is improved from 0.3002 to 0.2835 with FPS from 38 to 52. All experiments in <ref type="table" target="#tab_1">Table 1a</ref> deploy ResNet-50 as backbone and use the IDCT with r = 64 for dense prediction.</p><p>Hyperparameters of horizon-to-dense. We compare the two operations-linear interpolation (spatial domain) and IDCT (frequency domain)-applied to dense prediction un-  der different basis setups. As shown in <ref type="table" target="#tab_1">Table 1b</ref>, learning to predict in frequency domain (with IDCT) is consistently better than predicting in spatial domain (with linear interpolation) for dense depth estimation upon the compact LHFeat. Interestingly, the number of components r is not monotonic to the resulting accuracy, and we find r = 64 is the best setting for our model. As the compared operations introduce negligible computational cost, the FPSs are almost identical even if we increase r. All experiments in <ref type="table" target="#tab_1">Table 1b</ref> share the same deep net setting that consists of ResNet-50, the proposed EHC, and the MHSA.</p><p>Comparison of the backbones. We compare the results of different backbones in  <ref type="table">Table 2</ref>: State-of-the-art comparison for depth estimation on real-world indoor 360 datasets-Matterport3D <ref type="bibr" target="#b1">[2]</ref> and Stan-ford2D3D <ref type="bibr" target="#b0">[1]</ref>. The evaluation protocol follows <ref type="bibr" target="#b24">[24]</ref>, where the depth is clipped to 10 meter without depth median alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Depth estimation 4.2.1 State-of-the-art comparison using the protocol of</head><p>Wang et al. <ref type="bibr" target="#b24">[24]</ref> Datasets and evaluation protocol. We compare Ho-HoNet with state-of-the-art 360 depth estimation methods on real-world datasets following the testing protocol of <ref type="bibr" target="#b24">[24]</ref>. Matterport3D <ref type="bibr" target="#b1">[2]</ref> has 10,800 panoramas, and its training split contains 61 houses, and the testing results are reported on the merged official validation and test split. Stanford2D3D <ref type="bibr" target="#b0">[1]</ref> contains 1,413 panoramas, and the fold-1 is used where the fifth area is for testing, and the other areas are for training. All the ERP images and depth maps are resized to 512?1024. Standard depth estimation evaluation metrics-MRE, MAE, RMSE, RMSE (log), and ?-are used. Depths are clipped to 10 meters without median alignment.</p><p>Implementation details. We employ ResNet-50 as the backbone with the proposed EHC module for LHFeat extraction; the latent size D of LHFeat is set to 256; IDCT with r = 64 components is applied to the model predictions. We use Adam <ref type="bibr" target="#b14">[14]</ref> to optimize the L1 loss for 60 epochs with a batch-size of 4. The learning rate is set to 1e-4, and we apply the polynomial learning rate decay with factor 0.9.</p><p>Results. <ref type="table">Table 2</ref> shows the comparisons with prior arts. We demonstrate that the proposed HoHoNet outperforms the previous state-of-the-art, BiFuse <ref type="bibr" target="#b24">[24]</ref>, by a large margin. Note also that BiFuse takes both ERP and cubemap as their model inputs and thus requires two backbone networks. Ho-HoNet has only one backbone and the compact LHFeat can achieve superior results, which shows the effectiveness of the proposed framework.</p><p>A qualitative comparison with BiFuse <ref type="bibr" target="#b24">[24]</ref> is provided in <ref type="figure">Fig. 6</ref>, where we download their code 1 and the pre-trained 1 https://github.com/Yeh-yu-hsuan/BiFuse weights for the comparison. We find that HoHoNet is good at capturing the overall structure of the scene. However, some drawbacks of HoHoNet are also observable through the visualization in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">State-of-the-art comparison using the protocol of</head><p>Jin et al. <ref type="bibr" target="#b13">[13]</ref> Dataset and evaluation protocol. We also compare Ho-HoNet with another set of methods following the testing protocol of <ref type="bibr" target="#b13">[13]</ref>. A subset of the real-  <ref type="bibr" target="#b32">[31]</ref> 0.590 0.187 0.084 0.711 RectNet <ref type="bibr" target="#b32">[31]</ref> 0.577 0.181 0.081 0.717 Sph. FCRN <ref type="bibr" target="#b22">[22]</ref> 0.523 0.145 0.067 0.783 U-Net <ref type="bibr" target="#b18">[18]</ref> 0.472 0.140 0.062 0.803 GeoReg360 <ref type="bibr" target="#b13">[13]</ref> ? 0.421 0.118 0.053 0.851 Ours * 0.408 0.111 0.050 0.867 Ours 0.394 0.104 0.048 0.896 * Using <ref type="bibr" target="#b13">[13]</ref> training protocol for a fair comparison. ?Using layout and semantic annotation.</p><p>(a) Quantitative comparison for dense depth on Stanford2D3D <ref type="bibr" target="#b0">[1]</ref> layout-available subset <ref type="bibr" target="#b33">[32]</ref>. We strictly follow <ref type="bibr" target="#b13">[13]</ref>  RGB-D Gauge Net <ref type="bibr" target="#b4">[5]</ref> 39.4 55.9 RGB-D UGSCNN <ref type="bibr" target="#b12">[12]</ref> 38.3 54.7 RGB-D HexRUNet <ref type="bibr" target="#b29">[28]</ref> 43.3 58.6 RGB-D TangentImg <ref type="bibr" target="#b9">[9]</ref> 37.5 50.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semantic segmentation</head><p>Dataset and evaluation protocol. We evaluate Ho-HoNet's semantic segmentation performance on Stan-ford2D3D <ref type="bibr" target="#b0">[1]</ref> dataset. As previous work, we report the averaged results from the official 3-fold cross-validation splits, using standard semantic segmentation evaluation metricsclass-wise mIoU and class-wise mAcc.</p><p>Implementation detail. The architecture setting of Ho-HoNet for semantic segmentation is almost the same as for depth estimation in Sec. 4.2.1 except the last layer has E = N r = 13 ? 64 = 832 channels. To compare with methods using a simple backbone under low resolution, we follow <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b29">28]</ref> to construct a shallow U-Net but purely with planar CNN. For results on high resolution, we use ResNet-101 as backbone. We use Adam <ref type="bibr" target="#b14">[14]</ref> to optimize the cross-entropy loss for 60 epochs with a batch-size of 4. The learning rate is 1e-4 with polynomial decay of factor 0.9.</p><p>Results. <ref type="table" target="#tab_6">Table 3b</ref> shows the comparison with previous methods. On the lowest resolution, HexRUNet <ref type="bibr" target="#b29">[28]</ref>, with a specially designed kernel on icosahedron representation, achieves the best result. Ours with purely planar CNNs and compact LHFeat is still competitive with the distortion mitigated methods under the low-resolution settings. When scaling to a high resolution, we achieve similar mACC with the recent state-of-the-art <ref type="bibr" target="#b9">[9]</ref>, while our mIoU is significantly better. Note that the results of <ref type="bibr" target="#b9">[9]</ref> are obtained from a stronger FCN-ResNet101 backbone and a higher input resolution. Limited by our device and ERP projection, we can only train on a lower 1024 ? 2048 resolution but still obtain competitive performance with the current state-of-the-art on 360?semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Room layout estimation</head><p>Dataset and evaluation protocol. We use MatterportLayout <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b25">25]</ref> dataset, which is a real-world 360 Manhattan layout dataset. The official evaluation function 2 for 2D IoU and 3D IoU is used directly, where the 2D IoU is measured by projecting floor corners to an aligned floor, while 3D IoU is for pop-up view considering both floor and ceiling corners.</p><p>Implementation details. HoHoNet is compatible with the 1D layout representation proposed by HorizonNet <ref type="bibr" target="#b21">[21]</ref>. Since our main focus is not to design a new method for layout reconstruction, we use <ref type="bibr" target="#b21">[21]</ref>'s loss, training protocol, and post-processing algorithm directly. We find HoHoNet with ResNet-34 shows slightly better accuracy than ResNet-50 in validation, so we use the simpler ResNet-34 as backbone.</p><p>Results. The comparison with previous methods on Mat-terportLayout is shown in <ref type="table" target="#tab_6">Table 3c</ref>. The FPSs are obtained using the official codes 2345 and measured by the averaged feed-forward times of the models on a GeForce RTX 2080 Ti. The result of AtlantaNet <ref type="bibr" target="#b17">[17]</ref> is obtained from their official new pre-trained weights 5 with aligned data split and re-evaluated by the official evaluation function 2 . Our result is on par with the state-of-the-art AtlantaNet but 22? faster. HoHoNet also outperforms HorizonNet <ref type="bibr" target="#b21">[21]</ref> by +0.77 3D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>BiFuse Ours Ground Truth Advantage <ref type="figure">Figure 6</ref>: Qualitative comparison of the estimated dense depth with the prior art-BiFuse <ref type="bibr" target="#b24">[24]</ref>. The 'Advantage' column shows the MAE difference between ours and BiFuse's where the blue color indicates ours is better and the red color for vice versa. We find HoHoNet achieves good results in capturing the overall structure, but we also find some drawback in the visualization. First, HoHoNet's depth boundary is blurrier comparing to those of BiFuse. Second, some high-frequency signal in a column is discarded by HoHoNet. See the last row for an example. We find that i) the boundary of the chairs in the left of the image is blurrier, and ii) the lamp at the middle of the image is poorly reconstructed by HoHoNet while it seems to be easier to reconstruct from the conventional dense features. The intuitive reason for the qualitatively identified drawback is that the LHFeat focuses on learning the most prominent signals of a column, which makes it easier to optimize the training criterion.</p><p>IoU and +0.61 2D IoU, and is 3.5? faster, which shows the effectiveness of the designed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on non-gravity-aligned views</head><p>In <ref type="figure">Fig. 2</ref>, we show that the structure signals of an image column suffer more losses in compression if the image's y-axis is not aligned with the gravity. Though the 360 data in all benchmarks we use are mostly well-aligned with gravity, the captured 360?views could be non-gravity-aligned in practice. In <ref type="table" target="#tab_7">Table 4</ref>, we show the vulnerability of our model to heavy pitch or roll rotation (see <ref type="figure">Fig. 2c</ref> and <ref type="figure">Fig. 2b</ref> for visualization). The pre-trained model in our ablation study takes the rotated images directly as input, and the output depth maps are rotated back to the original view for a fair comparison. As expected, the pre-trained model performs poorly when input 360?views are not gravity-aligned. Introducing 10?of pitch or roll rotation increases MAE from 28.45cm to more than 44cm. A simple solution is to use the IMU sensor or 360 vanishing point detection algorithm <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b33">32]</ref> to ensure gravity alignment (the VP alignment is also a standard step in 360 layout benchmark <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b34">33]</ref>).</p><p>We also show the results by training with U(?30?, 30?) pitch/roll rotation as data augmentation, which makes the model much more robust against the non-canonical view but sacrifices the test-time performance when input 360?view </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work presents a novel framework, HoHoNet, which is the first step to learning compact latent horizontal features for dense modalities modeling of omnidirectional images. HoHoNet is fast, versatile, and accurate for solving layout reconstruction, depth estimation, and semantic segmentation with accuracy on par with or better than the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material A. Network architecture diagram</head><p>We show the detailed architecture diagram in <ref type="figure" target="#fig_3">Fig. 7</ref>. The shape of each feature tensor is denoted as "# of channels, height, width" within the box. The height and width of the input panorama are assumed to be 512 and 1024 respectively. D and E are hyperparameters. The ConvSqueezeH layer is a depthwise convolution layer with kernel size set to the prior known input feature height without padding, which produces output feature height 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparing EHC block and HC block [21]</head><p>The height compression block aims to squeeze a 2D feature from the backbone to produce a 1D horizontal feature. <ref type="figure">Fig. 8</ref> shows the architecture of our Efficient Height Compression block (EHC block) and the one of HC block <ref type="bibr" target="#b21">[21]</ref> for comparison. The HC block <ref type="bibr" target="#b21">[21]</ref> employs a sequence of convolution layers to gradually reduce the number of channels and heights, while we first use a convolution layer for channel reduction and then use bilinear upsampling and ConvSqueezeH layer to produce the features in horizontal shape. We show in our ablation experiments that replacing the HC block <ref type="bibr" target="#b21">[21]</ref> with the proposed ECH block leads to better speed and accuracy.</p><p>(a) The HC block in <ref type="bibr" target="#b21">[21]</ref>.</p><p>(b) The proposed EHC block. <ref type="figure">Figure 8</ref>: Comparison of the proposed EHC block and the HC block in <ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Detailed semantic segmentation results</head><p>We show detailed per-class IoU and per-class Acc for semantic segmentation in <ref type="table" target="#tab_9">Table 5</ref>. We achieve the best IoU on 10 out of 13 classes and superior overall mIoU; we achieve best Acc on 7 out of 13 classes and comparable overall mAcc.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detailed layout estimation results</head><p>We show detailed quantitative results for room layout under different numbers of ground truth 2D corners in <ref type="table">Table 6</ref>. Our training protocol and layout formalization are identical to HorizonNet <ref type="bibr" target="#b21">[21]</ref>, while we observe improvements (except rooms with six corners) by using our network architecture. In comparison with the most recent state-of-the-art-AtlantaNet <ref type="bibr" target="#b17">[17]</ref>, we show better results on scenes with fewer corners and similar accuracy on overall scenes; meanwhile, our model is 22? faster than AtlantaNet <ref type="bibr" target="#b17">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Metric  <ref type="table">Table 6</ref>: Detailed quantitative comparison for room layout estimation on MatterportLayout <ref type="bibr" target="#b34">[33]</ref> under different numbers of ground-truth corners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More qualitative comparisons for depth estimation</head><p>We show more qualitative comparisons with the prior art-BiFuse [24]-in <ref type="figure">Fig. 9</ref>. BiFuse's results are obtained from their official released model trained on the real-world Matterport3D <ref type="bibr" target="#b1">[2]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>BiFuse Ours Ground Truth Advantage <ref type="figure">Figure 9</ref>: More qualitative comparisons of the estimated dense depth with the prior art-BiFuse <ref type="bibr" target="#b24">[24]</ref>. The 'Advantage' column shows the MAE difference between ours and BiFuse's where the blue color indicates ours is better and the red color for vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative results for semantic segmentation</head><p>Qualitative results for semantic segmentation on Stanford2D3D <ref type="bibr" target="#b0">[1]</ref> dataset are shown in <ref type="figure" target="#fig_4">Fig. 10</ref>. We fail to build the prior art <ref type="bibr" target="#b9">[9]</ref> from their public release for semantic segmentation on high-resolution panorama, so we only show our results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Qualitative comparisons for layout estimation</head><p>We show qualitative comparisons for room layout estimation with the prior art-AtlantaNet [17]-in <ref type="figure">Fig. 11</ref>. The results of AtlantaNet are obtained from their official code and pre-trained weights. We use <ref type="bibr" target="#b21">[21]</ref> post-processing algorithm to produce Manhattan layouts; AtlantaNet <ref type="bibr" target="#b17">[17]</ref>'s algorithm generates less restrictive Atlanta layouts. Our model achieves promising results comparable to the most recent AtlantaNet <ref type="bibr" target="#b17">[17]</ref>, while our model runs 22? faster. <ref type="figure">Figure 11</ref>: Qualitative comparisons for room layout estimated with the competitive AtlantaNet <ref type="bibr" target="#b17">[17]</ref>. The green, magenta, and blue are the ground truth layout, AtlantaNet's results, and our results respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Gravity-aligned 360 image columns are easier to compress.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The proposed efficient height compression (EHC) module. The sizes of 2D and 1D features are denoted as [C, H, W ] and [C, W ] respectively. The ConvSqueezeH layer is a depthwise convolution layer with kernel size set to the prior known input feature height without padding, which produces output feature height 1. See Sec. 3.2 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The predictions at each column act as the weights for the linear combination of components in basis M . Ho-HoNet learns to predict in the spatial domain if M implements linear interpolation, and learns in the frequency domain if M implements IDCT. See Sec. 3.4 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>The detailed network architecture with ResNet50<ref type="bibr" target="#b10">[10]</ref> backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative results for semantic segmentation on Stanford2D3D<ref type="bibr" target="#b0">[1]</ref> dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Comparison of the components for LHFeat extraction. The 'HC' column indicates the height compression block, which produces the initial LHFeat from the backbone features. We compare the results of 'no feature refinement', 'refined by bidirectional LSTM'<ref type="bibr" target="#b11">[11]</ref> (Bi-LSTM), and 'refined by multi-head self-attention'<ref type="bibr" target="#b23">[23]</ref> (MHSA). Refinement with MHSA achieves the most favorable results.</figDesc><table><row><cell cols="2">HC Refine</cell><cell>MAE? RMSE?</cell><cell>? 1 ?</cell><cell>FPS?</cell></row><row><cell>[21] EHC</cell><cell>-</cell><cell cols="2">0.3090 0.5238 0.8158 0.3022 0.5102 0.8204</cell><cell>49 54</cell></row><row><cell cols="2">[21] Bi-LSTM EHC</cell><cell cols="2">0.3002 0.5147 0.8254 0.2928 0.5036 0.8294</cell><cell>38 41</cell></row><row><cell cols="2">[21] MHSA EHC</cell><cell cols="2">0.2915 0.5035 0.8331 0.2835 0.4916 0.8389</cell><cell>47 52</cell></row><row><cell>(a) r</cell><cell>Basis</cell><cell>MAE? RMSE?</cell><cell>? 1 ?</cell><cell>FPS?</cell></row><row><cell>32</cell><cell cols="3">Interp. 0.2886 0.5013 0.8356 IDCT 0.2847 0.4935 0.8369</cell><cell>52 52</cell></row><row><cell>64</cell><cell cols="3">Interp. 0.2880 0.4996 0.8351 IDCT 0.2835 0.4916 0.8389</cell><cell>52 52</cell></row><row><cell>128</cell><cell cols="3">Interp. 0.2926 0.5043 0.8308 IDCT 0.2850 0.4955 0.8405</cell><cell>52 52</cell></row><row><cell>256</cell><cell cols="3">Interp. 0.2937 0.5059 0.8260 IDCT 0.2903 0.5028 0.8334</cell><cell>52 52</cell></row><row><cell>512</cell><cell cols="3">Interp. 0.3045 0.5189 0.8227 IDCT 0.2913 0.5040 0.8341</cell><cell>52 52</cell></row><row><cell cols="3">Backbone MAE? RMSE?</cell><cell>? 1 ?</cell><cell>FPS?</cell></row><row><cell cols="2">ResNet34</cell><cell cols="2">0.2854 0.4976 0.8397</cell><cell>110</cell></row><row><cell cols="2">ResNet50</cell><cell cols="2">0.2835 0.4916 0.8389</cell><cell>52</cell></row></table><note>(b) Comparison on the different settings of the proposed horizon-to- dense module. The parameter r denotes the number of components in a basis. We compare the two bases that implement the linear interpolation (Interp.) and the inverse discrete cosine transform (IDCT).(c) Comparison of the results with different backbones.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Ablation study on depth modality using the ablation split of Matterport3D [2]. More details are in Sec. 4.1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1c</head><label>1c</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>MRE</cell><cell cols="2">MAE RMSE RMSE (log)</cell><cell>? 1</cell><cell>? 2</cell><cell>? 3</cell></row><row><cell></cell><cell>FCRN [15]</cell><cell cols="2">0.2409 0.4008 0.6704</cell><cell>0.1244</cell><cell cols="3">0.7703 0.9174 0.9617</cell></row><row><cell></cell><cell cols="3">OmniDepth (bn) [31] 0.2901 0.4838 0.7643</cell><cell>0.1450</cell><cell cols="3">0.6830 0.8794 0.9429</cell></row><row><cell></cell><cell>Equi [24]</cell><cell cols="2">0.2074 0.3701 0.6536</cell><cell>0.1176</cell><cell cols="3">0.8302 0.9245 0.9577</cell></row><row><cell>Matterport3D</cell><cell>Cube [24]</cell><cell cols="2">0.2505 0.3929 0.6628</cell><cell>0.1281</cell><cell cols="3">0.7556 0.9135 0.9612</cell></row><row><cell></cell><cell>BiFuse [24]</cell><cell cols="2">0.2048 0.3470 0.6259</cell><cell>0.1134</cell><cell cols="3">0.8452 0.9319 0.9632</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">0.1488 0.2862 0.5138</cell><cell>0.0871</cell><cell cols="3">0.8786 0.9519 0.9771</cell></row><row><cell></cell><cell>FCRN [15]</cell><cell cols="2">0.1837 0.3428 0.5774</cell><cell>0.1100</cell><cell cols="3">0.7230 0.9207 0.9731</cell></row><row><cell></cell><cell cols="3">OmniDepth (bn) [31] 0.1996 0.3743 0.6152</cell><cell>0.1212</cell><cell cols="3">0.6877 0.8891 0.9578</cell></row><row><cell></cell><cell>Equi [24]</cell><cell cols="2">0.1428 0.2711 0.4637</cell><cell>0.0911</cell><cell cols="3">0.8261 0.9458 0.9800</cell></row><row><cell>Stanford2D3D</cell><cell>Cube [24]</cell><cell cols="2">0.1332 0.2588 0.4407</cell><cell>0.0844</cell><cell cols="3">0.8347 0.9523 0.9838</cell></row><row><cell></cell><cell>BiFuse [24]</cell><cell cols="2">0.1209 0.2343 0.4142</cell><cell>0.0787</cell><cell cols="3">0.8660 0.9580 0.9860</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">0.1014 0.2027 0.3834</cell><cell>0.0668</cell><cell cols="3">0.9054 0.9693 0.9886</cell></row></table><note>, where we find that em- ploying ResNet-34 can almost double the FPS with only a little drop in accuracy comparing to ResNet-50.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>world Stanford2D3D<ref type="bibr" target="#b0">[1]</ref> dataset with extra layout annotation is used, where there are only 404 and 113 panoramas for training and testing. All the ERP images and depth maps are resized to 256 ? 512. Standard evaluation metrics-RMSE, MRE, log10, and ? 1for depth estimation are used. Neither depth clipping nor median alignment is applied during evaluation. Implementation details. The network and the training details are the same as in Sec. 4.2.1. However, we find the training strategy of<ref type="bibr" target="#b13">[13]</ref> is very different from ours. For a fair comparison, we also report the results of training Ho-HoNet with the training protocol of<ref type="bibr" target="#b13">[13]</ref>-SGD optimizer with a batch-size of 8, learning rate of 0.01, and weight decay set to 5e-4.</figDesc><table><row><cell>Method</cell><cell>RMSE MRE log10</cell><cell>? 1</cell></row><row><cell>FCRN [15]</cell><cell cols="2">0.534 0.164 0.073 0.749</cell></row><row><cell>UResNet</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Results. The comparison on the Stanford2D3D subset is</cell></row><row><cell></cell><cell></cell><cell>shown in Table 3a. HoHoNet achieves the best accuracy</cell></row><row><cell></cell><cell></cell><cell>under the same training protocol, and using Adam optimizer</cell></row><row><cell></cell><cell></cell><cell>with our training setting can further improve the results. Note</cell></row><row><cell></cell><cell></cell><cell>that GeoReg360 [13] employs a ResNet-50 and a ResNet-34,</cell></row><row><cell></cell><cell></cell><cell>and the network is jointly trained with the additional layout</cell></row><row><cell></cell><cell></cell><cell>and semantic annotation. Conversely, HoHoNet employs a</cell></row><row><cell></cell><cell></cell><cell>single ResNet-50 and is only trained with depth modality,</cell></row><row><cell></cell><cell></cell><cell>but still shows superior results, which further demonstrates</cell></row><row><cell></cell><cell></cell><cell>the effectiveness of the proposed framework.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>State-of-the-art comparison on various datasets and different modalities.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Roll 28.35 44.32 61.90 75.11 30.92 31.32 31.80 32.90 Vulnerability to non-gravity-aligned views. are gravity-aligned (MAE? from 28.35cm to 30.92cm).</figDesc><table><row><cell>Training</cell><cell>Testing</cell><cell>MAE (cm)</cell></row><row><cell>Rot. Aug.</cell><cell>Cam. Rot.</cell><cell>0?10?20?30?P</cell></row><row><cell></cell><cell>itch</cell><cell>28.35 44.88 62.77 75.79 30.92 31.30 31.80 32.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Method overall beam board bookcase ceiling chair clutter column door floor sofa table wall window 84.7 55.5 41.4 76.7 96.9 70.3 73.9 80.1 74.3 Ours 68.9 16.7 79.0 71.8 96.4 79.2 59.7 26.9 77.7 98.2 58.0 79.6 85.9 66.3</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Low-resolution RGB-D</cell><cell></cell><cell></cell></row><row><cell cols="2">UGSCNN [12] 38.3</cell><cell>8.7 32.7</cell><cell>33.4</cell><cell>82.2 42.0 25.6</cell><cell cols="2">10.1 41.6 87.0 7.6 41.7 61.7 23.5</cell></row><row><cell cols="3">HexRUNet [28] 43.3 10.9 39.7</cell><cell>37.2</cell><cell>84.8 50.5 29.2</cell><cell cols="2">11.5 45.3 92.9 19.1 49.1 63.8 29.4</cell></row><row><cell cols="3">TangentImg [9] 37.5 10.9 26.6</cell><cell>31.9</cell><cell>82.0 38.5 29.3</cell><cell>5.9</cell><cell>36.2 89.4 12.6 40.4 56.5 26.7</cell></row><row><cell>Ours</cell><cell>40.8</cell><cell>3.6 43.5</cell><cell>40.6</cell><cell>81.8 41.3 27.7</cell><cell>9.2</cell><cell>52.0 92.2 9.4 44.6 61.6 23.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>High-resolution RGB-D</cell><cell></cell><cell></cell></row><row><cell cols="2">TangentImg [9] 51.9</cell><cell>4.5 49.9</cell><cell>50.3</cell><cell>85.5 71.5 42.4</cell><cell cols="2">11.7 50.0 94.3 32.1 61.4 70.5 50.0</cell></row><row><cell>Ours</cell><cell>56.3</cell><cell>7.4 62.3</cell><cell>55.5</cell><cell>87.0 66.4 44.3</cell><cell cols="2">19.2 66.5 96.1 43.3 60.1 72.9 51.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Per-class IoU (%).</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="6">overall beam board bookcase ceiling chair clutter column door floor sofa table wall window</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Low-resolution RGB-D</cell><cell></cell><cell></cell></row><row><cell cols="3">UGSCNN [12] 54.7 19.6 48.6</cell><cell>49.6</cell><cell>93.6 63.8 43.1</cell><cell cols="2">28.0 63.2 96.4 21.0 70.0 74.6 39.0</cell></row><row><cell cols="3">HexRUNet [28] 58.6 23.2 56.5</cell><cell>62.1</cell><cell>94.6 66.7 41.5</cell><cell cols="2">18.3 64.5 96.2 41.1 79.7 77.2 41.1</cell></row><row><cell cols="3">TangentImg [9] 50.2 25.6 33.6</cell><cell>44.3</cell><cell>87.6 51.5 44.6</cell><cell cols="2">12.1 64.6 93.6 26.2 47.2 78.7 42.7</cell></row><row><cell>Ours</cell><cell>52.1</cell><cell>9.5 56.5</cell><cell>56.6</cell><cell>95.1 57.9 40.7</cell><cell cols="2">12.5 64.5 96.8 10.6 69.1 79.3 28.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>High-resolution RGB-D</cell><cell></cell><cell></cell></row><row><cell cols="3">TangentImg [9] 69.1 22.6 62.0</cell><cell>70.0</cell><cell>90.3</cell><cell></cell><cell></cell></row></table><note>(b) Per-class Acc (%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Detailed quantitative per-class results on Stanford2D3D [1] with RGB-D as input.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>75.82 81.35 72.33 67.45 63.00 DuLa-Net v2 [26] 75.07 77.02 78.79 71.03 63.27 HorizonNet [21] 79.11 81.88 82.26 71.78 68.32 AtlantaNet [17] 80.02 82.09 82.08 75.19 71.61 Ours 79.88 82.64 82.16 73.65 69.26 LayoutNet v2 [33] 2D IoU (%) 78.73 84.61 75.02 69.79 65.14 DuLa-Net v2 [26] 78.82 81.12 82.69 74.00 66.12 HorizonNet [21] 81.71 84.67 84.82 73.91 70.58 AtlantaNet [17] 82.09 84.42 83.85 76.97 73.18 Ours 82.32 85.26 84.81 75.59 70.98</figDesc><table><row><cell></cell><cell></cell><cell># of corners</cell><cell></cell><cell></cell></row><row><cell>overall</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10+</cell></row><row><cell>LayoutNet v2 [33]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D IoU (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/zouchuhang/LayoutNetv2 3 https://github.com/SunDaDenny/DuLa-Net 4 https://github.com/sunset1995/HorizonNet 5 https://github.com/crs4/AtlantaNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was supported in part by the MOST, Taiwan under Grants 110-2634-F-001-009 and 110-2634-F-007-016, MOST Joint Research Center for AI Technology and All Vista Healthcare. We thank National Center for High-performance Computing (NCHC) for providing computational and storage resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno>abs/1702.01105</idno>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matterport3d: Learning from RGB-D data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision</title>
		<meeting><address><addrLine>Qingdao, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural topological SLAM for visual navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12872" to="12881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cube padding for weakly-supervised saliency prediction in 360?videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsien-Tzu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hung</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Kai</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gauge equivariant convolutional networks and the icosahedral CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkay</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spherenet: Learning spherical representations for detection and classification in omnidirectional images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Coors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Paul Condurache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Proceedings, Part IX</title>
		<meeting>Part IX<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11213</biblScope>
			<biblScope unit="page" from="525" to="541" />
		</imprint>
	</monogr>
	<note>European Conference</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pano popups: Indoor 3d reconstruction with a plane-aware network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Eder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Moulon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision, 3DV 2019</title>
		<meeting><address><addrLine>Qu?bec City, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tangent images for mitigating spherical distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Eder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spherical cnns on unstructured grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Chiyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric structure based and regularized depth estimation from 360 indoor imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="886" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision, 3DV 2016</title>
		<meeting><address><addrLine>Stanford, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spherephd: Applying cnns on a spherical polyhedron representation of 360deg images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeon</forename><surname>Kun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeseok</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Seob Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjune</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Atlantanet: Inferring the 3D indoor layout from a single 360 image beyond the manhattan world assumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Pintore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Agus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Gobbetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The European Conference on Computer Vision (ECCV)</title>
		<meeting>The European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015 -18th International Conference</title>
		<editor>Nassir Navab, Joachim Hornegger, William M. Wells III, and Alejandro F. Frangi</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning spherical convolution for fast features from 360?imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kernel transformer networks for compact spherical convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9442" to="9451" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HorizonNet: learning room layout with 1d representation and pano stretch data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wei</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distortion-aware convolutional filters for dense prediction in panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part XVI</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bifuse: Monocular 360 depth estimation via bi-projection fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Layoutmp3d: Layout annotation of matter-port3d. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dula-net: A dual-projection network for estimating room layouts from a single RGB panorama</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Ta</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Han</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Kuo</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Joint 3d layout and depth prediction from a single indoor panorama image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sezer</forename><surname>Karaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Gevers</surname></persName>
		</author>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<biblScope unit="page" from="2020" to="2036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Proceedings, Part XVI</title>
		<meeting>Part XVI<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12361</biblScope>
			<biblScope unit="page" from="666" to="682" />
		</imprint>
	</monogr>
	<note>European Conference</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Orientation-aware semantic segmentation on icosahedron spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Panocontext: A whole-room 3d context model for panoramic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<editor>David J. Fleet, Tom?s Pajdla, Bernt Schiele, and Tinne Tuytelaars</editor>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="668" to="686" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured3d: A large photo-realistic dataset for structured 3d modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The European Conference on Computer Vision (ECCV)</title>
		<meeting>The European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Omnidepth: Dense depth estimation for indoors spherical panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Zioulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonis</forename><surname>Karakottas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Zarpalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Daras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11210</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Layoutnet: Reconstructing the 3d room layout from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2051" to="2059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">3d manhattan room layout reconstruction from a single 360 image. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jheng-Wei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Han</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Kuo</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1910" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
