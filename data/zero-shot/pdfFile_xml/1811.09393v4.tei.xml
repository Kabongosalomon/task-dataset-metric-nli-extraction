<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07">2020. July 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acm Reference Format: Mengyu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Chu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Mayer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Leal-Taix?</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thuerey</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Trans. Graph</title>
						<imprint>
							<biblScope unit="volume">39</biblScope>
							<biblScope unit="issue">4</biblScope>
							<date type="published" when="2020-07">2020. July 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3386569.3392457</idno>
					<note>This is the author&apos;s version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in ACM Transactions on Graphics, https://doi.org/10.1145/3386569.3392457. confirm the rankings computed with these metrics. Code, data, models, and results are provided at https://github.com/thunil/TecoGAN. The project page https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental materials.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Authors&apos; address: Mengyu Chu, mengyuchu@tumde</term>
					<term>You Xie, youxie@tumde</term>
					<term>Jonas Mayer, jonasamayer@tumde</term>
					<term>Laura Leal-Taix?, lealtaixe@tumde</term>
					<term>Nils Thuerey, nilsthuerey@tumde, CCS Concepts: ? Computing methodologies ? Neural networks</term>
					<term>Image processing Additional Key Words and Phrases: Generative adversarial network, temporal cycle-consistency, self-supervision, video super-resolution, unpaired video translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input TecoGAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For Unpaired Video Translation (UVT)</head><p>For Video Super-Resolution (VSR) TecoGAN Input TecoGAN Input <ref type="figure">Fig. 1</ref>. Using the proposed approach for temporal self-supervision, we achieve realistic results with natural temporal evolution for two inherently different video generation tasks: unpaired video translation (left) and video super-resolution (right). While the resulting sharpness can be evaluated via the still images above, the corresponding videos in our supplemental web-page (Sec. 1 and Sec.2) highlight the high quality of the temporal changes. Obama and Trump video courtesy of the White House (public domain).</p><p>Our work explores temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationships in the generated data are much less explored. Natural temporal changes are crucial for sequential generation tasks , e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L 2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. Additionally, we propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies * Both authors contributed equally to the paper</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Self-Supervision</head> <ref type="figure">Fig. 1</ref><p>. Using the proposed approach for temporal self-supervision, we achieve realistic results with natural temporal evolution for two inherently different video generation tasks: unpaired video translation (left) and video super-resolution (right). While the resulting sharpness can be evaluated via the still images above, the corresponding videos in our supplemental web-page (Sec. 1 and Sec.2) highlight the high quality of the temporal changes. Obama and Trump video courtesy of the White House (public domain).</p><p>Our work explores temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationships in the generated data are much less explored. Natural temporal changes are crucial for sequential generation tasks , e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L 2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. Additionally, we propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generative adversarial networks (GANs) have been extremely successful at learning complex distributions such as natural images ]. However, for sequence generation, directly applying GANs without carefully engineered constraints typically results in strong artifacts over time due to the significant difficulties introduced by the temporal changes. In particular, conditional video generation tasks are very challenging learning problems where generators should not only learn to represent the data distribution of the target domain but also learn to correlate the output distribution over time with conditional inputs. Their central objective is to faithfully reproduce the temporal dynamics of the target domain and not resort to trivial solutions such as features that arbitrarily appear and disappear over time.</p><p>In our work, we propose a novel adversarial learning method for a recurrent training approach that supervises both spatial contents as well as temporal relationships. As shown in <ref type="figure">Fig. 1</ref>, we apply our approach to two video-related tasks that offer substantially different challenges: video super-resolution (VSR) and unpaired video translation (UVT). With no ground truth motion available, the spatio-temporal adversarial loss and the recurrent structure enable our model to generate realistic results while keeping the generated structures coherent over time. With the two learning tasks we demonstrate how spatio-temporal adversarial training can be employed in paired as well as unpaired data domains. In addition to the adversarial network which supervises the short-term temporal coherence, long-term consistency is self-supervised using a novel bi-directional loss formulation, which we refer to as "Ping-Pong" (PP) loss in the following. The PP loss effectively avoids the temporal accumulation of artifacts, which can potentially benefit a variety of recurrent architectures. We also note that most existing image metrics focus on spatial content only. We fill the gap of temporal assessment with a pair of metrics that measures the perceptual similarity over time and the similarity of motions with respect to a ground truth reference. User studies confirm these metrics for both tasks.</p><p>The central contributions of our work are:</p><p>? a spatio-temporal discriminator unit together with a careful analysis of training objectives for realistic and coherent video generation tasks, ? a novel PP loss supervising long-term consistency, ? in addition to a set of metrics for quantifying temporal coherence based on motion estimation and perceptual distance.</p><p>Together, our contributions lead to models that outperform previous work in terms of temporally-coherent detail, which we qualitatively and quantitatively demonstrate with a wide range of content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Deep learning has made great progress for image generation tasks. While regular losses such as L 2 <ref type="bibr" target="#b26">[Kim et al. 2016;</ref><ref type="bibr" target="#b27">Lai et al. 2017</ref>] offer good performance for image super-resolution (SR) tasks in terms of PSNR metrics, previous work found adversarial training <ref type="bibr" target="#b14">[Goodfellow et al. 2014</ref>] to significantly improve the perceptual quality in multi-modal settings such as image generation <ref type="bibr" target="#b5">[Brock et al. 2019]</ref>, colorization <ref type="bibr" target="#b17">[He et al. 2018]</ref>, super-resolution <ref type="bibr" target="#b28">[Ledig et al. 2016]</ref>, and translation  tasks. Besides representing natural images, GAN-based frameworks are also successful at static graphic representations including geometry synthesis <ref type="bibr" target="#b52">[Wu et al. 2019</ref>] and city modeling <ref type="bibr" target="#b24">[Kelly et al. 2018]</ref>. Sequential generation tasks, on the other hand, require the generation of realistic content that changes naturally over time <ref type="bibr" target="#b53">Xie et al. 2018]</ref>. It is especially so for conditional video generation tasks <ref type="bibr" target="#b19">[Jamri?ka et al. 2019;</ref><ref type="bibr" target="#b44">Sitzmann et al. 2018;</ref><ref type="bibr" target="#b51">Wronski et al. 2019;</ref><ref type="bibr" target="#b54">Zhang et al. 2019]</ref>, where specific correlations between the input and the generated spatio-temporal evolution are required when ground-truth motions are not provided. Hence, motion estimation <ref type="bibr" target="#b10">[Dosovitskiy et al. 2015;</ref><ref type="bibr" target="#b32">Liu et al. 2019</ref>] and compensation become crucial for video generation tasks. The compensation can take various forms, e.g., explicitly using variants of optical flow networks <ref type="bibr" target="#b6">[Caballero et al. 2017;</ref><ref type="bibr" target="#b42">Shi et al. 2016]</ref> and implicitly using deformable convolution layers <ref type="bibr" target="#b49">[Wang et al. 2019a;</ref><ref type="bibr" target="#b57">Zhu et al. 2019]</ref> or dynamic up-sampling <ref type="bibr" target="#b20">[Jo et al. 2018</ref>]. In our work, a network is trained to estimate the motion and we show that it can help generators and discriminators in spatio-temporal adversarial training.</p><p>For VSR, recent work improve the spatial detail and temporal coherence by either using multiple low-resolution (LR) frames as inputs <ref type="bibr" target="#b16">[Haris et al. 2019;</ref><ref type="bibr" target="#b20">Jo et al. 2018;</ref><ref type="bibr" target="#b31">Liu et al. 2017;</ref><ref type="bibr" target="#b45">Tao et al. 2017]</ref> or recurrently using previously estimated outputs . The latter has the advantage to re-use high-frequency details over time. In general, adversarial learning is less explored for VSR and applying it in conjunction with a recurrent structure gives rise to a special form of temporal mode collapse, as we will explain below. For video translation tasks, GANs are more commonly used but discriminators typically only supervise the spatial content. E.g.,  focuses on images without temporal constrains and generators can fail to learn the temporal cycle-consistency for videos. In order to learn temporal dynamics, RecycleGAN <ref type="bibr" target="#b0">[Bansal et al. 2018]</ref> proposes to use a prediction network in addition to a generator, while a concurrent work  chose to learn motion translation in addition to the spatial content translation. Being orthogonal to these works, we propose a spatio-temporal adversarial training for both VSR and UVT and we show that temporal self-supervision is crucial for improving spatio-temporal correlations without sacrificing spatial detail.</p><p>While L 1 and L 2 temporal losses based on warping are generally used to enforce temporal smoothness in video style transfer tasks <ref type="bibr" target="#b8">[Chen et al. 2017;</ref><ref type="bibr" target="#b39">Ruder et al. 2016</ref>] and concurrent <ref type="bibr">GAN-based VSR [P?rez-Pellitero et al. 2018]</ref> and UVT <ref type="bibr" target="#b36">[Park et al. 2019</ref>] work, it leads to an undesirable smooth over spatial detail and temporal changes in outputs. Likewise, the L 2 temporal metric represents a sub-optimal way to quantify temporal coherence. For image similarity evaluation, perceptual metrics <ref type="bibr" target="#b38">[Prashnani et al. 2018;</ref><ref type="bibr" target="#b55">Zhang et al. 2018</ref>] are proposed to reliably consider semantic features instead of pixel-wise errors. However, for videos, perceptual metrics that evaluate natural temporal changes are unavailable up to now. To address this open issue, we propose two improved temporal metrics and demonstrate the advantages of temporal self-supervision over direct temporal losses. Due to its complexity, VSR has also led to workshop challenges like NTIRE19 <ref type="bibr" target="#b34">[Nah et al. 2019]</ref>, where algorithms such as EDVR <ref type="bibr" target="#b49">[Wang et al. 2019a</ref>] perform best w.r.t. PSNR-based metrics. We compare to these methods and give additional details in Appendix A.</p><p>Previous work, e.g. tempoGAN for fluid flow <ref type="bibr" target="#b53">[Xie et al. 2018</ref>] and vid2vid for video translation <ref type="bibr" target="#b48">[Wang et al. 2018a]</ref>, has proposed adversarial temporal losses to achieve time consistency. While tem-poGAN employs a second temporal discriminator with multiple aligned frames to assess the realism of temporal changes, it is not suitable for videos, as it relies on ground truth motions and employs single-frame processing that is sub-optimal for natural images. On the other hand, vid2vid focuses on paired video translations and proposes a video discriminator based on a conditional motion input that is estimated from the paired ground-truth sequences. We focus on more difficult unpaired translation tasks instead and demonstrate the gains in the quality of our approach in the evaluation section. <ref type="bibr" target="#b1">Bashkirova et al. [2018]</ref> solve UVT tasks as a 3D extension of the 2D image translation. In DeepFovea <ref type="bibr" target="#b22">[Kaplanyan et al. 2019</ref>], a 3D discriminator is used to supervise video in-painting results with 32 frames as a single 3D input. Since temporal evolution differs from a spatial distribution, we show how a separate handling of the temporal dimension can reduce computational costs, remove training restrictions, and most importantly improve inference quality.</p><p>For tracking and optical flow estimation, L 2 -based time-cycle losses <ref type="bibr" target="#b50">[Wang et al. 2019b</ref>] were proposed to constrain motions and tracked correspondences using symmetric video inputs. By optimizing indirectly via motion compensation or tracking, this loss improves the accuracy of the results. For video generation, we propose a PP loss that also makes use of symmetric sequences. However, we directly constrain the PP loss via the generated video content, which successfully improves the long-term temporal consistency in the video results. The PP loss is effective by offering valid information in forward as well as backward passes of image sequences. This concept is also used in robotic control algorithms, where reversed trajectories starting from goal positions have been used as training data <ref type="bibr" target="#b35">[Nair et al. 2018</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING TEMPORALLY COHERENT CONDITIONAL VIDEO GENERATION</head><p>We first propose the concepts of temporal self-supervision for GANbased video generation (Sec. 3.1 and Sec. 3.2), before introducing solutions for VSR and UVT tasks (Sec. 3.3 and Sec. 3.4) as example applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatio-Temporal Adversarial Learning</head><formula xml:id="formula_0">G D 0/1 Frame- Recurrent Generator Motion Compensation ?1 Generated Triplets ?1 +1 ?1 +1 Target Triplets ?1 +1 ?1 +1 , 0/1 a) b) c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. a) A spatial GAN for image generation. b) A frame recurrent</head><p>Generator. c) A spatio-temporal Discriminator. In these figures, letter a, b, and ?, stand for the input domain, the output domain and the generated results respectively. G and D stand for the generator and the discriminator.</p><p>While GANs are popular and widely used in image generation tasks to improve perceptual quality, their spatial adversarial learning inherently introduces temporal problems for tasks such as video generation. Thus, we propose an algorithm for spatio-temporal adversarial learning that is easy to integrate into existing GANbased image generation approaches. Starting from a standard GAN for images, as shown in <ref type="figure">Fig. 2 a)</ref>, we propose to use a frame-recurrent generator (b) together with a spatio-temporal discriminator (c). As shown in <ref type="figure">Fig. 2 b)</ref>, our generator produces an output ? t from an input frame a t and recursively uses the previously generated output ? t ?1 . Following previous work , we warp this frame-recurrent input to align it with the current frame. This allows the network to more easily re-use previously generated details. The high-level structure of the generator can be summarized as:</p><formula xml:id="formula_1">v t = F(a t ?1 , a t ), ? t = G(a t ,W (? t ?1 , v t )).</formula><p>(1)</p><p>Here, the network F is trained to estimate the motion v t from frame a t ?1 to a t and W denotes warping. The central building block of our approach is a novel spatiotemporal discriminator D s,t that receives triplets of frames, shown in <ref type="figure">Fig. 2 c)</ref>. This contrasts with typically used spatial discriminators that supervise only a single image. By concatenating multiple adjacent frames along the channel dimension, the frame triplets form an important building block for learning as they can provide networks with gradient information regarding the realism of spatial structures as well as short-term temporal information, such as firstand second-order time derivatives.</p><p>We propose a D s,t architecture that primarily receives two types of triplets: three adjacent frames and the corresponding warped ones. We warp later frames backward and previous ones forward. The network F is likewise used to estimate the corresponding motions. While original frames contain the full spatio-temporal information, warped frames more easily yield temporal information with their aligned content. For the input variants we use the following notations:</p><formula xml:id="formula_2">I ? = {? t ?1 , ? t , ? t +1 }, I b = {b t ?1 , b t , b t +1 }; I w? = {W (? t ?1 , v t ), ? t ,W (? t +1 , v ? t )}, I wb = {W (b t ?1 , v t ), b t ,W (b t +1 , v ? t )}.</formula><p>A subscript a denotes the input domain, while the b subscript denotes the target domain. The quotation mark in v ? indicates that quantities are estimated from the backward direction.</p><p>Although the proposed concatenation of several frames seems like a simple change that has been used in a variety of other contexts, we show that it represents an important operation that allows discriminators to understand spatio-temporal data distributions. As will be shown below, it can effectively reduce temporal problems encountered by spatial GANs. While L 2 ?based temporal losses are widely used in the field of video generation, the spatio-temporal adversarial loss is crucial for preventing the inference of blurred structures in multi-modal data-sets. Compared to GANs using multiple discriminators, the single D s,t network that we propose can learn to balance the spatial and temporal aspects according to the reference data and avoid inconsistent sharpness as well as overly smooth results. Additionally, by extracting shared spatio-temporal features, it allows for smaller network sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-Supervision for Long-term Temporal Consistency</head><p>When relying on a previous output as input, i.e., for frame-recurrent architectures, generated structures easily accumulate frame by frame. In adversarial training, generators learn to heavily rely on previously generated frames and can easily converge towards strongly reinforcing spatial features over longer periods of time. For videos,</p><formula xml:id="formula_3">a) b) 0 1 ? ?1 ? 0 1 ? ?1 ? 0 1 ? ?1 ? 2 0 ? 1 ? ? ?1 ? ? ? 2 - - - - - recurrent c) Fig. 3. a)</formula><p>Result without PP loss. The VSR network is trained with a recurrent frame-length of 10. When inference on long sequences, frame 15 and latter frames of the foliage scene show the drifting artifacts. b) Result trained with PP loss. These artifacts are removed successfully for the latter. c) When inferring a symmetric PP sequence with a forward pass (Ping) and its backward counterpart (Pong), our PP loss constrains the output sequence to be symmetric. It reduces the L 2 distance between ? t and ? ? t , the corresponding frames in the forward and backward passes, shown via red circles with a minus sign. The PP loss reduces drifting artifacts and improves temporal coherence.</p><p>this especially occurs along directions of motion and these solutions can be seen as a special form of temporal mode collapse, where the training converges to a mostly constant temporal signal as a sub-optimal, trivial equilibrium. We have noticed this issue in a variety of recurrent architectures, examples are shown in <ref type="figure">Fig. 3</ref> a) and the Dst version in <ref type="figure">Fig. 8</ref>.</p><p>While this issue could be alleviated by training with longer sequences, it is computationally expensive and can fail for even longer sequences, as shown in Appendix D. We generally want generators to be able to work with sequences of arbitrary length for inference. To address this inherent problem of recurrent generators, we propose a new bi-directional "Ping-Pong" loss. For natural videos, a sequence with the forward order as well as its reversed counterpart offer valid information. Thus, from any input of length n, we can construct a symmetric PP sequence in form of a 1 , ...a n?1 , a n , a n?1 , ...a 1 as shown in <ref type="figure">Fig. 3 c)</ref>. When inferring this in a frame-recurrent manner, the generated result should not strengthen any invalid features from frame to frame. Rather, the result should stay close to valid information and be symmetric, i.e., the forward result ? t = G(a t , ? t ?1 ) and the one generated from the reversed part, ? ? should be identical.</p><p>Based on this observation, we train our networks with extended PP sequences and constrain the generated outputs from both "legs" to be the same using the loss:</p><formula xml:id="formula_4">L pp = n?1 t =1 ?? t ? ? t ? ? 2 .</formula><p>Note that in contrast to the generator loss, the L 2 norm is a correct choice here: We are not faced with multi-modal data where an L 2 norm would lead to undesirable averaging, but rather aim to constrain the recurrent generator to its own, unique version over time without favoring smoothness. The PP terms provide constraints for short term consistency via ?? n?1 ? ? n?1 ? ? 2 , while terms such as ?? 1 ? ? 1 ? ? 2 prevent long-term drifts of the results. This bi-directional loss formulation also helps to constrain ambiguities due to disocclusions that can occur in regular training scenarios.</p><formula xml:id="formula_5">Frame- Recurrent Generator ?1 a) VSR , 0/1 Conditional LR Triplet ?1 +1 Original Triplet ?1 +1 Warped Triplet ?1 +1 + + Original Triplet ?1 +1 Warped Triplet ?1 +1 b) Fig. 4. a) The frame-recurrent VSR Generator. b) Conditional VSR D s, t .</formula><p>As shown in <ref type="figure">Fig. 3 b)</ref>, the PP loss successfully removes drifting artifacts while appropriate high-frequency details are preserved. In addition, it effectively extends the training data set, and as such represents a useful form of data augmentation. A comparison is given in Appendix D to disentangle the effects of the augmentation of PP sequences and the temporal constraints. The results show that the temporal constraint is the key to reliably suppressing the temporal accumulation of artifacts, achieving consistency, and allowing models to infer much longer sequences than seen during training.</p><p>The majority of related work for video generation focuses on network architectures. Being orthogonal to architecture improvements, our work explores temporal self-supervision. The proposed spatio-temporal discriminator and the PP loss can be used in video generation tasks to replace simple temporal losses, e.g. the ones based on L 2 differences and warping. In the following subsections, solutions for VSR and UVT are presented as examples in paired and unpaired data domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Architecture for VSR</head><p>For video super-resolution (VSR) tasks, the input domain contains LR frames while the target domain contains high-resolution (HR) videos with more complex details and motions. Since one pattern in low-resolution can correspond to multiple structures in highresolution, VSR represents a multimodal problem that benefits from adversarial training. In the proposed spatio-temproal adversarial training, we use a ResNet architecture for the VSR generator G. Similar to previous work, an encoder-decoder structure is applied to F for motion estimation. We intentionally keep the generative part simple and in line with previous work, in order to demonstrate the advantages of the temporal self-supervision.</p><p>The VSR discriminator D s,t should guide the generator to learn the correlation between the conditional LR inputs and HR targets. Therefore, three LR frames I a = {a t ?1 , a t , a t +1 } from the input domain are used as a conditional input. The input of D s,t can be summarized as I b s,t = {I b , I wb , I a } labelled as real and the generated inputs I ? s,t = {I ? , I w? , I a } labelled as fake, as shown in <ref type="figure">Fig. 4</ref>. We concatenate all triplets together. In this way, the conditional D s,t will penalize G if I ? contains less spatial details or unrealistic artifacts in comparison to I a , I b . At the same time, temporal relationships between the generated images I w? and those of the ground truth I wb should match. With our setup, the discriminator profits from the</p><formula xml:id="formula_6">Domain A Domain B ? ?1 ? ? ? ?1 ? ? ? ? ? ? ? ? a) Static Triplet { }x3 Static Triplet { }x3 UVT , 0/1 Original Triplet ?1 +1 Warped Triplet ?1 +1 Original Triplet ?1 +1</formula><p>Warped Triplet warped frames to classify realistic and unnatural temporal changes, and for situations where the motion estimation is less accurate, the discriminator can fall back to the original, i.e. not warped, images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Architecture for UVT</head><p>While one generator is enough to map data from A to B for tasks such as VSR, unpaired generation tasks require a second generator to establish cycle consistency ]. For the UVT task, we use two recurrent generators, mapping from domain A to B and back. As shown in <ref type="figure">Fig. 5 a)</ref>, given ? a?b</p><formula xml:id="formula_7">t = G ab (a t ,W (? a?b t ?1 , v t )), we can use a t as the labeled data of ? a?b?a t = G ba (? a?b t , W (? a?b?a t ?1 , v t ))</formula><p>to enforce consistency. An encoder-decoder structure is applied to UVT generators and F .</p><p>In UVT tasks, we demonstrate that the temporal cycle-consistency between different domains can be established using the supervision of unconditional spatio-temporal discriminators. This is in contrast to previous work which focuses on the generative networks to form spatio-temporal cycle links. Our approach actually yields improved results, as we will show below. In practice, we found it crucial to ensure that generators first learn reasonable spatial features, and only then improve their temporal correlation. Therefore, different to the D s,t of VST that always receives 3 concatenated triplets as an input, the unconditional D s,t of UVT only takes one triplet at a time. Focusing on the generated data, the input for a single batch can either be a static triplet of I s? = {? t , ? t , ? t }, the warped triplet I w? , or the original triplet I ? . The same holds for the reference data of the target domain, as shown in <ref type="figure">Fig. 4 b)</ref>. Here, the warping is again performed via F . With sufficient but complex information contained in these triplets, transition techniques are applied so that the network can consider the spatio-temporal information step by step, i.e., we initially start with 100% static triplets I s? as the input. Then, over the course of training, 25% of them transit to I w? triplets with simpler temporal information, with another 25% transition to I ? afterwards, leading to a (50%,25%,25%) distribution of triplets. Details of the transition calculations are given in Appendix C. Sample triplets are visualized in the supplemental web-page (Sec. 7).</p><p>While non-adversarial training typically employs loss formulations with static goals, the GAN training yields dynamic goals due to discriminators discovering learning objectives over the course of the training run. Therefore, their inputs have a strong influence on the training process and the final results. Modifying the inputs in a controlled manner can lead to different results and substantial improvements if done correctly, as will be shown in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Functions</head><p>Perceptual Loss Terms. As perceptual metrics, both pre-trained NNs <ref type="bibr" target="#b21">[Johnson et al. 2016;</ref><ref type="bibr" target="#b47">Wang et al. 2018b</ref>] and GAN discriminators <ref type="bibr" target="#b53">[Xie et al. 2018]</ref> were successfully used in previous work. Here, we use feature maps from a pre-trained VGG-19 network [Simonyan and Zisserman 2014], as well as D s,t itself. In the VSR task, we can encourage the generator to produce features similar to the ground truth ones by increasing the cosine similarity of their feature maps. In UVT tasks without paired ground truth data, the generators should match the distribution of features in the target domain. Similar to a style loss for traditional style transfer tasks <ref type="bibr" target="#b21">[Johnson et al. 2016]</ref>, we thus compute the D s,t feature correlations measured by the Gram matrix for UVT tasks. The D s,t features contain both spatial and temporal information and hence are especially well suited for the perceptual loss.</p><p>Loss and Training Summary . We now explain how to integrate the spatio-temporal discriminator into the paired and unpaired tasks. We use a standard discriminator loss for the D s,t of VSR and a leastsquare discriminator loss for the D s,t of UVT. Correspondingly, a non-saturated L adv is used for the G and F of VSR and a leastsquares one is used for the UVT generators. As summarized in <ref type="table" target="#tab_1">Table 1</ref>, G and F are trained with the mean squared loss L content , adversarial losses L adv , perceptual losses L ? , the PP loss L PP , and a warping loss L warp , where again ?, b and ? stand for generated samples, ground truth images and feature maps of VGG or D s,t . We only show losses for the mapping from A to B for UVT tasks, as the backward mapping simply mirrors the terms. We refer to our full model for both tasks as TecoGAN below. The UVT data-sets are obtained from previous work <ref type="bibr" target="#b0">[Bansal et al. 2018]</ref> and each data domain has around 2400 to 3600 unpaired frames. For VSR, we download 250 short videos with 120 frames each from Vimeo.com. In line with other VSR projects, we down-sample these frames by a factor of 2 to get the ground-truth HR frames. Corresponding LR frames are achieved by applying a Gaussian blur and sampling every fourth pixel. A Gaussian blur step is important to mimic the information loss due to the camera sensibility in a real-life capturing scenario. Although the information loss is complex and not unified, a Gaussian kernel with a standard deviation of 1.5 is commonly used for a super-resolution factor of 4. Training parameters and details are given in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ANALYSIS AND EVALUATION OF LEARNING OBJECTIVES</head><p>In the following section, we illustrate the effects of temporal supervision using two ablation studies.   </p><formula xml:id="formula_8">VSR, D s, t ?E b?p b (b) [log D(I b s, t )] ? E a?pa(a) [log(1 ? D(I ? s, t ))] UVT, D b s, t E b?p(b) [D(I b s, t ) ? 1] 2 + E a?p(a) [D(I ? s, t )] 2 Loss for VSR, G &amp; F UVT, G ab L G, F ? w L warp + ? p L PP + ? a L adv + ? ? L ? + ? c L content L warp ?a t ? W (a t ?1 , F(a t ?1 , a t )) ? 2 L PP n?1 t =1 ?? t ? ? t ? ? 2 L adv ?E a?pa(a) [log D s, t (I ? s, t )] ?E a?pa(a) [D b s, t (I ? a?b s, t )] 2 L ? 1.0 - ?(I ? s, t ) * ?(I b s, t ) ?(I ? s, t ) * ?(I b s, t ) G M (?(I ? s, t )) ? G M (?(I b s, t )) 2 L content ?? t ? b t ? 2 ? a )b )a t ? a t 2 + ? b )a )b t ? b t 2</formula><p>evaluations below, we also refer the reader to our supplemental material which contains a web-page with video clips that more clearly highlight the temporal differences. Video Super-Resolution. For VSR, we first train a DsOnly model that uses a frame-recurrent G and F with a VGG loss and only the regular spatial discriminator. Compared to ENet, which exhibits strong incoherence due to the lack of temporal information, DsOnly improves temporal coherence thanks to the frame-recurrent connection, but there are noticeable high-frequency changes between frames. The temporal profiles of DsOnly in <ref type="figure">Fig. 6</ref> and 7, correspondingly contain sharp and broken lines. When adding a temporal discriminator in addition to the spatial one (DsDt), this version generates more coherent results, and its temporal profiles are sharp and coherent. However, DsDt often produces the drifting artifacts discussed in Sec. 3, as the generator learns to reinforce existing details from previous frames to fool D s with sharpness, and satisfying D t with good temporal coherence in the form of persistent detail. While this strategy works for generating short sequences during training, the strengthening effect can lead to very undesirable artifacts for long-sequence inferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Loss Ablation Study</head><p>By adding the self-supervision for long-term temporal consistency L pp , we arrive at the DsDtPP model, which effectively suppresses these drifting artifacts with an improved temporal coherence. In <ref type="figure">Fig. 6</ref> and <ref type="figure" target="#fig_2">Fig. 7</ref>, DsDtPP results in continuous yet detailed temporal profiles without streaks from temporal drifting. Although Ds-DtPP generates good results, it is difficult in practice to balance the generator and the two discriminators. The results shown here were achieved only after numerous runs manually tuning the weights of the different loss terms. By using the proposed D s,t discriminator instead, we get a first complete model for our method, denoted as TecoGAN ? . This network is trained with a discriminator that achieves an excellent quality with an effectively halved network size, as illustrated on the right of <ref type="figure" target="#fig_5">Fig. 14.</ref> The single discriminator correspondingly leads to a significant reduction in resource usage. Using two discriminators requires ca. 70% more GPU memory, and leads to a reduced training performance by ca. 20%. The TecoGAN ? model yields similar perceptual and temporal quality to DsDtPP with a significantly faster and more stable training.</p><p>Since the TecoGAN ? model requires less training resources, we also trained a larger generator with 50% more weights. In the following, we will focus on this larger single-discriminator architecture with PP loss as our full TecoGAN model for VSR. Compared to the TecoGAN ? model, it can generate more details, and the training process is more stable, indicating that the larger generator and D s,t are more evenly balanced. Result images and temporal profiles are shown in <ref type="figure" target="#fig_2">Fig. 6 and Fig. 7</ref>. Video results are shown in Sec. 4 of the supplemental web-page.</p><p>Unpaired Video Translation. We carry out a similar ablation study for the UVT task. Again, we start from a single-image GAN-based model, a CycleGAN variant which already has two pairs of spatial generators and discriminators. Then, we train the DsOnly variant by adding flow estimation via F and extending the spatial generators to frame-recurrent ones. By augmenting the two discriminators to use the triplet inputs proposed in Sec. 3, we arrive at the Dst model with spatio-temporal discriminators, which does not yet use the PP loss. By adding the PP loss we complete the TecoGAN model for UVT. Although UVT tasks substantially differ from VSR tasks, the comparisons in <ref type="figure">Fig. 8</ref> and Sec. 4.7 of our supplemental web-page illustrate that UVT tasks profit from the proposed approach in a very similar manner to VSR.</p><p>We use renderings of 3D fluid simulations of rising smoke as our unpaired training data. These simulations are generated with randomized numerical simulations using a resolution of 64 3 for domain A and 256 3 for domain B, and both are visualized with images of size 256 2 . Therefore, video translation from domain A to B is a tough task, as the latter contains significantly more turbulent and small-scale motions. With no temporal information available, the CycleGAN variant generates HR smoke that strongly flickers. The DsOnly model offers better temporal coherence by relying on its frame-recurrent input, but it learns a solution that largely ignores the current input and fails to keep reasonable spatio-temporal cycleconsistency links between the two domains. On the contrary, our D s,t enables the Dst model to learn the correlation between the spatial and temporal aspects, thus improving the cycle-consistency. However, without L pp , the Dst model (like the DsDt model of VSR) reinforces detail over time in an undesirable way. This manifests itself as inappropriate smoke density in empty regions. Using our full TecoGAN model which includes L pp , yields the best results, with detailed smoke structures and very good spatio-temporal cycleconsistency.</p><p>For comparison, a DsDtPP model with a larger number of networks, i.e. four discriminators, two frame-recurrent generators and the F , is trained. By weighting the temporal adversarial losses from Dt with 0.3 and the spatial ones from Ds with 0.5, we arrived at a balanced training run. Although this model performs similarly to the TecoGAN model on the smoke dataset, the proposed spatiotemporal D s,t architecture represents a more preferable choice in practice, as it learns a natural balance of temporal and spatial components by itself, and requires fewer resources. Continuing along this direction, it will be interesting future work to evaluate variants, such as a shared D s,t for both domains, i.e. a multi-class classifier network. Besides the smoke dataset, an ablation study for the Obama and Trump dataset from <ref type="figure">Fig. 8</ref> shows a very similar behavior, as can be seen in the supplemental web-page (Sec. 4.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spatio-temporal Adversarial Equilibriums</head><p>Our evaluation so far highlights that temporal adversarial learning is crucial for achieving spatial detail that is coherent over time for VSR, and for enabling the generators to learn the spatio-temporal correlation between domains in UVT. Next, we will shed light on the complex spatio-temporal adversarial learning objectives by varying the information provided to the discriminator network. In the following tests, shown in <ref type="figure" target="#fig_3">Fig. 9</ref> and Sec. 5 of the supplemental document, D s,t networks are identical apart from changing inputs, and we focus on the smoke dataset.</p><p>In order to learn the spatial and temporal features of the target domain as well as their correlation, the simplest input for D s,t consists of only the original, unwarped triplets, i.e. {I ? or I b }. Using these, we train a baseline model, which yields a sub-optimal quality: it lacks sharp spatial structures and contains coherent but dull motions. Despite containing the full information, these input triplets prevent D s,t from providing the desired supervision. For paired video translation tasks, the vid2vid network achieves improved temporal coherence by using a video discriminator to supervise the output sequence conditioned with the ground-truth motion. With no ground-truth data available, we train a vid2vid variant by using the estimated motions and original triplets,</p><formula xml:id="formula_9">i.e {I ? + F (? t ?1 , ? t ) + F (? t +1 , ? t ) or I b + F (b t ?1 , b t ) + F (b t +1 , b t )},</formula><p>as the input for D s,t . However, the result do not significantly improve. The motions are only partially reliable, and hence don't help for the difficult unpaired translation task. Therefore, the discriminator still fails to fully correlate spatial and temporal features.</p><p>We then train a third model, concat, using the original triplets and the warped ones, i.e. {I ? + I w? or I b + I wb }. In this case, the model learns to generate more spatial details with a more vivid motion. I.e., the improved temporal information from the warped triplets gives the discriminator important cues. However, the motion still does not fully resemble the target domain. We arrive at our final TecoGAN model for UVT by controlling the composition of the input data: as outlined above, we first provide only static triplets {I s? or I sb }, and then apply the transitions of warped triplets {I w? or I wb }, and original triplets {I ? or I b } over the course of training. In this way, the network can first learn to extract spatial features and build on them to establish temporal features. Finally, discriminators learn features about the correlation of spatial and temporal content by analyzing the original triplets and provide gradients such that the generators learn to use the motion information from the input and establish a correlation between the motions in the two unpaired domains. Consequently, the discriminator, despite receiving only a single triplet at once, can guide the generator to produce detailed structures that move coherently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND METRIC EVALUATION</head><p>For the VSR task, we test our model on a wide range of video data, including the widely used Vid4 dataset shown in <ref type="figure" target="#fig_2">Fig. 6, 7</ref> and 12, detailed scenes from the movie Tears of Steel (ToS) <ref type="bibr">[2011]</ref> shown in <ref type="figure" target="#fig_4">Fig. 12</ref>, and others shown in <ref type="figure">Fig. 11</ref>. Besides ENet, FRVSR and DUF as baselines, we further compare our TecoGAN model to RBPN <ref type="bibr" target="#b16">[Haris et al. 2019</ref>] and EDVR <ref type="bibr" target="#b49">[Wang et al. 2019a</ref>]. Note that in contrast to TecoGAN with 3 million trainable weights, the latter two use substantially larger networks which have more than 12 and 20 million weights respectively, and EDVR is trained using bi-cubic down-sampling. Thus, in the following quantitative and qualitative comparisons, results and metrics for EDVR are calculated using bi-cubic down-sampled images, while other models use LR inputs with a Gaussian blur. In the supplemental web-page, Sec. 2 contains video results for the stills shown in <ref type="figure">Fig. 11</ref>, while Sec. 3 shows video comparisons of Vid4 scenes.</p><p>Trained with down-sampled inputs with Gaussian blur, the VSR TecoGAN model can similarly work with original images that were not down-sampled or filtered, such as a data-set of real-world photos. In <ref type="figure">Fig. 13</ref>, we compared our results to two other methods <ref type="bibr" target="#b29">[Liao et al. 2015;</ref><ref type="bibr" target="#b45">Tao et al. 2017</ref>] that have used the same dataset. With the help of adversarial learning, our model is able to generate improved and realistic details in down-sampled images as well as captured images.</p><p>For UVT tasks, we train models for Obama and Trump translations, LR-and HR-smoke simulation translations, as well as translations between smoke simulations and real-smoke captures. While smoke simulations usually contain strong numerical viscosity with details limited by the simulation resolution, the real smoke from <ref type="bibr" target="#b11">Eckert et al. [2018]</ref> contains vivid motions with many vortices and high-frequency details. As shown in <ref type="figure">Fig. 10</ref>, our method can be used to narrow the gap between simulations and real-world phenomena.</p><p>While visual results discussed above provide a first indicator of the quality our approach achieves, quantitative evaluations are crucial for automated evaluations across larger numbers of samples. Below we focus more on the VSR task as ground-truth data is available. We conduct user studies and present evaluations of the different models w.r.t. established spatial metrics. We also motivate and propose two novel temporal metrics to quantify temporal coherence.</p><p>For evaluating image SR, <ref type="bibr" target="#b2">Blau and Michaeli [2018]</ref> demonstrated that there is an inherent trade-off between the perceptual quality of the result and the distortion measured with vector norms or lowlevel structures such as PSNR and SSIM. On the other hand, metrics based on deep feature maps such as LPIPS <ref type="bibr" target="#b55">[Zhang et al. 2018]</ref> can capture more semantic similarities. We measure the PSNR and LPIPS using the Vid4 scenes. With a PSNR decrease of less than 2dB over [ <ref type="bibr" target="#b29">Liao et al. 2015]</ref> Ours <ref type="bibr" target="#b29">[Liao et al. 2015]</ref> Ours <ref type="bibr" target="#b45">[Tao et al. 2017</ref>] Ours <ref type="figure">Fig. 13</ref>. VSR comparisons for different captured images in order to compare to previous work <ref type="bibr" target="#b29">[Liao et al. 2015;</ref><ref type="bibr" target="#b45">Tao et al. 2017]</ref>.   <ref type="bibr" target="#b8">[Chen et al. 2017]</ref>, can be easily deceived by very blurry results, e.g. bicubic interpolated ones, we propose to use a tandem of two new metrics, tOF and tLP, to measure the consistence over time. tOF measures the pixel-wise difference of motions estimated from sequences, and tLP measures perceptual changes over time using deep feature map:</p><formula xml:id="formula_10">-diff = ?? t ? W (? t ?1 , v t )? 1</formula><formula xml:id="formula_11">tOF = ?O F (b t ?1 , b t ) ? O F (? t ?1 , ? t ) ? 1 and tLP = ?LP (b t ?1 , b t ) ? LP (? t ?1 , ? t ) ? 1 .<label>(2)</label></formula><p>OF represents an optical flow estimation with the Farneback <ref type="bibr">[2003]</ref> algorithm and LP is the perceptual LPIPS metric. In tLP, the behavior of the reference is also considered, as natural videos exhibit a certain degree of change over time. In conjunction, both pixel-wise differences and perceptual changes are crucial for quantifying realistic temporal coherence. While they could be combined into a single score, we list both measurements separately, as their relative importance could vary in different application settings. Our evaluation with these temporal metrics in <ref type="table" target="#tab_4">Table 2</ref> shows that all temporal adversarial models outperform spatial adversarial ones and the full TecoGAN model performs very well: With a large amount of spatial detail, it still achieves good temporal coherence, on par with non-adversarial methods such as DUF, FRVSR, RBPN and EDVR. These results are also visualized in <ref type="figure" target="#fig_5">Fig. 14.</ref> For VSR, we have confirmed these automated evaluations with several user studies (details in Appendix B). Across all of them, we find that the majority of the participants considered the TecoGAN results to be closest to the ground truth, when comparing to bi-cubic interpolation, ENet, FRVSR and DUF.</p><p>For the UVT tasks, where no ground-truth data is available, we can still evaluate tOF and tLP metrics by comparing the motion and the perceptual changes of the output data w.r.t. the ones from the input data , i.e., <ref type="bibr">tOF</ref> </p><formula xml:id="formula_12">= O F (a t ?1 , a t ) ? O F (? a?b t ?1 , ? a?b t ) 1 and tLP= LP (a t ?1 , a t ) ? LP (? a?b t ?1 , ? a?b t ) 1 .</formula><p>With sharp spatial features and coherent motion, TecoGAN outperforms CycleGAN and Recycle-GAN on the Obama&amp;Trump dataset, as shown in <ref type="table" target="#tab_5">Table 3</ref>, although it is worth pointing out that tOF is less informative in this case, as the motion in the target domain is not necessarily pixel-wise aligned with the input. While RecycleGAN uses an L2-based cycle loss that leads to undesirable smoothing, <ref type="bibr" target="#b36">Park et al. [2019]</ref> propose to use temporal-cycle losses in together with a VGG-based content preserving loss (we will refer to this method as STC-V2V below). While the evaluation of temporal metrics for TecoGAN and STC-V2V is very close, <ref type="figure">Fig. 8</ref> shows that our results contain sharper spatial details, such as the eyes and eyebrows of Obama as well as the wrinkles of Trump. This is illustrated in Sec. 2.2 of the supplemental web-page. Overall, TecoGAN successfully generates spatial details, on par with CycleGAN. TecoGAN also achieves very good tLP scores thanks to the supervision of temporal coherence, on par with previous work <ref type="bibr" target="#b0">[Bansal et al. 2018;</ref><ref type="bibr" target="#b36">Park et al. 2019]</ref>, despite inferring outputs with improved spatial complexity.</p><p>In line with VSR, a perceptual evaluation by humans in a user study confirms our metric evaluations for the UVT task. The participants consistently prefer TecoGAN results over CycleGAN and RecycleGAN. The corresponding scores are given in the right column of <ref type="table" target="#tab_5">Table 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND LIMITATIONS</head><p>In paired as well as unpaired data domains, we have demonstrated that it is possible to learn stable temporal functions with GANs thanks to the proposed discriminator architecture and PP loss. We have shown that this yields coherent and sharp details for VSR problems that go beyond what can be achieved with direct supervision. In UVT, we have shown that our architecture guides the training process to successfully establish the spatio-temporal cycle consistency between two domains. These results are reflected in the proposed metrics and confirmed by user studies. While our method generates very realistic results for a wide range of natural images, our method can lead to temporally coherent yet sub-optimal details in certain cases such as under-resolved faces and text in VSR, or UVT tasks with strongly different motion between two domains. For the latter case, it would be interesting to apply both our method and motion translation from concurrent work ]. This can make it easier for the generator to learn from our temporal self-supervision. The proposed temporal self-supervision also has potential to improve other tasks such as video in-painting and video colorization. In these multi-modal problems, it is especially important to preserve long-term temporal consistency. For our method, the interplay of the different loss terms in the non-linear training procedure does not provide a guarantee that all goals are fully reached every time. However, we found our method to be stable over a large number of training runs and we anticipate that it will provide a very useful basis for a wide range of generative models for temporal data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEARNING TEMPORAL COHERENCE VIA SELF-SUPERVISION FOR GAN-BASED VIDEO GENERATION THE APPENDIX</head><p>In the following, we first give details of the proposed temporal evaluation metrics, and present the corresponding quantitative comparison of our method versus a range of others (Appendix A). The user studies we conduected are in support of our TecoGAN network and proposed temporal metrics, and explained in Appendix B. Then, we give technical details of our spatio-temporal discriminator (Appendix C) and the proposed PP loss (Appendix D). Details of network architectures and training parameters are listed (Appendix E, Appendix F). Lastly, we discuss the performance of our approach in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A METRICS AND QUANTITATIVE ANALYSIS A.1 Spatial Metrics</head><p>In order to be able to compare our results with single-image methods, we evaluate all VSR methods with the purely spatial metrics PSNR together with the human-calibrated LPIPS metric <ref type="bibr" target="#b55">[Zhang et al. 2018</ref>]. While higher PSNR values indicate a better pixel-wise accuracy, lower LPIPS values represent better perceptual quality and closer semantic similarity. Note that both metrics are agnostic to changes over time, and hence do not suffice to really evaluate video data.</p><p>Mean values of the Vid4 scenes <ref type="bibr" target="#b30">[Liu and Sun 2011]</ref> are shown on the top of <ref type="table" target="#tab_6">Table 4</ref>. Trained with direct vector norms losses, FRVSR, DUF, EDVR, and RBPN achieve high PSNR scores. However, the undesirable smoothing induced by these losses manifests themselves in larger LPIPS distances. ENet, on the other hand, with no information from neighboring frames, yields the lowest PSNR and achieves an LPIPS score that is only slightly better than DUF and FRVSR. The TecoGAN model with adversarial training achieves an excellent LPIPS score, with a PSNR decrease of less than 2dB over DUF. This is very reasonable, since PSNR and perceptual quality were shown to be anti-correlated <ref type="bibr" target="#b2">[Blau and Michaeli 2018]</ref>, especially in regions where PSNR is very high. Based on good perceptual quality and reasonable pixel-wise accuracy, TecoGAN outperforms all other methods by more than 30% for LPIPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Temporal Metrics</head><p>For both VSR and UVT, evaluating temporal coherence without ground-truth motion is very challenging. L 2 -based temporal metrics such as T-diff = ?? t ? W (? t ?1 , v t )? 1 was used <ref type="bibr" target="#b8">[Chen et al. 2017</ref>] as a rough assessment of temporal differences, and we give corresponding numbers for comparison. As shown at the bottom of <ref type="table" target="#tab_6">Table 4</ref>, T-diff, due to its local nature, is easily deceived by blurry method such as the bi-cubic interrelation and can not correlate well with visual assessments of coherence.</p><p>By using the proposed metrics, i.e. measuring the pixel-wise motion differences using tOF together with the perceptual changes over time using tLP, a more nuanced evaluation can be achieved, as   <ref type="figure">Fig. 15</ref>. Bar graphs of temporal metrics for Vid4. <ref type="figure">Fig. 16</ref>. Spatial metrics for Vid4. <ref type="figure" target="#fig_2">Fig. 17</ref>. Metrics for ToS.</p><p>ENet there. By adding temporal information to discriminators, our DsDt, DsDt+PP, TecoGAN ? and TecoGAN improve in terms of temporal metrics. Especially the full TecoGAN model stands out here. For the UVT tasks, temporal motions are evaluated by comparing to the input sequence. With sharp spatial features and coherent motion, TecoGAN outperforms previous work on the Obama&amp;Trump dataset, as shown in <ref type="table" target="#tab_5">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Spatio-temporal Evaluations</head><p>Since temporal metrics can trivially be reduced for blurry image content, we found it important to evaluate results with a combination of spatial and temporal metrics. Given that perceptual metrics are already widely used for image evaluations, we believe it is the right time to consider perceptual changes in temporal evaluations, as we did with our proposed temporal coherence metrics.</p><p>Although not perfect, they are not as easily deceived as simpler metrics. Specifically, tOF is more robust than a direct pixel-wise metric as it compares motions instead of image content. In the supplemental material (Sec. 6), we visualize the motion difference and it can well reflect the visual inconsistencies. On the other hand, we found that our formulation of tLP is a general concept that can work reliably with different perceptual metrics: When repeating the tLP evaluation with the PieAPP metric <ref type="bibr" target="#b38">[Prashnani et al. 2018]</ref> instead of LP, i.e., tPieP = ?f (b t ?1 , b t ) ? f (? t ?1 , ? t ) ? 1 , where f(?) indicates the perceptual error function of PieAPP, we get close to identical results, as shown in <ref type="figure">Fig. 18</ref>. The conclusions from tPieP also closely match the LPIPS-based evaluation: our network architecture can generate realistic and temporally coherent detail, and the metrics we propose allow for a stable, automated evaluation of the temporal perception of a generated video sequence. Besides the previously evaluated the Vid4 dataset, with graphs shown in <ref type="figure">Fig. 15, 16</ref>, we also get similar evaluation results on the Tears of Steel data-sets (room, bridge, and face, in the following referred to as ToS scenes) and corresponding results are shown in <ref type="table" target="#tab_7">Table 5</ref> and <ref type="figure" target="#fig_2">Fig. 17</ref>. In all tests, we follow the procedures of previous work <ref type="bibr" target="#b20">[Jo et al. 2018;</ref>] to make the outputs of all methods comparable, i.e., for all result images, we first exclude spatial borders with a distance of 8 pixels to the image sides, then further shrink borders such that the LR input image is divisible by 8 and for spatial metrics, we ignore the first two and the last two frames, while for temporal metrics, we ignore first three and last two frames, as an additional previous frame is required for inference. Below, we additionally show user study results for the Vid4 scenes. By comparing the user study results and the metric breakdowns shown in <ref type="table" target="#tab_6">Table 4</ref>, we found our metrics to reliably capture the human temporal perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B USER STUDIES</head><p>We conducted several user studies for the VSR task comparing five different methods: bi-cubic interpolation, ENet, FRVSR, DUF and TecoGAN. The established 2AFC design <ref type="bibr" target="#b13">[Fechner and Wundt 1889;</ref><ref type="bibr" target="#b46">Um et al. 2017</ref>] is applied, i.e., participants have a pair-wise choice, with the ground-truth video shown as reference. One example setup can be seen in <ref type="figure" target="#fig_3">Fig. 19</ref>. The videos are synchronized and looped until participants make the final decision. With no control to stop videos, users cannot stop or influence the playback, and hence can focus more on the whole video, instead of specific spatial details. Videos positions (left/A or right/B) are randomized. After collecting 1000 votes from 50 users for every scene, i.e. twice for all possible pairs (5 ? 4/2 = 10 pairs), we follow common procedure and compute scores for all models with the Bradley-Terry model <ref type="bibr">(1952)</ref>. The outcomes for the Vid4 scenes can be seen in <ref type="figure">Fig. 20</ref> (overall scores are listed in <ref type="table" target="#tab_4">Table 2</ref> of the main document).</p><p>From the Bradley-Terry scores for the Vid4 scenes we can see that the TecoGAN model performs very well, and achieves the first place in three cases, as well as a second place in the walk scene. The latter is most likely caused by the overall slightly smoother images of the walk scene, in conjunction with the presence of several human faces, where our model can lead to the generation of unexpected details. However, overall the user study shows that users preferred the TecoGAN output over the other two deep-learning methods with a 63.5% probability.  <ref type="figure" target="#fig_3">Fig. 19</ref>. A sample setup of user study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>The Bradley-Terry scores (standard error) calendar foliage city walk Bi-cubic 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) 0.000 (0.000) ENet This result also matches with our metric evaluations. In <ref type="table" target="#tab_6">Table 4</ref>, while TecoGAN achieves spatial (LPIPS) improvements in all scenes, DUF and FRVSR are not far behind in the walk scene. In terms of temporal metrics tOF and tLP, TecoGAN achieves similar or lower scores compared to FRVSR and DUF for calendar, foliage and city scenes. The lower performance of our model for the walk scene is likewise captured by higher tOF and tLP scores. Overall, the metrics confirm the performance of our TecoGAN approach and match the results of the user studies, which indicate that our proposed temporal metrics successfully capture important temporal aspects of human perception.</p><p>For UVT tasks which have no ground-truth data, we carried out two sets of user studies: One uses an arbitrary sample from the target domain as the reference and the other uses the actual input from the source domain as the reference. On the Obama&amp;Trump data-sets, we evaluate results from CycleGAN, RecycleGAN, and TecoGAN following the same modality, i.e. a 2AFC design with 50 users for each  run. E.g., on the left of <ref type="figure" target="#fig_7">Fig. 21</ref>, users evaluate the generated Obama in reference with the input Trump on the y-axis, while an arbitrary Obama video is shown as the reference on the x-axis. Ultimately, the y-axis is more important than the x-axis as it indicates whether the translated result preserves the original expression. A consistent ranking of TecoGAN &gt; RecycleGAN &gt; CycleGAN is shown on the y-axis with clear separations, i.e. standard errors don't overlap. The x-axis indicates whether the inferred result matches the general spatio-temporal content of the target domain. Our TecoGAN model also receives the highest scores here, although the responses are slightly more spread out. On the right of <ref type="figure" target="#fig_7">Fig. 21</ref>, we summarize both studies in a single graph highlighting that the TecoGAN model is consistently preferred by the participants of our user studies. <ref type="figure">Fig. 22</ref>. Near image boundaries, flow estimation is less accurate and warping often fails to align content. The first two columns show original and warped frames, the third one shows differences after warping (ideally all black). The top row shows that structures moving into the view can cause problems, visible at the bottom of the images. The second row has objects moving out of the view.</p><formula xml:id="formula_13">b t ?1 W (b t ?1 , vt ) ?W (b t ?1 , vt ) ? bt ? 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C TECHNICAL DETAILS OF THE SPATIO-TEMPORAL DISCRIMINATOR C.1 Motion Compensation Used in Warped Triplet</head><p>In the TecoGAN architecture, D s,t detects the temporal relationships between I w? and I wb with the help of the flow estimation network F. However, at the boundary of images, the output of F is usually less accurate due to the lack of reliable neighborhood information. There is a higher chance that objects move into the field of view, or leave suddenly, which significantly affects the images warped with the inferred motion. An example is shown in <ref type="figure">Fig. 22</ref>. This increases the difficulty for D s,t , as it cannot fully rely on the images being aligned via warping. To alleviate this problem, we only use the center region of I w? and I wb as the discriminator inputs and we reset a boundary of 16 pixels. Thus, for an input resolution of I w? and I wb of 128 ? 128 for the VSR task, the inner part in size of 96 ? 96 is left untouched, while the border regions are overwritten with zeros.</p><p>The flow estimation network F with the loss L G, F should only be trained to support G in reaching the output quality as determined by D s,t , but not the other way around. The latter could lead to F networks that confuse D s,t with strong distortions of I w? and I wb . In order to avoid the this undesirable case, we stop the gradient back propagation from I w? and I wb to F. In this way, gradients from D s,t to F are only back propagated through the generated samples ? t ?1 , ? t and ? t +1 into the generator network. As such, D s,t can guide G to improve the image content, and F learns to warp the previous frame in accordance with the detail that G can synthesize. However, F does not adjust the motion estimation only to reduce the adversarial loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Curriculum Learning for UVT Discriminators</head><p>As mentioned in the main document, we train the UVT D s,t with 100% spatial triplets at the very beginning. During training, 25% of them gradually transition to warped triplets and another 25% transition to original triplets. The transitions of the warped triplets are computed with linear interpolation: (1 ? ?)I c? + ?I w? , with ? growing form 0 to 1. For the original triplets, we additionally fade the "warping" operation out by using (1 ? ?)</p><formula xml:id="formula_14">I c? + ? {W (? t ?1 , v t * ?), ? t ,W (? t +1</formula><p>, v ? t * ?)}, again with ? growing form 0 to 1 and ? decreasing from 1 to 0. We found this smooth transition to be helpful for a stable training run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DATA AUGMENTATION AND TEMPORAL CONSTRAINS IN THE PP LOSS</head><p>Since training with sequences of arbitrary length is not possible with current hardware, problems such as the "streaking artifacts" discussed above generally arise for recurrent models. In the proposed PP loss, both the Ping-Pong data augmentation and the temporal consistency constraint contribute to solving these problems. In order to show their separated contributions, we trained another TecoGAN variant that only employs the data augmentation without the constraint (i.e., ? p = 0 in <ref type="table" target="#tab_1">Table 1</ref>). Denoted as "PP-Augment", we show its results in comparison with the DsDt and TecoGAN ? models in <ref type="figure">Fig. 23</ref>. Video results are shown in the in the supplemental material (Sec. 4.5.). During training, the generator of DsDt receives 10 frames, and generators of PP-Augment and TecoGAN ? see 19 frames. While DsDt shows strong recurrent accumulation artifacts early on, the PP-Augment version slightly reduces the artifacts. In <ref type="figure">Fig. 23</ref>, it works well for frame 15, but shows artifacts from frame 32 on. Only our regular model (TecoGAN ? ) successfully avoids temporal accumulation for all 40 frames. Hence, with the PP constraint, the model avoids recurrent accumulation of artifacts and works well for sequences that are substantially longer than the training length. Among others, we have tested our model with ToS sequences of lengths 150, 166 and 233. For all of these sequences, the TecoGAN model successfully avoids temporal accumulation or streaking artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E NETWORK ARCHITECTURE</head><p>In this section, we use the following notation to specify all network architectures used: conc() represents the concatenation of two tensors along the channel dimension; C/CT (input, kernel_size, out-put_channel, stride_size) stands for the convolution and transposed convolution operation, respectively; "+" denotes element-wise addition; BilinearUp2 up-samples input tensors by a factor of 2 using bi-linear interpolation; BicubicResize4(input) increases the resolution of the input tensor to 4 times higher via bi-cubic up-sampling;</p><p>Dense(input, output_size) is a densely-connected layer, which uses Xavier initialization for the kernel weights.</p><p>The architecture of our VSR generator G is:</p><p>conc(a t ,W (? t ?1 , v t )) ? l in ; C(l in , 3, 64, 1), ReLU ? l 0 ; ResidualBlock(l i ) ? l i+1 with i = 0, ..., n ? 1; CT (l n , 3, 64, 2), ReLU ? l up2 ; CT (l up2 , 3, 64, 2), ReLU ? l up4 ; C(l up4 , 3, 3, 1), ReLU ? l r es ; BicubicResize4(a t ) + l r es ? ? t .</p><p>In TecoGAN ? , there are 10 sequential residual blocks in the generator ( l n = l 10 ), while the TecoGAN generator has 16 residual blocks ( l n = l 16 ). Each ResidualBlock(l i ) contains the following operations: C(l i , 3, 64, 1), ReLU ? r i ; C(r i , 3, 64, 1) + l i ? l i+1 .</p><p>The VSR D s,t 's architecture is:</p><p>I ? s,t or I b s,t ? l in ; C(l in , 3, 64, 1), Leaky ReLU ? l 0 ; C(l 0 , 4, 64, 2), BatchNorm, Leaky ReLU ? l 1 ; C(l 1 , 4, 64, 2), BatchNorm, Leaky ReLU ? l 2 ; C(l 2 , 4, 128, 2), BatchNorm, Leaky ReLU ? l 3 ; C(l 3 , 4, 256, 2), BatchNorm, Leaky ReLU ? l 4 ;</p><p>Dense(l 4 , 1), sigmoid ? l out .</p><p>VSR discriminators used in our variant models, DsDt, DsDtPP and DsOnly, have a the same architecture as D s,t . They only differ in terms of their inputs.</p><p>The flow estimation network F has the following architecture:</p><p>C(l 0 , 4, 128, 2), InstanceNorm, Leaky ReLU ? l 1 ; C(l 1 , 4, 256, 2), InstanceNorm, Leaky ReLU ? l 2 ; C(l 2 , 4, 512, 2), InstanceNorm, Leaky ReLU ? l 3 ; Dense(l 3 , 1) ? l out .</p><p>Again, all ablation studies use the same architecture and only differ in terms of their inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F TRAINING DETAILS</head><p>We use the non-saturated GAN for VSR and LSGAN <ref type="bibr" target="#b33">[Mao et al. 2017]</ref> for UVT and both of them can prevent the gradient vanishing problem of a standard GAN <ref type="bibr" target="#b14">[Goodfellow et al. 2014</ref>]. We employ a dynamic discriminator updating strategy, i.e. discriminators are not updated when there is a large difference between D(I ? s,t ) and D(I b s,t ). While our training runs are generally very stable, the training process could potentially be further improved with modern GAN algorithms, e.g. Wasserstein GAN <ref type="bibr" target="#b15">[Gulrajani et al. 2017]</ref>.</p><p>To improve the stability of the adversarial training for the VSR task, we pre-train G and F together with a simple L 2 loss of ?? t ? b t ? 2 +? w L war p for 500k batches. Based on the pre-trained models, we use 900k batches for the proposed spatio-temporal adversarial training stage. Our training sequences has a length of 10 and a batch size of 4. A black image is used as the first previous frame of each video sequence. I.e., one batch contains 40 frames and with the PP loss formulation, the NN receives gradients from 76 frames in total for every training iteration.</p><p>In the pre-training stage of VSR, we train the F and a generator with 10 residual blocks. An ADAM optimizer with ? = 0.9 is used throughout. The learning rate starts from 10 ?4 and decays by 50% every 50k batches until it reaches 2.5 * 10 ?5 . This pre-trained model is then used for all TecoGAN variants as initial state. In the adversarial training stage of VSR, all TecoGAN variants are trained with a fixed learning rate of 5 * 10 ?5 . The generators in DsOnly, DsDt, DsDtPP and TecoGAN ? have 10 residual blocks, whereas the TecoGAN model has 6 additional residual blocks in its generator. Therefore, after loading 10 residual blocks from the pre-trained model, these additional residual blocks are faded in smoothly with a factor of 2.5 * 10 ?5 . We found this growing training methodology <ref type="bibr" target="#b23">[Karras et al. 2017]</ref>, to be stable and efficient in our tests. When training the VSR DsDt and DsDtPP, extra parameters are used to balance the two cooperating discriminators properly. Through experiments, we found D t to be stronger. Therefore, we reduce the learning rate of D t to 1.5 * 10 ?5 in order to keep both discriminators balanced. At the same time, a factor of 0.0003 is used on the temporal adversarial loss to the generator, while the spatial adversarial loss has a factor of 0.001. During the VSR training, input LR video frames are cropped to a size of 32 ? 32. In all VSR models, the Leaky ReLU operation uses a tangent of 0.2 for the negative half space. Additional training parameters are listed in <ref type="table" target="#tab_11">Table 6.</ref> For the UVT task, a pre-training is not necessary for generators and discriminators since temporal triplets are gradually faded in. Only a pre-trained F model is reused. Trained on specialized datasets, we found UVT models to converge well with 100k batches of sequences in length of 6 and batch size of 1.</p><p>For all UVT tasks, we use a learning rate of 10 ?4 to train the first 90k batches and the last 10k batches are trained with the learning rate decay from 10 ?4 to 0. Images of the input domain are cropped  The additional training parameters are also listed in <ref type="table" target="#tab_11">Table 6</ref>. For UVT, L content and L ? are only used to improve the convergence of the training process. We fade out L content in the first 10k batches and L ? is used for the first 80k and faded out in last 20k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G PERFORMANCE</head><p>TecoGAN is implemented in TensorFlow. While generator and discriminator are trained together, we only need the trained generator network for the inference of new outputs after training, i.e., the whole discriminator network can be discarded. We evaluate the models on a Nvidia GeForce GTX 1080Ti GPU with 11G memory, the resulting VSR performance for which is given in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>The VSR TecoGAN ? model and FRVSR have the same number of weights (843587 in the generator network and 1.7M in F), and thus show very similar performance characteristics with around 37 ms per frame. The larger VSR TecoGAN model with 1286723 weights in the generator is slightly slower than them, spending 42 ms per frame. In the UVT task, generators spend around 60 ms per frame with a size of 512 ? 512. However, compared with models of DUF, EDVR and RBPN, which have 6 to 20 million weights, the performance of TecoGAN is significantly better thanks to its reduced size.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a) The UVT cycle link formed by two recurrent generators. b) Unconditional UVT D s, t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FRVSRFig. 7 .</head><label>7</label><figDesc>VSR temporal profile comparisons of the calendar scene (time shown along y-axis), cf. Sec. 4.1-4.6 of the supplemental web-page. TecoGAN models lead to natural temporal progression, and our final model closely matches the desired ground truth behavior over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 9 .</head><label>9</label><figDesc>Adversarial training arrives at different equilibriums when discriminators use different inputs. The baseline model (supervised on original triplets) and the vid2vid variant (supervised on original triplets and estimated motions) fail to learn the complex temporal dynamics of a highresolution smoke. The warped triplets improve the result of the concat model and the full TecoGAN model performs better spatio-temporally. Video comparisons are shown in Sec 5. of the supplemental web-page.Input TecoGANFig. 10. Video translations between renderings of smoke simulations and real-world captures for smokes. . Additional VSR comparisons, with videos in Sec 2 of the supplemental web-page. The TecoGAN model generates sharp details in both scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 12 .</head><label>12</label><figDesc>ACM Trans. Graph.,Vol. 39, No. 4, Article 75. Publication date: July 2020.75:10 ? Chu, M.; Xie, Y.; Mayer, J.; Leal-Taix?, L.; Thuerey, N. Detail views of the VSR results of ToS scenes (first three columns) and Vid4 scenes (two right-most columns) generated with different methods: from top to bottom. ENet [Sajjadi et al. 2017], FRVSR [Sajjadi et al. 2018], DUF [Jo et al. 2018], RBPN [Haris et al. 2019], EDVR [Wang et al. 2019a], TecoGAN, and the ground truth. Tears of Steel (ToS) movie (CC) Blender Foundation | mango.blender.org.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 14 .</head><label>14</label><figDesc>Visual summary of VSR models. a) LPIPS (x-axis) measures spatial detail and temporal coherence is measured by tLP (y-axis) and tOF (bubble size with smaller as better). b) The red-dashed-box region of a), containing our ablated models. c) The network sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>AVGFig. 21 .</head><label>21</label><figDesc>Tables and graphs of Bradley-Terry scores and standard errors for Obama&amp;Trump UVT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>. 1st &amp; 2nd row: Frame 15 &amp; 40 of the Foliage scene. While DsDt leads to strong recurrent artifacts early on, PP-Augment shows similar artifacts later in time (2nd row, middle). TecoGAN ? model successfully removes these artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>of 256?256 when training, the original size being 288?288.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>In the first one, models trained with ablated loss functions show how L adv and L PP change the overall learning objectives. Next, full UVT models are trained with different D s,t inputs. This highlights how differently the corresponding discriminators converge to different spatio-temporal equilibriums and the general importance of providing suitable data distributions from the target domain. While we provide qualitative and quantitative In VSR of the foliage scene, adversarial models (ENet, DsOnly, DsDt, DsDtPP, TecoGAN ? and TecoGAN) yield better perceptual quality than methods using L 2 loss (FRVSR and DUF). In temporal profiles on the right, DsDt, DsDtPP and TecoGAN show significantly less temporal discontinuities compared to ENet and DsOnly. The temporal information of our discriminators successfully suppresses these artifacts. Corresponding video clips can be found in Sec. 4.1-4.6 of the supplemental web-page .</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ENet</cell><cell>FRVSR</cell><cell>DUF</cell><cell>GTa</cell></row><row><cell>DsOnly</cell><cell>ENet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DsDt</cell><cell cols="2">FRVSR</cell><cell>Foliage scene, full frame</cell><cell></cell><cell>Temporal profiles</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">GTa</cell><cell>DsOnly</cell><cell>DsDt</cell><cell>DsDtPP</cell><cell>TecoGAN</cell></row><row><cell>DsDtPP</cell><cell>DUF</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TecoGANaa -</cell><cell>TecoGAN</cell><cell cols="2">Ours:</cell><cell>Ours:</cell></row><row><cell cols="3">DsDtPP DsDt TecoGAN TecoGAN ? Fig. 6. GTa Temporal profiles</cell><cell>Ours:</cell><cell></cell></row><row><cell></cell><cell>DUF</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ENet</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Summary of loss terms.</figDesc><table /><note>L Ds,t for</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Below we compare variants of our full TecoGAN model to En-hanceNet (ENet) [Sajjadi et al. 2017], FRVSR [Sajjadi et al. 2018], and DUF [Jo et al. 2018] for VSR. CycleGAN [Zhu et al. 2017] and RecycleGAN<ref type="bibr" target="#b0">[Bansal et al. 2018</ref>] are compared for UVT. Specifically, ENet and CycleGAN represent state-of-the-art single-image adversarial models without temporal information, FRVSR and DUF are state-of-the-art VSR methods without adversarial losses, and Recy-cleGAN is a spatial adversarial model with a prediction network learning the temporal evolution.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>When learning a mapping between Trump and Obama, the CycleGAN model gives good spatial features but collapses to essentially static outputs of Obama. It manages to transfer facial expressions back to Trump using tiny differences encoded in its Obama outputs, without understanding the cycle-consistency between the two domains. Being able to establish the correct temporal cycle-consistency between domains, ours, RecycleGAN and STC-V2V can generate correct blinking motions, shown in Sec. 4.7 of the supplemental web-page. Our model outperforms the latter two in terms of coherent detail that is generated. Obama and Trump video courtesy of the White House (public domain).</figDesc><table><row><cell>Input</cell><cell>CycleGAN</cell><cell>DsOnly</cell><cell>RecycleGAN</cell><cell>STC-V2V</cell><cell>TecoGAN</cell></row><row><cell>Input</cell><cell>CycleGAN</cell><cell>DsOnly</cell><cell>RecycleGAN</cell><cell>STC-V2V</cell><cell>TecoGAN</cell></row><row><cell></cell><cell>Cycle-</cell><cell></cell><cell></cell><cell></cell><cell>Teco-</cell></row><row><cell>Input</cell><cell>GAN</cell><cell>DsOnly</cell><cell>Dst</cell><cell>DsDtPP</cell><cell>GAN</cell></row><row><cell>Fig. 8.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>PSNR?</cell><cell>LPIPS? ?10</cell><cell>T-diff? ?100</cell><cell>tOF? ?10</cell><cell>tLP? ?100</cell><cell>User Study ?</cell><cell>Model Size(M) ?</cell><cell>Processing Time(ms/frame) ?</cell></row><row><cell>DsOnly</cell><cell>24.14</cell><cell>1.727</cell><cell>6.852</cell><cell>2.157</cell><cell>2.160</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DsDt</cell><cell>24.75</cell><cell>1.770</cell><cell>5.071</cell><cell>2.198</cell><cell>0.614</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DsDtPP</cell><cell>25.77</cell><cell>1.733</cell><cell>4.369</cell><cell>2.103</cell><cell>0.489</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TecoGAN ?</cell><cell>25.89</cell><cell>1.743</cell><cell>4.076</cell><cell>2.082</cell><cell>0.718</cell><cell>-</cell><cell>0.8(G)+1.7(F)</cell><cell>37.07</cell></row><row><cell>TecoGAN</cell><cell>25.57</cell><cell>1.623</cell><cell>4.961</cell><cell>1.897</cell><cell>0.668</cell><cell>3.258</cell><cell>1.3(G)+1.7(F)</cell><cell>41.92</cell></row><row><cell>ENet</cell><cell>22.31</cell><cell>2.458</cell><cell>9.281</cell><cell>4.009</cell><cell>4.848</cell><cell>1.616</cell><cell>-</cell><cell>-</cell></row><row><cell>FRVSR</cell><cell>26.91</cell><cell>2.506</cell><cell>3.648</cell><cell>2.090</cell><cell>0.957</cell><cell>2.600</cell><cell>0.8(SRNet)+1.7(F)</cell><cell>36.95</cell></row><row><cell>DUF</cell><cell>27.38</cell><cell>2.607</cell><cell>3.298</cell><cell>1.588</cell><cell>1.329</cell><cell>2.933</cell><cell>6.2</cell><cell>942.21</cell></row><row><cell>Bi-cubic</cell><cell>23.66</cell><cell>5.036</cell><cell>3.152</cell><cell>5.578</cell><cell>2.144</cell><cell>0.0</cell><cell>-</cell><cell>-</cell></row><row><cell>RBPN</cell><cell>27.15</cell><cell>2.511</cell><cell>-</cell><cell>1.473</cell><cell>0.911</cell><cell>-</cell><cell>12.7</cell><cell>510.90</cell></row><row><cell>EDVR</cell><cell>27.34</cell><cell>2.356</cell><cell>-</cell><cell>1.367</cell><cell>0.982</cell><cell>-</cell><cell>20.7</cell><cell>299.71</cell></row><row><cell cols="5">DUF (which has twice the model size), the LPIPS score of TecoGAN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">shows an improvement of more than 40%. The other baselines are</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">outperformed by similar margins. Even compared to the large EDVR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">model using down-sampled inputs without Gaussian blur, TecoGAN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">still yields a 30% improvement in terms of LPIPS.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">While traditional temporal metrics based on vector norm differ-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ences of warped frames, e.g. T</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Averaged VSR metric evaluations for the Vid4 data set with the following metrics, PSNR: pixel-wise accuracy. LPIPS (AlexNet): perceptual distance to the ground truth. T-diff: pixel-wise differences of warped frames. tOF: pixel-wise distance of estimated motions. tLP: perceptual distance between consecutive frames. User study: Bradley-Terry scores [Bradley and Terry 1952]. Performance is averaged over 500 images up-scaled from 320x134 to 1280x536. More details can be found in Appendix A and Sec. 3 of the supplemental web-page.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>For the Obama&amp;Trump dataset, the averaged tLP and tOF evaluations closely correspond to our user studies. The table below summarizes user preferences as Bradley-Terry scores. Details are given in Appendix B and Sec. 3 of the supplemental web-page.</figDesc><table><row><cell cols="5">UVT scenes Trump?Obama Obama?Trump</cell><cell>AVG</cell><cell cols="2">User Studies?, ref. to</cell></row><row><cell>metrics</cell><cell>tLP?</cell><cell>tOF?</cell><cell>tLP?</cell><cell>tOF?</cell><cell>tLP? tOF?</cell><cell>original input</cell><cell>arbitrary target</cell></row><row><cell cols="6">CycleGAN 0.0176 0.7727 0.0277 1.1841 0.0234 0.9784</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell cols="8">RecycleGAN 0.0111 0.8705 0.0248 1.1237 0.0179 0.9971 0.994 0.202</cell></row><row><cell>STC-V2V</cell><cell cols="5">0.0143 0.7846 0.0168 0.927 0.0156 0.8561</cell><cell>-</cell><cell>-</cell></row><row><cell>TecoGAN</cell><cell cols="7">0.0120 0.6155 0.0191 0.7670 0.0156 0.6913 1.817 0.822</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Metrics evaluated for the VSR Vid4 scenes.</figDesc><table><row><cell>PSNR? calendar</cell><cell cols="3">BIC ENet FRVSR DUF RBPN EDVR TecoGAN Teco GAN ? DsOnly DsDt DsDtPP 20.27 19.85 23.86 24.07 23.88 23.97 23.21 23.35 22.23 22.76 22.95</cell></row><row><cell>foliage</cell><cell cols="2">23.57 21.15 26.35 26.45 26.32 26.42 24.26</cell><cell>25.13 22.33 22.73 25.00</cell></row><row><cell>city</cell><cell cols="2">24.82 23.36 27.71 28.25 27.51 27.76 26.78</cell><cell>26.94 25.86 26.52 27.03</cell></row><row><cell>walk</cell><cell cols="2">25.84 24.90 29.56 30.58 30.59 30.92 28.11</cell><cell>28.14 26.49 27.37 28.14</cell></row><row><cell>average</cell><cell cols="2">23.66 22.31 26.91 27.38 27.15 27.34 25.57</cell><cell>25.89 24.14 24.75 25.77</cell></row><row><cell cols="4">LPIPS ??10 BIC ENet FRVSR DUF RBPN EDVR TecoGAN Teco GAN ? DsOnly DsDt DsDtPP calendar 5.935 2.191 2.989 3.086 2.654 2.296 1.511 2.142 1.532 2.111 2.112</cell></row><row><cell>foliage</cell><cell cols="2">5.338 2.663 3.242 3.492 3.613 3.485 1.902</cell><cell>1.984 2.113 2.092 1.902</cell></row><row><cell>city</cell><cell cols="2">5.451 3.431 2.429 2.447 0.233 2.264 2.084</cell><cell>1.940 2.120 1.889 1.989</cell></row><row><cell>walk</cell><cell cols="2">3.655 1.794 1.374 1.380 1.362 1.291 1.106</cell><cell>1.011 1.215 1.057 1.051</cell></row><row><cell>average</cell><cell cols="3">5.036 2.458 2.506 2.607 2.511 2.356 1.623 1.743 1.727 1.770 1.733</cell></row><row><cell cols="4">tOF ??10 BIC ENet FRVSR DUF RBPN EDVR TecoGAN Teco GAN ? DsOnly DsDt DsDtPP calendar 4.956 3.450 1.537 1.134 1.068 0.986 1.342 1.403 1.609 1.683 1.583</cell></row><row><cell>foliage</cell><cell cols="2">4.922 3.775 1.489 1.356 1.234 1.144 1.238</cell><cell>1.444 1.543 1.562 1.373</cell></row><row><cell>city</cell><cell cols="2">7.967 6.225 2.992 1.724 1.584 1.446 2.612</cell><cell>2.905 2.920 2.936 3.062</cell></row><row><cell>walk</cell><cell cols="2">5.150 3.203 2.569 2.127 1.994 1.871 2.571</cell><cell>2.765 2.745 2.796 2.649</cell></row><row><cell>average</cell><cell cols="2">5.578 4.009 2.090 1.588 1.473 1.367 1.897</cell><cell>2.082 2.157 2.198 2.103</cell></row><row><cell cols="4">tLP ??100 BIC ENet FRVSR DUF RBPN EDVR TecoGAN Teco GAN ? DsOnly DsDt DsDtPP calendar 3.258 2.957 1.067 1.603 0.802 0.622 0.165 1.087 0.872 0.764 0.670</cell></row><row><cell>foliage</cell><cell cols="2">2.434 6.372 1.644 2.034 1.927 1.998 0.894</cell><cell>0.740 3.422 0.493 0.454</cell></row><row><cell>city</cell><cell cols="2">2.193 7.953 0.752 1.399 0.432 1.060 0.974</cell><cell>0.347 2.660 0.490 0.140</cell></row><row><cell>walk</cell><cell cols="2">0.851 2.729 0.286 0.307 0.271 0.171 0.653</cell><cell>0.635 1.596 0.697 0.613</cell></row><row><cell>average</cell><cell cols="3">2.144 4.848 0.957 1.329 0.911 0.982 0.668 0.718 2.160 0.614 0.489</cell></row><row><cell cols="4">T-diff ??100 BIC ENet FRVSR DUF TecoGAN TecoGAN ? DsOnly DsDt DsDtPP GT</cell></row><row><cell>calendar</cell><cell>2.271 9.153 3.212 2.750 4.663</cell><cell>3.496</cell><cell>6.287 4.347 4.167 6.478</cell></row><row><cell>foliage</cell><cell>3.745 11.997 3.478 3.115 5.674</cell><cell>4.179</cell><cell>8.961 6.068 4.548 4.396</cell></row><row><cell>city</cell><cell>1.974 7.788 2.452 2.244 3.528</cell><cell>2.965</cell><cell>4.929 3.525 2.991 4.282</cell></row><row><cell>walk</cell><cell>4.101 7.576 5.028 4.687 5.460</cell><cell>5.234</cell><cell>6.454 5.714 5.305 5.525</cell></row><row><cell>average</cell><cell>3.152 9.281 3.648 3.298 4.961</cell><cell>4.076</cell><cell>6.852 5.071 4.369 5.184</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Metrics evaluated for VSR of ToS scenes.</figDesc><table><row><cell cols="2">PSNR? BIC ENet FRVSR DUF RBPN EDVR TecoGAN</cell></row><row><cell>room 26.90 25.22 29.80 30.85 26.82 31.12</cell><cell>29.31</cell></row><row><cell>bridge 28.34 26.40 32.56 33.02 28.56 32.88</cell><cell>30.81</cell></row><row><cell>face 33.75 32.17 39.94 40.23 33.74 41.57</cell><cell>38.60</cell></row><row><cell>average 29.58 27.82 34.04 34.60 29.71 35.02</cell><cell>32.75</cell></row><row><cell cols="2">LPIPS ??10 BIC ENet FRVSR DUF RBPN EDVR TecoGAN</cell></row><row><cell>room 5.167 2.427 1.917 1.987 2.054 2.232</cell><cell>1.358</cell></row><row><cell>bridge 4.897 2.807 1.761 1.684 1.845 1.663</cell><cell>1.263</cell></row><row><cell>face 2.241 1.784 0.586 0.517 0.728 0.613</cell><cell>0.590</cell></row><row><cell>average 4.169 2.395 1.449 1.414 1.565 1.501</cell><cell>1.086</cell></row><row><cell cols="2">tOF ??10 BIC ENet FRVSR DUF RBPN EDVR TecoGAN</cell></row><row><cell>room 1.735 1.625 0.861 0.901 0.821 0.769</cell><cell>0.737</cell></row><row><cell>bridge 5.485 4.037 1.614 1.348 1.354 1.140</cell><cell>1.492</cell></row><row><cell>face 4.302 2.255 1.782 1.577 1.612 1.383</cell><cell>1.667</cell></row><row><cell>average 4.110 2.845 1.460 1.296 1.287 1.113</cell><cell>1.340</cell></row><row><cell cols="2">tLP ??100 BIC ENet FRVSR DUF RBPN EDVR TecoGAN</cell></row><row><cell>room 1.320 2.491 0.366 0.307 0.263 0.252</cell><cell>0.590</cell></row><row><cell>bridge 2.237 6.241 0.821 0.526 0.821 1.030</cell><cell>0.912</cell></row><row><cell>face 1.270 1.613 0.290 0.314 0.364 0.396</cell><cell>0.379</cell></row><row><cell>average 1.696 3.827 0.537 0.403 0.531 0.628</cell><cell>0.664</cell></row><row><cell cols="2">shown for the VSR task in the middle of Table 4. Not surprisingly,</cell></row><row><cell cols="2">the results of ENet show larger errors for all metrics due to their</cell></row><row><cell cols="2">strongly flickering content. Bi-cubic up-sampling, DUF, and FRVSR</cell></row><row><cell cols="2">achieve very low T-diff errors due to their smooth results, repre-</cell></row><row><cell cols="2">senting an easy, but undesirable avenue for achieving coherency.</cell></row><row><cell cols="2">However, the overly smooth changes of the former two are identi-</cell></row><row><cell cols="2">fied by the tLP scores. While our DsOnly model generates sharper</cell></row><row><cell cols="2">results at the expense of temporal coherence, it still outperforms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>.202 0.118 RecycleGAN 0.994 0.135 TecoGAN 0.822 0.123 TecoGAN 1.817 0.150</figDesc><table><row><cell>X-axis: Generated Obama vs. Obama CycleGAN</cell><cell>Bradley Terry scores 0</cell><cell>Std. Error 0</cell><cell cols="2">Y-axis: Generated Obama vs. Trump CycleGAN</cell><cell>Bradley Terry scores 0</cell><cell>Std. Error 0</cell><cell cols="4">X-axis: Generated Trump vs. Trump CycleGAN 0.806 0.177 CycleGAN Bradley Terry scores Std. Error X-axis: Generated Trump vs. Obama</cell><cell>Bradley Terry scores 0</cell><cell>Std. Error 0</cell><cell>X-axis: Generated vs. Arbitrary Target CycleGAN</cell><cell cols="2">Bradley Terry scores 0</cell><cell>Std. Error 0</cell><cell>Y-axis: Generated vs. the Input CycleGAN</cell><cell>Bradley Terry scores 0</cell><cell>Std. Error 0</cell></row><row><cell cols="15">RecycleGAN 1.322 0.197 RecycleGAN 1.410 0.208 TecoGAN 1.520 0.201 TecoGAN 1.958 0.222 RecycleGAN TecoGAN 0.5 1 1.5 2 2.5 In ref. with the input Trump Trump ? Obama RecycleGAN 0CycleGAN RecycleGAN 0 0 RecycleGAN 0.623 0.182 TecoGAN 1.727 0.208 TecoGAN 1.092 0.182 RecycleGAN TecoGAN 0.4 0.6 0.8 1 1.2 1.4 In ref. with the input Obama Obama ? Trump 0.5 1 1.5 2 2.5 In ref. with the input 0 In ref. with an arbitrary Obama CycleGAN 0 0.2 In ref. with an arbitrary Trump CycleGAN 0</cell><cell cols="2">RecycleGAN</cell><cell>TecoGAN In ref. with an arbitrary target</cell></row><row><cell>0</cell><cell>0.5</cell><cell></cell><cell>1</cell><cell cols="2">1.5</cell><cell>2</cell><cell>0</cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell><cell>0</cell><cell cols="2">0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .</head><label>6</label><figDesc>Training parameters</figDesc><table><row><cell cols="3">VSR Param DsOnly DsDt</cell><cell>DsDtPP</cell><cell cols="2">TecoGAN ? TecoGAN</cell></row><row><cell>? a</cell><cell>1e-3</cell><cell cols="2">Ds: 1e-3, Dt: 3e-4</cell><cell>1e-3</cell><cell>1e-3</cell></row><row><cell>? p</cell><cell>0.0</cell><cell>0.0</cell><cell></cell><cell>0.5</cell></row><row><cell>? ?</cell><cell></cell><cell cols="3">0.2 for VGG and 1.0 for Discriminator</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = G(a t , ? ? t +1 ),</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 39, No. 4, Article 75. Publication date: July 2020.Learning Temporal Coherence via Self-Supervision for </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 39, No. 4, Article 75. Publication date: July 2020.Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation ? 75:1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 39, No. 4, Article 75. Publication date: July 2020.Learning Temporal Coherence via Self-Supervision for </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the ERC Starting Grant realFlow (StG-2015-637014) and the Humboldt Foundation through the Sofja Kovalevskaja Award. We would like to thank Kiwon Um for helping with the user studies.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Residual Block (l 2 + i) contains the following operations: C(l 2+i , 3, 128, 1), InstanceNorm, ReLU ? t 2+i ;C(t 2+i , 3, 128, 1), InstanceNorm ? r 2+i ; r 2+i + l 2+i ? l 3+i . We use 10 residual blocks for all UVT generators.</p><p>Since UVT generators are larger than the VSR generator, we also use a larger D s,t architecture: <ref type="bibr">4,</ref><ref type="bibr">64,</ref><ref type="bibr">24)</ref>, ReLU ? l 0 ;</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recycle-GAN: Unsupervised Video Retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised video-to-video translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Bashkirova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03698</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="6228" to="6237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rank analysis of incomplete block designs: I. The method of paired comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Ralph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milton</forename><forename type="middle">E</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="324" to="345" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1xsqj09Fm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Blender Foundation | mango.blender.org</title>
		<ptr target="https://mango.blender.org/.Online" />
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Coherent online video style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Computer Vision (ICCV)</title>
		<meeting>Intl. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09514</idno>
		<title level="m">Mocycle-GAN: Unpaired Video-to-Video Translation</title>
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE inter</title>
		<meeting>the IEEE inter</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-L</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Publication date</note>
	<note>Coupled fluid density and motion from single views</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Farneb?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian conference on Image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Elemente der Psychophysik: erster Theil</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Theodor</forename><surname>Fechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><forename type="middle">Max</forename><surname>Wundt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1889" />
			<publisher>Breitkopf &amp; H?rtel</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent backprojection network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Exemplar-Based Colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image-To-Image Translation With Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stylizing Video by Example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Jamri?ka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">??rka</forename><surname>Sochorov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Texler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Luk??</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Fi?er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>S?kora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3224" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepFovea: neural reconstruction for foveated rendering and video compression using learned statistics of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Anton S Kaplanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sochenov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Leimk?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Okunev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gizem</forename><surname>Goodall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rufo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FrankenGAN: guided detail synthesis for building mass models using style-synchonized GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transport-Based Neural Style Transfer for Smoke Simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vinicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solenthaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video super-resolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="531" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Bayesian approach to adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2526" to="2534" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selflow: Self-supervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4571" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">NTIRE 2019 Challenge on Video Deblurring and Super-Resolution: Dataset and Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Time reversal as self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01128</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Preserving Semantic and Temporal Consistency for Unpaired Video-to-Video Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanyong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3350864</idno>
		<ptr target="https://doi.org/10.1145/3343031.3350864" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia<address><addrLine>Nice, France; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>MM ???19)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>P?rez-Pellitero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07930</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Photorealistic Video Super Resolution. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PieAPP: Perceptual Image-Error Assessment through Pairwise Preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekta</forename><surname>Prashnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasamin</forename><surname>Mostofi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1808" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Artistic style transfer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Frame-Recurrent Video Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-end optimization of optics and image processing for achromatic extended depth of field and super-resolution imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Dun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Detail-Revealing Deep Video Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of liquid simulation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiwon</forename><surname>Um</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">143</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Perceptual adversarial networks for image-to-image transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="4066" to="4079" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video-to-Video Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">EDVR: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2566" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Handheld Multi-Frame Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartlomiej</forename><surname>Wronski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Garcia-Dorado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Kai</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
	<note>18 pages</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">SAGNet: Structure-aware Generative Network for 3D-Shape Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 2019)</title>
		<meeting>SIGGRAPH 2019)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyu</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep Exemplar-based Video Colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Bermak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8052" to="8061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<title level="m">Leaky ReLU, MaxPooling ? l 1</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Leaky ReLU ? l</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<title level="m">Leaky ReLU, MaxPooling ? l 3</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<title level="m">Leaky ReLU ? l 4</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<title level="m">Leaky ReLU, MaxPooling ? l 5</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Leaky ReLU ? l</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leaky</forename><surname>Relu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<title level="m">Leaky ReLU ? l 8</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<title level="m">Leaky ReLU, BilinearUp2 ? l 9</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Leaky ReLU ? l</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leaky</forename><surname>Relu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Leaky ReLU ? l</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<title level="m">tanh ? l out ; l out * MaxVel ? v t</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">MaxVel is a constant vector, which scales the network output to the normal velocity range. While F is the same for UVT tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Here</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UVT generators have an encoder-decoder structure</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Residual Block(l 2 + i) ? l 3+i with i = 0</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<title level="m">InstanceNorm, ReLU ? l n+3</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>CT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
