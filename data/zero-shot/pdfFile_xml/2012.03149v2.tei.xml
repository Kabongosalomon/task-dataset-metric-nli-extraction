<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Weighted Discriminator for Training Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasily</forename><surname>Zadorozhnyy</surname></persName>
							<email>vasily.zadorozhnyy@uky.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Cheng</surname></persName>
							<email>qiang.cheng@uky.edu</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute for Biomedical Informatics</orgName>
								<orgName type="department" key="dep2">Departments of Computer Science and Internal Medicine</orgName>
								<orgName type="institution">University of Kentucky</orgName>
								<address>
									<postCode>40506-0027</postCode>
									<settlement>Lexington</settlement>
									<region>Kentucky</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Weighted Discriminator for Training Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial network (GAN) has become one of the most important neural network models for classical unsupervised machine learning. A variety of discriminator loss functions have been developed to train GAN's discriminators and they all have a common structure: a sum of real and fake losses that only depends on the actual and generated data respectively. One challenge associated with an equally weighted sum of two losses is that the training may benefit one loss but harm the other, which we show causes instability and mode collapse. In this paper, we introduce a new family of discriminator loss functions that adopts a weighted sum of real and fake parts, which we call adaptive weighted loss functions or aw-loss functions. Using the gradients of the real and fake parts of the loss, we can adaptively choose weights to train a discriminator in the direction that benefits the GAN's stability. Our method can be potentially applied to any discriminator model with a loss that is a sum of the real and fake parts. Experiments validated the effectiveness of our loss functions on unconditional and conditional image generation tasks, improving the baseline results by a significant margin on CIFAR-10, STL-10, and CIFAR-100 datasets in Inception Scores (IS) and Fr?chet Inception Distance (FID) metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Network (GAN) <ref type="bibr" target="#b15">[15]</ref> has become one of the most important neural network models for unsupervised machine learning. The origin of this idea lies in the combination of two neural networks, one generative and one discriminative, that work simultaneously. The task of the generator is to generate data of a given distribution, while the discriminator's purpose is to try to rec-ognize which data are created by the generative model and which are the original ones. While a variety of GAN models have been developed, many of them are prone to issues with training such as instability where model parameters might destabilize and not converge, mode collapse where the generative model produces a limited number of different samples, diminishing gradients where the generator gradient vanishes and training does not occur, and high sensitivity to hyperparameters.</p><p>In this paper, we focus on the discriminative model to rectify the issues of instability and mode collapse in training GAN. In the GAN architecture, the discriminator model takes samples from the original dataset and the output from the generator as input and tries to classify whether a particular element in those samples is real or fake data <ref type="bibr" target="#b15">[15]</ref>. The discriminator updates its parameters by maximizing a discriminator loss function via backpropagation through the discriminator network. In many of the proposed models <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b28">28]</ref>, the discriminator loss function consists of two equally weighted parts: the "real part" that purely relies on the original dataset and the "fake part" that depends on the generator network and its output; for simplicity we will call them L r and L f for real and fake losses, respectively. For example, in the original GAN paper <ref type="bibr" target="#b15">[15]</ref>, the discriminator loss function L D is written as</p><formula xml:id="formula_0">L D = L r + L f ,<label>(1)</label></formula><p>with L r = E x?p d log D(x) and L f = E z?pz log(1 ? D(G(z))) , where D and G are the discriminative and generative models, respectively, p d is the probability distribution of the real data, and p z is the probability distribution of the generator parameter z. The goal of the GAN discriminator training is to increase both L r and L f so that the discriminator D(?) assigns high scores to real data and low scores to fake data. This is done in (1) by placing equal weights on L r and L f <ref type="bibr" target="#b15">[15]</ref>. However, the training with L D is not performed equally on L r and L f . Indeed, a gradient ascent training step along the ?L D may decrease L r (or L f ), depending on the angle be-tween ?L D and ?L r (or ?L f ). For example, if we have a large obtuse angle between ?L r and ?L f , which is the case in most training steps (see ?5.1), training along the direction of ?L D may potentially decrease either L r or L f by going in the opposite direction to ?L r or ?L f (see ?3 and ?5.2). We suggest that this reduction on the real loss may destabilize training and cause mode collapses. Specifically, if a generator is converging with its generated samples close to the data distribution (or a particular mode), a training step that increases the fake loss will reduce the discriminator scores on the fake data and, by the continuity of D(?), reduce the scores on the nearby real data as well. With the updated discriminator now assigning lower scores to the regions of data where the generator previously approximated well, the generator update is likely to move away from that region and to the regions with higher discriminator scores (possibly a different mode). Hence, we see instability or mode collapse. See ?5.3 for experimental results.</p><p>We propose a new approach in training the discriminative model by modifying the discriminator loss function and introducing adaptive weights in the following way,</p><formula xml:id="formula_1">L aw D = w r ? L r + w f ? L f .<label>(2)</label></formula><p>We adaptively choose w r and w f weights to calibrate the training in the real and fake losses. Using the information of ?L r and ?L f , we can control the gradient direction, ?L aw D , by either training in the direction that benefits both L r and L f or increasing one loss while not changing the other. This attempts to avoid a situation where training may benefit one loss but significantly harm the other. A more detailed mathematical approach is presented in ?3.</p><p>Our proposed method can be applied to any GAN model with a discriminator loss function composed of two parts as in <ref type="bibr" target="#b0">(1)</ref>. For our experiments we have applied adaptive weights to the SN-GAN <ref type="bibr" target="#b34">[34]</ref>, AutoGAN <ref type="bibr" target="#b14">[14]</ref>, and BigGAN <ref type="bibr" target="#b5">[5]</ref> models for unconditional as well as conditional image generating tasks. We have achieved significant improvements on them for CIFAR-10, STL-10 and CIFAR-100 datasets in both Inception Scores (IS) and Fr?chet Inception Distance (FID) metrics, see ?4. Our code is available at https://github.com/vasily789/adaptiveweighted-gans.</p><p>Notation: We use ?, ? 2 to denote the Euclidean inner product, x 2 the Euclidean 2-norm, and ? 2 (x, y) := arccos x,y 2 x 2 y 2 the angle between vectors x and y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>GAN was first proposed in <ref type="bibr" target="#b15">[15]</ref> for creating generative models via simultaneous optimization of a discriminative and a generative model. The original GAN may suffer from vanishing gradients during training, non-convergence of the model(s), and mode collapse; see <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39]</ref> for discussions. Several papers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b30">30]</ref> have addressed the issues of vanishing gradients by introducing new loss functions. The LSGAN proposed in <ref type="bibr" target="#b30">[30]</ref> adopted the least squares loss function for the discriminator that relies on minimizing the Pearson ? 2 divergence, in contrast to the Jensen-Shannon divergence used in GAN. The WGAN model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">16]</ref> introduced another way to solve the problem of convergence and mode collapse by incorporating Wasserstein-1 distance into the loss function. As a result, WGAN has a loss function associated with image quality which improves learning stability and convergence. The hinge loss function introduced in <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b43">43]</ref> achieved smaller error rates than cross-entropy, being stable against different regularization techniques, and having a low computational cost <ref type="bibr" target="#b12">[12]</ref>. The models in <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b45">45]</ref> adopted a loss function called maximum mean discrepancy (MMD). A repulsive function to stabilize the MMD-GAN training was employed in <ref type="bibr" target="#b46">[46]</ref>, and the MMD loss function was weighted in <ref type="bibr" target="#b11">[11]</ref> according to the contribution of data to the loss function. <ref type="bibr" target="#b37">[37]</ref> presented a dual discriminator GAN that combines two discriminators in a weighted sum.</p><p>New loss functions are not the only way of improving GAN's framework. DCGAN <ref type="bibr" target="#b38">[38]</ref>, one of the first and more significant improvements in the GAN architecture, was the incorporation of deep convolutional networks. The Progressive Growing GAN <ref type="bibr" target="#b21">[21]</ref> was created based on <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">16]</ref> with the main idea of progressively adding new layers of higher resolution during training, which helps to create highly realistic images. <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b42">42]</ref> developed neural architecture search methods to find an optimal neural network architecture to train GAN for a particular task.</p><p>There are many works dedicated to the conditional GAN, for example BigGAN <ref type="bibr" target="#b5">[5]</ref> which utilized a model with a large number of parameters and larger batch sizes showing a significant benefit of scaling.</p><p>There are many works devoted to improving or analyzing GAN training. <ref type="bibr" target="#b33">[33]</ref> trained the generator by optimizing a loss function unrolled from several training iterations of the discriminator training. SN-GAN <ref type="bibr" target="#b34">[34]</ref>, normalized the spectral norm of each weight to stabilize the training. Recent work <ref type="bibr" target="#b40">[40]</ref> introduced stable rank normalization that simultaneously controls the Lipschitz constant and the stable rank of a layer. <ref type="bibr" target="#b27">[27]</ref> developed an analysis to suggest that first-order approximations of the discriminator lead to instability and mode collapse. <ref type="bibr" target="#b36">[36]</ref> proved local stability under the model that both the generator and the discriminator are updated simultaneously via gradient descent. <ref type="bibr" target="#b9">[9]</ref> analyzed the stability of GANs through stationarity of the generator. <ref type="bibr" target="#b32">[32]</ref> points out that absolute continuity is necessary for GAN training to converge. Relativistic GAN <ref type="bibr" target="#b20">[20]</ref> addressed the observation that with generator training increasing the probability that fake data is real, the probability of real data being real would decrease. <ref type="bibr" target="#b1">[2]</ref> proposed a method of re-weighting training samples to correct for mass shift between the transferred distributions in the domain transfer setup. <ref type="bibr" target="#b7">[7]</ref> viewed GAN as an energy-based model and proposed an MCMC sampling based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adaptive Weighted Discriminator</head><p>In GAN training, if we maximize L D to convergence in training discriminator D, we should meet the goal to increase both L r and L f . However, in practice, we train with a gradient ascent step along ?L D = ?L r + ?L f , which may be dominated by either ?L r or ?L f . Then, the training may be done primarily on one of the losses, either L r or L f . Consider a gradient ascent training iteration for L D ,</p><formula xml:id="formula_2">? 1 ?? ? 0 + ??L D ,<label>(3)</label></formula><p>where ? is a learning rate. Then using the Taylor Theorem, we can expand both L r and L f about ? 0 ,</p><formula xml:id="formula_3">L r (? 1 ) = L r (? 0 ) + ??L T r ?L D + O(? 2 ) (4) = L r (? 0 ) + ? ?L r 2 ?L D 2 cos (? 2 (?L r , ?L D )) + O(? 2 )<label>(5)</label></formula><p>and</p><formula xml:id="formula_4">L f (? 1 ) = L f (? 0 ) + ? ?L f 2 ?L D 2 cos (? 2 (?L f , ?L D )) + O(? 2 ),<label>(6)</label></formula><p>where we have omitted the evaluation point ? 0 in all gradients (i.e. ?L * = ?L * (? 0 )) to avoid cumbersome expressions. If one of ? 2 (?L r , ?L D ) and ? 2 (?L f , ?L D ) is obtuse, then to the first order approximation, the corresponding loss is decreased. This causes a decrease in the discriminator assigning a correct score D(?) to the real (or fake) data. Thus, a gradient ascent step with loss (1) may turn out to decrease one of the losses if the angle</p><formula xml:id="formula_5">? 2 (?L r , ?L f ) &gt; 90 ? .</formula><p>This situation occurs quite often in GAN training; see ?5.1 for some experimental results illustrating this. This undesirable situation is expected to happen in GAN training when the generator has produced samples close to the data distribution or its certain modes. If a training step in the direction ?L D results in an increase in the fake loss or equivalently a decrease in the discriminator scores D(G(z)) on the fake data, it will decrease the scores D(x) on the real data as well by the continuity of D(?). Equivalently, this reduces the real loss. With the updated discriminator assigning lower scores to the regions of the data where the generator previously approximated well, the generator update using the new discriminator will likely move in the direction where the discriminator scores are higher and hence leave the region it was converging to. We suggest that this is one of the causes of instability in GAN training. If the regions with high discriminator scores contain only a few modes of the data distribution, this leads to mode collapse; see the study in ?5.3.</p><p>To remedy this situation, we propose to modify the training gradient ?L D to encourage high discriminator scores for real data. We propose a new family of discriminator loss functions, which we call adaptive weighted loss function or aw-loss function; see equation <ref type="bibr" target="#b1">(2)</ref>.</p><p>We first show that the proposed weighted discriminator (2) with fixed weights carries the same theoretical properties of the original GAN as stated in <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b32">32]</ref> for binarycross-entropy loss function, i.e. when the min-max problem is solved exactly, we recover the data distribution. Theorem 1. Let p d (x) and p g (x) be the density functions for the data and model distributions,</p><formula xml:id="formula_6">P d and P g , respec- tively. Consider L aw (D, p g ) = w r E x?p d log D(x) + w f E x?pg log(1 ? D(x)) with fixed w r , w f &gt; 0. 1. Given a fixed p g (x), L aw (D, p g ) is maximized by D * (x) = wrp d (x) wrp d (x)+w f pg(x) for x ? supp(p d ) ? supp(p g ). 2. min pg max D L aw (D, p g ) = w r log wr wr+w f + w f log w f wr+w f with the minimum attained by p g (x) = p d (x).</formula><p>See Appendix A for a proof of Theorem 1. To choose the weights w r and w f , we propose an adaptive scheme, where the weights w r and w f are determined using gradient information of both L r and L f . This structure allows us to adjust the direction of the gradient of the discriminator loss function to achieve the goal of training to increase both L r and L f , or at least not to decrease either loss. We propose Algorithm 1 based on the following gradient relations with various weight choices.</p><formula xml:id="formula_7">Theorem 2. Consider L aw D in (2) and the gradient ?L aw D . 1. If w r = 1 ?Lr 2 and w f = 1 ?L f 2 , then ?L aw D is the angle bisector of ?L r and ?L f , i.e. ? 2 (?L aw D , ?L r ) = ? 2 (?L aw D , ?L f ) = ? 2 (?L r , ?L f ) /2. 2. If w r = 1 ?Lr 2 and w f = ? ?Lr,?L f 2 ?L f</formula><p>See Appendix A for a proof of Theorem 2. The first case in the above theorem allows us to choose weights for <ref type="bibr" target="#b1">(2)</ref> such that we can train L r and L f by going in the direction of the angle bisector. However, sometimes the direction of the angle bisector might not be optimal. For example, if the angle between ?L r and ?L f is close to 180 ? , then the bisector direction will effectively not train either loss. During training, L f is often easier to train than L r meaning that the fake gradient has a larger magnitude. In this situation, we might want to train just on the real gradient direction by simply choosing w f = 0, but if the angle between ?L r and ?L f is obtuse, we will increase L r but significantly decrease L f which is undesirable. The second case in Theorem 2 suggests a direction that forms an acute angle with ?L r and orthogonal to ?L f (see <ref type="figure" target="#fig_1">Figure 1)</ref>; such a direction will increase L r and to the first order approximation will leave L f unchanged. When L r is high, the third case in Theorem 2 would allow us to increase L f while minimizing changes to L r .</p><p>Inspired by the Theorem 2 and observations that we have made, we can calibrate discriminator training in a way that produces and maintains high real loss to reduce fluctuations in the real loss (or real discriminator scores) to improve stability. Algorithm 1 describes the procedure for updating weights of the aw-loss function in (2) during training using the information of ?L r and ?L f . if ? 2 (?L r , ?L f ) &gt; 90 ? then 13: Algorithm 1 is designed to first avoid, up to the first order approximation, decreasing L r or L f during a gradient ascent iteration. Furthermore, it chooses to favor training real loss unless the mean real score is greater than the mean fake score (i.e. s f ? s r ) and the real mean score is at least ? 1 = 0.5 (i.e. ? 1 ? s r ). Here the mean discriminator scores s r and s f represent the mean probability that the discriminator assigns to x i 's and y j 's respectively as real data. When s r is highly satisfactory with s r ? ? 2 = 0.75 (the midpoint between the minimum probability 0.5 and the maximum probability 1 for correct classifications of real data), we favor training the fake loss; otherwise, we train both equally. By maintaining these training criteria, we will reduce the fluctuations in real and fake discriminator scores and hence avoid instability. See study in ?5.3. Note that we impose a small gap ? = 0.05 in s f ? ? &gt; s r to account for situations when s r is nearly identical to s f .</p><formula xml:id="formula_8">w r = ? ?Lr,?L f 2 ?Lr 2 2 ? ?L f 2 + ?; w f = 1 ?L f 2 + ?;</formula><p>The way we favor training the real or fake loss depends on whether the angle between ?L r and ?L f is obtuse or not. In Algorithm 1, the first and the third cases are concerned with the more frequent situation (see ?5.1 and <ref type="figure" target="#fig_4">Figure  4)</ref> where the angle between ?L r and ?L f is obtuse. These cases are the ones that are developed in Theorem 2. In the first case, we favor training real loss by going in the direction orthogonal to the ?L f , illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. In the third case, we favor the fake loss by going in the direction orthogonal to ?L r . In a similar manner, the second and the fourth cases are concerned with the situation when the angle between ?L r and ?L f is acute. We use the same criteria to decide if training should favor the real or fake directions, but in this case we favor training the real or fake loss by using the direction of the corresponding gradient. Lastly, in the fifth case, it is desirable to increase both s r and s f without either taking priority, so we choose to train in the direction of the angle bisector between ?L r and ?L f .</p><p>The two threshold ? 1 and ? 2 in Algorithm 1 can be treated as hyperparameters. Our ablation studies show that the default ? 1 = 0.5 and ? 2 = 0.75 as discussed earlier are indeed good choices, see Appendix B.</p><p>All weights stated in Algorithm 1 normalize both the real and fake gradients for the purpose of avoiding differ- ently sized gradients, which has the effect of preventing exploding gradients and speeds up training, i.e. achieves better IS and FID with fewer epochs, see <ref type="figure" target="#fig_3">Figure 3</ref>. With this implementation, we implement a linear learning rate decay to ensure convergence. However, aw-method performs well without normalization, and achieves comparable results. We list the detailed results in Appendix B.</p><p>A small constant ? is added to all the weights in Algorithm 1 to avoid numerical discrepancies in cases that would prevent the discriminator model from training/updating. As an example, there are cases when our algorithm would set w r = 0 but at the same time ?L f would be almost zero, which will result in ?L aw D being practically zero. We have set ? = 0.05 in all of our experiments.</p><p>Algorithm 1 has a small computational overhead. At each iteration we compute inner products and norms that are used for computing w r and w f , and then use these weight to update ?L aw D . If we have k trainable parameters, then it takes an order of 6k operations to compute inner products between real-fake, real-real, and fake-fake gradients, and an order of 3k operations to form ?L aw D , totalling to an order of 9k operations in Algorithm 1. This is a fraction of total computational complexity for one training iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments &amp; Results</head><p>We implement our Adaptive Weighted Discriminator for SN-GAN <ref type="bibr" target="#b34">[34]</ref> and AutoGAN <ref type="bibr" target="#b14">[14]</ref> models, and for SN-GAN <ref type="bibr" target="#b34">[34]</ref> and BigGAN <ref type="bibr" target="#b5">[5]</ref> models, on unconditional and conditional image generating tasks, respectively (commonly referred to as unconditional and conditional GANs). AutoGAN is an architecture based on neural search. In our experiments; we do not invoke a neural search with our awloss, we have simply implemented the aw-method on the model and architecture exactly provided by <ref type="bibr" target="#b14">[14]</ref>.</p><p>We test our method on three datasets: CIFAR-10 <ref type="bibr" target="#b25">[25]</ref>, STL-10 <ref type="bibr" target="#b10">[10]</ref>, and CIFAR-100 <ref type="bibr" target="#b25">[25]</ref>. The datasets and implementation details are provided in Appendix B. We present the implementation with normalized gradients using linear learning rate decay. We also give results of non-normalized version without learning rate decay in Appendix B.</p><p>All of the above mentioned models train the discriminator by minimizing the negative hinge loss <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b43">43]</ref>. Our aw-loss also uses the negative hinge loss as follows:</p><formula xml:id="formula_9">L aw D = ? w r ? E x?p d min(0, D(x) ? 1) ? w f ? E z?pz min(0, ?1 ? D(G(z)) ,<label>(7)</label></formula><p>with w r and w f updated every iteration using Algorithm 1.</p><p>To evaluate the performance of the models, we employ the widely used Inception Score <ref type="bibr" target="#b39">[39]</ref> (IS) and Fr?chet Inception Distance <ref type="bibr" target="#b18">[18]</ref> (FID) metrics; see <ref type="bibr" target="#b29">[29]</ref> for more details. We compute these metrics every 5 epochs and we report the best IS and FID achieved by each model within the 320 (SN-GAN), 300 (AutoGAN), and BigGAN (1,000) training epochs as in the corresponding original works.</p><p>We first present the results for the unconditional GAN for the datasets CIFAR-10, STL-10 and CIFAR-100 in Tables 1, 2, and 3 respectively. In addition to baseline results, we have included top published results for each dataset for comparison purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>IS ? FID ? Imp. MMD GAN <ref type="bibr" target="#b46">[46]</ref> 8  For CIFAR-10 in  For STL-10 in <ref type="table" target="#tab_2">Table 2</ref>, our methods also significantly improve SN-GAN and AutoGAN baseline results. Our aw-SN-GAN achieved the highest IS and aw-AutoGAN achieved the lowest FID score among comparisons.  <ref type="table">Table 3</ref>: Unconditional GAN: CIFAR-100; * -results from our test; ? -quoted from <ref type="bibr" target="#b44">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>For CIFAR-100 in <ref type="table">Table 3</ref>, our methods improve IS sig-nificantly for AutoGAN but modestly for SN-GAN. Our aw-Auto-GAN achieved the highest IS and the lowest FID score among comparisons. We have also included some visual examples that were randomly generated by our aw-Auto-GAN model in <ref type="figure" target="#fig_2">Figure  2</ref>. We also consider the convergence of our method against training epochs by plotting in <ref type="figure" target="#fig_3">Figure 3</ref> the IS and FID scores of 50,000 generated samples at every 5 epochs for Auto-GAN vs aw-AutoGAN. For all the datasets, our model consistently achieves faster convergence than the baseline.</p><p>We next consider our aw-method for a class conditional image generating task using two base models, SN-GAN <ref type="bibr" target="#b34">[34]</ref> and BigGAN <ref type="bibr" target="#b5">[5]</ref>, on CIFAR-10 and CIFAR-100 datasets. Results are listed in <ref type="table">Table 4</ref>.  <ref type="table">Table 4</ref>: Conditional GAN: CIFAR-10 and CIFAR-100; ? -quoted from <ref type="bibr" target="#b41">[41]</ref>; BigGAN <ref type="bibr" target="#b5">[5]</ref> CIFAR-100 results based on our test using the code from <ref type="bibr" target="#b4">[4]</ref>. <ref type="table">Table 4</ref> shows that our method works well for the conditional GAN too. The aw-method significantly improves the SN-GAN and BigGAN baselines. Indeed, our aw-BigGAN achieved the best FID for both CIFAR-10 and CIFAR-100 among comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Exploratory &amp; Ablation Studies</head><p>In this section, we present three studies to illustrate potential problems of equally weighted GAN loss and advan-tages of our adaptive weighted loss. The hinge loss is implemented in the first and second studies, and a binary crossentropy loss function is used for the third. In the first study, we examine the angles between ?L r , ?L f , ?L D (or ?L aw D ). We use the CIFAR-10 dataset with the DCGAN architecture <ref type="bibr" target="#b38">[38]</ref> and we look at 50 iterations in the first epoch of training. In <ref type="figure" target="#fig_4">Figure 4</ref>, we plot the following 3 angles: ? 2 (?L r , ?L f ), ? 2 (?L r , ?L D ) and ? 2 (?L f , ?L D ) against iterations for the original loss L D (1) on the top and for the aw-loss L aw D on the bottom. For the original loss (Left), ? 2 (?L r , ?L f ) (blue) stays greater then 90 ? , closer to 180 ? . ? 2 (?L r , ?L D ) (green) often goes above 90 ? and so the training is often done to decrease the real loss. ? 2 (?L f , ?L D ) also goes above 90 ? , though to a lesser extent. With the aw-loss (Right), ? 2 (?L r , ?L aw D ) and ? 2 (?L f , ?L aw D ) stay below the 90 ? line and indicate that we train in the direction of ?L r and orthogonal to ?L f in most steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Angles between Gradients</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Real Discriminator Scores and Real-Fake Gap after Training</head><p>Our second experiment is an ablation study to show that aw-loss increases the discriminator scores for real data and increases the gap between real and fake discriminator scores. We again apply the DCGAN model with the original loss L D to CIFAR-10 and at every iteration we examine the mean discriminator score for the mini-batch of the real set and the mean discriminator scores for the mini-batch of the fake dataset generated by the generator. We use the logit output of the discriminator network as the score. We plot these two mean scores against each iteration before training in the first row of <ref type="figure">Figure 5</ref> and after training (with the original loss L D ) in the second row. At each of the above training iterations, we replace L D by the aw-loss L aw D (2) and train for one iteration with the same training mini-batch. We plot the mean discriminator scores for the mini-batches of the real and fake dataset after this training in the third row of <ref type="figure">Figure 5</ref>. We further present the gaps between the two scores before training and after training using the original loss and using aw-loss in the fourth row of <ref type="figure">Figure 5</ref>. <ref type="figure">Figure 5</ref> shows that training with aw-loss leads to higher real discriminator scores (0.921 epoch average) than training with the original loss (0.248 epoch average). The average gap between real and fake scores is also larger with the aw-loss at 1.413 vs. 1.262 of the original loss. Therefore, with the same model and the same training mini-batch, the aw-loss produces higher discriminator scores for the real dataset and larger gaps between real and fake scores. These are two important properties of a discriminator for the generator training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Instability and Real Discriminator Scores</head><p>Our third study examines benefits of high discriminator scores for a real dataset with respect to instability and mode collapse of GAN training. We use a synthetic dataset with a mixture of Gaussian distributions used to test unrolled GAN in <ref type="bibr" target="#b33">[33]</ref>. The dataset consists of eight 2D Gaussian distributions centered at eight equally distanced points on the unit circle. We train with a plain GAN as in <ref type="bibr" target="#b33">[33]</ref> and we plot samples of (fake) data generated by the generator every 5,000 iterations on the first row of of <ref type="figure" target="#fig_6">Figure 6</ref>. We see that the generated data converging to two or three points but then moving off, demonstrating instability and mode collapse. To understand this phenomenon, at each of the iterations that we study in <ref type="figure" target="#fig_6">Figure 6</ref>, we generate 100 (real) data points from each of the eight classes and compute their mean discriminator scores (as the logit output of the discriminator). We plot the mean scores against the classes in the second row of <ref type="figure" target="#fig_6">Figure 6</ref>. We observe that the discriminator scores for the real data do not increase much during training, staying around 0, which corresponds to 0.5 probability after the logistic sigmoid function. The scores are also uneven among different classes. We believe these cause the instability in the generator training.</p><p>We compare the GAN results with aw-GAN that applies our adaptive weighted discriminator to the plain GAN and we present the corresponding plots of generated data points (fake) in the third row of <ref type="figure" target="#fig_6">Figure 6</ref> and the corresponding discriminator scores on the eight classes in the bottom row. In this case, the generator gradually converges to all eight classes and the discriminator scores stay high for all eight classes. Even though the generator was starting to converge to a few classes (step 5,000), the discriminator scores remain high for all classes. Then the generator continues to converge while convergence to other classes occurs. We believe the high real discriminator scores maintains stability and prevents mode collapse in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>This paper pinpoints potential negative effects of the traditional GAN training on the real loss (and fake loss) and points out that this is a potential cause of instability and mode collapse. To remedy these issues, we have proposed the Adaptive Weighted Discriminator method to increase and maintain high real loss. Our experiments demonstrate the benefits and the competitiveness of this method.</p><formula xml:id="formula_10">1 ?L r 2 L r + 1 ?L f 2 L f .<label>(21)</label></formula><p>Using the definition of Euclidean inner product, cos (? 2 (?L aw D , ?L r )) = ?L aw D , ?L r 2 ?L aw D 2 ?L r 2 (1 + cos (? 2 (?L r , ?L f ))) (30) = 1 + cos (? 2 (?L r , ?L f )) 2 <ref type="bibr" target="#b31">(31)</ref> = cos (? 2 (?L r , ?L f ) /2) .</p><p>Thus, ? 2 (?L aw D , ?L r ) = ? 2 (?L aw D , ?L f ) = ? 2 (?L r , ?L f ) /2. </p><p>Using this aw-loss function, we have </p><formula xml:id="formula_14">?L aw D , ?L f 2 = 1 ?L r 2 ?L r ? ?L r , ?L f 2 ?L f 2 2 ?L r 2 ?L f , ?L f 2 (34) = ?L r , ?L f 2 ?L r 2 ? ?L r , ?L f 2 ?L f , ?L f 2 ?L f 2 2 ?L r 2 (35) = ?L r , ?L f 2 ?L r 2 ? ?L r , ?L f 2 ?L r 2 = 0,<label>(36)</label></formula><p>= ?L r 2 ? ?L r 2 = 0.</p><p>Thus, ? 2 (?L aw D , ?L f ) = 90 ? and ? 2 (?L aw D , ?L r ) ? 90 ? .</p><p>3. If w r = ? ?Lr,?L f 2 ?Lr 2 2 ? ?L f 2</p><formula xml:id="formula_17">and w f = 1 ?L f 2 then L aw D = ? ?L r , ?L f 2 ?L r 2 2 ? ?L f 2 L r + 1 ?L f 2 L f ,<label>(41)</label></formula><p>and similar argument as above proves that ? 2 (?L aw D , ?L r ) = 90 ? and ? 2 (?L aw D , ?L f ) ? 90 ? .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 : 1 : 11 :</head><label>1111</label><figDesc>Adaptive weighted discriminator method for one step of discriminator training. Given: P d and P g -data and model distributions; 2: Given: ? 1 = 0.5, ? 2 = 0.75, ? = 0.05, ? = 0.05; 3: Sample: x 1 , . . . , x n ? P d and y 1 , . . . , y n ? P g ;4:  Compute: ?L r , ?L f , s r = 1 n n i=1 ?(D(x i )), s f = 1 n n j=1 ?(D(y j )); 5: if s r &lt; s f ? ? or s r &lt; ? 1 then6:    if ? 2 (?L r , ?L f ) &gt; 90 ? then else if s r &gt; s f ? ? and s r &gt; ? 2 then 12:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Depiction of the second case of Theorem 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>aw-AutoGAN: CIFAR-10 (left), STL-10 (center), CIFAR-100 (right); samples randomly generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>AutoGAN vs aw-AutoGAN IS and FID plots for the first 40 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Angles between gradients at each iteration. Top: original loss; Bottom: aw-loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 D 1 DFigure 5 :</head><label>115</label><figDesc>(x) (av.=0.786) D(G(z)) (av.=0.022) ) (av.=0.248) D(G(z)) (av.=-1.014) (x) (av.=0.921) D(G(z)) (av.=-0.493) .=0.765) GAOT (av.=1.262) GAAWT (av.=1.413) Mean discriminator scores for real data D(x) and fake data D(G(z)) (Row 1: before training, Row 2: after original training with L D , Row 3: after training with awloss L aw D ) and their gap (Row 4: GBT -gap before training; GAOT -gap after original training; GAAWT -gap after awloss training)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Mixture of eight 2D Gaussian distributions centered at 8 points (right-most column). Row 1: GAN sample points produced by generators; Row 2: GAN mean discriminator scores for each of 8 classes; Row 3: aw-GAN sample points produced by generators; Row 4: aw-GAN mean discriminator scores for each of eight classes;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>?L f 2 ?L f 2 2</head><label>22</label><figDesc>?L r 2 L f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">, our methods significantly</cell></row><row><cell cols="3">improve the baseline results. Indeed, our aw-AutoGAN</cell></row><row><cell cols="3">achieves IS substantially above all comparisons other than</cell></row><row><cell cols="3">StyleGAN2. StyleGAN2 outperforms aw-AutoGAN but</cell></row><row><cell cols="3">uses 26.2M parameters vs. 5.4M for aw-AutoGAN.</cell></row><row><cell>Method ProbGAN [17]</cell><cell cols="2">IS ? 8.87?.09 46.74 FID ?</cell></row><row><cell>Imp. MMD GAN [46]</cell><cell>9.34</cell><cell>37.63</cell></row><row><cell>MGAN [19]</cell><cell>9.22?.11</cell><cell>-</cell></row><row><cell>MSGAN [44]</cell><cell>-</cell><cell>27.10</cell></row><row><cell>SN-GAN [34]</cell><cell cols="2">9.10?.04 40.10</cell></row><row><cell>aw-SN-GAN (Ours)</cell><cell cols="2">9.53?.10 36.41</cell></row><row><cell>AutoGAN [14]</cell><cell cols="2">9.16?.12 31.01</cell></row><row><cell>aw-AutoGAN (Ours)</cell><cell cols="2">9.41?.09 26.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Unconditional GAN: STL-10.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>?L r , ?L r 2 + ?L f , ?L r 2 ?L r , ?L f 2 + ?L f , ?L f 2 ?L r , ?L f 2 ?L aw D 2 ?L r 2 ?L f 2(27)We can rewrite ?L awD 2 in term of ? 2 (?L r , ?L f ), that is ?L r , ?L f 2 ?L r 2 ?L f 2 = 2 (1 + cos (? 2 (?L r , ?L f ))) .(28)Notice that (24) can be rewritten using ? 2 (?L r , ?L f ) as</figDesc><table><row><cell></cell><cell>=</cell><cell cols="2">1 ?L aw D 2</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>= =</cell><cell cols="2">?Lr 2 1 ?L aw D 2</cell><cell>+</cell><cell cols="2">?L f 2 D 2 ?L r 2 ?L aw ?L r , ?L f 2 ?L aw D 2 ?L r 2 ?L f 2</cell><cell>(23) (24)</cell></row><row><cell>cos (? 2 (?L aw D , ?L f )) =</cell><cell cols="4">?L aw D , ?L f 2 ?L aw D 2 ?L f 2 1</cell><cell>1</cell><cell>(25)</cell></row><row><cell>= =</cell><cell cols="2">?Lr 2 1 D 2 ?L aw</cell><cell>+</cell><cell cols="2">?L f 2 D 2 ?L f 2 ?L aw</cell><cell>(26)</cell></row><row><cell cols="6">?L aw D 2 cos (? 2 (?L aw 2 2 = ?L aw D 2 D , ?L aw = 1 ?L r 2 ?L r + 1 ?L f 2 ?L f , ?L r 2 1 = ?L r , ?L r 2 ?L r 2 2 + ?L f , ?L f 2 ?L f 2 2 + D , ?L r )) = 1 ?L aw D 2 + ?L r , ?L f 2 ?L r + ?L f 2 1 ?L aw D 2 ?L r 2 ?L f 2 ?L f</cell><cell>2</cell><cell>(29)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Devin Willmott and Xin Xing for their initial efforts and helpful advice. We thank Rebecca Calvert for reading the manuscript and providing us with many valuable comments/suggestions. We also thank the University of Kentucky Center for Computational Sciences and Information Technology Services Research Computing for their support and use of the Lipscomb Compute Cluster.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>Appendix A.</p><p>A.1 Proof of Theorem 1 Theorem 1. Let p d (x) and p g (x) be the density functions for the data and model distributions, P d and P g , respectively. Consider L aw (D, p g ) = w r E x?p d log D(x) + w f E x?pg log(1 ? D(x)) with fixed w r , w f &gt; 0.</p><p>1. Given a fixed p g (x), L aw (D, p g ) is maximized by D * (x) = wrp d (x) wrp d (x)+w f pg(x) for x ? supp(p d ) ? supp(p g ).</p><p>2. min pg max D L aw (D, p g ) = w r log wr wr+w f + w f log w f wr+w f with the minimum attained by p g (x) = p d (x).</p><p>Proof.</p><p>1. First, the function f (t) = a log t + b log(1 ? t) has its maximum in [0, 1] at t = a a+b . Given a fixed p g (x), w r &gt; 0 and w f &gt; 0.</p><p>.</p><p>where the equality holds if</p><p>On the other hand, </p><p>and minimum is attained when p g = p d . </p><p>2. If w r = 1 ?Lr 2</p><p>3.</p><p>Proof.</p><p>B.1 Ablation study of ? 1 and ? 2 parameters We have considered the choice of ? 1 and ? 2 by experimenting with aw-AutoGAN models on the CIFAR-10 dataset. We recorded IS and FID scores after the first and the fifth epochs where the models were trained with the parameters in the grids shown in <ref type="figure">Figure 7</ref> with all other settings fixed. We present the IS and FID scores in heat map plots in <ref type="figure">Figure 7</ref>. The results show that neighbors around the point (0.5, 0.75) lead to good scores; the point (0.5, 0.75) itself produces one of the highest IS and one of the lowest FID. In particular, the performance is not too sensitive to the selections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Dataset and Implementation Details</head><p>We test our aw-method on the following datasets:</p><p>? The CIFAR-10 dataset <ref type="bibr" target="#b25">[25]</ref> consists of 60,000 color images with 50,000 for training and 10,000 for testing. All images have resolution 32 ? 32 pixels and are divided equally into 10 classes, with 6,000 images per class. No data augmentation;</p><p>? The STL-10 is a dataset proposed in <ref type="bibr" target="#b10">[10]</ref> and designed for image recognition and unsupervised learning. STL-10 consists of 100,000 unlabeled images with 96 ? 96 pixels and split into 10 classes. All images are resized to 48 ? 48 pixels, without any other data augmentation;</p><p>? The CIFAR-100 from <ref type="bibr" target="#b25">[25]</ref> is a dataset similar to CIFAR-10 that consists of 60,000 color 32 ? 32 pixel images that are divided into 100 classes. No data augmentation.</p><p>We follow the original implementations of SN-GAN, AutoGAN and BigGAN-PyTorch <ref type="bibr" target="#b4">[4]</ref> that use the following hyperparameters:</p><p>? Generator: learning rate: 0.0002; batch size: 128 (SN-GAN, AutoGAN) and 50 (BigGAN); optimizer: Adam optimizer with ? 1 = 0 and ? 2 = 0.999 <ref type="bibr" target="#b24">[24]</ref>; loss: hinge <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b43">43]</ref>; spectral normalization: False; learning rate decay: linear; # of training epochs: 320 (SN-GAN), 300 (AutoGAN) and 1000 (BigGAN);</p><p>? Discriminator: learning rate: 0.0002; batch size: 64 (SN-GAN, AutoGAN) and 50 (BigGAN); optimizer: Adam optimizer with ? 1 = 0 and ? 2 = 0.999; loss: hinge; spectral normalization: True; learning rate decay: linear; training iterations ratio: 3 (SN-GAN) and 2 <ref type="figure">(AutoGAN, BigGAN)</ref>.</p><p>Experiments based on SN-GAN and AutoGAN models are performed on a single NVIDIA QUADRO P5000 GPU running Python 3.6.9 with PyTorch v1.1.0 for AutoGAN based models and Chainner v4.5.0 for SN-GAN based models. Experiments based on BigGAN-Pytorch <ref type="bibr" target="#b4">[4]</ref> model are performed on two NVIDIA Tesla V100 GPU running Python 3.6.12 with PyTorch v1.4.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Aw-method with non-normalized gradients</head><p>In section 3, we introduced Algorithm 1 which was developed using Theorem 2 where the weights w r and w f include normalization of the gradients ?L r and ?L f . This is not necessary and we can consider using non-normalized gradients directly where one of the weights is chosen as 1. The corresponding results are stated as the following theorem. , then</p><p>2. If w r = ? ?Lr,?L f 2 ?Lr 2 2 and w f = 1, then</p><p>Proof. Identical to the proof of Theorem 2.</p><p>Similar to section 3, we have developed Algorithm 2 using Theorem 3. The key difference between Algorithms 1 and 2 is normalization of the gradients; the rest of the algorithm is unchanged including the values for ? 1 = 0.5, ? 2 = 0.75, ? = 0.05 and ? = 0.05.</p><p>Similar to Section 4, we tested Algorithm 2 on unconditional and conditional image generating tasks, with results provided in <ref type="table">Tables 5 and 6</ref>, respectively. The implementation details are the same as in Section 4.</p><p>For the unconditional GAN, we tested our non-normalized aw-method on SN-GAN and AutoGAN baselines on three datasets: CIFAR-10, STL-10, and CIFAR-100. Our non-normalized methods also significantly improve SN-GAN baseline, in particular achieving the highest IS for the STL-10 dataset among comparisons in <ref type="table">Table 2</ref>, and aw-AutoGAN non-normalized models improve the baseline AutoGAN models as well on all the datasets.</p><p>For the conditional GAN, we tested our non-normalized aw-method on SN-GAN and BigGAN models as baselines on CIFAR-10 and CIFAR-100 datasets. Both non-normalized aw-SN-GAN and aw-BigGAN significantly improve baseline models for both datasets.</p><p>On average, normalized weights achieve better results than non-normalized ones. We advocate the normalized version (Algorithm 1), but both produce quite competitive results and should be considered in implementations.</p><p>Algorithm 2: Adaptive weighted discriminator method w/o normalization for one step of discriminator training. 1: Given: P d and P g -data and model distributions; 2: Given: ? 1 = 0.5, ? 2 = 0.75, ? = 0.05, ? = 0.05; 3: Sample: x 1 , . . . , x n ? P d and y 1 , . . . , y n ? P g ; <ref type="bibr">4:</ref> Compute: ?L r , ?L f , s r = 1 n n i=1 ?(D(x i )), s f = 1 n n j=1 ?(D(y j )); 5: if s r &lt; s f ? ? or s r &lt; ? 1 then <ref type="bibr">6:</ref> if ? 2 (?L r , ?L f ) &gt; 90 ? then 7:   <ref type="table">Table 6</ref>: Conditional GAN: CIFAR-10 and CIFAR-100 scores for the normalized (Algorithm 1) and non-normalized (Algorithm 2) versions of aw-method; ? -quoted from <ref type="bibr" target="#b41">[41]</ref>; * -results from our tests based on <ref type="bibr" target="#b4">[4]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno>PMLR. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikolaj</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Batch weight for domain adaptation with mass shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1844" to="1853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miko?aj</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Demystifying mmd gans. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biggan-Pytorch</surname></persName>
		</author>
		<ptr target="https://github.com/ajbrock/BigGAN-PyTorch.6" />
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02136</idno>
		<title level="m">Mode regularized generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Your gan is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12146" to="12155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smoothness and stability in gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Minami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR. 5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Geoffrey Gordon, David Dunson, and Miroslav Dud?k</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Importance weighted generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Diesendruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ethan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinead</forename><forename type="middle">A</forename><surname>Shakkottai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="249" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards a deeper understanding of adversarial losses under a discriminative adversarial network setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Degas: Differentiable efficient generator search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Doveh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autogan: Neural architecture search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probgan: Towards probabilistic gan with theoretical guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-He</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MGAN: Training generative adversarial nets with multiple generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Tu Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The relativistic discriminator: a key element missing from standard GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A multi-class hinge loss for conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="1290" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mmd gan: Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2203" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the limitations of first-order approximation in GAN dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<idno>PMLR. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
	<note>Jennifer Dy and Andreas Krause</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Are gans created equal? a largescale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc. 5</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y K</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The numerics of gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1825" to="1835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient descent gan optimization is locally stable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishnavh</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5585" to="5595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Dual discriminator generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Tu Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><forename type="middle">Q</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
		<idno>abs/1709.03831</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amartya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet K</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dokania</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04659</idno>
		<title level="m">Stable rank normalization for improved generalization in neural networks and gans</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">How good is my gan?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Off-policy reinforcement learning for efficient and effective gan architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Fink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">supervised gan: Analysis and improvement with multi-class minimax game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-Hung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Ngoc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Coulomb gans: Provably optimal nash equilibria via potential fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?nter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>abs/1708.08819</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving MMD-GAN training with repulsive loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saman</forename><surname>Halgamuge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
