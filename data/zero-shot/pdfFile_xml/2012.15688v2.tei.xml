<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ERNIE-DOC: A Retrospective Long-Document Modeling Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Ding</surname></persName>
							<email>dingsiyu@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
							<email>shangjunyuan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
							<email>wangshuohuan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
							<email>tianhao@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wuhua@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<email>wanghaifeng@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ERNIE-DOC: A Retrospective Long-Document Modeling Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-DOC, a document-level language pretraining model based on Recurrence Transformers . Two welldesigned techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-DOC 1 , which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-DOC to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-DOC improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> have achieved remarkable improvements in a wide range of natural language tasks, including language modeling , text classification , and question answering <ref type="bibr" target="#b12">(Devlin et al., 2018;</ref>. This success is largely due to the self-attention mechanism, which enables the network to capture contextual information from the *indicates equal contribution. 1 Source code and pre-trained checkpoints can be found at https://github.com/PaddlePaddle/ERNIE/ tree/repro/ernie-doc. Figure 1: Available contextual information utilized by Transformer variants, where a long document D is partitioned into three segments S i (i ? [1, 2, 3]). When training on S 2 , (a) and (b) optimize the pretraining objective depending only on the contextual information from the current segment or segments in the forward pass, whereas ERNIE-DOC utilizes the contextual information of the entire document for each segment. entire input sequence. Nevertheless, the memory usage and computation complexity caused by the self-attention mechanism grows quadratically with the sequence length, incurring excessive cost when processing a long document on existing hardware. Currently, the most prominent pretrained models, such as BERT <ref type="bibr" target="#b12">(Devlin et al., 2018)</ref>, are used on fixed-length input segments of a maximum of 512 tokens owing to the aforementioned limitation. Thus, a long document input must be partitioned into smaller segments of manageable sizes. However, this leads to the loss of important crosssegment information, that is, the context fragmentation problem , as shown in <ref type="figure">Fig. 1(a)</ref>. To mitigate the problem of insufficient interactions among the partitioned segments of long documents, Recurrence Transformers <ref type="bibr" target="#b26">Rae et al., 2019)</ref> permit the use of contextual information from previous segments in computing the hidden states for a new segment by maintaining a memory component from the previous activation; this enables the modeling of long documents. In addition, Sparse Attention Transformers <ref type="bibr" target="#b35">Tay et al., 2020;</ref><ref type="bibr" target="#b3">Beltagy et al., 2020;</ref><ref type="bibr">Zaheer et al., 2020)</ref> focus on reducing the complexity of self-attention operations to explicitly improve the modeling length, but only up to a restricted context length (4,096) due to resource limitations.</p><p>We argue that existing strategies are not sufficiently effective or reliable, because the contextual information of a complete document is still not available for each segment during the training phase. As depicted in <ref type="figure">Fig. 1</ref>, when training on segment S 2 , the model is ideally optimized by maximizing P (y | (S 1 , S 2 , S 3 )) conditioned on the contextual information of the entire document D = {S 1 , S 2 , S 3 }, in contrast to the following suboptimal solutions: P (y | S 2 ) for Vanilla/Sparse Transformers 2 and P (y | (S 1 , S 2 )) for Recurrence Transformers.</p><p>To address this limitation, we propose ERNIE-DOC (A Retrospective Long-Document Modeling Transformer) based on the Recurrence Transformer paradigm. Inspired by the human reading behavior of skimming a document first and then looking back upon it attentively, we design a retrospective feed mechanism in which segments from a document are fed twice as input. As a result, each segment in the retrospective phase could explicitly fuse the semantic information of the entire document learned in the skimming phase, which prevents context fragmentation.</p><p>However, simply incorporating the retrospective feed mechanism into Recurrence Transformers is infeasible because the maximum effective context length is limited by the number of layers , as shown in <ref type="figure">Fig. 1 (b)</ref>. Thus, we present an enhanced recurrence mechanism, a drop-in replacement for a Recurrence Transformer, by changing the shifting-one-layer-downwards recurrence to the same-layer recurrence. In this manner, the maximum effective context length can be expanded, and past higher-level representations can be exploited to enrich future lower-level representations.</p><p>Moreover, we introduce a segment-reordering objective to pretrain a document-level model. Specifically, it is a document-aware task of predicting the correct order of the permuted set of segments of a document, to model the relationship among segments directly. This allows ERNIE-DOC to build full document representations for prediction. This is analogous to the sentencereordering task in ERNIE 2.0 <ref type="bibr" target="#b32">(Sun et al., 2020b)</ref> but at a segment level of granularity, spanning (commonly) multiple training steps.</p><p>We first evaluate ERNIE-DOC on autoregressive word-level language modeling using the enhanced recurrence mechanism, which, in theory, allows the model to process a document with infinite words. ERNIE-DOC achieves state-of-theart (SOTA) results on the WiKiText-103 benchmark dataset, demonstrating its effectiveness in long-document modeling. Then, to evaluate the potential of ERNIE-DOC on document-level natural language understanding (NLU) tasks, we pretrained the English ERNIE-DOC on the text corpora utilized in BigBird (Zaheer et al., 2020) from the RoBERTa-released checkpoint, and the Chinese ERNIE-DOC on the text corpora utilized in ERNIE 2.0 <ref type="bibr" target="#b32">(Sun et al., 2020b)</ref> from scratch. After pretraining, we fine-tuned ERNIE-DOC on a wide range of English and Chinese downstream tasks, including text classification, question answering and keypharse extraction. Empirically, ERNIE-DOC consistently outperformed RoBERTa on various benchmarks and showed significant improvements over other high-performance long-text pretraining models for most tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sparse Attention Transformers have been extensively explored <ref type="bibr" target="#b35">Tay et al., 2020;</ref><ref type="bibr" target="#b3">Beltagy et al., 2020;</ref><ref type="bibr">Zaheer et al., 2020)</ref>. The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L ? L), where L is the sequence length. Reformer <ref type="bibr" target="#b19">(Kitaev et al., 2020)</ref> further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers <ref type="bibr" target="#b44">(Ye et al., 2019</ref>) employs a binary partition for the input sequence. Recently, Longformer <ref type="bibr" target="#b3">(Beltagy et al., 2020)</ref> and <ref type="bibr">BigBird (Zaheer et al., 2020)</ref> have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in <ref type="bibr">Zaheer et al. (2020)</ref> that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires ?(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use. In this study, we adopt a different approach to adapting Recurrence Transformers for a pretraining-then-finetuning setting, to model a long document.</p><p>Recurrence Transformers <ref type="bibr" target="#b26">Rae et al., 2019)</ref> have been successfully applied in generative language modeling. They employ the Transformer decoder as a parametric model for each conditional distribution in p(x) = L t=1 p(x t |x &lt;t ), where x denotes a text sequence. To capture long dependencies, they process the text in segments from left to right based on the segment recurrence mechanism . This mechanism maintains a memory bank of past activations at each layer to preserve a history of context. Compressive Transformer <ref type="bibr" target="#b26">(Rae et al., 2019)</ref> adds a compressive memory bank to sufficiently store old activations instead of discarding them, which facilitates long-range sequence learning. However, these methods operate from left to right, which limits their capacity for discriminative language understanding tasks that require bidirectional information. XLNet  proposed a permutation language modeling objective to construct bidirectional information and achieve superior performance in multiple NLP tasks; however, its application to long-document modeling tasks remains largely unexplored. ERNIE-DOC builds on the ideas of the Recurrence Transformers to 1) tackle the limitation of Recurrence Transformers for utilizing bidirectional contextual information and 2) improve the behavior of the segment recurrence mechanism to capture longer dependencies.  have enabled significant progress on numerous document-level tasks, such as document summarization  and document ranking . Similar to Vanilla Transformers, Hierarchical Transformers also split long documents into shorter segments with manageable lengths and then feed them independently to produce corresponding segment-level semantic representations. Unlike in Vanilla Transformers, however, separate Transformer layers are used in Hierarchical Transformers to process the concatenation of these representations. Hierarchical Transformers ignore the contextual information from the remaining segments when processing each segment of a long document, thus suffering from the context fragmentation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Transformers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we first describe the background (Sec. 3.1) that ERNIE-DOC builds on. Then, we present the implementation of ERNIE-DOC, including the retrospective feed mechanism in Sec. 3.2, the enhanced recurrence mechanism in Sec. 3.3, and the segment-reordering objective in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>Formally, a long document D is sliced into T sequential segments, denoted as {S 1 , S 2 , ..., S T }, where S ? = {x ?,1 , x ?,2 , ..., x ?,L } is the ? -th segment with L tokens; x denotes a single token. Vanilla, Sparse, and Recurrence Transformers employ different strategies to produce the hidden state h n ? ? R L?d for segment S ? at the n-th layer:</p><formula xml:id="formula_0">h n?1 ? +1 = h n?1 ? +1 , Vanilla or Sparse Transformers [SG(h n?1 ? ) ? h n?1 ? +1 ], Recurrence Transformers, q n ? +1 , k n ? +1 , v n ? +1 = h n?1 ? +1 W q , h n?1 ? +1 W k , h n?1 ? +1 W v . h n ? +1 = Transformer-Block (q n ? +1 , k n ? +1 , v n ? +1 ).<label>(1)</label></formula><p>where q ? R L?d , k, and v ? R (L+m)?d are the query, key and value vectors, respectively with hidden dimension d and memory length m (Note that m = 0 for Vanilla or Sparse Transformers); h n?1 ? +1 ? R (L+m)?d is the extended context; W * ? R d * ?d represents learnable linear projection parameters; the function SG(?) denotes the stop-gradient operation; and the notation [?] denotes the concatenation of two hidden states along the length dimension. In contrast to Vanilla or Sparse Transformers, where h n ? +1 is produced using only itself, Recurrence Transformers introduce a segment-level recurrence mechanism to promote interaction across segments. The hidden state computed for the previous segment h n?1 ? is cached as an auxiliary context to help process the current segment h n ? . However, from the concatenation part in Eq. 1, i.e., [SG(h n?1 ? ) ? h n?1 ? +1 ], there is apparently a constraint that the current hidden state can only fuse information from the previous segments. In   When training on S 4 , it can only fuse the contextual information of the previous two consecutive segments S 2 , S 3 , since the largest effective context length grows linearly w.r.t the number of layers. ERNIE-DOC (lower):The effective context length is much larger aided by the enhanced recurrence mechanism (Sec. 3.3). Thus, S 4 can fuse the information of S 1 discarded by Recurrence Transformers. Moreover, segments in the retrospective phase contains the contextual information of an entire document, powered by the retrospective feed mechanism (Sec. 3.2).</p><p>other words, the contextual information of an entire document is not available for each segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retrospective Feed Mechanism</head><p>ERNIE-DOC employs a retrospective feed mechanism to address the unavailability of the contextual information of a complete document for each segment. The segments from a long document are twice fed as input. Mimicking the human reading behavior, we refer to the first and second inputtaking phases as the skimming and retrospective phases, respectively. In the skimming phase, we employ a recurrence mechanism to cache the hidden states for each segment. In the retrospective phase, we reuse the cached hidden states from the skimming phase to enable bi-directional information flow. Naively, we can rewrite Eq. 1 to obtain the contextual information of an entire document in the skimming phase to be utilized in the retrospective phase as follows,</p><formula xml:id="formula_1">H = [ H 1 1:T ? H 2 1:T ? ? ? ? H N 1:T ], (skim. phase) h n?1 ? +1 = [SG( H ? h n?1 ? ) ? h n?1 ? +1 ], (retro. phase)<label>(2)</label></formula><p>where H ? R (L * T * N )?d denotes the cached hidden states in the skimming phase with T segments, L length of each segment and total N layers, and</p><formula xml:id="formula_2">H i 1:T = [ h i 1 ? h i 2 ? ? ? ? h i T ]</formula><p>is the concatenation of i-th layer's hidden states of the skimming phase. Thus, the extended context h n?1 ? +1 is guaranteed to capture the bidirectional contextual information of the entire document. However, it will incur massive memory and computation cost for directly employing H in self-attention mechanism. Henceforth, the main issue is how H should be implemented in a memory-and computation-efficient manner.</p><p>By rethinking segment-level recurrence , we observe that the largest possible context dependency length increases linearly w.r.t the number of layers (N ). For instance, at i-th layer, h i ? have the longest dependency to h 1 ? ?(i?1) . Thus, to minimize memory and computation consumption, hidden states from the N -th layer (toplayer) are included at a stride of N , which is sufficient to build the contextual information of an entire document. Formally, H can be reduced to</p><formula xml:id="formula_3">H r = [ h N N ? h N 2 * N ? ? ? ? h N T /N * N ] (</formula><p>Note that when T is not evenly divisible by N , the last hidden state h N T need to be included). However, for a long document input, the extra computational and memory cost of H r ? R T /N ?d where T N is still excessive on existing hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Enhanced Recurrence Mechanism</head><p>To effectively utilize the retrospective feed mechanism in practice, an ideal strategy is to ensure that the cached hidden state h n?1 ? already contains the contextual information of an entire document without explicitly taking H or H r as input. Essentially, we should tackle the problem of limited effective context length in the segment-level recurrence mechanisms. Herein, we introduce the enhanced recurrence mechanism, a drop-in replacement for the segment-level recurrence mechanism, by changing the shifting-one-layer-downwards recurrence to the same-layer recurrence as follows:</p><formula xml:id="formula_4">h n?1 ? +1 = [ SG(h n ? ) ? h n?1 ? +1 ]<label>(3)</label></formula><p>where the cached hidden state h n?1 ? in Eq. 1 and Eq. 2 is replaced with h n ? in Eq. 3. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, when the retrospective feed mechanism is combined with the enhanced recurrence mechanism, every segment in the retrospective phase (shown in the box with a green dotted border) has bidirectional contextual information of the entire text input. We successfully modeled a larger effective context length (shown in the box with a orange dotted border) than traditional Recurrence Transformers can without extra memory and computation costs. Another benefit of the enhanced recurrence scheme is that past higher-level representations can be exploited to enrich future lower-level representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Segment-Reordering Objective</head><p>In addition to the masked language model (MLM) objective <ref type="bibr" target="#b12">(Devlin et al., 2018)</ref>, we introduce an additional document-aware task called segment-reordering objective for pretraining. Benefitting from the much larger effective context length provided by the enhanced recurrence mechanism, the goal of the segment-reordering objective is to predict the correct order for the permuted set of segments of a long document, to explicitly learn the relationships among segments. During the pretraining process of this task, a long text input D is first randomly partitioned into 1 to m chunks; then, all the combinations are shuffled in a random order. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, D is partitioned into three chunks and then permuted, that is,</p><formula xml:id="formula_5">D = {C 1 , C 2 , C 3 } =?D = {C 2 , C 3 , C 1 },</formula><p>where C i denotes the i-th chunk. Subsequently, the permuted long contextD is split into T sequential segments as a common practice, denoted asD = {S 1 , S 2 , ..., S T }. We let the pretrained model reorganize these permuted segments, modeled as a K-class classification problem, where K = m i=1 i!. The pretraining objective is summarized as follows for the ? -th input segment:</p><formula xml:id="formula_6">max ? log p ? (S ? |? ? ) + 1 ? =T log p ? (D|D)</formula><p>where? ? is the corrupted version of S ? , which is obtained by randomly setting a portion of tokens </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Autoregressive Language Modeling</head><p>Autoregressive language modeling aims to estimate the probability distribution of an existing token/character based on previous tokens/characters in an input sequence. For comparison with previous work, we conducted experiments on wordlevel LM, that is, WikiText-103 <ref type="bibr" target="#b24">(Merity et al., 2016)</ref>, which is a document-level language modeling dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Experimental Setup</head><p>For autoregressive language modeling, we use a memory-enhanced Transformer-XL , that is, we employ our enhanced recurrence mechanism to replace the primitive one used in the Transformer-XL. Additionally, as proposed by Segatron <ref type="bibr" target="#b2">(Bai et al., 2020)</ref>, we introduce the segment-aware mechanism into Transformer-XL. Based on Transformer-XL, we trained a base-size model (L=16, H=410, A=10) and a large-size model (L=18, H=1,024, A=16) 3 . The models were trained for 200K/400K steps using a batch size of 64/128 for the base/large configurations. During the training phase, the sequence length and memory length were limited to 150 and 384 for the base and the large model, respectively. The remaining hyper-parameters were identical to those of Transformer-XL.  <ref type="bibr" target="#b23">(Merity et al., 2018)</ref> 151M 33.0 Transformer-XL Base  151M 24.0 SegaTransformer-XL Base <ref type="bibr" target="#b2">(Bai et al., 2020)</ref>     and duplicated the pretraining data 10 times. Chinese Data. The Chinese text corpora used in ERNIE 2.0 <ref type="bibr" target="#b32">(Sun et al., 2020b)</ref> were adopted for pretraining ERNIE-DOC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Experimental Setup</head><p>Pretraining. We trained three sizes of models for English tasks: small (L=6, H=256, A=4), base (L=12, H=768, A=12), and large (L=24, H=1,024, Models IMDB HYP Acc. F1 F1 RoBERTa  95.3 95.0 87.8 Longformer <ref type="bibr" target="#b3">(Beltagy et al., 2020)</ref>   A=16). For Chinese tasks, we used only one size, i.e., base (L=12, H=768, A=12). We limited the length of the sentences in each mini-batch to 512 tokens and the length of the memory to 128. The models were trained for 500K/400K/100K steps using a batch size of 2,560/2,560/3,920 sentences for the small/base/large configurations. ERNIE-DOC was optimized with the Adam (Kingma and Ba, 2014) optimizer. The learning rate was warmed up over the first 4,000 steps to a peak value of 1e-4, and then it linearly decayed. The remaining pretraining hyperparameters were the same as those of RoBERTa  (see Tab. 12). Additionally, we employed relative positional embedding <ref type="bibr" target="#b28">(Shaw et al., 2018)</ref> in our model pretraining because it is necessary for reusing hidden state without causing temporal confusion .</p><p>Finetune. In contrast to previous models, such as BERT, RoBERTa, and XLNet, the proposed model employs the retrospective feed mechanism and the enhanced recurrence mechanism during the finetuning phase to fully utilize the advantages of these two strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Results on English Tasks</head><p>Results on Long-Text Classification Tasks. We consider two datasets: IMDB reviews <ref type="bibr" target="#b22">(Maas et al., 2011)</ref> and Hyperpartisan News Detection (HYP) <ref type="bibr" target="#b17">(Kiesel et al., 2019)</ref>. The former is a widely used sentiment analysis dataset containing 50,000 movie reviews, labeled as positive or negative. The latter contains news that takes extreme left-wing or right-wing standpoints.    <ref type="bibr" target="#b16">(Joshi et al., 2017)</ref> and distractor setting of HotpotQA (HQA) <ref type="bibr" target="#b43">(Yang et al., 2018)</ref>) to evaluate the reasoning ability of the models over long documents. TQA and HQA are extractive QA tasks, and we follow the simple QA model of BERT <ref type="bibr" target="#b12">(Devlin et al., 2018)</ref> to predict an answer with the maximum sum of start and end logits across multiple segments of a sample. In addition, we use a modified cross-entropy loss <ref type="bibr" target="#b5">(Clark and Gardner, 2017)</ref> for the TQA dataset and use a two-stage model <ref type="bibr" target="#b14">(Groeneveld et al., 2020)</ref> with the backbone of ERNIE-DOC for the HQA dataset. Tab. 4. shows that ERNIE-DOC outperforms RoBERTa and Longformer by a considerable margin on these two datasets, and is comparable to current SOTA long-document model, i.e., BigBird on HQA in large-size model setting.</p><p>Results on the Keyphrase Extraction Task. We include OpenKP <ref type="bibr" target="#b40">(Xiong et al., 2019)</ref> dataset to eval-uate ERNIE-DOC's ability to extract keyphrases from a long document. Each document contains up to three short keyphrases and we follow the model setting of JointKPE <ref type="bibr" target="#b31">(Sun et al., 2020a)</ref> and ETC <ref type="bibr" target="#b0">(Ainslie et al., 2020)</ref> by applying CNNs on BERT's output to compose n-gram embeddings for classification. We report the results of basesize models in Tab. 5 under no-visual-features setting for easy and fair comparison with baselines. ERNIE-DOC performs stably better on all metrics on the OpenKP dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Results on Chinese Tasks</head><p>We conducted extensive experiments on seven Chinese natural language understanding (NLU) tasks, including machine reading comprehension (CMRC2018 <ref type="bibr" target="#b9">(Cui et al., 2018)</ref>, DRCD <ref type="bibr" target="#b27">(Shao et al., 2018)</ref>, DuReader <ref type="bibr" target="#b15">(He et al., 2017)</ref>, C 3 (Sun et al., 2019a)), semantic similarity (CAIL2019-SCM (CAIL) <ref type="bibr" target="#b39">(Xiao et al., 2019)</ref>), and long-text classification (IFLYTEK (IFK) <ref type="bibr" target="#b41">(Xu et al., 2020)</ref>, THUCNews (THU) 5 <ref type="bibr" target="#b30">(Sun et al., 2016)</ref>). The documents in all the aforementioned datasets are sufficiently long to be used to evaluate the effectiveness of ERNIE-DOC on long-context tasks (see detailed datasets statistics in Tab. 9). We reported the mean results with five runs for the seven Chinese tasks in Tab. 6, and summarized the hyperparameters in Tab. 16. ERNIE-DOC outperforms previous models across these Chinese NLU tasks by a significant margin in the base-size model group.   <ref type="table">Table 6</ref>: Results on seven Chinese NLU tasks for ERNIE-DOC-base model.The results of the models with " * " are from <ref type="bibr" target="#b8">Cui et al. (2019)</ref>. The XLNet-zh is the abbreviation of Chinese-XLNet. Notably, the result of BERT on CAIL was obtained from <ref type="bibr" target="#b39">Xiao et al. (2019)</ref>, where BERT was post-pretrained with a legal dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Ablation Studies</head><p>No.IV and No.V, we see that segment-level recurrence is necessary for modeling long documents and produces 2.74 and 3.95 % points improvement on the TQA and HYP dateset, respectively. Moreover, a substantial improvement is achieved using the enhance recurrence mechanism (2.29% point on TQA and 1.40% point on HYP, see No.III -IV).</p><p>Retrospective feed mechanism further improves 0.21% point on TQA and 1.33% point on HYP (No.II -No.III). Considering different types of tasks, we observe that on HYP, an extremely long text classification dataset, a substantial improvement is achieved using the segment-reordering objective (1.5% point). This indicates that the [CLS] token, pretrained using the segment-reordering objective, is more adaptable to the document-level text classification task. Effect of enhanced recurrence mechanism with regard to different maximum sequence lengths.</p><p>As depicted in <ref type="figure" target="#fig_4">Fig. 4</ref>, the enhanced recurrence mechanism plays an important role in pretraining an effective language model with lower PPL and higher accuracy under both the maximum sequence input lengths of 128 and 512. The effect of the enhanced recurrence mechanism is more significant under a smaller maximum sequence length, even makes the ERNIE-DOC-Small (max-len:128) comparable to ERNIE-DOC-Small w/o en recur (max-len:512) w.r.t accuracy. This intriguing property of the enhanced recurrence mechanism enables more efficient model training and inference by reducing maximum sequence length while remaining comparable modeling capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed ERNIE-DOC, a document-level language pretraining model based on the Recurrence Transformers paradigm. Two well-designed mechanisms, namely the retrospective feed mechanism and the enhanced recurrent mechanism, enable ERNIE-DOC, which theoretically has the longest possible dependency, to model bidirectional contextual information of a complete document. Additionally, ERNIE-DOC is pretrained with a document-aware segment-reordering objective to explicitly learn the relationship among segments of a long context. Experiments on various downstream tasks demonstrate that ERNIE-DOC outperforms existing strong pretraining models such as RoBERTa, Longformer, and BigBird and achieves SOTA results on several language modeling and language understanding benchmarks.</p><p>In future studies, we will evaluate ERNIE-DOC on language generation tasks, such as generative question answering and text summarization. We will also investigate its potential applicability in other areas, such as computational biology. Another possibility is to incorporate graph neural networks into ERNIE-DOC to enhance its modeling capability for tasks that require multi-hop reasoning and longdocument modeling ability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Tasks</head><p>Following previous work, we evaluate ERNIE-DOC on various tasks that require the ability to model a long document.</p><p>Document-level Language Modeling Task. We employ WikiText-103 <ref type="bibr" target="#b24">(Merity et al., 2016)</ref> in language modeling experiments. WikiText-103 is the largest available word-level benchmark with long-term dependency for language modeling, which consists of 28K articles, where each article has 3.6K tokens on average, thus 103M training tokens in total.</p><p>Long Text classification. We consider two English datasets: IMDB reviews <ref type="bibr" target="#b22">(Maas et al., 2011)</ref> and Hyperpartisan news detection <ref type="bibr" target="#b17">(Kiesel et al., 2019</ref>) (see Tab. 8), and two Chinese datasets: IFLY-TEK <ref type="bibr" target="#b41">(Xu et al., 2020)</ref> and THUCNews <ref type="bibr" target="#b30">(Sun et al., 2016</ref>) (see Tab. 9). IMDB is a widely used sentiment analysis dataset containing 50,000 movie reviews labeled as positive or negative. Training and dev dataset is equally split. Hyperpartisan contains news that takes an extreme left-wing or right-wing standpoint. Documents are extremely long in Hyperpartisan which makes it a good test for long text classification. We use the same split as Longformer by dividing 654 documents into train/dev/test sets. IFLYTEK contains 17,332 app descriptions. The task is to assign each description into one of 119 categories, such as food, car rental and education. THUCNews is generated by filtering historical data of Sina News RSS subscription channel from 2005 to 2011, including 740,000 news documents and 14 categories. In this paper, we employ the subset version instead of the full one 6 , which contains 10 categories, each with 5,000 pieces of data. For the above four long text classification datasets, we concatenate [CLS] token with each segment and takes as input multiple segments of a text sequentially. Each segment is generated by slicing the text with a sliding window of 128 tokens. We apply binary cross entropy loss on the [CLS] token of the last segment.</p><p>Long Text Semantic Similarity. Considering that there is no available long text semantic similarity dataset in English, we evaluate the effectiveness of ERNIE-DOC on semantic similarity task only depending on Chinese dataset CAIL2019-SCM. According to <ref type="bibr" target="#b39">Xiao et al. (2019)</ref>, CAIL2019-SCM is a sub-task of the Chinese AI and Law Challenge (CAIL) competition in 2019, which contains 8,964 triplets of legal documents collected from China Judgments Online. Every document in a majority of triplet has more than 512 characters, therefore, the total length of a triplet is quite long. CAIL2019-SCM requires researchers to decide which two cases are more similar in a triplet. Specifically, given a triplet <ref type="figure">(A, B, C)</ref>, where A, B, C are fact descriptions of three cases. The model needs to predict whether sim(A, B) &gt; sim(A, C) or sim(A, C) &gt; sim <ref type="figure">(A, B)</ref>, in which sim denotes the similarity between two cases. Instead of separately feeding the document A, B, C into the model to get the feature h, we use the combinations of (A, B) and (A, C) as input. We generate multiple segments for (A, B) or (A, C) with a sliding window of 128 tokens and feed them as input sequentially. The binary cross entropy loss is applied to the difference of [CLS] token output of each segment.</p><p>Document-level Question answering. We utilize two English question answering datasets (Trivi-aQA <ref type="bibr" target="#b16">(Joshi et al., 2017)</ref>, HotpotQA <ref type="bibr" target="#b43">(Yang et al., 2018)</ref>) (see Tab. 8) and four Chinese question answering datasets (CMRC2018 <ref type="bibr" target="#b9">(Cui et al., 2018)</ref>, DRCD <ref type="bibr" target="#b27">(Shao et al., 2018)</ref>, DuReader <ref type="bibr" target="#b15">(He et al., 2017)</ref>, C 3 <ref type="bibr" target="#b29">(Sun et al., 2019a)</ref>) (see Tab. 9) to evaluate models' reasoning ability over long documents.</p><p>TriviaQA is a large scale QA dataset that contains over 650K question-answer pairs. We evaluate models on its Wikipedia setting where documents are Wikipedia articles, and answers are named entities mentioned in multiple documents. The dataset is distantly supervised meaning that there is no golden span, thus we find all superficial identical answers in provided documents 7 . We use the following input format for each segment: "[CLS] context [q] question [/q]" where context is generated by slicing multidocuments input with a sliding window of 128 tokens. We take as input multiple segments of a sample sequentially and attach a linear layer to each token in a segment to predict the answer span. We <ref type="table">Table 9</ref>: Chinese Datasets statistics. use a modified cross entropy loss <ref type="bibr" target="#b5">(Clark and Gardner, 2017)</ref> assuming that each segment contains at least one correct answer span. The final prediction for each question is a span with the maximum sum of start and end logit across multiple segments.</p><p>HotpotQA is a QA dataset where golden spans of an answer and sentence-level supporting facts are provided. Thus, it contains two tasks namely, answer span prediction and supporting facts prediction. In the distractor setting, each question is associated with 10 documents where only 2 documents contain supporting facts. It requires the model to find and reason over multiple documents to find answers, and explain the predicted answers using predicted supporting facts. Following <ref type="bibr" target="#b14">Groeneveld et al. (2020)</ref>, we implemented a two-stage model based on ERNIE-DOC and use the following input format for each segment: " <ref type="bibr">[</ref> representing a sentence and a paragraph separately. Then we use binary cross entropy loss to do binary classification. For answer span prediction, we train the model with a multi-task objective: 1) question type (yes/no/span) classification on the [CLS] token. 2) supporting evidence prediction on [SEP] and <ref type="bibr">[p]</ref>. 3) span prediction on the start and end token of a golden span.</p><p>CMRC2018, DRCD and DuReader are common Chinese QA datasets with same format, which have been evaluated in numerous popular pretrain-ing models, such as BERT <ref type="bibr" target="#b12">(Devlin et al., 2018)</ref>, ERNIE 1.0 <ref type="bibr" target="#b34">(Sun et al., 2019b)</ref>, ERNIE 2.0 <ref type="bibr" target="#b32">(Sun et al., 2020b)</ref> and etc. The detailed descriptions of three datasets can refer to <ref type="bibr" target="#b9">Cui et al. (2018)</ref>, <ref type="bibr" target="#b27">Shao et al. (2018)</ref> and <ref type="bibr" target="#b15">He et al. (2017)</ref>. We adopt the same input format as TriviaQA for each segment, denotes as "[CLS] context [SEP] question [SEP]" where context is generated by slicing multi-documents input with a sliding window of 128 tokens. We take as input multiple segments of a sample sequentially and attach a linear layer to each token in a segment to predict the answer span. Then, we apply a softmax and use the cross entropy loss with the correct answer. The final prediction for each question is a span with the maximum sum of start and end logit across multiple segments.</p><p>The multiple Choice Chinese machine reading Comprehension dataset (C 3 ) <ref type="bibr" target="#b29">(Sun et al., 2019a)</ref> is the first Chinese free-form multi-choice dataset where each question is associated with at most four choices and a single document. According to <ref type="bibr" target="#b29">(Sun et al., 2019a)</ref>, m segments are constructed for a question, in which m denotes the number of choice for that question. We use the following input format for each segment: "[CLS] context [SEP] question [SEP] choice i [SEP] " where context is generated by slicing document input with a sliding window of 128 tokens stride. We take as input multiple segments of a sample in a single batch and attach a linear layer to [CLS] that outputs an unnormalized logit. Then we obtain the final prediction for a question by applying a softmax layer over the unnormalized logits of all choices  <ref type="table">Table 10</ref>: Performance of ERNIE-DOC-small after ablating each proposed component. (so denotes the segmentreordering objective, re denotes the retrospective feed mechanism, en-rec denotes the enhanced recurrence mechanism, and recur denotes the segment-level recurrence module. We used the Acc. metric for IMDB, F1 metric for TriviaQA and Hyperpartisan, Joint-F1 for HotpotQA.) associated with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keyphrase Extraction.</head><p>We include OpenKP <ref type="bibr" target="#b40">(Xiong et al., 2019)</ref> dataset 8 to evaluate ERNIE-DOC's ability to extract keyphrases from a long document. Each document contains up to three short keyphrases and we follow the model setting of JointKPE <ref type="bibr" target="#b31">(Sun et al., 2020a)</ref> and ETC <ref type="bibr" target="#b0">(Ainslie et al., 2020)</ref> by applying CNNs on BERT's output to compose n-gram embeddings for classification. We clean the dataset by removing some nonsense words such as the HTTP links. In detail, we apply five CNNs on BERT's output with the kernel size ranging from 1 to 5. Since each word is composed of several sub-tokens, we take the first token's embedding as the input for CNNs. Finally, we use the binary cross entropy loss as the optimization objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Ablation Studies</head><p>Tab. 10 shows the performance of ERNIE-DOC-Small on English tasks after ablating each proposed component. All models were pretrained and finetuned with the same experimental setup, and we report the mean results of five runs. In the last column in Tab. 10, we see that the segment-reordering objective is improved ERNIE-DOC by 0.81% on average (#1 -#0), the retrospective feed mechanism is improved ERNIE-DOC by an average of 0.58% (#2 -#1), and the enhanced recurrence mechanism makes a large contribution of 2.55 percentage points on average (#3 -#2). By comparing #3 with #4, we see that segment-level recurrence is necessary for modeling long documents and produces a 4.92 percentage point improvement on average. Considering different types of tasks, we observe that on Hyperpartisan, an extremely long text classification dataset, a substantial improvement is achieved using the segment-reordering ob-8 The dataset can be downloaded from https:// github.com/thunlp/BERT-KPE jective (1.5% point). This indicates that the [CLS] token, pretrained using the segment-reordering objective, is more adaptable to the document-level text classification task. Moreover, we observed a stable performance gain across all tasks using the enhanced recurrence mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyperparameters for Language Modeling</head><p>In Tab. 11, we present the detailed hyperparameters used for our experiments, which are the same as the hyperparameters employed in Transformer-XL .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Hyperparameters for Fine-Tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.1 Long Text Classification tasks</head><p>The finetuning hyperparameters for IMDB <ref type="bibr" target="#b22">(Maas et al., 2011)</ref> and Hyperpartisan <ref type="bibr" target="#b17">(Kiesel et al., 2019)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.2 Document-level Question answering tasks</head><p>The finetuning hyperparameters for TriviaQA <ref type="bibr" target="#b38">(Welbl et al., 2018)</ref> and HotpotQA <ref type="bibr" target="#b43">(Yang et al., 2018)</ref> are presented in Tab. 14. HQA-sent. is the model for coarse-grained evidence prediction, and we choose the evidence with the probability larger than a pre-defined threshold 1e-3 and 1e-5 for base and large models, respectively. HQA-span. is the model for span prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.3 Keyphrase Extraction task</head><p>The finetuning hyperparameters for the OpenKP <ref type="bibr" target="#b40">(Xiong et al., 2019)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.4 Chinese NLU tasks</head><p>Tab. 16 lists the finetuning hyperparameters for Chinese NLU tasks including IFLYTEK <ref type="bibr" target="#b41">(Xu et al., 2020)</ref>, THUCNews <ref type="bibr" target="#b30">(Sun et al., 2016)</ref>, CMRC2018 <ref type="bibr" target="#b9">(Cui et al., 2018)</ref>, DRCD <ref type="bibr" target="#b27">(Shao et al., 2018)</ref>, DuReader <ref type="bibr" target="#b15">He et al. (2017)</ref>, C 3 <ref type="bibr" target="#b29">(Sun et al., 2019a)</ref> and CAIL2019-SCM <ref type="bibr" target="#b39">(Xiao et al., 2019)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Attention Complexity</head><p>Given a long document with length L, Longformer and BigBird usually applies a local attention with a window size of 512 tokens on the entire input resulting in L * 512 token-to-token calculations. While the long document is fed twice as input and each input is sliced with a sliding window size of 512 tokens in ERNIE-DOC, which resulting in 2 * L 512 * 512 * (512 + m) token-to-token calculations where m is the memory length. Since 512 L and m L, the attention complexity of ERNIE-DOC is comparable to Longformer and BigBird which scales linearly with respect to the input length L, i.e., O(L). Notably, the segments produced from the long document are fed one by one in ERNIE-DOC, leading to the lower spatial complexity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustrations of ERNIE-DOC and Recurrence Transformers, where models with three layers take as input a long document D which is sliced into four segments S i , i ?[1, 2, 3, 4]. Recurrence Transformers (upper-right):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Work] Sparse attention based transformers are largely explored ? C 3 :[Proposed Method] In this section, we firstly describe the background of proposed ERNIE-DOC ? C 1 :[Introduction] Transformers have achieved remarkable improvements ? Illustrations of segment-reordering objective. to [MASK];D is the permutated version of D; ? is the model parameter; and 1 ? =T indicates that the segment-reordering objective is optimized only at the T -th step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Acc. (dotted line) and PPL (solid line) metrics for variants of our small models with different maximum sequence length during pretraining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>English datasets used for pretraining.</figDesc><table /><note>English Data. To allow ERNIE-DOC to capture long dependencies in pretraining, we compiled a corpus from four standard datasets: WIKIPEDIA, BOOKSCORPUS (Zhu et al., 2015), CC-NEWS 4 , and STORIES (Trinh and Le, 2018) (details listed in Tab. 2). We tokenized the corpus using the RoBERTa wordpieces tokenizer</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on the IMDB and HYP dataset for long-text classification.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results on TQA and HQA dev dataset for document-level QA. HQA metrics are F1.</figDesc><table><row><cell>OpenKP dataset</cell><cell cols="3">F1@1 F1@3 F1@5</cell></row><row><cell cols="2">BLING-KPE (Xiong et al., 2019) 26.7</cell><cell>29.2</cell><cell>20.9</cell></row><row><cell>JointKPE (Sun et al., 2020a)</cell><cell>39.1</cell><cell>39.8</cell><cell>33.8</cell></row><row><cell>ETC (Ainslie et al., 2020)</cell><cell>-</cell><cell>40.2</cell><cell>-</cell></row><row><cell>ERNIE-DOC</cell><cell>40.2</cell><cell>40.5</cell><cell>34.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">: Results on OpenKP dev dataset. The baseline</cell></row><row><cell cols="4">results are obtained from corresponding papers under</cell></row><row><cell cols="3">no-visual-features setting.</cell><cell></cell></row><row><cell cols="4">mance gain compared with RoBERTa. This is be-</cell></row><row><cell cols="4">cause nearly 90% of the samples in the dataset</cell></row><row><cell cols="4">consist of fewer than 569 tokens. Unlike on IMDB,</cell></row><row><cell cols="4">ERNIE-DOC surpasses the baseline models on</cell></row><row><cell cols="4">HYP by a substantial margin, demonstrating its</cell></row><row><cell cols="4">capability of utilizing information from a long doc-</cell></row><row><cell cols="4">ument input. Note that we include XLNet-Large,</cell></row><row><cell cols="4">the previous SOTA pretraining model on the IMDB</cell></row><row><cell cols="4">dataset, as the baseline for a large model setting;</cell></row><row><cell cols="4">ERNIE-DOC achieves a result comparable to that</cell></row><row><cell cols="2">of XLNet-Large.</cell><cell></cell><cell></cell></row><row><cell>Results</cell><cell>on</cell><cell>Document-level</cell><cell>Question-</cell></row><row><cell cols="4">Answering Tasks. We utilized two document-</cell></row><row><cell cols="4">level QA datasets (Wikipedia setting of TriviaQA</cell></row><row><cell>(TQA)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Performance of ERNIE-DOC-Small after ablating each proposed component (F1 result is reported).</figDesc><table><row><cell>Effect of proposed components. Tab. 7 shows</cell></row><row><cell>the performance of ERNIE-DOC-Small on two</cell></row><row><cell>English tasks after ablating each proposed compo-</cell></row><row><cell>nent. All models were pretrained and fine-tuned</cell></row><row><cell>with the same experimental setup, and we report</cell></row><row><cell>the mean results of five runs. We observed a stable</cell></row><row><cell>performance gain across these two tasks by incor-</cell></row><row><cell>porating each proposed component. By comparing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Hyperparameters used for WikiText-103.A.4 Hyperparameters for Pre-TrainingAs shown in Tab. 12, we present the detailed hyperparameters adopted to pretraining ERNIE-DOC on English text corpora and Chinese text corpora. For comparisons, we follow the same optimization hyperparameters of RoBERTa BASE or RoBERTa LARGE for base-size or large-size model in English domain. As for Chinese ERNIE-DOC, we follow the same optimization hyperparameters of ERNIE 2.0 BASE .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>Hyperparameters used for ERNIE-DOC pretraining.are presented in Tab. 13.</figDesc><table><row><cell>Hyperparameters</cell><cell cols="2">BASE</cell><cell cols="2">LARGE</cell></row><row><cell></cell><cell cols="4">IMDB HYP IMDB HYP</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>16</cell></row><row><cell>Learning rate</cell><cell>7e-5</cell><cell>1e-4</cell><cell>1e-5</cell><cell>4e-6</cell></row><row><cell>Epochs</cell><cell>3</cell><cell>15</cell><cell>3</cell><cell>15</cell></row><row><cell>LR schedule</cell><cell cols="4">linear linear linear linear</cell></row><row><cell>Layerwise LR decay</cell><cell>1</cell><cell>0.7</cell><cell>0.9</cell><cell>1</cell></row><row><cell>Warmup proportion</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Weight decay</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Hyperparameters used for finetuning on IMDB and Hyperpartisan (HYP).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>dataset are presented in Tab. 15. . BASE LARGE TQA HQA-sent. HQA-span. TQA HQA-sent. HQA-span.</figDesc><table><row><cell>HyperBatch size</cell><cell>64</cell><cell>128</cell><cell>128</cell><cell>64</cell><cell>32</cell><cell>32</cell></row><row><cell>Learning rate</cell><cell>3e-5</cell><cell>3e-5</cell><cell>1.5e-4</cell><cell>5e-6</cell><cell>5e-6</cell><cell>1.5e-5</cell></row><row><cell>Epochs</cell><cell>5</cell><cell>6</cell><cell>6</cell><cell>3</cell><cell>5</cell><cell>5</cell></row><row><cell>LR schedule</cell><cell>linear</cell><cell>linear</cell><cell>linear</cell><cell>linear</cell><cell>linear</cell><cell>linear</cell></row><row><cell>Layer-decay</cell><cell>0.8</cell><cell>1</cell><cell>0.8</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>Warmup prop.</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Weight decay</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>Finetuning hyperparameters on the TQA and HQA for base-and large-size ERNIE-DOC.</figDesc><table><row><cell>Hyperparameters</cell><cell>OpenKP</cell></row><row><cell>Batch size</cell><cell>32</cell></row><row><cell>Learning rate</cell><cell>1.5e-4</cell></row><row><cell>Epochs</cell><cell>5</cell></row><row><cell>LR schedule</cell><cell>linear</cell></row><row><cell>Layerwise LR decay</cell><cell>0.8</cell></row><row><cell>Warmup proportion</cell><cell>0.1</cell></row><row><cell>Weight decay</cell><cell>0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 15 :</head><label>15</label><figDesc>Finetuning hyperparameters on the OpenKP for base-size ERNIE-DOC.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 16 :</head><label>16</label><figDesc>Hyperparameters used for finetuning on Chinese NLU tasks. Note that the warmup proportion are set to 0.1 and the layerwise learning rate decay rate are set to 0.8 for all tasks.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For Sparse Transformers, the length of segment S2 could be up to 4,096 in Beltagy et al. (2020); Zaheer et al. (2020).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We denote the number of Transformer layers as L, the hidden size as H, and the number of self-attention heads as A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We used news-please to crawl English news articles published between September 2016 and February 2019 and adopted Message Digest Algorithm5 (MD5) for deduplication.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We use a subset of THUCNews (https://github. com/gaussic/text-classification-cnn-rnn).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The subset version is also released and can be downloaded from the official website of THUCTC.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We use the same preprocessing code for TriviaQA dataset as BigBird, see https://github.com/ tensorflow/models/blob/master/official/ nlp/projects/triviaqa/preprocess.py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Key Research and Development Project of China (No. 2018AAA0101900).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMDB</head><p>Hyperpartisan <ref type="table">TriviaQA  HotpotQA  OpenKP  split  train  dev  train  dev  test  train  dev  train  dev  train  dev  # samples  25,000  2,000  516  64  65  61,888  7,993  90,432  7,404  134,</ref>   <ref type="figure">133 2,599 50,000 5,000 5,102 1,500 10,121 3,219 15,763 1,628 11,869 3,816 26,936 3,524</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Etc: Encoding long and structured inputs in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10853</idno>
		<title level="m">Adaptive input representations for neural language modeling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segatron: Segment-aware transformer for language modeling and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10723</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Revisiting pretrained models for chinese natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13922</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revisiting pretrained models for Chinese natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="657" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08101</idno>
		<title level="m">Pre-training with whole word masking for chinese bert</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A span-extraction dataset for chinese machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07366</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>length context. CoRR, abs/1901.02860</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04426</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06753</idno>
		<title level="m">A simple yet strong pipeline for hotpotqa</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05073</idno>
		<title level="m">Dureader: a chinese machine reading comprehension dataset from real-world applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semeval-2019 task 4: Hyperpartisan news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payam</forename><surname>Adineh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="829" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06467</idno>
		<title level="m">Pretrained transformers for text ranking: Bert and beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An analysis of neural language modeling at multiple scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08240</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<idno>abs/1911.05507</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih Chieh</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trois</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Tsai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00920</idno>
		<title level="m">Drcd: a chinese machine reading comprehension dataset</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Self-attention with relative position representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Probing prior knowledge needed in challenging chinese machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno>abs/1904.09679</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Thuctc: an efficient chinese text classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>GitHub Repository</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Bao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13639</idno>
		<title level="m">Joint keyphrase chunking and salience ranking with bert</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ernie 2.0: A continual pre-training framework for language understanding</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">Ernie: Enhanced representation through knowledge integration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11296</idno>
		<title level="m">Sparse sinkhorn attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cail2019-scm: A dataset of similar case matching in legal domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Daniel Campos, and Arnold Overwijk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02671</idno>
	</analytic>
	<monogr>
		<title level="m">Open domain web keyphrase extraction beyond language modeling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yechen</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05986</idno>
		<title level="m">Clue: A chinese language understanding evaluation benchmark</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<title level="m">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04070</idno>
		<title level="m">Bp-transformer: Modelling long-range context via binary partitioning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
