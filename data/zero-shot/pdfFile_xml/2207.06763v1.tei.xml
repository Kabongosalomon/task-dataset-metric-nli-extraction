<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neighbor Correspondence Matching for Flow-based Video Frame Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Jia</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
							<email>yanlu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neighbor Correspondence Matching for Flow-based Video Frame Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Motion capture</term>
					<term>Matching</term>
					<term>Im- age compression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: A difficult example of video frame interpolation in X4K1000FPS <ref type="bibr" target="#b31">[33]</ref>. From top to bottom are the synthesised results, the zoomed details, and the residuals between synthesised images and ground truth. Previous methods fail on the car with large motion and produce blurs or flickers, while our method can generate high-quality results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>Video frame synthesis, which consists of interpolation and extrapolation, is an essential video processing technique that can be applied to various scenarios. However, most existing methods cannot handle small objects or large motion well, especially in high-resolution videos such as 4K videos. To eliminate such limitations, we introduce a neighbor correspondence matching (NCM) algorithm for flow-based frame synthesis. Since the current frame is not available in video frame synthesis, NCM is performed in a current-frameagnostic fashion to establish multi-scale correspondences in the spatial-temporal neighborhoods of each pixel. Based on the powerful motion representation capability of NCM, we further propose to estimate intermediate flows for frame synthesis in a heterogeneous coarse-to-fine scheme. Specifically, the coarse-scale module is designed to leverage neighbor correspondences to capture large motion, while the fine-scale module is more computationally efficient to speed up the estimation process. Both modules are trained progressively to eliminate the resolution gap between training dataset and real-world videos. Experimental results show that NCM achieves state-of-the-art performance on several benchmarks. In addition, NCM can be applied to various practical scenarios such as video compression to achieve better performance. * This work was done when Zhaoyang Jia was an intern at Microsoft Research Asia.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Video frame synthesis is a classic video processing task to generate frame in-between (interpolation) or subsequent to (extrapolation) reference frames. It can be applied to many practice applications, such as video compression <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b37">39]</ref>, video view synthesis <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">14]</ref>, slow-motion generation <ref type="bibr" target="#b10">[12]</ref> and motion blur synthesis <ref type="bibr" target="#b3">[4]</ref>. Recently, various deep-learning-based algorithms have been proposed to handle video frame synthesis problem. Most of them focus on interpolation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b24">[26]</ref><ref type="bibr" target="#b25">[27]</ref><ref type="bibr" target="#b26">[28]</ref><ref type="bibr" target="#b31">33]</ref>, while some others <ref type="bibr" target="#b18">[20]</ref> can deal with both interpolation and extrapolation in a unified framework.</p><p>Among existing video frame synthesis algorithms, flow-based schemes predict the current frame by warping reference frames with estimated optical flows. Many flow-based interpolation schemes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">12]</ref> first compute bi-directional optical flows, and then approximate intermediate flows by flow reversal <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b17">19]</ref>. On the contrary, some recent schemes <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b26">28]</ref> directly estimate intermediate flows and achieve superior performance. However, most of existing methods fail in estimating large motion or motion of small objects, <ref type="figure">Figure 2</ref>: Illustration of neighbor correspondence matching. Blue regions in the reference frames denote the matching regions. The correspondence matching <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">37]</ref> is performed between the current frame and the reference frame, while our neighbor correspondence matching is performed in a current-frame-agnostic fashion to match pixels in the spatial-temporal neighborhoods of the current frame.</p><p>as shown in <ref type="figure">Fig.1</ref>. It is mainly caused by the limited receptive field and motion capture capability of CNNs.</p><p>Correspondence matching is proven to be effective in capturing long-term correlations in multimedia tasks like video object segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">42]</ref> and optical flow estimation <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b35">37]</ref>. In these scenarios, by matching pixels of the current frame in the reference frame, a correspondence matrix can be established to guide the generation of mask or flow <ref type="figure">(Fig.2, left)</ref>. However, in video frame synthesis, we only have two inference frames and the current frame is not available. As a result, the correspondence matching cannot be performed directly. So how to perform correspondence matching in video frame synthesis is still an unanswered question.</p><p>In this paper, we introduce a neighbor correspondence matching (NCM) algorithm to enhance flow estimation in video frame synthesis, which can establish correspondences in a current-frameagnostic fashion. Observing that objects usually move continuously and locally within a small region in natural videos, we propose to perform correspondence matching between the spatial-temporal neighbors of each pixel. Specifically, for each pixel in the current frame, we only use pixels in the local windows of adjacent reference frames to calculate the correspondence matrix <ref type="figure">(Fig.2</ref>, middle and right), so the current frame is not required in this process. The matched neighbor correspondence matrix can effectively model the object correlations, from which we can infer sufficient motion cues to guide the generation of flows. In addition, multi-scale neighbor correspondence matching is further preformed to extend the receptive field and capture large motion.</p><p>Compared with previous cost-volume-based schemes <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37</ref>] that guide the matching region by estimated flow, NCM works better on video frame synthesis. Due to the complexity of frame synthesis, the estimated flows are usually inaccurate at the beginning of estimation. In this case, if the matching region is guided by the inaccurate flow, the matching region may move away from where the current pixel is, leading to ineffective matching. On the contrary, NCM directly matches correspondences in fixed neighbor regions to avoid misleading of inaccurate flows, which is more stable and more efficient since the correspondences only need to be computed once to be applied in different stages of estimation.</p><p>Based on NCM, we further propose a unified video frame synthesis network for both interpolation and extrapolation. The proposed model can accurately estimate intermediate flows in a heterogeneous coarse-to-fine scheme. Specifically, the coarse-scale module is designed to utilize multi-scale neighbor correspondence matrix to capture accurate motion, while the fine-scale module refines the coarse flows in a computationally efficient fashion. With the proposed heterogeneous coarse-to-fine structure, our model is not only effective but also efficient, especially for high-resolution videos.</p><p>For flow-based video frame synthesis schemes, another existing problem is the resolution gap between training dataset and realworld high-resolution videos. To eliminate such gap, we propose to train coarse and fine-scale modules using a progressive training strategy. Combining all above designs, we can augment RIFE <ref type="bibr" target="#b11">[13]</ref> framework to a novel NCM-based network, which demonstrates new state-of-the-art results in several video frame synthesis benchmarks. Specifically, In challenging X4F1000FPS benchmark, our model improves PSNR by 1.47dB (from 30.16dB of ABME [28] to 31.63dB), which shows its capability in capturing large motion and handling real-scenario videos.</p><p>NCM can be extended to many practical applications such as video compression, motion effects generation, and jitters removal in real-time communication. We build a video compression algorithm based on NCM, and results show out model can save 10% bits in HEVC dataset compared with H265-HM [10].</p><p>In summary, the main contributions in this paper are :</p><p>(1) We introduce a neighbor correspondence matching algorithm for video frame synthesis, which is simple yet effective in capturing large motion or small objects. (2) We propose a heterogeneous coarse-to-fine structure, which can generate intermediate flows both accurately and efficiently. We further train them in a progressive fashion to eliminate the resolution gap between training and inference. (3) Combining all designs above, we propose a unified framework for video frame synthesis. It achieves the new state-ofthe-art in both interpolation and extrapolation, which improves PSNR by 1.47dB (30.16dB?31.63dB) compared with previous SOTA ABME <ref type="bibr" target="#b26">[28]</ref> on X4K1000FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS 2.1 Video Frame Interpolation</head><p>Video frame interpolation (VFI) is a sub-task of video frame synthesis, which aims to predict the intermediate frame between input frames. Learning-based VFI methods can be categorised as kernelbased methods <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b24">26]</ref> and flow-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b26">28]</ref>. Kernel-based VFI learns motion implicitly using dynamic kernels <ref type="bibr" target="#b24">[26]</ref> and deformable kernels <ref type="bibr" target="#b14">[16]</ref>, which can preserve structural stability but might generate blurry frames because of the lack of explicit motion guidance. On the contrary, flow-based VFI explicitly model the motion with dense pixel-wise flows and perform forward-warp <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b23">25]</ref> or backward-warp <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b26">28]</ref> to predict the frame, which can achieve superior performance. Since forward-warping can cause holes and overlaps in the warped image, backward-warping is more widely exploited and applied in flow-based VFI. For flow-based VFI that perform backward-warping, the key is how to estimate the intermediate flows. The intermediate flows should be spatially aligned with the current synthesised frame, but such spatial information is agnostic during inference. That makes it difficult to estimate accurate intermediate flows. Early flow-based VFI leverage advanced optical flow methods <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37]</ref> to estimate bi-directional flows, and perform flow reversal to generate intermediate flows. Later, Park et al. <ref type="bibr" target="#b25">[27]</ref> estimates symmetric bilateral motion with a bilateral cost volume, which is further improved by Park et al. <ref type="bibr" target="#b26">[28]</ref> through introducing asymmetric motion to achieve superior performance. Recently, Huang et al. <ref type="bibr" target="#b9">[11]</ref> proposed to estimate intermediate flows directly with a privileged distillation supervision, which shows a new paradigm for intermediate flow estimation. However, these schemes cannot handle large motion of small objects well, and are limited by the solution gap between training and inference. It inspires us to explore more effective motion representations for intermediate flow estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video Frame Extrapolation</head><p>Video frame extrapolation aims to predict the frame subsequent to input frames. It is much more challenging than interpolation because unseen objects may exist in the current frame. Liu et al. <ref type="bibr" target="#b18">[20]</ref> proposed a unified framework for both interpolation and extrapolation, which models intermediate flow as a 3D voxel flow and synthesises current frame by trilinear sampling. However, due to the difficulty of synthesis frame only by two previous frames, many following works focus more on multi-frame extrapolation or video prediction <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b36">38]</ref>. These works can generate more accurate results, but they usually need a sequence of frames to warm up, which are computationally expensive. In this paper, our synthesis algorithm is more like Liu et al. <ref type="bibr" target="#b18">[20]</ref>, that only needs two frames as input and can adapt to both interpolation and extrapolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Correspondence Matching</head><p>Correspondence matching is a technique to establish correspondences between images, which has been widely used in many computer vision and graphics tasks. In many 3D vision tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">31]</ref>, correspondences are computed between different views to explore the 3D structure. In video object segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">42]</ref>, correspondence matching is performed to search the similar pixels in the reference frames to propagate the mask. Benefiting from the longterm correlation modeling capability of correspondence matching, these schemes achieve remarkable performance.</p><p>Recently, correspondence is leveraged in flow estimation <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b35">37]</ref> and achieve superior performance. RAFT <ref type="bibr" target="#b35">[37]</ref> builds an all-pair correspondence matrix and looks up it to refine estimated optical flow recurrently, but it cannot be effectively applied in video frame synthesis because the current frame is not available to compute correspondences. BMBC <ref type="bibr" target="#b25">[27]</ref> establishes a bilateral cost volume in video frame interpolation, but it is limited by the symmetric linear motion assumption. In addition, these schemes introduce correspondence as a means to refine estimated flows, which can be easily misled if inaccurate flows are given. In this paper, we rethink the correspondence matching in flow estimation. Based on the assumption that objects usually move continuously and locally in natural videos, we introduce neighbor correspondence matching as a new manner for motion correlation matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>The overview of the proposed video frame synthesis network is shown in <ref type="figure">Fig.3</ref>. The network consists of three parts : 1) neighbor correspondence matching with a feature pyramid (yellow in <ref type="figure">Fig.3</ref>), 2) heterogeneous coarse-to-fine motion estimation(blue in <ref type="figure">Fig.3</ref>), and 3) frame synthesis. For completeness, we first briefly introduce RIFE <ref type="bibr" target="#b9">[11]</ref> from which we adopt some block designs, and then demonstrate details of each module in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>We base our network design on the RIFE <ref type="bibr" target="#b9">[11]</ref> framework. In RIFE, given a pair of reference frames 0 , 1 ? R 3? ? , three IFBlocks ( <ref type="figure" target="#fig_2">Fig.5</ref>, left) are used to estimate intermediate flows from the coarse to fine-scale. The flows = ( ?0 , ?1 ) and fusion map are refined by residual estimation in each IFBlock, and the current image at time can be generated by:</p><formula xml:id="formula_0">= ? ( 0 , ?0 ) + (1 ? ) ? ( 1 , ?1 ) (1)</formula><p>where ? denotes pixel-wise product and means backward warping operation. Then , and are fed to a U-Net-like refine network (i.e., the synthesis network) to generate the synthesised frame?.</p><p>RIFE is light weight and real-time, but the synthesis quality is not satisfactory due to the limited receptive field and motion capture capability of the designed fully-convolutional network. In addition, RIFE cannot adapt to high-resolution videos where the motion is even larger. To eliminate these limitations, we propose neighbor correspondence matching and a heterogeneous coarseto-fine structure for video frame synthesis, which can effectively estimate accurate intermediate flows even on 4K videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neighbor Correspondence Matching</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Overview. Based on the observation that an object usually move continuously and locally within a small region in natural videos, the core idea of NCM is to explore the motion information by establishing spatial-temporal correlation between the neighboring <ref type="figure">Figure 3</ref>: Overview of the proposed network. Our network is based on RIFE <ref type="bibr" target="#b9">[11]</ref>. We augment RIFE with : 1) the neighbor correspondence matching (NCM, yellow color) with a feature pyramid, 2) the heterogeneous coarse-to-fine modules (blue color). IFBlocks estimate flows with features 0,1 , correspondences or frame 0,1 as inputs, which will be illustrated in <ref type="figure" target="#fig_2">Fig.5</ref>.</p><p>regions. In detail, we compute the correspondences between the local windows of two adjacent reference frames for each pixel, as shown in <ref type="figure">Fig.2</ref>. It means we need no information about the current frame, so the matching can be performed in a currentframe-agnostic fashion to meet the need of frame synthesis.</p><p>It is worth noting that the position of local windows are determined by the position of pixel, which is different from the costvolume-based manners <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b25">27]</ref> that establish a cost volume around where the estimated flows point. The flow-centric manners have a potential problem that if the estimated flow is inaccurate, the matching may be performed in a wrong region. As a result, the cost volume cannot compute effective correlations to refine the estimated flows. On the contrary, NCM is pixel-centric and will not be misled by the inaccurate flows. Experiments also show that compared with flow-guided matching, NCM can lead to better performance and stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Mathematical Formulation. Given a pair of reference frames</head><formula xml:id="formula_1">0 , 1 ? R 3? ? , a -layer feature pyramid ? R ? ? , ? {0,</formula><p>1} is first extracted with several residual blocks <ref type="bibr" target="#b8">[9]</ref>, where ? {1, . . . , } denotes different layers and , , are the channel number, height and width of the feature from the -th layer. The first ? 1 features only serve as the image features for subsequent modules, while feature from the deepest layer is used for correspondence matching to generate motion feature.</p><p>For a pixel at spatial position ( , ), we perform NCM to compute the correspondences in ? windows :</p><formula xml:id="formula_2">0 ( , ) = { 0 ( + 0 , + 0 ) ? 1 ( + 1 , + 1 )} , 0,1<label>(2)</label></formula><p>where , 0,1 ? {? /2, ? /2 + 1, . . . , /2} denote different location pairs in the window, and ? denotes the channel-wise dot production. The computed correspondence matrix</p><formula xml:id="formula_3">0 ? R 4 ? ?</formula><p>contains correlations of all pairs in the neighborhoods, which can be further leveraged to extract motion information.</p><p>To enlarge the receptive field and capture large motion, we further perform multi-scale correspondence matching. As shown in  </p><formula xml:id="formula_4">( , ) = { 0 ( + 0 , + 0 )? 1 ( + 1 , + 1 )} , 0,1 (3) where ( , ) = ( /2 , /2 )</formula><p>is the position of pixel in the downsampled feature map. We use bilinear interpolation for non-interger position. And the final multi-scale neighbor correspondences can be generated by simply concatenating correspondences at different levels:</p><p>= 0 | 1 | ? ? ? | (4) where | denotes channel concatenation. In this paper, we extract = 4 layers feature pyramid with 1, 1/2, 1/4, 1/8 resolutions of input frames, and perform NCM in 4 scales ( = 3) with window size = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Heterogeneous Coarse-to-Fine flow estimation</head><p>Existing coarse-to-fine flow estimation manners <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b34">36]</ref> usually adopt the same upsampling factor and the same model structure In the left we show the original IFBlock in IFNet-HD <ref type="bibr" target="#b9">[11]</ref>, which is used as fine-scale IFBlock in the proposed network. In the right we show our modification to add image features and motion features into IFBlock, which is used only in coarse-scale IFBlock .</p><p>from coarse to fine-scale. However, it may not be the best solution for coarse-to-fine scheme, because the coarse-scale and fine-scale have different focus on motion estimation. In coarse-scale, the flows need to be estimated from scratch, so strong motion capture capability is preferred. In fine-scale, we only need to refine the coarse-scale-flows, which can be done with fewer cost. Based on such idea, we propose a heterogeneous coarse-to-fine structure to adopt different module designs for coarse and fine-scale. Our heterogeneous coarse-to-fine structure comprises of a coarsescale module and a fine-scale module. To adapt to different input resolutions, we downsample 0 , 1 to (?, ) to feed into the coarsescale module, and the value of (?, ) can be decided by the resolution of the input video. The estimated coarse flow is upsampled back to original resolution and fed to the subsequent fine-scale module.</p><p>The coarse-scale module is designed to leverage the neighbor correspondences for more accurate flows. In detail, we perform NCM to obtain feature pyramid and neighbor correspondences , which are fed into three augmented IFBlocks to estimate the coarse-scale flows. As shown in <ref type="figure" target="#fig_2">Fig.5</ref> right, in each IFBlock, we warp to generate an image feature , and fuse and flows for a motion feature . The residual flows and mask ? , ? are estimated to refine that of the previous block :</p><formula xml:id="formula_5">= ?1 + ?<label>(5)</label></formula><formula xml:id="formula_6">= ?1 + ?<label>(6)</label></formula><p>For the fine-scale module, we directly adopt original IFBlocks to be more computationally efficient. Two IFBlocks receive the finescale flows as input, and refine it only using the high-resolution frames. Finally, the estimated intermediate flows are fed into the synthesis network to generate the synthesized frame?as output.</p><p>The estimation resolution in each IFBlock can be controlled flexibly by the size (?, ) and the downsample factor , to adapt to the resolution of the input video. Assume &lt; , we use parameter to control the the size by (?, ) = ( , / ? ). In the fine-scale module, we set = (2, 1) if / &lt; 1/2 otherwise set = (1, 1). We set = (4, 2, 1) in the coarse-scale module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Progressive Learning</head><p>Many existing frame synthesis schemes cannot be well extended to applications due to the resolution gap between training and inference. That is, the training data is low-resolution (e.g., 256?256) but the resolution of real-world data may be much higher (e.g., 1080p or 4K). To address this problem, we design a progressive learning scheme for the proposed network. The basic idea is to separate the end-to-end training into two stages to simulate the inference on high-resolution videos:</p><p>? In stage I, only the coarse-scale module is trained on lowresolution 256 ? 256 frames. It can be regarded as training on the low-resolution version of real-world high-resolution images. ? In stage II, the coarse-scale module is fixed, and the fine-scale module is trained to refine the coarse-scale flows to highresolution. We randomly downsample images to 64 ? 64, 128 ? 128 or keep 256 ? 256 in each mini-batch to generate pseudo low-high resolution data pairs for training.</p><p>In inference, the coarse-module can estimate accurate low-resolution flows with stage I, and the fine-scale module can refine such flows to high-resolution with stage II. As a result, our model can be effectively adapted to high-resolution videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss function</head><p>Following RIFE, we adopt a self-supervised privileged distillation scheme to supervise the estimated flows directly. In detail, an additional teacher IFBlock is stacked to refine the estimated flows using the current frame as input, and the generated and can supervise the intermediate flows with a distillation loss:</p><formula xml:id="formula_7">= ?? ? {0,1} || ? ? ? ||<label>(7)</label></formula><p>which is applied over all estimated flows from each IFBlock. The gradient of the distillation loss is stopped for the teacher module. The overall training loss consists of the reconstruction loss of the student , the teacher and the privileged distillation loss :</p><formula xml:id="formula_8">= + +<label>(8)</label></formula><p>where the reconstruction loss is defined as the 1 loss between the Laplacian pyramid representations of the synthesized frame and the ground truth. is set to 0.01 by default.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experimental Setup</head><p>Training Data. We use the Vimeo-90k <ref type="bibr" target="#b38">[40]</ref> training split, which has 51,312 triplets with a resolution of 448 ? 256. We augment the dataset by randomly flipping, temporal order revering, and cropping 256 ? 256 patches.</p><p>Training Strategy. We use AdamW <ref type="bibr" target="#b19">[21]</ref> to optimize our model with weight decay 10 ?3 . The learning rate is gradually reduced from 3 ? 10 ?4 to 3 ? 10 ?5 using cosine annealing for each stage in progressive learning. The batch size is set to 64, and we use 4 Telsa V100 GPU to train the coarse-scale module for 230k iterations in stage I and train the other parts for 76k iterations in stage II. It takes about 40 hours for training in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmarks</head><p>We evaluate our scheme on various benchmarks to verify its performance and generalization ability. On each dataset, we measure the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) for quantitative evaluation.</p><p>X4K1000FPS <ref type="bibr" target="#b31">[33]</ref>: It is a high-quality dataset of 4K videos, which is challenging due to the high resolution, occlusion, large motion and the scene diversity. The provided X-TEST set supports 8? interpolation evaluation on two frames with a temporal distance of 32 frames, and we also use it to evaluate 2? extrapolation by synthesising the 32?nd frame using the 0?th and the 16-th.</p><p>SNU-FILM <ref type="bibr" target="#b5">[6]</ref>: It contains 1,240 triplets of 240fps videos with resolution from 640 ? 368 to 1280 ? 720. Four settings -Easy, Medium, Hard, and Extreme -are provided to evaluate from small motion to large motion, and the temporal distance of each setting increases from 2 (120fps ? 240fps) to 16 (15fps ? 30fps).</p><p>UCF101 <ref type="bibr" target="#b32">[34]</ref>: Liu et al. <ref type="bibr" target="#b18">[20]</ref> selected 379 triplets from UCF101 human actions dataset for video frame synthesis evaluation. However, there are some dirty data in the test set. We will demonstrate it in detail in the supplementary material.</p><p>Vimeo90K <ref type="bibr" target="#b38">[40]</ref>: The test set of Vimeo90K contains 3,782 triplets with resolution of 448 ? 256. We evaluate on it to verify the robustness of our model on low-resolution videos.</p><p>Different downsample size (?, ) is set to adapt to the resolution of different benchmark. We set = 384 for X4K1000FPS and = 256 for other benchmarks (the definition of is in Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Previous Methods</head><p>For video frame interpolation, we compare the proposed scheme with previous methods : DAIN <ref type="bibr" target="#b1">[2]</ref>, AdaCoF <ref type="bibr" target="#b14">[16]</ref>, XVFI <ref type="bibr" target="#b31">[33]</ref>, Soft-Splat <ref type="bibr" target="#b23">[25]</ref>, BMBC <ref type="bibr" target="#b25">[27]</ref>, ABME <ref type="bibr" target="#b26">[28]</ref> and RIFE <ref type="bibr" target="#b9">[11]</ref>. For extrapolation, we compare with DVF <ref type="bibr" target="#b18">[20]</ref>, and re-implement RIFE on the extrapolation task for comparison.</p><p>Following RIFE <ref type="bibr" target="#b9">[11]</ref>, we also introduce a Large version of our model to meet the need of different scenarios with different computation cost. Two modification are performed : 1) test-time augmentation, to inference twice with the original input frames and the flipped frames then average the results, and 2) model scaling, to double the resolution of feature map in each IFBlock and the synthesis network by removing the first stride. More information can be found in the supplementary material.   <ref type="bibr" target="#b18">[20]</ref> and RIFE <ref type="bibr" target="#b9">[11]</ref>. Best viewed in zoom.</p><p>We report the quantitative results of interpolation in <ref type="table">Table.</ref>1. Our large model achieves the best performance in all benchmarks except for the SNU-FILM Easy setting. Compared with previous state-ofthe-art scheme ABME, our base model improves the average PSNR by 0.27dB with 17 times faster runtime in 1080p videos, and our large model improves by 0.46dB with 3.3 times faster runtime. It is worth noting that our base and large models outperform ABME by surprising 1.47dB (30.16dB?31.63dB) and 1.70dB (30.16dB?31.86dB) in the most challenging X4K1000FPS benchmark, which shows its capability in capturing large motion in high-resolution videos. Compared with the real-time scheme RIFE-Large, our base model shows 0.50 dB improvement with 4.2 times faster runtime. Visual comparison can be found in <ref type="figure">Fig.1 and the supplementary material.</ref> We also report the quantitative results of extrapolation in <ref type="table">Table.</ref>2, and the visual comparison is shown in <ref type="figure" target="#fig_3">Fig.6</ref>. Compared with RIFE, our model achieves better performance in all benchmarks except for SNU-FILM Easy and Medium setting, leading to an average improvement of 0.23dB (base model) and 2.08dB (large model) in terms of PSNR. And we find that RIFE-Large shows much worse performance than RIFE in high-resolution videos (e.g., 4.49dB on X4K1000FPS and 2.02dB drop on SNU-FILM), which is also observed on the interpolation benchmark where RIFE-Large shows 0.2dB drop in X4K1000FPS. It is because when doubling feature resolution, the receptive field of the network is halved. However, our model is not affected much by the resolution of feature map because the receptive field is guaranteed by NCM instead of convolution layers. It means our scheme can be better extended to the large version for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Add-on Study.</head><p>To understand the effectiveness of the proposed components, we perform an ablation study to add each component on a baseline in Table3. The baseline (last line in the table) consists of five IFBlocks, and between each two blocks the upsampling rate is set to 2 in both training and inference.</p><p>Progressive learning. It makes the model adapt to high resolution 4K videos (0.93 dB improvement on X4K1000FPS). However, the performance on 256p videos sightly drops. This is because the baseline is only trained for low-resolution 256p videos, while progressive learning is designed to refine high-resolution videos on low-resolution 256p videos. So if we resize 256p videos to 480p, such limitation is eliminated and achieves 0.14dB improvement.</p><p>Neighbor correspondence matching comprises of a feature pyramid to extract image features and a matching process to extract motion features. The image features mainly improves performance on low-resolution videos, and the matching-based motion features can enhance performance on both high-resolution (0.65dB) and low-resolution (0.12dB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.2</head><p>Coarse-to-fine and the matching region. We also perform an ablation study on the coarse-to-fine structure and the matching region in NCM in <ref type="table">Table.</ref>4.</p><p>Heterogeneous coarse-to-fine. Compared with using NCM in both coarse and fine-scale (normal coarse-to-fine in <ref type="table">Table.</ref>4), the proposed heterogeneous coarse-to-fine causes 3 times shorter runtime with better performance on 4K videos and comparable performance on 256p videos. It shows the efficiency to design different module structure for the coarse and fine-scale modules. Using NCM in the fine-scale module in 4K videos can cause performance drop, because in such high-resolution videos, the content is so similar in the neighbor regions that the correspondence is not effective to capture the motion.</p><p>Matching region. We compare the proposed neighbor correspondence matching with the flow-guided matching. Guiding matching by flow causes performance to drop by 0.07dB in X4K1000FPS. It is because the matching region may be misled by the inaccurate estimated flows. And runtime is also longer by 10ms since more matching operations are needed. In addition, we find that using flow-guided matching is more likely to cause collapse in training. It indicates the flow-guided matching is less stable since the matching region is influenced by estimated flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATION : VIDEO COMPRESSION</head><p>The proposed scheme can be applied to various scenarios due to the powerful capability in capture large motion in high-resolution videos. For example, it can be applied in video compression, jitters removal in real-time communication system, and motion effects generation. In this section, we use video compression as an example to show its potential. Experiments and demos about other applications an be found in the supplementary material.</p><p>Most video compression schemes <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b39">41]</ref> adopt a motion estimation and motion compensation (MEMC) paradigm. They predict the current frame using the optical flow, and then compress the prediction residual to reconstruct the compressed frame. Many works <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b39">41]</ref> focus on improving the residual compression process, but the temporal redundancy in the optical flow is not well considered. We can eliminate such redundancy partly by using the proposed module for motion prediction.We design a scalable bi-directional video compression model using NCM. It can serve as a plugin-in on any uni-direction codec (e.g., P-frame codec [10, 32]) to compress B-frame, and the whole video can be compressed in order of I-B-P-B-? ? ? . The model supports a bit-free mode for low-cost compression and a bit-need mode for high-quality compression.</p><p>The bit-free mode simply adopts our video frame synthesis model to interpolate the B-frame. It means the codec does not need to encode and decode code streams of B-frame, and this frame can be interpolated using much less cost. We evaluate it on the state-of-theart learned P-frame codec TCM <ref type="bibr" target="#b30">[32]</ref>, and the rate-distortion curve on HEVC test videos <ref type="bibr" target="#b33">[35]</ref> is shown in <ref type="figure" target="#fig_4">Fig.7</ref>. Our model can save 10.4% bits (0.065bpp?0.058bpp) under PSNR=32.0dB and save 17.2% bits (0.063bpp?0.052bpp) under MS-SSIM=0.9650. In addition, our model can inference about 9 times faster in B-frames, from 500ms encoding and 252ms decoding time of TCM to 0ms encoding and 82ms interpolation time. It demonstrates the efficiency of bit-free mode in low-cost scenarios such as video communication.</p><p>The bit-need mode adopts our video frame synthesis model as a motion prediction module, and compress the residual motion instead of the entire motion. Experiments show it can save 17.5% bits (from 0.065bpp to 0.053bpp) under PSNR=32.0 on TCM. We also compare it with H.265-HM [10] under the same sequence order in <ref type="figure" target="#fig_5">Fig.8</ref>, and results show 10% bits saving (from 0.111bpp to 0.100bpp) on the whole sequence under PSNR=34.0dB. It shows our model can serve as a superior motion prediction module for video compression. More details of experiment settings and experiment results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS</head><p>Even the proposed neighbor correspondence matching algorithm can capture large motion for high-resolution videos, it may estimate inaccurate flows on similar details in the video. Because the matching is guided by feature similarity, neighbor regions with similar appearance cannot be distinguished by matching. As shown in <ref type="figure" target="#fig_6">Fig.9</ref>, our model fails on estimating repeated stripes on the house. It also   limits NCM to be used in fine-scale module in 4K videos because in such scale the neighbor region is usually full of similar regions. We hope to solve it in the future by distinguishing regions with similar appearance using techniques like positional embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we propose a neighbor correspondence matching algorithm for video frame synthesis, which can capture large motion even in high-resolution videos. With the proposed heterogeneous coarse-to-fine structure design and the progressive learning, our model is both effective and efficient and can adapt to high-resolution videos. Experiments show the superiority of our model in both interpolation and extrapolation. In addition, our model can be used in many applications, and we use video compression as an example to show its potential. We hope it can be extended to more applications in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILS OF EXPERIMENTS A.1 Test-Time Augmentation and Model Scaling</head><p>Following RIFE <ref type="bibr" target="#b9">[11]</ref>, we also introduce a Large version of our model by performing test-time augmentation and model scaling. We use the same strategy as in RIFE : 1) We flip the input frames horizontally and vertically as the augmented test data, and infer the results on the augmented data. The results are flipped back and then averaged to generate the final results. 2) We double the resolution of the feature map. For each IFBlocks and the synthesis network, we remove the stride on the first convolution layer and the last transposed convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Runtime Analysis</head><p>We also visualize the runtime of different components in <ref type="figure" target="#fig_7">Fig.10</ref>.</p><p>Since the feature pyramid and the neighbor correspondence matching are only performed in coarse-scale, it will not cause much time cost. The main time cost comes from stacked convolutional layers in IFBlocks and the synthesis network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Extrapolation Settings</head><p>The extrapolation task is more difficult to interpolation, so we adopt different experimental settings. First, We remove the distillation loss because we find that it leads to collapse in training. Second, instead of directly estimate the intermediate flow ( ?0 , ?1 ), we estimate ( 01 , ?1 ) and compute ?0 = 2 ? ?1 + 01 . It avoids the unbalance temporal distance between current frame and two reference frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Baseline Settings</head><p>All baselines are tested using the available official weights except for RIFE for extrapolation, because RIFE is not implemented for extrapolation in the original paper. We re-implement RIFE as the baseline for extrapolation. We find setting base learning rate to 3 ? 10 ?4 cases collapse in training, so we set it to 1 ? 10 ?4 . The distillation loss is not used since it also causes collapse. In RIFE, we can change the downsample rate to adapt to different resolutions. It is set to (4, 2, 1) in SNU-FILM, UCF101 and Vimeo90K benchmark like the official recommendation, and is set to <ref type="bibr" target="#b14">(16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1)</ref> in X4K1000FPS to enlarge receptive fields. We find it to perform much better than (4, 2, 1) in X4K1000FPS (from PSNR=24.57dB to 28.94dB in interpolation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Dirty Data in UCF101</head><p>Liu et al. <ref type="bibr" target="#b18">[20]</ref> selected 378 triplets from UCF101 as test set for video frame synthesis. However, we find some dirty data (98 triplets) in the test set. As shown in <ref type="figure" target="#fig_8">Fig.11</ref>, such dirty data have two same frames in the triplet (e.g., the 2nd G.T. and 3d frame are the same), which leads to wrong ground truth for evaluation. We exclude these dirty data and perform comparison experiments with RIFE in <ref type="table">Table.</ref>5. We display ID of these dirty data in <ref type="table">Table.</ref>6 for reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B VISUAL COMPARISON</head><p>We conduct the visual comparison with XVFI <ref type="bibr" target="#b31">[33]</ref> and ABME <ref type="bibr" target="#b26">[28]</ref> for interpolation, and with RIFE and DVF <ref type="bibr" target="#b18">[20]</ref> for extrapolation.</p><p>Compared with previous methods, our network can capture larger motion and more details. In addition, our network can preserve more complete structural information.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C APPLICATION : VIDEO COMPRESSION</head><p>In the paper, we apply the proposed frame synthesis network in video compression, and propose a plugin-and-in bi-directional video codec. It has two modes : a bit-free mode for low-cost and lowlatency scenarios like video communication, and a bit-need mode for high-quality compression. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Model Structure</head><p>The bit-free mode adopts the proposed frame synthesis network to interpolate the B-frame directly. The bit-need mode consists of three parts as shown in <ref type="figure" target="#fig_11">Fig.13</ref> :</p><p>C.1.1 Motion Prediction. It use the intermediate flow estimation part of proposed frame synthesis network to predict the intermediate flow and mask (?,?). The predicted motion can be obtained in both encoder and decoder without bits transmission.</p><p>C.1.2 Motion Residual Refine. When we train the frame synthesis network, a Teacher block is also trained for the distillation loss. We directly adopt this block to refine the predicted flows with the current frame as input.</p><p>C.1.3 Residual-Motion Coding. We compress the residual motion between the refined flows and the predicted flows. We use a convolution layer to learn a motion context?from the predicted flows, and perform contextual coding <ref type="bibr" target="#b16">[18]</ref> to compression the residual flow. Using the compressed residual motion, more accurate motion ,^can be reconstructed in the decoder.</p><p>C.1.4 Contextual Coding. Following TCM <ref type="bibr" target="#b30">[32]</ref>, we perform multiscale context extracting using the compressed flows. The extracted contexts0 ,1,2 are leveraged to compression image residual by contextual coding. Finally, we use a frame generator to reconstruct the decoded frame^.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Experimental Settings</head><p>The proposed bi-directional video codec can serve as a plugin-in on any existing uni-directional video codec to compress the whole video sequence in I-B-P-B order.  </p><formula xml:id="formula_9">= ? ( ,^) + +<label>(9)</label></formula><p>where and denote the bit rate in residual motion coding and contextual coding. is set to 256. We use AdamW <ref type="bibr" target="#b19">[21]</ref> and batch size of 4 to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.3</head><p>Benchmark. We test the model in HEVC standard testing videos <ref type="bibr" target="#b33">[35]</ref>. It contains 16 sequences including Class B, C, D, and E with different resolutions.</p><p>C.2.4 Baseline. In the paper we perform two experiment to compare with TCM <ref type="bibr" target="#b30">[32]</ref> and H.265-HM <ref type="bibr">[10]</ref>. TCM is the state-of-the-art learned P-frame codec. HM is the reference software for Rec. ITU-T H.265 | ISO/IEC 23008-2 High Efficiency Video Coding (HEVC). The intra period is set to 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Experiment Results</head><p>We use TCM as the P-frame codec to compare with TCM in <ref type="figure" target="#fig_0">Fig.14.</ref> Our bit-free model is better or comparable with TCM on MS-SSIM, and outperforms TCM in HEVC D and E class on PSNR. In addition, the runtime on B-frame is about 9 times faster, from 500ms encoding and 252ms decoding time of TCM to 0ms encoding and 82ms interpolation time of the proposed model. Our bit-need model outperforms TCM in all datasets.</p><p>We further use HM as the P-frame codec to compare the bitneed mode with HM in <ref type="figure" target="#fig_2">Fig.15</ref>. We test HM with the same I-B-P-B sequence order as ours, and results show that our bit-need model outperforms HM in all datasets.</p><p>An example is shown in <ref type="figure" target="#fig_3">Fig.16</ref>, where we show the 17th and 18th frames of a compressed video. The proposed codec cost more bits in P-frame while it saves much more in B-frame. As a result, the overall performance in the whole sequence is better than the baseline (TCM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D APPLICATION : MOTION EFFECT</head><p>We can use the proposed frame synthesis network to generate motion effects like motion blur and slow motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Motion Blur Generation</head><p>Motion blur is the apparent streaking of moving objects in a sequence of frames. In film or animation, simulating motion blur can lead to more realistic visuals. Simulating motion blur can also generate dataset for deblurring networks.</p><p>Our interpolation model can be applied to generate such motion blur. We interpolate 31 frames between reference frames, and simply average them to generate the blurred image. As shown in <ref type="figure" target="#fig_4">Fig.17</ref>, the generated images are blurred in the fast moving regions while keep high quality in static regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Slow Motion Generation</head><p>Slow motion (slo-mo) is a common effect in film-making where time seems to be slowed down. Jiang et al. <ref type="bibr" target="#b10">[12]</ref> proposed to use interpolation to generate slo-mo in videos. Our model can also be applied to generate slo-mo by interpolating multiple intermediate frames between given frames. Examples can be found in the video demos.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E APPLICATION : JITTER REMOVAL</head><p>In real-time communication (RTC), a long-standing problem is video jitter. There are many reasons for video jitter, such as bandwidth limitation and network transmission limitation. Here we mainly focus on the bandwidth limitation problem. Bandwidth limitation is a common problem in terminal devices. If the bandwidth is limited, the video must be compressed at a low rate, resulting in poor visual quality and user experience. Our model can be applied to remove such jitters to improve user experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Baselines</head><p>When the bandwidth is limited, in practice there are two straightforward solutions : 1) transmit high frame-rate video at low compression quality, and 2) transmit low frame-rate video at high compression quality. We denote them as "scheme1" and "scheme2" correspondingly.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Methods</head><p>We can transmit video at low frame-rate and high compression quality (scheme2), and use the proposed frame synthesis network   to synthesis untransmitted frames. We propose two schemes to meet the need of different scenarios.</p><p>The first scheme (denoted as "Ours-Interpolation") utilize interpolation to synthesis untransmitted frames. As shown in <ref type="figure" target="#fig_6">Fig.19</ref>, after decoding the 5th frame, we wait for the 9th frame, and interpolate the 6th, 7th, 8th frames for display. This scheme can synthesis high quality frames, but it causes 3 frames delay in display.</p><p>The second scheme (denoted as "Ours-Extrapolation") utilize extrapolation to synthesis untransmitted frames without delay. As shown in <ref type="figure" target="#fig_18">Fig.20</ref>, after decoding the 5th frame, we use the 1st and 5th frames to extrapolate the 9th frame, and then interpolate the 6th, 7th, 8th frames. Since the non-linear motion and large temporal distance in low frame rate, the extrapolated frame may differ a lot from the raw frame. It leads to temporal inconsistency between the 4th and 5th (or between 8th and 9th) frames. To eliminate such limitation, we further use the 4th and 6th frame to interpolate the 5th frame for display. It can improve temporal smoothness in video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Results</head><p>We show an example in <ref type="figure" target="#fig_5">Fig.18</ref>, where 5 frames in a video is shown. Scheme 1 suffers from the low visual quality and compression artifacts, and scheme 2 produces stuck playback. Compared with them, our methods is smoother and have higher quality, resulting in a better user experience. Please see the video demo for better visual comparison. Note that scheme 1 is not shown in video demo since the compression of video demo will influence the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of neighbor correspondence matching at the -th scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 ,</head><label>4</label><figDesc>we first downsample to = 1/2 resolution to generate multi-scale features , ? {0, 1, . . . , }. For each level , the correspondences can be computed by :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Structure of IFBlock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Visual comparisons on extrapolation with DVF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Apply the proposed frame synthesis model in video compression. The designed plugin-and-in bi-directional codec can achieve higher compression rate compared with the baseline TCM<ref type="bibr" target="#b30">[32]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Compare the proposed B-frame video codec with H.265-HM [10].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>A failure case. Our model cannot estimate accurate flows on large region of repeated stripes, which may cause artifacts in the synthesised frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Runtime of each components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>An example of dirty data in UCF101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>C</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Visual quality comparison. Best view in zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Model structure of the proposed bit-need bi-direction video codec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Compression experiment results on HM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>An exmaple of proposed video compression. Best view in zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 :</head><label>17</label><figDesc>Motion blur generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Compression experiment results on TCM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 18 :</head><label>18</label><figDesc>Jitter removal in bandwidth limited real-time communication. Best view in zoom in. Please see the video demo for better visual comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 19 :</head><label>19</label><figDesc>Jitter removal by interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 20 :</head><label>20</label><figDesc>Jitter removal by extrapolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison(PSNR/SSIM) of video frame interpolation results. The best result in each set in shown in red and the second best is shown in blue. Runtime and parameters are tested on the same device under the same settings, except for results with * copied from the original papers.Large  28.94/0.8721 40.23/0.9907 35.86/0.9792 30.19/0.9332 24.81/0.8540 35.41/0.9700 36.13/0.9800 33.Large 31.86/0.9225 40.14/0.9905 36.12/0.9793 30.88/0.9370 25.70/0.8647 35.43/0.9700 36.22/0.9807 33.76/0.</figDesc><table><row><cell>Model</cell><cell>X4K1000FPS</cell><cell>Easy</cell><cell cols="2">SNU-FILM Medium Hard</cell><cell>Extreme</cell><cell>UCF101</cell><cell>Vimeo90K</cell><cell>Average</cell><cell cols="3">Runtime (ms) Parameters 480p 1080p (Million)</cell></row><row><cell>DAIN</cell><cell cols="9">26.78/0.8065 39.73/0.9902 35.46/0.9780 30.17/0.9335 25.09/0.8584 34.99/0.9683 34.71/0.9756 32.42/0.9300 130*</cell><cell>-</cell><cell>24.0*</cell></row><row><cell>AdaCoF</cell><cell cols="8">23.90/0.7271 39.80/0.9900 35.05/0.9754 29.46/0.9244 24.31/0.9439 34.90/0.9680 34.47/0.9730 31.70/0.9288</cell><cell>36</cell><cell>210</cell><cell>21.8</cell></row><row><cell>XVFI</cell><cell cols="8">30.12/0.8704 39.92/0.9902 35.37/0.9777 29.57/0.9272 24.17/0.8448 35.18/0.9685 35.07/0.9756 32.77/0.9363</cell><cell>60</cell><cell>398</cell><cell>5.7</cell></row><row><cell>SoftSplat</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">35.39/0.9700 36.10/0.9800</cell><cell>-</cell><cell>135*</cell><cell>-</cell><cell>7.7*</cell></row><row><cell>BMBC</cell><cell cols="9">29.35/0.8791 39.90/0.9902 35.31/0.9774 29.33/0.9270 23.92/0.8432 35.15/0.9689 35.01/0.9764 32.57/0.9374 894</cell><cell>5787</cell><cell>11.0</cell></row><row><cell>ABME</cell><cell cols="9">30.16/0.8793 39.59/0.9901 35.77/0.9789 30.58/0.9364 25.42/0.8639 35.38/0.9698 36.18/0.9805 33.30/0.9425 226</cell><cell>1386</cell><cell>17.6</cell></row><row><cell>RIFE</cell><cell cols="8">29.14/0.8765 40.02/0.9905 35.73/0.9787 30.08/0.9328 24.82/0.8530 35.28/0.9690 35.61/0.9779 32.95/0.9398</cell><cell>12</cell><cell>56</cell><cell>10.1</cell></row><row><cell cols="9">RIFE-08/0.9399</cell><cell>54</cell><cell>350</cell><cell>10.1</cell></row><row><cell cols="9">Ours-Base 31.63/0.9185 39.98/0.9903 35.94/0.9788 30.72/0.9359 25.55/0.8624 35.36/0.9695 35.88/0.9795 33.58/0.9478</cell><cell>38</cell><cell>82</cell><cell>12.1</cell></row><row><cell cols="10">Ours-9492 122</cell><cell>419</cell><cell>12.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Large 21.85/0.8014 33.02/0.9501 29.83/0.9274 25.78/0.8666 22.10/0.7941 31.78/0.9447 32.07/0.9611 28.06/0.Large 28.05/0.8786 36.40/0.9814 32.24/0.9561 27.41/0.8895 23.00/0.8120 31.81/0.9449 32.10/0.9614 30.14/0.9177 122 419 12.1</figDesc><table><row><cell>Model</cell><cell>X4K1000FPS</cell><cell>Easy</cell><cell>SNU-FILM Medium Hard</cell><cell>Extreme</cell><cell>UCF101</cell><cell>Vimeo90K</cell><cell>Average</cell><cell cols="2">Runtime (ms) Parameters 480p 1080p (Million)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78/0.9115</cell><cell>12</cell><cell>56</cell><cell>10.1</cell></row><row><cell cols="8">RIFE-8922</cell><cell>54</cell><cell>350</cell><cell>10.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>01/0.9154</cell><cell>38</cell><cell>82</cell><cell>12.1</cell></row><row><cell>Ours-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Quantitative comparison(PSNR/SSIM) of video frame extrapolation results. The best result in each set in shown in red and the second best is shown in blue.DVF 19.18/0.6879 25.39/0.8728 23.30/0.8279 21.41/0.7798 19.49/0.7251 31.29/0.9433 26.21/0.8815 23.75/0.8169 40 256 3.8 RIFE 26.34/0.8456 36.76/0.9821 32.31/0.9566 27.10/0.8880 22.59/0.8061 31.69/0.9441 31.68/0.9580 29.Ours-Base 27.69/0.8694 36.46/0.9814 32.21/0.9557 27.28/0.8881 22.84/0.8090 31.72/0.9444 31.84/0.9595 30.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Add on study of progressive learning and NCM. PSNR/SSIM and the runtime(ms) are reported.</figDesc><table><row><cell>Progressive Learning</cell><cell cols="3">NCM image feature motion feature</cell><cell>X4K1000FPS</cell><cell>256p</cell><cell cols="2">Vimeo90K</cell><cell>480p</cell><cell>Runtime @1080p</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell cols="4">31.63/0.9185 35.88/0.9795 36.44/0.9816</cell><cell>82</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>30.98/0.9087</cell><cell cols="2">35.76/0.9788</cell><cell>36.35/0.9812</cell><cell>69</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>30.93/0.9088</cell><cell cols="2">35.36/0.9773</cell><cell>36.07/0.9803</cell><cell>63</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>30.00/0.8937</cell><cell cols="2">35.53/0.9779</cell><cell>35.93/0.9796</cell><cell>63</cell></row><row><cell cols="5">Table 4: Ablation study on coarse-to-fine manner and match-</cell><cell></cell><cell></cell></row><row><cell cols="4">ing region. PSNR/SSIM and runtime(ms) are reported.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Setting</cell><cell>X4K1000FPS</cell><cell>Vimeo90K</cell><cell>Runtime @1080p</cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/ normal coarse-to-fine</cell><cell cols="2">31.35/0.9151 35.93/0.9797</cell><cell>244</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">w/ flow-guided matching 31.56/0.9180</cell><cell>35.83/0.9793</cell><cell>92</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours-base</cell><cell cols="2">31.63/0.9185 35.88/0.9795</cell><cell>82</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(Heterogeneous coarse-to-fine, neighbor matching)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Quantitative comparison(PSNR/SSIM) on UCF101. Dirty data are excluded.</figDesc><table><row><cell>Model</cell><cell cols="2">Interpolation Extrapolation</cell></row><row><cell>RIFE</cell><cell>35.56/0.9745</cell><cell>31.95/0.9536</cell></row><row><cell cols="2">RIFE-Large 35.67/0.9752</cell><cell>32.06/0.9545</cell></row><row><cell>Ours-Base</cell><cell>35.64/0.9752</cell><cell>31.98/0.9541</cell></row><row><cell cols="3">Ours-Large 35.75/0.9757 32.08/0.9548</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Dirty data IDs in UCF101.</figDesc><table><row><cell>1</cell><cell>141</cell><cell>171</cell><cell>191</cell><cell>231</cell><cell>241</cell><cell>281</cell><cell>291</cell><cell>371</cell></row><row><cell>401</cell><cell>441</cell><cell>451</cell><cell>511</cell><cell>571</cell><cell>611</cell><cell>661</cell><cell>711</cell><cell>721</cell></row><row><cell>731</cell><cell>971</cell><cell cols="7">981 1001 1021 1041 1081 1091 1111</cell></row><row><cell cols="9">1221 1231 1241 1251 1261 1321 1361 1401 1411</cell></row><row><cell cols="9">1441 1451 1471 1501 1511 1531 1551 1601 1721</cell></row><row><cell cols="9">1741 1811 1901 1911 1961 2051 2061 2111 2191</cell></row><row><cell cols="9">2281 2291 2301 2361 2381 2401 2431 2441 2451</cell></row><row><cell cols="9">2471 2481 2491 2531 2671 2691 2711 2751 2761</cell></row><row><cell cols="9">2781 2881 2921 2971 3011 3061 3071 3111 3121</cell></row><row><cell cols="9">3131 3171 3181 3211 3231 3241 3311 3321 3341</cell></row><row><cell cols="8">3361 3381 3391 3441 3601 3651 3661 3721</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.2.1 Training Data. We use Vimeo-90K[40] training set. Videos are randomy cropped to 256 ? 256 to train the bit-need model. C.2.2 Training Strategy. We train the model to jointly optimize the rate-distortion cost :</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3703" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="933" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to synthesize motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6840" to="6848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking space-time networks with improved memory coverage for efficient video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Ho Kei Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Channel attention is all you need for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10663" to="10671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rife: Real-time intermediate flow estimation for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06294</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to estimate hidden motions with global motion aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9772" to="9781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning-based view synthesis for light field cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Nima Khademi Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CCVS: Contextaware Controllable Video Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><forename type="middle">Le</forename><surname>Moing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adacof: Adaptive collaboration of flows for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeoh</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Pak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5316" to="5325" />
		</imprint>
	</monogr>
	<note>Yuseok Ban, and Sangyoun Lee</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video prediction recalling long-term motion context via memory alignment learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dae Hwi</forename><surname>Hak Gu Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung-Il</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3054" to="3063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep contextual video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced quadratic video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="41" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4463" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dvc: An end-to-end deep video compression framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11006" to="11015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An end-to-end learning framework for video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3292" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5437" to="5446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="109" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Asymmetric bilateral motion estimation for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14539" to="14548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extending neural p-frame codecs for bframe coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Pourreza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6680" to="6689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Temporal Context Mining for Learned Video Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihua</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13850</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">XVFI: Extreme video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjun</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14489" to="14498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Overview of the high efficiency video coding (HEVC) standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens-Rainer</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woo-Jin</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1649" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">PredRNN: A recurrent neural network for spatiotemporal predictive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09504</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video compression through image interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="416" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning for video compression with recurrent auto-encoder and recurrent probability model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="388" to="401" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by multi-scale foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
