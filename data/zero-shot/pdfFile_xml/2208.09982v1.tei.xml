<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
							<email>qianqian.xie@manchester.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chancefocus</forename><forename type="middle">Amc</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tulika</forename><surname>Saha</surname></persName>
							<email>tulika.saha@manchester.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
							<email>sophia.ananiadou@manchester.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">National Centre for Text Mining</orgName>
								<orgName type="institution" key="instit2">The University of Manchester</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">National Centre for Text Mining</orgName>
								<orgName type="institution">The University of Manchester</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, neural topic models (NTMs) have been incorporated into pre-trained language models (PLMs), to capture the global semantic information for text summarization. However, in these methods, there remain limitations in how they capture and integrate the global semantic information. In this paper, we propose a novel model, Graph contRastivE Topic Enhanced Language model (GRETEL), that incorporates the graph contrastive topic model with the pre-trained language model, to fully leverage both the global and local contextual semantics for long document extractive summarization. To better capture and incorporate the global semantic information into PLMs, the graph contrastive topic model integrates the hierarchical transformer encoder and the graph contrastive learning to fuse the semantic information from the global document context and the gold summary. To this end, GRE-TEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary, rather than redundant sentences that cover sub-optimal topics. Experimental results 1 on both general domain and biomedical datasets demonstrate that our proposed method outperforms SOTA methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the well-known limitation of pre-trained language models (PLMs) <ref type="bibr" target="#b9">(Devlin et al., 2019;</ref> that they fail to capture long-range dependencies <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref>, attempts have been proposed to integrate neural topic models (NTMs) <ref type="bibr" target="#b3">(Cao et al., 2015;</ref><ref type="bibr" target="#b26">Peng et al., 2018;</ref> into PLMs, which have shown significant improvement in the performance of the text summarization task <ref type="bibr" target="#b32">(Wang et al., 2020b;</ref><ref type="bibr" target="#b7">Cui and Hu, 2021b;</ref><ref type="bibr" target="#b25">Nguyen et al., 2021;</ref><ref type="bibr" target="#b10">Fu et al., 2020)</ref>. In addition to the local contextual information captured in PLMs, NTMs can provide an approximation of the global semantics captured from document contents, i.e., latent topics, as well as their posterior topic representations. The global semantics are further used to guide the model, to generate coherent summaries which cover the most relevant topics discussed within the document, via the attention mechanism <ref type="bibr" target="#b32">(Wang et al., 2020b;</ref><ref type="bibr" target="#b0">Aralikatte et al., 2021;</ref><ref type="bibr" target="#b25">Nguyen et al., 2021;</ref><ref type="bibr" target="#b10">Fu et al., 2020)</ref> or graph neural networks (GNNs) <ref type="bibr" target="#b7">(Cui and Hu, 2021b;</ref><ref type="bibr" target="#b8">Cui et al., 2020)</ref>.</p><p>However, there exists the semantic gap between latent topics as the approximation global semantics, and the true global semantics due to two major limitations for existing methods. The first limitation concerns the nature of unsupervised topic inference in these methods, where topics and posterior topic distributions are learned from documents in an unsupervised manner, without considering the key semantic information conveyed in the gold summary (mostly the abstract). Existing methods using document-word features, without accessing the semantic information of the gold summary, can extract sub-optimal topics with high-frequency words. However, sub-optimal topics with highfrequency words, do not necessarily cover the true global semantics that is condensed in the gold summary. This results in the wrong assignment of the document with sub-optimal topics, and con-sequently the model extract redundant sentences containing high-frequency topic words.</p><p>Another limitation is that existing methods rely on document word features such as Bag-of-Words (BOWs) to extract latent topics, and disregard the sequential and syntactic dependency between words. This may lead to sequentially and syntactically correlated words being allocated to different topics. One solution is to provide NTMs with contextual representations from PLMs, which is nevertheless challenging to directly apply in existing methods for text summarization. PLMs in existing methods are forced to truncate the input to a limited length owing to the complexity of language models. Thus the representations from partial content of the document cannot necessarily help NTMs to mine informative topics that cover the whole content of the document, especially for long documents. Overall, although existing methods encourage the summary with sentences that are topically similar to the document topic distribution, the summary focuses more on sentences with high-frequency words and can have low semantic similarity with the gold summary. To help understand these limitations, we make a detailed analysis based on the benchmark datasets in section 3.</p><p>In response to the above, we propose a novel Graph contRastivE Topic Enhanced Language model (GRETEL), that incorporates the graph contrastive topic model (GCTM) empowered by the semantic information of the gold summary and the global document context, with PLMs for long document extractive summarization. The first distinguishing feature of GRETEL is the employment of the hierarchical transformer encoder (HTE) to fully embed the global context of long documents, to inform topic representations of documents and sentences. The global contextual information captured in HTE but missing in the BOW feature, allows the model to learn more discriminative document and sentence topic representations, and coherent topics.</p><p>Secondly, it utilizes graph contrastive learning with the supervised information from the gold summary. It pushes close topic representations of documents and sentences that have high semantic similarity with the gold summary and pulls away otherwise. This encourages the model to capture better global semantic information: latent topics that describe the most key information in the original document content. Therefore, it allows the model to select key sentences that are topically similar to the gold summary. Experimental results demonstrate that our method can effectively distinguish salient sentences from documents with both global and local semantics, leading to superior performance compared to previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Topic enhanced PLMs for Text Summarization. Many studies have investigated the application of PLMs for extractive summarization, including BERTSum <ref type="bibr" target="#b19">(Liu and Lapata, 2019)</ref>, DiscoBert <ref type="bibr" target="#b36">(Xu et al., 2020)</ref>, MatchSum <ref type="bibr" target="#b38">(Zhong et al., 2020)</ref> et al. However, these methods fail to capture the global context of long documents due to the limitation of PLMs. To address it, several studies combined topic modeling with PLMs to introduce global semantic information. <ref type="bibr" target="#b32">Wang et al. (2020b)</ref> proposed to extract firstly latent topics independently and then use them to improve the summarization model. Other studies <ref type="bibr" target="#b0">(Aralikatte et al., 2021;</ref><ref type="bibr" target="#b25">Nguyen et al., 2021;</ref><ref type="bibr" target="#b8">Cui et al., 2020)</ref> proposed to use the BOW as input features for neural topic modeling and improved the transformer encoder and decoder with the extracted latent topics with an attention mechanism for abstractive summarization <ref type="bibr" target="#b0">(Aralikatte et al., 2021;</ref><ref type="bibr" target="#b10">Fu et al., 2020;</ref><ref type="bibr" target="#b25">Nguyen et al., 2021)</ref>. <ref type="bibr" target="#b8">Cui et al. (2020)</ref>; <ref type="bibr" target="#b7">Cui and Hu (2021b)</ref> proposed to use the graph neural network to infuse topics into contextual representations from PLMs, for multidocument abstractive summarization and extractive summarization. <ref type="bibr" target="#b10">Fu et al. (2020)</ref> considered extract both document and paragraph-level topic distribution, and use them to guide the abstractive summarization.</p><p>PLMs for Long Document Summarization. To address the limitation of encoding the full context of long documents using PLMs, another direction is to design the efficient sparse self-attention or using a sliding window. <ref type="bibr" target="#b24">Narayan et al. (2020)</ref> proposed a step-wise model with a structured transformer.  proposed a computationally efficient method based on the head-wise positional strides, to identify salient content for long documents. <ref type="bibr" target="#b20">Liu et al. (2021)</ref> employed a transformer with multi-granularity sparse attentions. <ref type="bibr" target="#b6">Cui and Hu (2021a)</ref> used a sliding selector network with dynamic memory, in which the sliding window is used to encode input documents, segment by segment. <ref type="bibr" target="#b11">Grail et al. (2021)</ref> divided long documents into multiple blocks and encoded them by independent transformer networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset-dependent Analysis for Limitations</head><p>To better illustrate the limitations of existing methods, we present a dataset-dependent analysis, with the aim to answer two key questions: 1) Do the extracted topics from topic models tend to focus on high-frequency words? and 2) Due to it, would there be a semantic gap between topics as the approximation of global semantics and true global semantics in the gold summary?</p><p>We first present the top-10 words of topics learned by the traditional topic model LDA on the PubMed <ref type="bibr" target="#b5">(Cohan et al., 2018)</ref> dataset, as shown in <ref type="table" target="#tab_0">Table 1</ref>. It shows that there is a high overlap between words in learned topics and high-frequency words. This is also reported in previous studies <ref type="bibr" target="#b12">(Griffiths and Steyvers, 2002;</ref><ref type="bibr" target="#b28">Steyvers and Griffiths, 2007;</ref><ref type="bibr" target="#b4">Chi et al., 2019)</ref>, that words are mentioned more frequently, have a higher probability conditioned on topics on average. Since, they infer the posterior distribution of documents over topics, according to the co-occurrences of words in the whole document collections.  <ref type="bibr">patients, study, using, cells, group, treatment, et, one, al, data, studies, two, patient, results, cell, time, however, figure, significant, reported, high, disease, analysis, clinical, found, age, years, associated, showed, different, compared, risk, levels</ref>  In <ref type="table" target="#tab_2">Table 2</ref>, we further compare the mean score of ROUGE-1 <ref type="bibr" target="#b18">(Lin and Hovy, 2003)</ref> F1 and ROUGE-2 F1 of the oracle summary, and summary based on the generated topics among all datasets used in our experiments. It shows a much lower rouge  score on all datasets for summaries using generated topics, which indicates a semantic gap between the latent topics and the gold summary. The latent topics would guide the method to select sentences that are topically similar to the posterior distribution of Oracle summary: this case report illustrates three learning points about cervical fractures in ankylosing spondylitis, and it highlights the need to manage these patients with the neck initially stabilised in flexion. We describe a case of cervical pseudoarthrosis that is a rare occurrence after fracture of the cervical spine with ankylosing spondylitis. This went undetected until the development of myelopathic symptoms many months later. The neck was initially stabilised in flexion using tongs, and then slowly extended before anterior and posterior fixation was performed. (Mean score on ROUGE-1 F1 and ROUGE-2 F1: 0.994) Summary based on topic words: A patient's neurological condition may be made worse by extension of the neck, as the spinal cord may be compromised by the angle that is formed between the upper and lower rigid bony segments of the cervical spine. Over the previous 5 weeks, he had been experiencing increasing, although intermittent, symptoms including: sharp pains in the posterior aspect of his neck with head movement, abdominal pain and paraesthesia with numbness of his fingers and toes. Certainly significant trauma to a rigid and osteoporotic spine will cause fracture, and then the effect of instability at the fracture site (the fused spinal segments can be thought of as a long bone) will produce a pseudoarthrosis. (Mean score on ROUGE-1 F1 and ROUGE-2 F1 : 0.172) Top-topic words: cervical, spine, neck, ankylosing, fractures spondylitis, fixation, spinal, stabilised, anterior, posterior, flexion, fracture, c7, trauma, traction, paraesthesia, weakness, immobilisation, immobilised, bony, limb, head, cord, post-operatively..... <ref type="table">Table 3</ref>: An example document. High-frequency topic words that appeared in sentences are marked with a red color.</p><p>documents, rather than informative sentences, that cover the semantics in the gold summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>To address the aforementioned limitations of existing methods, we propose our method GRETEL, to better capture and incorporate the global semantics to improve PLMs, for long document extractive summarization. Given u sentences {s 1 , ? ? ? , s u } of a document i from the corpus D, extractive summarization aims to select v informative sentences from u sentences (v u) as the summary S for the document i. This task can be formulated as a binary sentence classification problem. We assign label y i,j = 1 to sentence s i,j (j ? {1, ? ? ? , u}) for the summary, or y i,j = 0 otherwise.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, different from previous methods, we leverage the contextual representations from PLMs, and gold summary to guide the topic inference. To this end, we first employ the hierarchical transformer encoder (HTE) to fully encode the global context of long documents, and then design the supervised graph contrastive loss, to push close the document topic distribution and topic distributions of salient sentences. This helps our method to capture better global semantics, that effectively distinguish between salient and nonsalient sentences, according to their contextual and semantic connections to the gold summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hierarchical Transformer Encoder</head><p>To fully encode the document contents, especially for long documents, we propose to use a the Hierarchical Transformer Encoder (HTE) based on  Block transformer encoder. We first split the document d = {blk 1 , blk 2 , ? ? ? , blk m } into m blocks with fixed length, in which each block blk l = {x l,0 , ? ? ? , x l,n }(l ? {1, ? ? ? , m}) has n tokens. Subsequently, each token x l,p (p ? {1, ? ? ? , n}) at block blk l is represented by the vector E l,p , which is the sum of the token embedding, the block embedding, and the position embedding. Take E l as the input embedding, the contextual representations of tokens in each block blk l can be learned by the PLMs based transformer encoder: h l = BERT (E l ). Following previous studies <ref type="bibr" target="#b19">(Liu and Lapata, 2019)</ref>, we insert the [CLS] and [SEP] tokens at the start and end of each sentence in the block. We consider the representations of [CLS] tokens as the contextual representations of their corresponding sentences: h s = {h s 1 , ? ? ? , h s u }, which capture the local contextual semantic in each block.</p><p>Document transformer encoder. To further model the correlations among intra-block, we stack the document transformer encoder on h s to yield the document context-aware sentence representations:h s = T ransf ormer(h s ). To denote the position of each block, we add the block position embedding <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> to h s . Finally, we use the pooling layer to generate the document representation h d based onh s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Contrastive Topic Model</head><p>Next, we introduce the graph contrastive topic model, to capture global semantics empowered by the semantic information from HTE and the gold summary. It consists of a probabilistic topic encoder with HTE and supervised graph contrastive learning and a probabilistic decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Probabilistic Topic Encoder Enhanced with HTE</head><p>We assume that ? s and ? d refer to sentence topic distributions and document topic distribution, ? represents the topics (topic word distributions in the vocabulary), and X i is the BoW feature of document i. Different from existing methods <ref type="bibr" target="#b32">(Wang et al., 2020b;</ref><ref type="bibr" target="#b0">Aralikatte et al., 2021;</ref><ref type="bibr" target="#b10">Fu et al., 2020)</ref> considering only the BoW features, we further employ the representations from HTE to leverage the semantic and syntactic dependencies among words to generate more coherent topics and topic distributions for documents and sentences. For document topic distribution, we sample it from the logistic normal distribution 2 . We first generate the mean and covariance of a multinomial distribution variable and then use the softmax activation function to convert it into the logistic normal distribution variable. Based on the contextual hidden representations from HTE and BoW features, for each document i, we have:</p><formula xml:id="formula_0">h d i = fX (Xi) +h d i ? d i = f d ? (h d i ), ? d = diag(f d ? (h d i )) ? d i = sof tmax(? d i + (? d i ) 1 2 d i )<label>(1)</label></formula><p>where d i ? N (0, I) is the sampled noise variable, f d ? and f d ? are the feed-forward neural networks which takes input as the BoW feature X i and the contextual hidden representationsh d i of document i from HTE respectively.</p><p>The sentence topic distribution is sampled with only the document context-aware hidden represen-2 Following the previous method <ref type="bibr" target="#b27">(Srivastava and Sutton, 2017)</ref>, we use the logistic normal distribution to approximate the Dirichlet distribution. tations of sentences from HTE:</p><formula xml:id="formula_1">? s i,j = f s ? (h s i,j ), ? s i,j = diag(f s ? (h s i,j )) ? s i,j = ? s i,j + (? s i,j ) 1 2 s i,j<label>(2)</label></formula><p>where s i,j ? N (0, I) is the sampled noise variable, f s ? and f s ? are the feed-forward neural networks which takes the same input as the representation from HTE for the sentence j in the document i. Notice that the BoW features for sentences are not considered since they would be too sparse to introduce external noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Supervised Graph Contrastive Learning</head><p>Although the posterior topic distributions can now utilize the sequential dependencies of words from HTE, they still cannot distinguish between important and redundant topics without the semantic information from the gold summary. Thus, to fill the gap between the posterior topic distributions from NTMs and the key semantics in the gold summary, we propose the supervised graph contrastive learning to explicitly guide the document topic and sentence topic representations with the gold summary.</p><p>Graph Construction. For each document, we first build the graph G = {V, E} with nodes V as the document and all its sentences. Edges E can be represented by the adjacency matrix A, in which the edge between two nodes (i, j) is defined as:</p><formula xml:id="formula_2">Ai,j = ? ? ? ? ? ? ? ? ?</formula><p>1, i is the document node, j is the sentence node, and j ? S + 1, i = j 0, otherwise</p><p>where S + is the oracle summary of each document which has the maximum semantic similarity with the gold summary. Notice that the graph is a bipartite graph with only connections between the document and sentences.</p><p>Graph Contrastive Representation Learning. Based on the bipartite graph embedded with the supervision information, we argue that the representation of the document should be similar to representations of informative sentences in the oracle summary and dissimilar to representations of redundant sentences that are not mentioned. Therefore, we design the following loss for the graph contrastive representation learning:</p><formula xml:id="formula_4">Lcon = ? 1 |V | |V | i=1 log( 0&lt;A i,j ?Ai,jcos(xi, xj) A i,j =0 cos(xi, xj) )<label>(4)</label></formula><p>where |V | is the number of nodes, cos denotes the cosine similarity, and x i ? ? d i , ? s i means the features of node i. This loss explicitly pushes close the topic distributions of the nodes with connections in the graph, i.e., the document node and the sentence node in the oracle summary, and pulls away otherwise. It guides the model to learn more discriminative document and sentence distributions that are semantically related to the gold summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Probabilistic Decoder</head><p>Based on sampled sentences and document representations, we use the probabilistic decoder to generate the observed words and predict the labels of sentences in each document. For each document i, we assume the v-th word w d i,v is generated from the multinomial distribution based on the dot product of the document representations and topics:</p><formula xml:id="formula_5">p(w d i,v |(? d i , ?); ?) = M ult([? d i ? ?])<label>(5)</label></formula><p>where ? is the parameter set of the probabilistic decoder. The topics ? are randomly initialized. We assume the j-th sentence label? i,j of document i is generated from the feed-forward neural network f y with the sigmoid activation function, based on the sentence representation ? s i,j :</p><formula xml:id="formula_6">p(?i,j|? s i,j ; ?) = fy(? s i,j )<label>(6)</label></formula><p>Since we fill the gap between the approximation and true global semantics with supervised contrastive learning based on both the contextual representations from HTE and BoW features, our method allows us to directly use the sentence topic representations to predict the labels of sentences, without any further distillation or fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Optimization</head><p>We optimize the loss function from both graph contrastive topic modeling and extractive summarization to support joint inference. The final loss of GRETEL is the sum of the evidence lower bound (ELBO) and the graph contrastive loss:</p><formula xml:id="formula_7">L = L(?, ?; Xi) + ?Lcon<label>(7)</label></formula><p>where ? is the parameter to control the sensitivity of the contrastive normalization, L con is the contrastive loss, and L(?, ?; X i ) is:</p><formula xml:id="formula_8">L(?, ?; Xi) = E q ? (? s i |h s i ) [logP?(yi|? s i )] + E q ? (? d i |X i ,h d i ) [logP?(wi|? d i , ?)] ? DKL[q?(? d i |Xi,h d i )||P?(? d i )]<label>(8)</label></formula><p>where y i is the ground truth labels of sentences in document i. The ELBO is composed of three terms, including the sentence label prediction loss for the extractive summarization, the word reconstruction loss of the neural topic modeling, and the KL divergence between the variational posterior and the prior of ? d , which uses the prior P ? (? d |?) to normalize the document topic representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimentation Details</head><p>In this section, we present the details of the datasets used, evaluation metrics, and different baselines.</p><p>Datasets. To evaluate the effectiveness of GRE-TEL, we conducted experiments on four benchmark datasets and two biomedical domain-specific long document datasets: 1) CNN/DM <ref type="bibr" target="#b13">(Hermann et al., 2015)</ref>: a commonly used news dataset; 2) Arxiv <ref type="bibr" target="#b5">(Cohan et al., 2018)</ref>: a dataset containing long scientific documents from the Arxiv website; 3) PubMed-Long <ref type="bibr" target="#b5">(Cohan et al., 2018)</ref>: a dataset containing long scientific documents from biomedicine; 4) PubMed-Short <ref type="bibr" target="#b38">(Zhong et al., 2020)</ref>: adapted PubMed-Long to use only the introduction of the document as input and filter noisy documents; 5) CORD-19 <ref type="bibr" target="#b31">(Wang et al., 2020a)</ref>: an openly released dataset including long biomedical scientific papers related to COVID-19. We use the version of the dataset which was released on 2020-06-28 ; 6) S2ORC <ref type="bibr" target="#b22">(Lo et al., 2020)</ref>: a publicly released dataset that includes long scientific papers from several domains. We sample a random subset of articles from only the biomedical domain . We show the statistics of the datasets in <ref type="table" target="#tab_5">Table 4</ref>. We use abstracts of documents as the gold summary. For CNN/DM and Arxiv, we extract 3 and 7 sentences respectively to formulate the final summary, following previous methods <ref type="bibr" target="#b38">(Zhong et al., 2020)</ref>. For the remaining datasets, we extract 6 sentences to formulate the summary .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>Our method is implemented by Pytorch and Huggingface <ref type="bibr" target="#b33">(Wolf et al., 2020)</ref>. We investigated the RoBERTa  implemented in Huggingface as the encoder. We use the base size of it. We set the learning rate to 2e-3, dropout rate to 0.0, warmup steps to 5000, topic number between{100, 200, 300, 400, 500}, the parameter to control the negative samples of the contrastive loss ? to 1, and the weight parameter ? to 0.5. We set the hidden size of the transformer in HTE to 768. Due to the memory limitations of GPU, we set the max tokens of input documents as 6000. We train the model with 50000 steps and save the model checkpoint at every 1000 steps. We select the best checkpoint according to the loss in the validation and report the results in the test. To extract the sentence label for training the model, we use the greedy search algorithm <ref type="bibr" target="#b23">(Nallapati et al., 2017)</ref> to select the oracle summary of each document, via maximizing the ROUGE-2 score against the gold summary. we use the pyrouge 3 to calculate the ROUGE <ref type="bibr" target="#b17">(Lin, 2004)</ref> metric.</p><p>Baselines and Metrics. We compare our model with SOTA extractive summarization methods including: 1) PLMs based methods: BERTSum <ref type="bibr" target="#b19">(Liu and Lapata, 2019)</ref> and MatchSum <ref type="bibr" target="#b38">(Zhong et al., 2020)</ref>; 2) PLMs based models for long documents: HIBERT <ref type="bibr" target="#b37">(Zhang et al., 2019)</ref>, ETCSum <ref type="bibr" target="#b24">(Narayan et al., 2020)</ref>, SSN-DM (Cui and Hu, 2021a), GBT-EXTSUM <ref type="bibr" target="#b11">(Grail et al., 2021)</ref>, Longformer-Ext <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref>, Reformer-Ext <ref type="bibr" target="#b15">(Kitaev et al., 2019)</ref>, BERTSum+SW <ref type="bibr" target="#b19">(Liu and Lapata, 2019)</ref> which uses the BERTSum to sequentially encode the full context with the sliding window; 3) topic enhanced transformer method: Topic-GraphSum <ref type="bibr" target="#b7">(Cui and Hu, 2021b)</ref>, which is the only PLMs-based model with topic modeling for extractive summarization. Following <ref type="bibr" target="#b19">(Liu and Lapata, 2019)</ref>, we report the unigram (ROUGE-1), bigram F1 (ROUGE-2), and the longest common subsequence (ROUGE-L) between the generated summary and the gold summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>A series of experiments were conducted to demonstrate the efficacy of the proposed method.      <ref type="bibr" target="#b11">(Grail et al., 2021;</ref><ref type="bibr" target="#b38">Zhong et al., 2020;</ref><ref type="bibr" target="#b6">Cui and Hu, 2021a</ref>).</p><formula xml:id="formula_9">Datasets CNN/DM Arxiv PubMed-Long PubMed-Short CORD-19 S2ORC Metrics R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L LEAD</formula><formula xml:id="formula_10">- - - - - - - - - - - - - - ETCSum 43.84 20.80 39.77 - - - - - - - - - - - - - - - Longformer-</formula><formula xml:id="formula_11">40.37 - - - - - - - - - - - - - - - SSN-DM - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Main Results</head><p>We first present the ROUGE F1 results of different models on all datasets in <ref type="table" target="#tab_8">Table 5</ref>, which shows that our method GRETEL outperforms all existing baseline methods in all datasets. It demonstrates the superiority of our method GRETEL to other methods, via capturing better global semantics with the guidance of the gold summary and the leverage of contextual information and word features simultaneously. Our methods and Topic-GraphSum both present superior performance over methods without the topic information, such as BERTSum and MatchSum, indicating the importance of modeling the global semantic information with the approximation of latent topics. When comparing with Topic-GraphSum incorporating latent topics, our method yields better performance on all datasets. This proves the benefit of the supervision from the gold summary and the integration of contextual representations to exploit the better global semantics. It is also proved from the improvement of our method when compared with methods that encode full document contents but ignore the topic information, such as Longformer-Ext, SSN-DM et al.</p><p>Moreover, our methods and other PLMs-based methods that address the truncation issue to encode full contents, such as Longformer-Ext, SSN-DM et al, achieve better performance on all long document datasets, when comparing methods with the input length limit, such as BERTSum, and Match-Sum. It shows that the content loss can inhibit their ability to model the contextual information in the document, which also limits the employment of the contextual representations from existing methods in the topic generation. On the contrary, for CNN/DM and PubMed-Short whose documents are relatively short, the improvement is insignifi-cant, since truncating these short documents would not miss much context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>PubMed-Long Arxiv  </p><formula xml:id="formula_12">Metrics R-1 R-2 R-L R-1 R-2 R-L GRETEL</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Study</head><p>We further verify the attribution of each component to the performance improvement of GRETEL in this section, as shown in  dataset . It shows that the performance of our method generally increases with the growing number of topics on PubMed-Long, while it soon achieves the best of 300 topics on CORD-19 since it contains fewer topics with relatively shorter content and much fewer documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>PubMed-Long CORD-19  </p><formula xml:id="formula_13">Metrics R-1 R-2 R-L R-1 R-2 R-L<label>K=100</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Topic Analysis</head><p>To verify the quality of our generated topics, we further evaluate the NPMI (Lau et al., 2014) score in <ref type="table">Table 8</ref>   <ref type="table">Table 8</ref>: NPMI score of different models on CORD-19 and PubMed-Long using different numbers of topics.</p><p>Gold A 53-year-old man with steroid dependent rheumatoid arthritis presented with fever and serious articular drainage. Oral antibiotics were initially prescribed. Subsequent hemodynamic instability was attributed to septic shock. Further evaluation revealed a pericardial effusion with tamponade. Pericardiocentesis of purulent fluid promptly corrected the hypotension. Proteus mirabilis was later isolated from both the infected joint and the pericardial fluid. This is the first report of combined proteus mirabilis septic arthritis and purulent pericarditis. It documents the potential for atypical transmission of gram-negative pathogens, to the pericardium, in patients with a high likelihood of preexisting pericardial disease. In immunocompromised patients, the typical signs and symptoms of pericarditis may be absent, and the clinical presentation of pericardial tamponade may be misinterpreted as one of septic shock. This case underscores the value of a careful physical examination and proper interpretation of ancillary studies. It further illustrates the importance of initial antibiotic selection and the need for definitive treatment of septic arthritis in immunocompromised patients.</p><p>Our ID 3: We report a case of purulent pericarditis with pericardial tamponade masquerading as septic shock related to proteus mirabilis septic arthritis.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Case Study</head><p>In <ref type="table" target="#tab_16">Table 9</ref>, we present the summaries of an example document generated by GRETEL and BERTSum, together with the top-5 topics (with the highest coherence) of the document from the PubMed-Long dataset. In table 10, we show the top-2 topics of selected sentences by our method for inclusion in the summary. It shows that our method generates a more coherent summary that contains more salient sentences than the summary generated by BERT-Sum, due to the integration of a better approximation of global semantics in our method. This is also proved by the selected sentences of our method are topically related to the captured topics about "treatment", "joints" and "infected", which are semantically similar to the meaning of the gold summary. Moreover, the positions of our selected sentence vary in every part of the document while the sentences of BERTSum are all located in the former part of the document. This is because the employment of HTE allows our method to encode the full contents of the document without truncation. In <ref type="figure" target="#fig_2">Figure 2</ref>, we further compare the position distribution of selected sentences by different models and the oracle summary on PubMed-Long. The distribution of our method is the most similar to the oracle summary, which pays more attention to the latter sentences compared with other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a novel framework GRE-TEL for extractive summarization of long texts, that furnishes PLMs with the neural topic inference, to fully incorporate the local and global semantics. Experimental results on both general and biomedical datasets show that our model outperforms existing state-of-the-art methods, and global semantics empowered by graph contrastive learning and PLMs can yield more discriminative sentence representations to select salient sentences, that are topically similar to the gold summary. For future work, we would explore the feasibility of extending this framework to abstractive and multidocument summarization tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>T1: type treatment consistent needed lower disorders sensitive patient acid way T2: male group treatment followed cells per side plasma american health T3: al dna clinical risk observed tube lower inflammatory et features T4: type al clinical mice bacteria high vs posterior conditions side T5: differences performed results side number higher size tube et patients T6: dna revealed smoking control mental number change sd light versus Top high-frequency words:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The model architecture of GRETEL blocks with two modules: the block transformer encoder and the document transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The position distribution of extracted sentences by different models on the PubMed-Long test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: top-10 words of topics learned by LDA on</cell></row><row><cell>Pubmed dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: The mean score of ROUGE-1 F1 and ROUGE-</cell></row><row><cell>2 F1 between different summaries with the gold sum-</cell></row><row><cell>mary, averaged on all documents.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Statistics of datasets. Ext denotes the number of sentences extracted in the final summary.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>40.11 17.54 36.32 33.66 8.94 22.19 36.19 11.82 32.96 37.58 12.22 33.44 32.40 8.97 29.30 36.62 16.57 33.11 ORACLE 56.22 33.74 52.19 53.88 23.05 44.90 50.26 28.32 46.33 45.12 20.33 40.19 46.20 22.86 42.08 58.34 34.48 54.36 BERTSum 43.25 20.24 39.63 41.24 13.01 36.10 41.09 15.51 36.85 41.05 14.88 36.57 36.25 10.83 32.85 40.53 16.31 37.50 GraphSum 44.02 20.81 40.55 44.03 18.52 32.41 45.95 20.81 33.</figDesc><table><row><cell>MatchSum</cell><cell>44.41 20.86 40.55</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">41.21 14.91 36.75</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="8">Topic-97</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HiBERT</cell><cell>42.37 19.95 38.83</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>20.65 39.67 47.86 19.17 42.50 46.36 19.67 42.49 42.07 15.10 37.29 42.51 15.72 38.58 46.21 19.73  43.01 GBT-EXTSUM 42.93 19.81 39.20 48.08 19.21 42.68 46.87 20.? 20.96 ? 40.69 ? 48.17 ? 20.31 ? 42.84 ? 48.20 ? 21.20 ? 43.16 ? 42.53 ? 16.55 ? 38.61 ? 43.91 ? 16.54 ? 40.01 ? 48.24 ? 23.34 ? 44.55 ?</figDesc><table><row><cell></cell><cell>45.03 19.03 32.58 46.73 21.00 34.10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">BERTSum+SW 43.78 19 42.68</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GRETEL</cell><cell>44.62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: ROUGE F1 results of different models on CNN/DM, Arxiv, PubMed-Long, PubMed-Short, CORD-19,</cell></row><row><cell>and S2ORC under 5 times running.  ? means outperform the existing model with best performance significantly</cell></row><row><cell>(p &lt; 0.05). Part results are from</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>ROUGE F1 results of our model under differ- ent settings on PubMed-Long and Arxiv.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>. It presents</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>ROUGE F1 results of our model with a different number of topics on PubMed-Long and CORD-19.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Example of extractive summarization conducted by our method on the PubMed-Long dataset. The gold summary is the abstract of the document. Sentences with deep color have a higher ROUGE score. Topic words are marked with the blue color.</figDesc><table><row><cell cols="2">Sentence ID Top-6 words</cell></row><row><cell>ID 3</cell><cell>adequate mice patients label understanding body infected report oxygen formation disease type exercise vegf nerve deaths shock joints drugs lower</cell></row><row><cell>ID 4</cell><cell>family report presented followed obesity side j macrophages high necrosis treatment infected severity year patients crp tract area arm isolated</cell></row><row><cell>ID 0</cell><cell>treatment infected severity year patients crp tract area arm isolated type treatment male sd well use statistically specific post mice</cell></row><row><cell>ID 75</cell><cell>treatment infected severity year patients crp tract area arm isolated treatment adjacent medication different motor min height stroke like rate</cell></row><row><cell>ID 30</cell><cell>et treatment lower diagnosis control observed could 7 number association infected congenital patients loss blood disorders compared high fig phase</cell></row><row><cell>ID 68</cell><cell>type treatment male sd well use statistically specific post mice effects performed revealed compared clinical observed diagnosis isolated cm family</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Top 10 words of top 2 topics in sentences, which are selected into the summary.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/xashely/GRETEL_ extractive</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/andersjo/pyrouge.git</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by the Alan Turing Institute and the Biotechnology and Biological Sciences Research Council (BBSRC), BB/P025684/1. We would like to thank Pan Du, Jennifer Bishop, and Guanghao Yang for their help and constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Focus attention: Promoting faithfulness and diversity in summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Aralikatte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11921</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gencomparesum: a hybrid unsupervised summarization method using salience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Workshop on Biomedical Language Processing</title>
		<meeting>the 21st Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="220" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Topic representation: Finding more representative words in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changchun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhua</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="53" to="60" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sliding selector network with dynamic memory for extractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5881" to="5891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Topic-guided abstractive multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11207</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhancing extractive text summarization with topic-aware graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanchao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5360" to="5371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Document summarization with vhtm: Variational hierarchical topic-aware mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinmao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenglu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7740" to="7747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Globalizing bert-based transformer architectures for long document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Grail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1792" to="1810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A probabilistic approach to semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the cognitive science society</title>
		<meeting>the annual meeting of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient attentions for long document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Parulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1419" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 human language technology conference of the North American chapter of the association for computational linguistics</title>
		<meeting>the 2003 human language technology conference of the North American chapter of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="150" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Hetformer: Heterogeneous transformer with sparse attention for long-text extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06388</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">S2orc: The semantic scholar open research corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stepwise extractive summarization and planning with structured transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Adamek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaz</forename><surname>Bratanic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4143" to="4159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Enriching and controlling global semantics for text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truc</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tho</forename><surname>Quan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10616</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural sparse topical coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Xiuzhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2332" to="2340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Autoencoding variational inference for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Probabilistic topic models. Handbook of latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="page" from="424" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pre-trained language models in biomedical domain: A systematic survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahuan</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prayag</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05006</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cord-19: The covid-19 open research dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoganand</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Reas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Katsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><forename type="middle">Michael</forename><surname>Kinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020</title>
		<meeting>the 1st Workshop on NLP for COVID-19 at ACL 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Friendly topic assistant for transformer based abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="485" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transformers: State-of-theart natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pre-trained language models with domain knowledge for biomedical extractive summarization. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Amy</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prayag</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">109460</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph topic neural network for document representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3055" to="3065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discourse-aware neural extractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5021" to="5031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hibert: Document level pre-training of hierarchical bidirectional transformers for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5059" to="5069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Extractive summarization as text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6197" to="6208" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
