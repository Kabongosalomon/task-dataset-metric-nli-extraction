<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XBNet: An Extremely Boosted Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Sarkar</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Analytica, Mumbai</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">XBNet: An Extremely Boosted Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>XBNet</term>
					<term>Boosted Gradient Descent</term>
					<term>XGBoost</term>
					<term>Neural networks</term>
					<term>Entropy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks have proved to be very robust at processing unstructured data like images, text, videos, and audio. However, it has been observed that their performance is not up to the mark in tabular data; hence tree-based models are preferred in such scenarios. A popular model for tabular data is boosted trees, a highly efficacious and extensively used machine learning method, and it also provides good interpretability compared to neural networks. In this paper, we describe a novel architecture XBNet (Extremely Boosted Neural Network), which tries to combine tree-based models with neural networks to create a robust architecture trained by using a novel optimization technique, Boosted Gradient Descent for Tabular Data which increases its interpretability and performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>to prevent it promptly; anomaly detection techniques help banks to check to prevent fraudulent transactions <ref type="bibr" target="#b1">[2]</ref>. The reason behind the success of these applications is the following: the utility of statistical models that maps complex data dependencies between disparate entities. Furthermore, the scalability of these architectures enables them to learn the relationships even in complex data sources to solve our problems and help us achieve our goals. Even in the algorithms that are commonly used in dealing with tabular data, gradient tree boosting is an approach that outshines others in several use cases <ref type="bibr" target="#b2">[3]</ref>. Deep neural networks have shown noteworthy success with unstructured data like images, text, videos, and audio <ref type="bibr" target="#b3">[4]</ref> [5] <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b6">[7]</ref>. For the above domains of problems, we try to precisely encode the information hidden in them into a vector space that forms the basis of our understanding and helps tackle the problem. Tabular data is the only data type where there is a minimum success with this approach.</p><p>Though it is the most common data type in the actual world of data science, which comprises any continuous and discrete features, deep learning for tabular data remains dormant, as ensemble methods built on decision trees continue to perform better on such data <ref type="bibr" target="#b7">[8]</ref>. Decision trees develop a piece-wise function for classifying the data and solving its objective, whereas deep neural networks develop a non-linear function for mapping the relationship between the input features and the target vectors. The activation function introduces this nonlinearity in the neural network. Different types of activation functions introduce different degrees of activation of the neuron in a particular layer that enables it to grasp the relation between the input features and output vectors. Since treebased models create a piece-wise function, it makes them more interpretable than neural networks and can also be used to understand how various features affect the dependent variable. We propose a new architecture for tabular data, XBNet, which attempts to combine gradient boosted trees with feed-forward neural networks to give rise to a new approach to robust architectures. XBNet inputs raw tabular data and is trained using an optimization technique Boosted Gradient Descent which is initialized with the feature importance of a gradient boosted tree, and it updates the weights of each layer in the neural network in two steps:</p><p>(1) Update weights by gradient descent.</p><p>(2) Update weights by using feature importance, of a gradient boosted tree in every intermediate layer <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Tree-based models and their variants like AdaBoost, Random Forest, XG-Boost, etc are widely used in classification and regression problems <ref type="bibr" target="#b9">[10]</ref>. They repeatedly split the input vector space and allot scores to the final node. Treebased models not only boost the performance in tabular data they also increase the interpretability, of the system which increases its usability in business scenarios <ref type="bibr" target="#b10">[11]</ref>. It is also frequently seen that ensemble techniques like Random Forest and XGBoost are used in most of the winning solutions in the case of tabular data <ref type="bibr" target="#b11">[12]</ref> [13] <ref type="bibr" target="#b13">[14]</ref>. These models perform better than neural networks at several classification and regression problems when it comes to tabular data. Some works have advanced to amalgamate neural networks and tree-based models like decision trees to develop a better architecture with the features of both neural network and tree-based models. An approach in this direction was Neural Decision Forests that attempts to combine trees with representational learning <ref type="bibr" target="#b14">[15]</ref>.</p><p>Another unique approach is DNDT which stands for Deep Neural Decision tree.</p><p>DNDTs have a unique architecture, where a particular set of weights maps to a distinct decision tree <ref type="bibr" target="#b15">[16]</ref>. Our proposed architecture XBNet is another step in that field and it differs from the previous methods in many approaches. The optimization technique acts as an extension of any gradient-based optimization technique with the help of tree-based models <ref type="bibr" target="#b16">[17]</ref>.</p><p>Recent advancement in this field is the attempt to use convolution layers for encoding the information to create a feature map which can then be fed to an extreme gradient boosted tree for inference <ref type="bibr" target="#b17">[18]</ref>. A similar approach is also followed for dealing with images by using a combination of convolution layers and XGBoost to propel the research and work in the field of breast cancer  <ref type="bibr" target="#b18">[19]</ref>. A convolution-based architecture coupled with a classifier for inference and prediction utilized for maximizing performance measures is also rooted in the above-discussed techniques <ref type="bibr" target="#b19">[20]</ref>.</p><p>In our approach, trees are trained in every layer of the architecture and their feature importance is used along with the weights determined by gradient descent for adjusting the weights of those layers respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">XBNet Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature importance from XGBoost</head><p>XGBoost is an ensemble technique that uses N trees for providing a prediction in the following way:</p><formula xml:id="formula_0">y = ?(x) = N n=1 g n (x)<label>(1)</label></formula><p>where x and y denote the input features and outputs respectively. g n (x) represents the score of leaf of the N th tree. Also, g n (x) ? M , where M represents the set of all the scores. We then regularize to avoid overfitting:</p><formula xml:id="formula_1">L(?) = i l(? i , y i ) + n ?(g n )<label>(2)</label></formula><p>where l denotes the loss function, and ?(g n ) is defined as:</p><formula xml:id="formula_2">?(g) = ?T + 1 2 ? T i=1 w 2 i<label>(3)</label></formula><p>where ? and ? help in reducing overfitting by directing regularization. T and w represent the number of leaves and their weights respectively.</p><p>The feature importance of the extremely gradient boosted tree which plays a great role in our architecture as well as training, is determined based on information gain from the features of the tree, which is a way of determining which attribute in a given set of feature vectors is most useful for distinguishing between the classes to be learned which in turn is dependent on entropy that is a common way of measuring impurity [21] <ref type="bibr" target="#b21">[22]</ref>. Impurity measures the homogeneity of the target variable at every node. Let P be a probability distribution such that</p><formula xml:id="formula_3">P = (p 1 , p 2 , ...., p n )<label>(4)</label></formula><p>where p i is the probability of a data point that belongs to a subset d i of dataset D Entropy can be defined as:</p><formula xml:id="formula_4">Entropy(P ) = n i=1 ?p i log 2 (p i )<label>(5)</label></formula><p>The information gain calculated is then used to determine the feature importance of the boosted tree which is used in Boosted Gradient Descent.</p><p>Inf ormationGain = Entropy bef oreSplit ? Entropy af terSplit (6)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gradient Descent</head><p>Gradient Descent is an optimization technique for finding the minima of the required differentiable loss function. Gradient descent is employed to determine the optimum values of a function's parameters that minimize the given loss function. Forward propagation happens in the way shown below:</p><formula xml:id="formula_5">z [l] = w [l] A [l?1] + b [l]<label>(7)</label></formula><formula xml:id="formula_6">A [l] = g [l] (z [l] )<label>(8)</label></formula><p>where w <ref type="bibr">[l]</ref> and b <ref type="bibr">[l]</ref> is the weight and the bias of the l th layer respectively g(x)</p><p>is the activation function, A <ref type="bibr">[l]</ref> is the output of the l th layer that is activated with the activation function g(x) and z <ref type="bibr">[l]</ref> is the output of the l th layer before applying the activation function.Then we compute the cost</p><formula xml:id="formula_7">J = 1 m L(? (i) , y (i) ) + ? 2m (||w [l] ||) 2 f<label>(9)</label></formula><p>where? (i) , y (i) are the predicted and actual values respectively, m is number of mini-batches of data during training and ? is the tuning parameter that controls the effect of the regularization. The presence of a regularization reduces overfitting by regulating penalty. Here we have used the L2 regularization that is proportional to (||(w [l] ||) 2 . Weights are adjusted during the backward propagation in the following way:</p><formula xml:id="formula_8">w [l] = w [l] ? ??w [l] (10) b [l] = b [l] ? ??b [l]<label>(11)</label></formula><p>where w <ref type="bibr">[l]</ref> and b <ref type="bibr">[l]</ref> is the weight and the bias of the l th layer respectively, ? is the learning rate and ?w represents the gradient.</p><p>Given below is the algorithm for gradient descent where we combine all the steps to provide the flow of this optimization technique. Our optimization strategy will add another step to the traditional gradient descent approach to boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Optimization using Boosted Gradient Descent</head><p>Now we combine the above subsections to create our architecture and optimizer. This optimizer can be used with any gradient-based optimization technique as the base but here we have given the example of Gradient Descent for </p><formula xml:id="formula_9">z [l] = w [l] A [l?1] + b [l] ; A [l] = g [l] (z [l] ); Compute cost J= 1 m L(? (i) , y (i) ) + ? 2m (||w [l] ||) 2 f Backward propagation on J t ; w [l] = w [l] ? ??w [l] ; b [l] = b [l] ? ??b [l] ;</formula><p>end easily elucidating the concept. Our architecture creates a Sequential structure of layers with the first and last being the input and output layers respectively.</p><p>The weights of the first layer are not initialized randomly but it is the feature importance of a gradient boosted tree which is trained at the time of initialization of the model.</p><p>w <ref type="bibr" target="#b0">[1]</ref> = tree.train(X, y).importance</p><p>Here w 1 is the weight of the first layer, X and y are the input and output vectors, respectively. Apart from this the architecture also contains a gradient boosted tree that is connected to each layer. At the time of training the model, the data that is fed completes a forward and backward propagation, and the weights of all the layers get updated according to gradient descent once and then instead of going to the next epoch of training it goes through all the layers again and updates its weights again based on the feature importance of the gradient boosted tree that is trained on the layers respectively. This is the step that is added during the forward propagation:</p><formula xml:id="formula_11">f [l] = tree.train(A [l]</formula><p>, y (i) ).importance <ref type="bibr" target="#b12">(13)</ref> where A <ref type="bibr">[l]</ref> is the output of the l th layer that is activated with an activation function g(x), y (i) is the outputs for the mini-batch that is fed to the system.</p><p>The number of layers of the neural network on which the tree should be trained is a hyperparameter. Further, the trees are trained on the hidden layers during the forward propagation of the feed-forward neural network and their feature importance is stored which is updated after the backward pass. So in this approach, the feature importance also plays the role of adjusting the weight which boosts the performance of the architecture. Weights are updated in the following way:</p><formula xml:id="formula_12">w [l] = w [l] ? ??w [l]<label>(14)</label></formula><p>where w <ref type="bibr">[l]</ref> and b <ref type="bibr">[l]</ref> is the weight and the bias of the l th layer respectively, ? is the learning rate and ?w represents the gradient.</p><formula xml:id="formula_13">w [l] = w [l] + f [l] ?(w [l] )<label>(15)</label></formula><p>where f [l] is the feature importance for the l th layer that was computed during the forward propagation.</p><formula xml:id="formula_14">?(w [l] ) = 10 log(min(w [l] ))<label>(16)</label></formula><p>Here ?(w <ref type="bibr">[l]</ref> ) is used to ensure that the contribution of the weights provided by the feature importance and the weights of gradient descent is in the same order. The feature importance is scaled down to the same power as that of the minimum value of the weights of the gradient descent algorithm to ensure that the feature importance complements the weights obtained through gradient descent. This is necessary because after some epochs the feature importance remains in the same order by virtue of its definition but the weights provided by gradient descent decrease by several orders. Only one gradient boosted tree is initialized inside the architecture as all the layers will have different inputs after the epoch and therefore the same tree is used for each layer and each epoch and the feature importance are stored before training the tree on the next layer which saves space while providing the required result. In the algorithm given below,? (i) , y (i) are the predicted and actual values respectively. </p><formula xml:id="formula_15">z [l] = w [l] A [l?1] + b [l] ; A [l] = g [l] (z [l] ); f [l] = tree.train(A [l] , y (i) ).importance; Compute cost J= 1 m L(? (i) , y (i) ) + ? 2m (||w [l] ||) 2 f Backward propagation on J t ; w [l] = w [l] ? ??w [l] ; f [l] = f [l] ? 10 log(min(w [l] )) ; w [l] = w [l] + f [l] ; b [l] = b [l] ? ??b [l] ;</formula><p>end Thus each layer has two components, the weight vector and the matrix containing the feature importances. The configuration of each layer is such that during training of a particular layer the weights are first calculated during forward propagation, which is followed by training of an XGBoost model taking inputs as the nodes of that layer, the feature importance of this ensemble model is stored in a matrix as they are required again during back-propagation. The weights are updated on the basis of gradient descent during back-propagation then they are adjusted according to the matrix of feature importances that was calculated during forward propagation. The results section contains a detailed discussion of the different configurations that were tried as well as their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference</head><p>During inference, there is no requirement of using the gradient boosted tree as the purpose of the feature importance of the tree is to only ensure the precise update of weights to improve the performance of the network which is done while training the model. Hence the prediction speed only depends on the number of layers as there is no contribution of trees during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Complexity</head><p>To understand the complexity of our proposed architecture, we will delve into the details of the complexities of the gradient boosted tree and the neural network. The training time and space complexity of a gradient boosted tree can be defined in the following way:</p><formula xml:id="formula_16">Complexity time = O(tn trees log(td))<label>(17)</label></formula><p>Complexity space = O(n nodes n trees + ?m) <ref type="bibr" target="#b17">(18)</ref> where t, n trees and d represent the number of data points, trees, and dimensions respectively; n nodes represents the number of nodes, and ?m represents the values that are processed by the leaves in the decision trees. As we are using the gradient boosted tree only for training the neural network, the complexities of the tree during testing are not mentioned. The training complexity of a deep neural network can be defined as follows:</p><formula xml:id="formula_17">Complexity time = O(nt( p?1 i=1 ?(l [i] )?(l [i+1] )))<label>(19)</label></formula><formula xml:id="formula_18">Complexity space = O(2q + 1)<label>(20)</label></formula><p>The testing complexity of a deep neural network can be defined as follows:</p><formula xml:id="formula_19">Complexity time = O(t( p?1 i=1 ?(l [i] )?(l [i+1] )))<label>(21)</label></formula><formula xml:id="formula_20">Complexity space = O(2q + 1)<label>(22)</label></formula><p>where n, t represent the number of epochs and training pairs, respectively;</p><formula xml:id="formula_21">l [i]</formula><p>represents the layer that is being processed, ?(x) number of nodes in the xth layer, q represents the vector containing all the parameters and p is the number of layers in the neural network. Now we derive the complexity of our architecture in the following way:</p><p>(1) During Training:</p><formula xml:id="formula_22">Complexity time = O(nt[( p?1 i=1 ?(l [i] )?(l [i+1] )) + n trees plog(td)])<label>(23)</label></formula><p>Complexity space = O(3q + n nodes n trees + ?m + 1)</p><p>For every layer, we train a gradient boosted tree, so the time complexity of each layer is added by that factor multiplied by the number of layers as indicated in the above notation. For space complexity, the complexities of the gradient boosted tree and neural network are added, along with the matrix for storing the feature importances during the training procedure.</p><p>(2) During Inference:</p><formula xml:id="formula_24">Complexity time = O(t[ p?1 i=1 ?(l [i] )?(l [i+1] )))<label>(25)</label></formula><formula xml:id="formula_25">Complexity space = O(2q + 1)<label>(26)</label></formula><p>The space complexity decreases as the gradient boosted tree is only used while training and thus can be discarded during inference. The time complexity during inference is the same as a vanilla neural network.</p><p>where n, t represent the number of epochs and training pairs, respectively; l <ref type="bibr">[i]</ref> represents the layer that is being processed, ?(x) number of nodes in the xth </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We evaluate our model on the Breast Cancer Dataset and we create several models with different numbers of layers, numbers of nodes, activation functions, and different numbers of boosted layers i.e number of hidden layers on which the tree is trained. Here is a summary of the results:</p><p>(1) When a model with 2 layers having 16,1 nodes respectively whose 1st layer is boosted with a default xgboost tree with no hyperparameter tuning is trained on 100 epochs it yields a loss of 0.12 on the training data and a loss of 0.08 on the validation data when BCE loss criterion was used as the loss function.  ROC AUC score of 99.6% was obtained on the breast cancer dataset. The optimal settings that we established were 100 estimators in the gradient boosted tree, ReLU activation after each layer, a fewer number of layers, and fewer nodes in each layer in comparison to a vanilla neural network and boosting applied to the first few layers. We also introduced a parameter and set its value to 0.001 to introduce Laplacian smoothing in the feature importances. Adam optimizer with a learning rate of 0.01 is used and the train-test split was stratified with 80% of the data being used for training and the rest for testing; mlogloss which computes log loss for multiclass problems was used as the evaluation metric for the gradient boosted trees.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Future Work</head><p>XBNet requires more time and resources for training as we train a gradient boosted tree in every layer. We minimize the training time of this gradient boosted tree by fixing the number of trees as 100. The optimal number of trees for training in each layer can also be parameterized to find the bestsuited number to minimize the number of trees and concurrently improve the evaluation. Currently, XBNet only works on tabular data and is unable to process unstructured data. If it is extended for usage in unstructured data, the number of parameters will see a drastic jump, and hence care has to be taken with respect to it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper about XBNet discussed the techniques that we employed for building this architecture and described the training, optimization, and inference techniques for this model. As the need and use for data keep increasing day by day in our current world and its impact on the daily life of people is motivating data-driven decisions in many sectors of the industry, this paper was an effort to combine neural networks and gradient boosted trees to provide an alternative approach to the currently used techniques which will pave the way for future work using this approach. The performance, interpretability, and scalability of this architecture will make it possible for professionals to optimally utilize the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>XBNet architecture detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>BCE Loss function vs Number of epochs layer, q represents the vector containing all the parameters; p is the number of layers in the neural network, d denotes the number of dimensions, n nodes represents the number of nodes, and ?m represents the values that are processed by the leaves in the decision trees and n trees represents the number of trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :( 2 )( 3 )( 4 )</head><label>3234</label><figDesc>Accuracy vs Number of epochs When a model with 2 layers having 8,1 nodes respectively whose 1st layer is boosted with a default xgboost tree with no hyperparameter tuning is trained on 100 epochs it yields a loss of 0.10 on the training data and a loss of 0.09 on the validation data when BCE loss criterion was used as the loss function. When a model with 2 layers having 8,1 nodes respectively whose 1st as well as 2nd layer is boosted with a default xgboost tree with no hyperparameter tuning is trained on 100 epochs it yields a loss of 0.13 on the training data and a loss of 0.11 on the validation data when BCE loss criterion was used as the loss function. When a model with 3 layers having 32,16,1 nodes respectively whose 1st layer is boosted with a default xgboost tree with no hyperparameter tuning is trained on 100 epochs it yields a loss of 21.78 on the training data and a loss of 19.45 on the validation data when BCE loss criterion was used as the loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>ROC of XBNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Precision Recall curve of XBNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>I</head><label></label><figDesc>would like to thank Chandan Sarkar, Mallika Sarkar, Disha Shah, Vaibhav Vasani, and Dr.Rupali Patil for their constant guidance and valuable feedback.I am also grateful to Aparna Sarkar, Sneha Kothi, and the entire XBNet community for their priceless suggestions which went a long way toward improving the architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on different Datasets</figDesc><table><row><cell>Dataset</cell><cell>Training</cell><cell>Testing Accuracy</cell></row><row><cell></cell><cell>Accuracy</cell><cell></cell></row><row><cell>Iris</cell><cell>100</cell><cell>100</cell></row><row><cell>Breast Cancer</cell><cell>96.7</cell><cell>96.49</cell></row><row><cell>Wine</cell><cell>97.22</cell><cell>97.22</cell></row><row><cell>Diabetes</cell><cell>77.09</cell><cell>78.78</cell></row><row><cell>Titanic</cell><cell>80.25</cell><cell>79.85</cell></row><row><cell>Digits</cell><cell>99.65</cell><cell>94.72</cell></row><row><cell>German Credit</cell><cell>69.8</cell><cell>71.33</cell></row><row><cell>Digit Completion</cell><cell>86.11</cell><cell>85.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Training Classification metrics on Breast Cancer</figDesc><table><row><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>f1-score</cell></row><row><cell>0</cell><cell>0.96</cell><cell>0.98</cell><cell>0.97</cell></row><row><cell>1</cell><cell>0.97</cell><cell>0.93</cell><cell>0.95</cell></row><row><cell>micro avg</cell><cell>0.96</cell><cell>0.96</cell><cell>0.96</cell></row><row><cell>macro avg</cell><cell>0.96</cell><cell>0.96</cell><cell>0.96</cell></row><row><cell>weighted avg</cell><cell>0.96</cell><cell>0.96</cell><cell>0.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Testing Classification metrics on Breast Cancer</figDesc><table><row><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>f1-score</cell></row><row><cell>0</cell><cell>0.99</cell><cell>0.93</cell><cell>0.96</cell></row><row><cell>1</cell><cell>0.89</cell><cell>0.98</cell><cell>0.93</cell></row><row><cell>micro avg</cell><cell>0.95</cell><cell>0.95</cell><cell>0.95</cell></row><row><cell>macro avg</cell><cell>0.94</cell><cell>0.96</cell><cell>0.94</cell></row><row><cell>weighted avg</cell><cell>0.95</cell><cell>0.95</cell><cell>0.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance Comparision of XBNet and XGBoost</figDesc><table><row><cell>Dataset</cell><cell>XBNet</cell><cell>XGBoost</cell></row><row><cell>Iris</cell><cell>100</cell><cell>97.7</cell></row><row><cell>Breast Cancer</cell><cell>96.49</cell><cell>96.47</cell></row><row><cell>Wine</cell><cell>97.22</cell><cell>97.22</cell></row><row><cell>Diabetes</cell><cell>78.78</cell><cell>77.48</cell></row><row><cell>Titanic</cell><cell>79.85</cell><cell>80.5</cell></row><row><cell>German Credit</cell><cell>71.33</cell><cell>77.66</cell></row><row><cell>Digit Completion</cell><cell>85.98</cell><cell>78.24</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning: Trends, perspectives, and prospects</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page" from="255" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t follow me: Spam detection in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on security and cryptography (SECRYPT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Google Scholar Google Scholar Cross Ref Cross Ref</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Malhotra, Notes from the ai frontier: Insights from hundreds of use cases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Manyika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miremadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Henke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<publisher>McKinsey Global Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Evaluating feature importance estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-class adaboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and its Interface</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="360" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning interpretable classification rules with boolean compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Malioutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transparent Data Mining for Big and Small Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python, the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="https://doi.org/10.1145/2939672.2939785" />
		<title level="m">Xgboost: A scalable tree boosting system, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16, Association for Computing Machinery</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural decision forests for semantic image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">G</forename><surname>Morillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06988</idno>
		<title level="m">Deep neural decision trees</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convxgb: A new deep learning model for classification problems based on cnn and xgboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thongsuwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaiyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Padcharoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nuclear Engineering and Technology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="522" to="531" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural network-xgboost for accuracy enhancement of breast cancer detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sugiharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arifudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wiyanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Susilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Physics: Conference Series</title>
		<imprint>
			<publisher>IOP Publishing</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1918</biblScope>
			<biblScope unit="page">42016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08417</idno>
		<title level="m">Learning convolutional neural network to maximize pos@ top performance measure</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stoffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theoretical comparison between the gini index and information gain criteria</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="77" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A spatial entropy-based decision tree for classification of geographical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Claramunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions in GIS</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="467" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
