<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyun</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soongsil University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyojung</forename><surname>Gu</surname></persName>
							<email>gyojung.gu@kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Korea Advanced Institute of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Nestyle Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Korea Advanced Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Choi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Korea Advanced Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
							<email>jchoo@kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Korea Advanced Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>High-Resolution Virtual Try-On</term>
					<term>Misalignment-Free</term>
					<term>Occlusion- Handling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>indicates equal contributions.</p><p>Fig. 1: Comparison of 1024?768 try-on synthesis results with VITON-HD [2].</p><p>(1st row) The red-colored areas indicate the artifact due to the misalignment between a warped clothing image and a segmentation map. (2nd row) The greencolored areas denote the pixel-squeezing due to the occlusion by the body parts. In contrast to the VITON-HD, our method successfully handles the misalignment and occlusion. Zoom in for the best view.</p><p>Abstract. Image-based virtual try-on aims to synthesize an image of a person wearing a given clothing item. To solve the task, the existing methods warp the clothing item to fit the person's body and generate the segmentation map of the person wearing the item before fusing the item with the person. However, when the warping and the segmentation generation stages operate individually without information exchange, the misalignment between the warped clothes and the segmentation map occurs, which leads to the artifacts in the final image. The information disconnection also causes excessive warping near the clothing regions occluded arXiv:2206.14180v2 [cs.CV] 20 Jul 2022 2 Lee et al.</p><p>by the body parts, so-called pixel-squeezing artifacts. To settle the issues, we propose a novel try-on condition generator as a unified module of the two stages (i.e., warping and segmentation generation stages). A newly proposed feature fusion block in the condition generator implements the information exchange, and the condition generator does not create any misalignment or pixel-squeezing artifacts. We also introduce discriminator rejection that filters out the incorrect segmentation map predictions and assures the performance of virtual try-on frameworks. Experiments on a high-resolution dataset demonstrate that our model successfully handles the misalignment and occlusion, and significantly outperforms the baselines. Code is available at https://github.com/sangyun884/HR-VITON.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: Comparison of 1024?768 try-on synthesis results with VITON-HD <ref type="bibr" target="#b1">[2]</ref>. (1st row) The red-colored areas indicate the artifact due to the misalignment between a warped clothing image and a segmentation map. (2nd row) The greencolored areas denote the pixel-squeezing due to the occlusion by the body parts. In contrast to the VITON-HD, our method successfully handles the misalignment and occlusion. Zoom in for the best view.</p><p>Abstract. Image-based virtual try-on aims to synthesize an image of a person wearing a given clothing item. To solve the task, the existing methods warp the clothing item to fit the person's body and generate the segmentation map of the person wearing the item before fusing the item with the person. However, when the warping and the segmentation generation stages operate individually without information exchange, the misalignment between the warped clothes and the segmentation map occurs, which leads to the artifacts in the final image. The information disconnection also causes excessive warping near the clothing regions occluded arXiv:2206.14180v2 [cs.CV] 20 Jul 2022 by the body parts, so-called pixel-squeezing artifacts. To settle the issues, we propose a novel try-on condition generator as a unified module of the two stages (i.e., warping and segmentation generation stages). A newly proposed feature fusion block in the condition generator implements the information exchange, and the condition generator does not create any misalignment or pixel-squeezing artifacts. We also introduce discriminator rejection that filters out the incorrect segmentation map predictions and assures the performance of virtual try-on frameworks. Experiments on a high-resolution dataset demonstrate that our model successfully handles the misalignment and occlusion, and significantly outperforms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As the importance of online shopping increases, a technology that allows customers to virtually try on clothes is expected to enrich the customer's experience. A virtual try-on task aims to change the clothing item on a person into a given clothing product. While there are 3D-based virtual try-on approaches that rely on the 3D measurement of garments <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">25,</ref><ref type="bibr">23,</ref><ref type="bibr">22]</ref>, we address image-based virtual try-on <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr">28,</ref><ref type="bibr">33,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref>, which only requires a garment and a person image, facilitating real-world applications.</p><p>To address this task, previous studies employ an explicit warping module that aligns the clothing image with the person's body. Moreover, predicting the segmentation map of the final image alleviates the difficulty of image generation as it guides the person's layout and separates regions to be generated and the ones to be preserved <ref type="bibr">[32]</ref>. The importance of the segmentation map increases as the image resolution grows. Most image-based virtual try-on methods include these stages <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">28,</ref><ref type="bibr">33,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15]</ref>, and the outputs of the warping and segmentation map generation modules greatly influence the final try-on results.</p><p>However, the virtual try-on frameworks that consist of warping and segmentation generation modules have misaligned regions between the warped clothes and the segmentation map, so-called misalignment. As shown in <ref type="figure">Fig. 1</ref>, the misalignment results in the artifacts in these regions, which harm the perceptual quality of the final result significantly, especially at the high resolution. The main cause of misalignment is that the warping module and the segmentation map generator operate separately without information exchange. Although a recent study <ref type="bibr" target="#b1">[2]</ref> tries to alleviate the artifacts in the misaligned regions, the existing methods are still not possible to solve the misalignment problem completely.</p><p>The information disconnection between two modules yields another problem (i.e., pixel-squeezing artifacts). As shown in <ref type="figure">Fig. 1</ref>, the results of the previous methods are significantly impaired when the body parts occlude the garment. Pixel-squeezing artifacts are caused by excessive warping of clothes near the occluded regions, which is due to the lack of information exchange between the warping and the segmentation map generation modules. The artifacts limit the possible poses of the person images, making it difficult to apply virtual try-on to the real world.</p><p>To settle the issues, we propose a novel try-on condition generator that unifies the warping and segmentation generation modules. The proposed module simultaneously predicts the warped garment and the segmentation map, which are perfectly aligned to each other. Our try-on condition generator can remove the misalignment completely and handle the occlusions by the body parts naturally. Extensive experiments show that the proposed framework successfully handles the occlusion and misalignment, and achieves state-of-the-art results on the high-resolution dataset (i.e., 1024?768), both quantitatively and qualitatively.</p><p>In addition, we introduce a discriminator rejection that filters out incorrect segmentation map predictions, which lead to unnatural final results. We demonstrate that the discriminator rejection assures the performance of virtual try-on frameworks, which is an important feature for real-world applications.</p><p>We summarize our contributions as follows:</p><p>-We propose a novel architecture that performs warping and segmentation map generation simultaneously. -Our method is inherently misalignment-free and can handle the occlusion of clothes by body parts naturally. -We adapt the discriminator rejection to filter out incorrect segmentation map predictions. -We achieve state-of-the-art performance on a high-resolution dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image-based Virtual Try-On</head><p>An image-based virtual try-on task aims to produce a person image wearing a target clothing item given a pair of clothes and person images. Recent virtual try-on methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">28,</ref><ref type="bibr">33,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15]</ref> generally consist of three separate modules: 1) segmentation map generation module, 2) clothing warping module, and 3) fusion module. The fusion module can generate the photo-realistic images by utilizing intermediate representations such as warped clothes and segmentation maps, which are produced by previous stages. Clothes Deformation. To preserve the details of a clothing item, previous approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3]</ref> rely on the explicit warping module to fit the input clothing item to a given person's body. VITON <ref type="bibr" target="#b8">[9]</ref> and CP-VTON[28] predict the parameters for thin plate spline (TPS) transformation to warp the clothing item. Since the warping modules based on the TPS transformation have a limited degree of freedom, an appearance flow is utilized to compute a pixel-wise 2D deformation field of the clothing image <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b2">3]</ref>. Although the warping modules have been consistently improved, the misalignment between the warped clothes and a person's body remains and results in the artifacts in the misaligned regions. Recently, VITON-HD <ref type="bibr" target="#b1">[2]</ref> proposed a normalization technique to alleviate the issue. However, we found that the normalization method fails to naturally fill the misaligned regions with clothing texture. In this paper, we propose a method that can generate warped clothes without misaligned regions. Segmentation Generation for Try-On Synthesis. To guide the try-on image synthesis, recent virtual try-on models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">32,</ref><ref type="bibr">18,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15]</ref> utilize the human segmentation maps of a person wearing the target clothes. The segmentation map disentangles the generation of appearance and shape, allowing the model to produce more spatially coherent results. In particular, the high-resolution virtual try-on methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref> generally include the segmentation generation module because the importance of the segmentation map increases as the image resolution grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Rejection Sampling</head><p>There are several studies that aim to reject the low-quality generator outputs to improve the fidelity of samples. Razavi et al.</p><p>[24] introduced rejection sampling based on the probability that the pre-trained classifier assigns to the correct class. Azadi et al. <ref type="bibr" target="#b0">[1]</ref> proposed the discriminator rejection sampling, where a discriminator rejects the generated samples at test time. Under strict assumptions, this allows exact sampling from the data distribution. Although there have been several follow-up works <ref type="bibr">[27,</ref><ref type="bibr">20]</ref>, this technique has not been commonly used for image-conditional generation. In this paper, we utilize the discriminator to filter out the low-quality samples at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Given a reference image I ? R 3?H?W of a person and a clothing image c ? R 3?H?W (H and W denote the image height and width, respectively), our goal is to synthesize an image? ? R 3?H?W of the person wearing c, where the pose and the body shape of I are maintained. Following the training procedure of VITON <ref type="bibr" target="#b8">[9]</ref>, we train the model to reconstruct I from a clothing-agnostic person representation and c that the person is wearing already. The clothing-agnostic person representation eliminates any clothing information in I, and it allows the model to generalize at test time when an arbitrary clothing image is given.</p><p>Our framework is composed of two stages: (1) a try-on condition generator ; (2) a try-on image generator (see <ref type="figure" target="#fig_0">Fig. 2</ref>). Given the clothing-agnostic person representation and c, our try-on condition generator deforms c and produces the segmentation map simultaneously. The generator does not create any misalignment or pixel-squeezing artifacts (Section 3.1). Afterward, the try-on image generator synthesizes the final try-on result using the outputs of the try-on condition generator (Section 3.2). At test time, we apply discriminator rejection that filters out incorrect segmentation map predictions (Section 3.3). Pre-Processing. In the pre-processing step, we obtain a segmentation map S ? L H?W of the person, a clothing mask c m ? L H?W , and a pose map P ? R 3?H?W with the off-the-shelf models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, where L is a set of integers indicating the semantic labels. For the pose map P , we utilize a dense pose <ref type="bibr" target="#b6">[7]</ref>, which maps all pixels of the person regions in the RGB image to the 3D surface of the person's body. For the clothing-agnostic person representation, we employ a clothing-agnostic person image I a and a clothing-agnostic segmentation map S a as those of VITON-HD <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Try-On Condition Generator</head><p>In this stage, we aim to generate the segmentation map? of the person wearing the target clothing item c and deform c to fit the body of the person. A warped clothing image? c and a generated segmentation map? are used as the conditions for the try-on image generator. <ref type="figure" target="#fig_1">Fig. 3</ref> (A) shows the overall architecture of our try-on condition generator. Our try-on condition generator consists of two encoders (i.e., a clothing encoder E c and a segmentation encoder E s ) and a decoder. Given (c, c m ) and (S a , P ), we first extract the feature pyramid {E c k } 4 k=0 and {E s l } 4 l=0 from each encoder, respectively. The extracted features are fed into the feature fusion blocks of the decoder, where the feature maps obtained from the two different feature pyramids are fused to predict the segmentation map and the appearance flow for warping the clothing image. Given the outputs of the last feature fusion block, we obtain? c ,? c , and? through condition aligning. Feature Fusion Block. As shown in <ref type="figure" target="#fig_1">Fig. 3 (B)</ref>, there are two pathways in the feature fusion block: the flow pathway and the seg pathway. The flow and seg pathway generate the appearance flow map F fi and the segmentation feature F si , respectively. These two pathways exchange information with each other to estimate the appearance flow and the segmentation map jointly, which is indicated by green and blue arrows. For the green arrow, F fi?1 is used to deform the feature extracted from c and c m , which is then concatenated with F si?1 and E si to generate F si . For the blue arrow, F si?1 is used to guide the flow estimation. These information exchanges are crucial in estimating the warped clothing and the segmentation map aligned each other. The feature fusion block estimates F fi and F si simultaneously, which are then used to refine each other at the next block. Condition Aligning. To prevent the misalignment, we obtain? by removing the non-overlapping regions of the clothing mask channel of? k,i,j raw with</p><formula xml:id="formula_0">W (c m , F f4 ):? k,i,j logit = ? k,i,j raw if k ? = ? S k,i,j raw ? W (c m , F f4 ) if k = C (1) S = ?(? logit ),<label>(2)</label></formula><p>where? raw is equivalent to F s4 and C denotes the index of the clothing mask channel. i, j, and k are indices across the spatial and channel dimensions. ? is depth-wise softmax. Note that we apply ReLU activation to assure that? raw is nonnegative.</p><p>I c and? c are obtained by applying the body part occlusion handling to W (c, F f4 ). As <ref type="figure" target="#fig_1">Fig. 3</ref> (C) demonstrates, the body parts of? are used to remove the occluded regions from W (c, F f4 ) and W (c m , F f4 ). Body part occlusion handling helps to eliminate the pixel-squeezing artifacts (see <ref type="figure" target="#fig_5">Fig. 7</ref>). Loss Functions. We use the pixel-wise cross-entropy loss L CE between predicted segmentation map? and S. Additionally, L1 loss and perceptual loss are used to encourage the network to warp the clothes to fit the person's pose. These loss functions are also directly applied to the intermediate flow estimations to prevent the intermediate flow maps from vanishing and improve the performance. Formally, L L1 and L V GG are as follows:</p><formula xml:id="formula_1">L L1 = 3 i=0 w i ? ||W (c m , F fi ) ? S c || 1 + ||? c ? S c || 1 ,<label>(3)</label></formula><formula xml:id="formula_2">L V GG = 3 i=0 w i ? ?(W (c, F fi ), I c ) + ?(? c , I c ),<label>(4)</label></formula><p>where w i determines the relative importance between each terms. L T V is a total-variation loss to enforce the smoothness of the appearance flow:</p><formula xml:id="formula_3">L T V = ||?F f4 || 1<label>(5)</label></formula><p>We found that regularizing only the last appearance flow F f4 is vital in learning the flow estimation at coarse scales. Totally, our try-on condition generator is trained end-to-end using the following objective function:</p><formula xml:id="formula_4">L T OCG = ? CE L CE + L cGAN + ? L1 L L1 + L V GG + ? T V L T V ,<label>(6)</label></formula><p>where L cGAN is conditional GAN loss between? and S, and ? CE , ? L1 , and ? T V denote the hyper-parameters controlling relative importance between different losses. For L cGAN , we used the least-squared GAN loss <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Try-On Image Generator</head><p>In this stage, we generate the final try-on image? by fusing the clothing-agnostic image I a , the warped clothing image? c , and the pose map P , guided by?. The try-on image generator consists of a series of residual blocks, along with upsampling layers. The residual blocks use SPADE [21] as normalization layers whose modulation parameters are inferred from?. Also, the input (I a ,? c , P ) is resized and concatenated to the activation before each residual block. We train the generator with the same losses used in SPADE and pix2pixHD <ref type="bibr">[29]</ref>. Details of the model architecture, hyperparameters, and the objective function are described in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discriminator Rejection</head><p>We propose a discriminator rejection method to filter out the low-quality segmentation map generated by the try-on condition generator at the test time. In the discriminator rejection sampling <ref type="bibr" target="#b0">[1]</ref>, the acceptance probability for an input x is</p><formula xml:id="formula_5">p accept (x) = p d (x) Lp g (x) ,<label>(7)</label></formula><p>where p d and p g are the data distribution and the implicit distribution given by the generator, and L is a normalizing constant. As we use the least-squares GAN loss, the optimal discriminator is derived as follows:</p><formula xml:id="formula_6">D * (x) = p d (x) p d (x) + p g (x)<label>(8)</label></formula><p>Afterward, the acceptance probability can be represented using the discriminator D(x):</p><formula xml:id="formula_7">p accept = D(x) L(1 ? D(x)) ,<label>(9)</label></formula><p>Where the equality is satisfied only if D = D * . L is written as follows:</p><formula xml:id="formula_8">L = max x D(x) (1 ? D(x)) ,<label>(10)</label></formula><p>which is intractable. In practice, we construct x from the segmentation map and input conditions (i.e., P, S a , c, and c m ) and obtain L using the entire training dataset. Azadi et al. <ref type="bibr" target="#b0">[1]</ref> sample ? ? U (0, 1) and reject x if ? &gt; p accept (x). Instead, we reject x if p accept (x) is below a certain threshold. The discriminator rejection enables us to filter out the incorrect segmentation maps faithfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>For the experiments, we use a high-resolution virtual try-on dataset introduced by VITON-HD <ref type="bibr" target="#b1">[2]</ref>, which contains 13,679 frontal-view woman and top clothing image pairs. The original resolution of the images is 1024?768, and the images are bicubically downsampled to the desired resolutions when needed. We split the dataset into a training and a test set with 11,647 and 2,032 pairs, respectively. For detailed information on the model training, see appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Qualitative Results</head><p>Comparison with Baselines. We compare our method with several state-ofthe-art baselines, including CP-VTON [28], ACGPN [32], and VITON-HD <ref type="bibr" target="#b1">[2]</ref>. We utilize the publicly available codes for baselines. <ref type="figure" target="#fig_2">Fig. 4</ref> shows that our method  generates more photo-realistic images compared to the baselines. Specifically, we observe that our model not only preserves the details of the target clothing images but also generates the neckline naturally. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, our try-on condition generator has the capability to produce the body shape more naturally compared to VITON-HD. These results demonstrate that the quality of the conditions for the try-on image generator is crucial in achieving perceptually convincing results. Furthermore, <ref type="figure" target="#fig_4">Fig. 6</ref> shows that VITON-HD fails to eliminate the artifacts in the misaligned regions completely. On the other hand, since our method can produce misalignment-free segmentation maps and warped clothing images, our method solves the misalignment problem inherently. Thus, our method successfully synthesizes the high-quality images. Effectiveness of Occlusion Handling. We analyze the impact of the occlusion handling process in our try-on condition generator. <ref type="figure" target="#fig_5">Fig. 7</ref> shows the effectiveness of the proposed body part occlusion handling. Without occlusion handling, the model excessively deforms the clothing image to fit the person's body shape, as shown in the 2nd column of <ref type="figure" target="#fig_5">Fig. 7</ref>. Due to the undesired deformation, the texture (e.g., logo and stripe) of the target clothing item is squeezed, causing the missing pattern in the final results (See the 3rd column of <ref type="figure" target="#fig_5">Fig. 7</ref>). On the other hand, the model with occlusion handling enables to warp the clothes without the pixel-squeezing, better preserving the high-frequency details of the garment. Effectiveness of Discriminator Rejection. To filter out the low-quality segmentation maps produced by our try-on condition generator, we propose a discriminator rejection method. <ref type="figure" target="#fig_6">Fig. 8</ref> shows the accepted and the rejected samples   of our discriminator rejection. Different from the accepted samples, the segmentation maps of the rejected samples are considerably impaired, as shown in the 2nd row of <ref type="figure" target="#fig_6">Fig. 8</ref>. We found that the incorrect segmentation maps are caused mainly by errors in the pre-processing step, such as obtaining the clothing mask. Most virtual try-on methods rely on multiple conditions such as segmentation map and pose information obtained in the pre-processing stage and thus are prone to these errors. We believe that our discriminator rejection method can be a simple and effective solution for filtering out the low-quality outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Results</head><p>Following previous studies, we evaluate a paired setting and an unpaired setting, where the paired setting is to reconstruct the person image with the original clothing image, and the unpaired setting is to change the clothing item of the  <ref type="table">Table 1</ref>: Ablation study in unpaired setting. We describes the KID as a value multiplied by 100. *Last row denotes that there is no information exchange.  . Additionally, to evaluate the unpaired setting, we measure Frechet Inception Distance (FID) <ref type="bibr" target="#b9">[10]</ref> and Kernel Inception Distance (KID), which is a more descriptive metric than FID when the number of data is small.</p><formula xml:id="formula_9">256?192 512?384 1024?768 LPIPS ? SSIM ? FID ? KID ? LPIPS ? SSIM ? FID ? KID ? LPIPS ? SSIM ? FID ? KID ? CP-VTON</formula><p>Ablation Study. <ref type="table">Table 1</ref> shows the effectiveness of the proposed feature fusion block and condition aligning. Indeed, the benefits of fusion block and condition aligning are largely additive. Notably, the model without feature fusion block and condition aligning yields suboptimal results, demonstrating the necessity of information exchange between the warping module and the segmentation map generator.</p><p>Comparison with Baselines. <ref type="table" target="#tab_1">Table 2</ref> demonstrates that our method outperforms the baselines for all evaluation metrics, especially at the 1024?768 resolution. The results indicate that CP-VTON and ACGPN can not handle the high-resolution images in the unpaired setting. Furthermore, it is noteworthy that our framework surpasses VITON-HD, one of the state-of-the-art methods for high-resolution virtual try-on. Although our try-on image generator is very similar to one of VITON-HD, our framework has superior performance due to the capability to produce high-quality conditions (i.e., segmentation map and warped clothing image). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Parser-free Virtual Try-on Methods</head><p>Recently, several approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4]</ref> propose virtual try-on models that do not rely on a predicted segmentation map. However, explicitly predicting a segmentation map helps the model distinguish the regions to be generated and the regions to be preserved, which is necessary for a high-resolution virtual try-on. To verify this, we compare our model with PF-AFN <ref type="bibr" target="#b3">[4]</ref> on the high-resolution dataset. <ref type="figure" target="#fig_8">Fig. 9</ref> demonstrates that PF-AFN fails to remove the original clothing regions as it can not differentiate the parts to be generated and the parts to be left, resulting in significant artifacts in the outputs. Moreover, <ref type="table" target="#tab_1">Table 2</ref> shows that our model outperforms PF-AFN by a large margin. The results indicate that it is difficult to obtain convincing high-resolution results without predicting a segmentation map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Limitation of Discriminator Rejection. The existing image-based virtual try-on approaches assume that test data is drawn from the same distribution as the training data. However, in the real-world scenario, it is prevalent that the input images are taken at a different camera view from the training images or even do not contain humans. Since the low-quality segmentation is often predicted due to such out-of-distribution inputs, our discriminator rejection is capable of filtering out the out-of-distribution inputs. We believe that our discriminator rejection can be a solution to enhance the user experience in virtual try-on applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel architecture for high-resolution virtual, which performs warping clothes and segmentation generation simultaneously while exchanging information with each other. The proposed try-on condition generator completely eliminates the misaligned region and solves the pixel-squeezing problem by handling the occlusion by body parts. We also demonstrate that the discriminator of the condition generator can filter out the impaired segmentation results, which is practically helpful for real-world virtual try-on applications. Extensive experiments show that our method outperforms the existing virtual try-on methods at 1024?768 resolution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>In this section, we describe the detailed architectures, hyper-parameters, and objective functions of the try-on condition generator and the image generator. Try-On Condition Generator. The try-on condition generator consists of two encoders and four feature fusion blocks, and each encoder is composed of five residual blocks. The features of the last residual blocks are concatenated and passed to a 3?3 convolutional layer, which generates the first flow map of the flow pathway. Also, the last feature of the segmentation encoder is used as the input of the segmentation pathway (i.e., seg pathway) after passing through two residual blocks. We employ two multi-scale discriminators for the conditional adversarial loss. The visualization of the try-on condition generator architecture is in <ref type="figure" target="#fig_10">Fig. 10</ref>.</p><p>During the training of our try-on condition generator, the model predict? I c ,? c , and? at 256?192 resolution. In the inference phase, before forwarding our try-on image generator, the segmentation map and the appearance flow obtained from the try-on condition generator are upscaled to 1024?768. We down-sampled the inputs for the discriminator of our try-on condition generator by a factor of 2 to increase the receptive field. In addition, we apply a dropout <ref type="bibr">[26]</ref> to the discriminator to stabilize the training. For hyper-parameters we used, ? CE , ? V GG , and ? T V are set to 10, 10, and 2, respectively. The batch sizes for training our try-on condition generator and image generator are set to 8 and 4, respectively. We train each module for 100,000 iterations. The learning rates of the generator and the discriminator of the try-on condition generator are set to 0.0002. Try-On Image Generator. We describe the detailed architecture of the tryon image generator as shown in <ref type="figure" target="#fig_11">Fig. 11</ref>. The generator is composed of a series of residual blocks with upsampling layers, and two multi-scale discriminators are employed for the conditional adversarial loss. Spectral normalization [19] is applied to all the convolutional layers.</p><p>To train the try-on image generator, we utilize the same losses used in SPADE [21] and pix2pixHD <ref type="bibr">[29]</ref>. Specifically, our full objective function consists of the conditional adversarial loss, the perceptual loss, and the feature matching loss. Formally, our objective function is as follows:</p><formula xml:id="formula_10">L T OIG = L T OIG cGAN + ? T OIG V GG L T OIG V GG + ? T OIG F M L T OIG F M ,<label>(11)</label></formula><p>where L T OIG cGAN , L T OIG V GG , and L T OIG are set to 10. The learning rates of the generator and the discriminator of the try-on image generator are set to 0.0001 and 0.0004, respectively. We adopt the Adam optimizer with ? 1 = 0.5 and ? 2 = 0.999 for both modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiments</head><p>Results on Different Resolutions. We provide the additional qualitative results for comparison across different resolutions ( <ref type="figure" target="#fig_0">Fig. 12, and Fig. 13</ref>).</p><p>Comparison with the Variant of VITON-HD. Previous studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b2">3]</ref> improve the performance of the geometric deformation for the target clothes by utilizing the appearance flow. However, simply increasing the degree of freedom of the warping module cannot perfectly remove the artifacts caused by misalignment and pixel-squeezing. To verify this, we further compare our method with VITON-HD*, the VITON-HD variant of which the clothes warping module is replaced by that of Clothflow <ref type="bibr" target="#b7">[8]</ref>. Since Clothflow is superior to the warping module of VITON-HD, VITON-HD* can reduce the misalignment region.</p><p>Despite the improvement of the warping module in VITON-HD, our model consistently outperforms the VITON-HD* in all evaluation metrics, as seen in <ref type="table">Table 3</ref>. Also, 2nd column in <ref type="figure" target="#fig_2">Fig. 14</ref> shows that VITON-HD* still suffers from  <ref type="table">Table 3</ref>: Quantitative comparison with VITON-HD* at the 1024?768 resolution. We describe the KID as a value multiplied by 100.</p><p>the artifacts due to the misalignment. Furthermore, increasing the degree of freedom of the warping module exacerbates the pixel-squeezing artifact, indicating that the use of appearance flow without proper occlusion handling can be harmful. On the other hand, our model successfully solves both the misalignment and the pixel-squeezing problems, as shown in 3rd column in <ref type="figure" target="#fig_2">Fig. 14</ref> User Study. We conduct a user study to further assess our model and other baselines at the 1024?768 resolution. Given the 30 sets of a reference image and a target garment image from the test set, the users are asked to choose an image among the synthesized results of our model and baselines according to the following questions: (1) Which image is the most photo-realistic? (2) Which image preserves the details of the given clothing the most? In addition, a total of 21 participants participate in the user study. <ref type="figure" target="#fig_3">Fig. 15</ref> shows that our model achieves the highest average selection rate for both questions, indicating that our model synthesizes more perceptually convincing results and preserves the detail of the clothing items better than other baselines. Effectiveness of Multi-Scale L1/VGG Losses. During the training of the try-on condition generator, L L1 and L V GG are directly applied to the inter-mediate flow estimations. As shown in 2nd row of <ref type="figure" target="#fig_4">Fig. 16</ref>, the model without the multi-scale losses has difficulty learning flow estimation in a coarse scale. Multi-scale losses enable the model to learn the meaningful intermediate flow estimation, which is crucial for the coarse-to-fine generation of appearance flow. Additional Results. We present additional qualitative results of our model. <ref type="figure" target="#fig_5">Fig. 17</ref> shows the combination of different clothes and different people, and <ref type="figure" target="#fig_0">Fig. 18-20</ref> shows the high-resolution synthesis results (i.e., 1024?768).   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the proposed framework (HR-VITON).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Architecture of try-on condition generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative comparison with baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Try-on synthesis results and corresponding segmentation maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Synthesis results and corresponding misaligned regions indicated by yellow colored areas. VITON-HD suffers from the artifacts caused by misalignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Effects of the body part occlusion handling. The green colored areas indicate the pixel-squeezing artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Examples of accepted (A) and rejected (B) segmentation maps by discriminator rejection, corresponding input clothes and clothing masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>0.159 0.739 30.11 2.034 0.141 0.791 30.25 4.012 0.158 0.786 43.28 3.762 ACGPN 0.074 0.833 11.33 0.344 0.076 0.858 14.43 0.587 0.112 0.850 43.29 3.730 VITON-HD 0.084 0.811 16.36 0.871 0.076 0.843 11.64 0.300 0.077 0.873 11.59 00.062 0.864 9.38 0.153 0.061 0.878 9.90 0.188 0.065 0.892 10.91 0.179</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Qualitative comparison with PF-AFN on 1024?768 resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>This work was supported by the Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government(MSIT) (No. 2019-0-00075, Artificial Intelligence Graduate School Program(KAIST) and No.2021-0-02068, Artificial Intelligence Innovation Hub) and the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. NRF-2022R1A2B5B02001913) 18. Minar, M.R., Ahn, H.: Cloth-vton: Clothing three-dimensional reconstruction for hybrid image-based virtual try-on. In: Proceedings of the Asian Conference on Computer Vision (2020) 19. Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for generative adversarial networks. In: International Conference on Learning Representations (2018) 20. Mo, S., Kim, C., Kim, S., Cho, M., Shin, J.: Mining gold samples for conditional gans. Advances in Neural Information Processing Systems 32 (2019) 21. Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with spatially-adaptive normalization. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2337-2346 (2019) 22. Patel, C., Liao, Z., Pons-Moll, G.: Tailornet: Predicting clothing in 3d as a function of human pose, shape and garment style. In: Proc. of the IEEE conference on computer vision and pattern recognition (CVPR). pp. 7365-7375 (2020) 23. Pons-Moll, G., Pujades, S., Hu, S., Black, M.J.: Clothcap: Seamless 4d clothing capture and retargeting. ACM Transactions on Graphics (TOG) 36(4), 1-15 (2017) 24. Razavi, A., Van den Oord, A., Vinyals, O.: Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems 32 (2019) 25. Sekine, M., Sugita, K., Perbet, F., Stenger, B., Nishiyama, M.: Virtual fitting by single-shot body shape estimation. In: Int. Conf. on 3D Body Scanning Technologies. pp. 406-413. Citeseer (2014) 26. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research 15(1), 1929-1958 (2014) 27. Turner, R., Hung, J., Frank, E., Saatchi, Y., Yosinski, J.: Metropolis-hastings generative adversarial networks. In: International Conference on Machine Learning. pp. 6345-6353. PMLR (2019) 28. Wang, B., Zheng, H., Liang, X., Chen, Y., Lin, L., Yang, M.: Toward characteristicpreserving image-based virtual try-on network. In: Proc. of the European Conference on Computer Vision (ECCV). pp. 589-604 (2018) 29. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: Highresolution image synthesis and semantic manipulation with conditional gans. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 8798-8807 (2018) 30. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13(4), 600-612 (2004) 31. Xie, Z., Zhang, X., Zhao, F., Dong, H., Kampffmeyer, M.C., Yan, H., Liang, X.: Was-vton: Warping architecture search for virtual try-on network. In: Proceedings of the 29th ACM International Conference on Multimedia. pp. 3350-3359 (2021) 32. Yang, H., Zhang, R., Guo, X., Liu, W., Zuo, W., Luo, P.: Towards photo-realistic virtual try-on by adaptively generating-preserving image content. In: Proc. of the IEEE conference on computer vision and pattern recognition (CVPR). pp. 7850-7859 (2020) 33. Yu, R., Wang, X., Xie, X.: Vtnfp: An image-based virtual try-on network with body and clothing feature preservation. In: Proc. of the IEEE international conference on computer vision (ICCV). pp. 10511-10520 (2019) 34. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: Proc. of the IEEE conference on computer vision and pattern recognition (CVPR) (2018) APPENDIX</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>The detailed architecture of the try-on condition generator. (ResBlock (n), Up/Down (f )) denotes a residual block where the scaling factor is f and the output channel is n. Conv (m) denotes a convolutional layer where the output channel is m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>The detailed architecture of the try-on image generator. (ResBlock (n), Up (f )) denotes a residual block, where the scaling factor is f , and the output channel is n. Conv (m) denotes a convolutional layer where the output channel is m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>F M denote the conditionalFig. 12 :</head><label>12</label><figDesc>adversarial loss, the perceptual loss, and the feature matching loss [29], respectively. We use ? T OIG V GG Qualitative comparison of the baselines (256?192) and ? T OIG F M for hyper-parameters controlling relative importance between different losses. For L T OIG GAN , we employ the Hinge loss [16]. ? T OIG V GG and ? T OIG F M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 :Fig. 14 :</head><label>1314</label><figDesc>Qualitative comparison of the baselines (512?384) Qualitative comparison with VITON-HD* (1024?768). VITON-HD* suffers from the misalignment and the pixel-squeezing artifacts indicated by green and red colored areas, respectively. LPIPS ? SSIM ? FID ? KID ? VITON-HD* 0.070 0.875 11.55 0.2993 Ours 0.065 0.892 10.91 0.1794</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>.Fig. 15 :Fig. 16 :</head><label>1516</label><figDesc>User study results. Effects of the multi-scale L1/VGG losses. 1st row: w/ multi-scale losses. 2nd row: w/o multi-scale losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 :</head><label>17</label><figDesc>Qualitative results of our model (1024?768).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 18 :</head><label>18</label><figDesc>Qualitative results of our model (1024?768). The reference image and the target clothes (left), the synthesis image (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 19 :</head><label>19</label><figDesc>Qualitative results of our model (1024?768). The reference image and the target clothes (left), the synthesis image (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison with baselines. We describes the KID as a value multiplied by 100. HR-VITON refers to our model.</figDesc><table><row><cell>person image. For paired setting, we evaluate our method using two widely-</cell></row><row><cell>used metrics: Structural Similarity (SSIM) [30] and Learned Perceptual Image</cell></row><row><cell>Patch Similarity (LPIPS) [34]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">Lee et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Fig. 20: Qualitative results of our model (1024?768). The reference image and the target clothes (left), the synthesis image (right).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06758</idno>
		<title level="m">Discriminator rejection sampling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Viton-hd: High-resolution virtual try-on via misalignment-aware normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14131" to="14140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zflow: Gated appearance flow-based virtual try-on with 3d priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5433" to="5442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parser-free virtual try-on via distilling appearance flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8485" to="8493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Instance-level human parsing via part grouping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="770" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Drape: Dressing any person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hirshberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clothflow: A flow-based model for clothed person generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>of the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10471" to="10480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Viton: An image-based virtual tryon network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7543" to="7552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information essing Systems</meeting>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do not mask what you do not need to mask: a parser-free virtual try-on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Issenhuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Calauz?nes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="619" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sievenet: A unified framework for robust image-based virtual try-on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jandial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Halwai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2182" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The conditional analogy gan: Swapping fashion articles on people images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jetchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bergmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision workshop (ICCVW)</title>
		<meeting>of the IEEE international conference on computer vision workshop (ICCVW)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2287" to="2292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Varadharajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<title level="m">Vogue: Try-on by stylegan interpolation optimization. arXiv e-prints pp</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward accurate and realistic outfits visualization with attention to details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15546" to="15555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<title level="m">Geometric gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
