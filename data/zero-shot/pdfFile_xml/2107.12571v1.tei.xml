<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Gudovskiy</surname></persName>
							<email>denis.gudovskiy@us.panasonic.com</email>
							<affiliation key="aff0">
								<orgName type="department">Panasonic AI Lab</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Ishizaka</surname></persName>
							<email>ishizaka.shun@jp.panasonic.com</email>
							<affiliation key="aff1">
								<orgName type="department">Panasonic Technology Division</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Kozuka</surname></persName>
							<email>kozuka.kazuki@jp.panasonic.com</email>
							<affiliation key="aff1">
								<orgName type="department">Panasonic Technology Division</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised anomaly detection with localization has many practical applications when labeling is infeasible and, moreover, when anomaly examples are completely missing in the train data. While recently proposed models for such data setup achieve high accuracy metrics, their complexity is a limiting factor for real-time processing. In this paper, we propose a real-time model and analytically derive its relationship to prior methods. Our CFLOW-AD model is based on a conditional normalizing flow framework adopted for anomaly detection with localization. In particular, CFLOW-AD consists of a discriminatively pretrained encoder followed by a multi-scale generative decoders where the latter explicitly estimate likelihood of the encoded features. Our approach results in a computationally and memory-efficient model: CFLOW-AD is faster and smaller by a factor of 10? than prior state-of-the-art with the same input setting. Our experiments on the MVTec dataset show that CFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by 1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source our code with fully reproducible experiments 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anomaly detection with localization (AD) is a growing area of research in computer vision with many practical applications e.g. industrial inspection <ref type="bibr" target="#b3">[4]</ref>, road traffic monitoring <ref type="bibr" target="#b19">[20]</ref>, medical diagnostics <ref type="bibr" target="#b45">[44]</ref> etc. However, the common supervised AD <ref type="bibr" target="#b32">[32]</ref> is not viable in practical applications due to several reasons. First, it requires labeled data which is costly to obtain. Second, anomalies are usually rare long-tail examples and have low probability to be acquired by sensors. Lastly, consistent labeling of anomalies is subjective and requires extensive domain expertise as illustrated in <ref type="figure">Figure 1</ref> with industrial cable defects. <ref type="bibr" target="#b0">1</ref> Our code is available at github.com/gudovskiy/cflow-ad AD AD <ref type="figure">Figure 1</ref>. An example of the proposed out-of-distribution (OOD) detector for anomaly localization trained on anomaly-free Dtrain (top row). Sliced cable images are from the MVTec dataset <ref type="bibr" target="#b3">[4]</ref>, where the bottom row illustrates Dtest ground truth masks for anomalies (red) and the middle row shows examples of anomalyfree patches (green). The OOD detector learns the distribution of anomaly-free patches x with pX (x) density and transforms it into a Gaussian distribution with pZ (z) density. Threshold ? separates in-distribution patches from the OOD patches with pZ (z) density.</p><p>With these limitations of the supervised AD, a more appealing approach is to collect only unlabeled anomaly-free images for train dataset D train as in <ref type="figure">Figure 1</ref> (top row). Then, any deviation from anomaly-free images is classified as an anomaly. Such data setup with low rate of anomalies is generally considered to be unsupervised <ref type="bibr" target="#b3">[4]</ref>. Hence, the AD task can be reformulated as a task of out-of-distribution detection (OOD) with the AD objective.</p><p>While OOD for low-dimensional industrial sensors (e.g. power-line or acoustic) can be accomplished using a common k-nearest-neighbor or more advanced clustering methods <ref type="bibr" target="#b9">[10]</ref>, it is less trivial for high-resolution images. Re-cently, convolutional neural networks (CNNs) have gained popularity in extracting semantic information from images into downsampled feature maps <ref type="bibr" target="#b3">[4]</ref>. Though feature extraction using CNNs has relatively low complexity, the postprocessing of feature maps is far from real-time processing in the state-of-the-art unsupervised AD methods <ref type="bibr" target="#b7">[8]</ref>.</p><p>To address this complexity drawback, we propose a CFLOW-AD model that is based on conditional normalizing flows. CFLOW-AD is agnostic to feature map spatial dimensions similar to CNNs, which leads to a higher accuracy metrics as well as a lower computational and memory requirements. We present the main idea behind our approach in a toy OOD detector example in <ref type="figure">Figure 1</ref>. A distribution of the anomaly-free image patches x with probability density function p X (x) is learned by the AD model. Our translation-equivariant model is trained to transform the original distribution with p X (x) density into a Gaussian distribution with p Z (z) density. Finally, this model separates in-distribution patches z with p Z (z) from the outof-distribution patches with pZ(z) using a threshold ? computed as the Euclidean distance from the distribution mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We review models 2 that employ the data setup from Figure 1 and provide experimental results for popular MVTec dataset <ref type="bibr" target="#b3">[4]</ref> with factory defects or Shanghai Tech Campus (STC) dataset <ref type="bibr" target="#b21">[22]</ref> with surveillance camera videos. We highlight the research related to a more challenging task of pixel-level anomaly localization (segmentation) rather than a more simple image-level anomaly detection.</p><p>Napoletano et al. <ref type="bibr" target="#b24">[25]</ref> propose to use CNN feature extractors followed by a principal component analysis and kmean clustering for AD. Their feature extractor is a ResNet-18 <ref type="bibr" target="#b12">[13]</ref> pretrained on a large-scale ImageNet dataset <ref type="bibr" target="#b15">[16]</ref>. Similarly, SPADE <ref type="bibr" target="#b6">[7]</ref> employs a Wide-ResNet-50 <ref type="bibr" target="#b44">[43]</ref> with multi-scale pyramid pooling that is followed by a k-nearestneighbor clustering. Unfortunately, clustering is slow at test-time with high-dimensional data. Thus, parallel convolutional methods are preferred in real-time systems.</p><p>Numerous methods are based on a natural idea of generative modeling. Unlike models with the discriminativelypretrained feature extractors <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref>, generative models learn distribution of anomaly-free data and, therefore, are able to estimate a proxy metrics for anomaly scores even for the unseen images with anomalies. Recent models employ generative adversarial networks (GANs) <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b37">36]</ref> and variational autoencoders (VAEs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">38]</ref>.</p><p>A fully-generative models <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b39">38]</ref> are directly applied to images in order to estimate pixel-level probability density and compute per-pixel reconstruction errors as anomaly scores proxies. These fully-generative models are unable to estimate the exact data likelihoods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref> and do not perform better than the traditional methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref> according to MVTec survey in <ref type="bibr" target="#b3">[4]</ref>. Recent works <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b14">15]</ref> show that these models tend to capture only low-level correlations instead of relevant semantic information. To overcome the latter drawback, a hybrid DFR model <ref type="bibr" target="#b38">[37]</ref> uses a pretrained feature extractor with multi-scale pyramid pooling followed by a convolutional autoencoder (CAE). However, DFR model is unable to estimate the exact likelihoods.</p><p>Another line of research proposes to employ a studentteacher type of framework <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b42">41]</ref>. Teacher is a pretrained feature extractor and student is trained to estimate a scoring function for AD. Unfortunately, such frameworks underperform compared to state-of-the-art models.</p><p>Patch SVDD <ref type="bibr" target="#b43">[42]</ref> and CutPaste <ref type="bibr" target="#b18">[19]</ref> introduce a selfsupervised pretraining scheme for AD. Moreover, Patch SVDD proposes a novel method to combine multi-scale scoring masks to a final anomaly map. Unlike the nearestneighbor search in <ref type="bibr" target="#b43">[42]</ref>, CutPaste estimates anomaly scores using an efficient Gaussian density estimator. While the self-supervised pretraining can be helpful in uncommon data domains, Schirrmeister et al. <ref type="bibr" target="#b35">[34]</ref> argue that large natural-image datasets such as ImageNet can be a more representative for pretraining compared to a small applicationspecific datasets e.g. industrial MVTec <ref type="bibr" target="#b3">[4]</ref>.</p><p>The state-of-the-art PaDiM <ref type="bibr" target="#b7">[8]</ref> proposes surprisingly simple yet effective approach for anomaly localization. Similarly to <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b43">42]</ref>, this approach relies on ImageNetpretrained feature extractor with multi-scale pyramid pooling. However, instead of slow test-time clustering in <ref type="bibr" target="#b6">[7]</ref> or nearest-neighbor search in <ref type="bibr" target="#b43">[42]</ref>, PaDiM uses a wellknown Mahalanobis distance metric <ref type="bibr" target="#b22">[23]</ref> as an anomaly score. The metric parameters are estimated for each feature vector from the pooled feature maps. PaDiM has been inspired by Rippel et al. <ref type="bibr" target="#b29">[29]</ref> who firstly advocated to use this measure for anomaly detection without localization.</p><p>DifferNet <ref type="bibr" target="#b30">[30]</ref> uses a promising class of generative models called normalizing flows (NFLOWs) <ref type="bibr" target="#b8">[9]</ref> for image-level AD. The main advantage of NFLOW models is ability to estimate the exact likelihoods for OOD compared to other generative models <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b38">37]</ref>. In this paper, we extend DifferNet approach to pixel-level anomaly localization task using our CFLOW-AD model. In contrast to Real-NVP <ref type="bibr" target="#b8">[9]</ref> architecture with global average pooling in <ref type="bibr" target="#b30">[30]</ref>, we propose to use conditional normalizing flows <ref type="bibr" target="#b1">[2]</ref> to make CFLOW-AD suitable for low-complexity processing of multi-scale feature maps for localization task. We develop our CFLOW-AD with the following contributions:</p><p>? Our theoretical analysis shows why multivariate Gaussian assumption is a justified prior in previous models and why a more general NFLOW framework objective converges to similar results with the less compute.</p><p>? We propose to use conditional normalizing flows for unsupervised anomaly detection with localization using computational and memory-efficient architecture.</p><p>? We show that our model outperforms previous stateof-the art in both detection and localization due to the unique properties of the proposed CFLOW-AD model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theoretical background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature extraction with Gaussian prior</head><p>Consider a CNN h(?) trained for classification task. Its parameters ? are usually found by minimizing Kullback-Leibler (D KL ) divergence between joint train data distribution Q x,y and the learned model distribution P x,y (?), where (x, y) is an input-label pair for supervised learning.</p><p>Typically, the parameters ? are initialized by the values sampled from the Gaussian distribution <ref type="bibr" target="#b11">[12]</ref> and optimization process is regularized as</p><formula xml:id="formula_0">arg min ? D KL [Q x,y P x,y (?)] + ?R(?),<label>(1)</label></formula><p>where R(?) is a regularization term and ? is a hyperparameter that defines regularization strength.</p><p>The most popular CNNs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">43]</ref> are trained with L 2 weight decay <ref type="bibr" target="#b16">[17]</ref> regularization (R(?) = ? 2 2 ). That imposes multivariate Gaussian (MVG) prior not only to parameters ?, but also to the feature vectors z extracted from the feature maps of h(?) <ref type="bibr" target="#b10">[11]</ref> intermediate layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A case for Mahalanobis distance</head><p>With the same MVG prior assumption, Lee et al. <ref type="bibr" target="#b17">[18]</ref> recently proposed to model distribution of feature vectors z by MVG density function and to use Mahalanobis distance <ref type="bibr" target="#b22">[23]</ref> as a confidence score in CNN classifiers. Inspired by <ref type="bibr" target="#b17">[18]</ref>, Rippel et al. <ref type="bibr" target="#b29">[29]</ref> adopt Mahalanobis distance for anomaly detection task since this measure determines a distance of a particular feature vector z to its MVG distribution. Consider a MVG distribution N (?, ?) with a density function p Z (z) for random variable z ? R D defined as</p><formula xml:id="formula_1">p Z (z) = (2?) ?D/2 det ? ?1/2 e ? 1 2 (z??) T ? ?1 (z??) ,<label>(2)</label></formula><p>where ? ? R D is a mean vector and ? ? R D?D is a covariance matrix of a true anomaly-free density p Z (z). Then, the Mahalanobis distance M (z) is calculated as</p><formula xml:id="formula_2">M (z) = (z ? ?) T ? ?1 (z ? ?),<label>(3)</label></formula><p>Since the true anomaly-free data distribution is unknown, mean vector and covariance matrix from (3) are replaced by the estimates? and? calculated from the empirical train dataset D train . At the same time, density function pZ(z) of anomaly data has different? and? statistics, which allows to separate out-of-distribution and indistribution feature vectors using M (z) from <ref type="formula" target="#formula_2">(3)</ref>.</p><p>This framework with MVG distribution assumption shows its effectiveness in image-level anomaly detection task <ref type="bibr" target="#b29">[29]</ref> and is adopted by the state-of-the-art PaDiM <ref type="bibr" target="#b7">[8]</ref> model in pixel-level anomaly localization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Relationship with the flow framework</head><p>Dinh et al. <ref type="bibr" target="#b8">[9]</ref> introduce a class of generative probabilistic models called normalizing flows. These models apply change of variable formula to fit an arbitrary density p Z (z) by a tractable base distribution with p U (u) density and a bijective invertible mapping g ?1 : Z ? U . Then, the loglikelihood of any z ? Z can be estimated by</p><formula xml:id="formula_3">logp Z (z, ?) = log p U (u) + log |det J | ,<label>(4)</label></formula><p>where a sample u ? p U is usually from standard MVG distribution (u ? N (0, I)) and a matrix J = ? z g ?1 (z, ?) is the Jacobian of a bijective invertible flow model (z = g(u, ?) and u = g ?1 (z, ?)) parameterized by vector ?.</p><p>The flow model g(?) is a set of basic layered transformations with tractable Jacobian determinants. For example, log | det J | in RealNVP <ref type="bibr" target="#b8">[9]</ref> coupling layers is a simple sum of layer's diagonal elements. These models are optimized using stochastic gradient descent by maximizing loglikelihood in <ref type="bibr" target="#b3">(4)</ref>. Equivalently, optimization can be done by</p><formula xml:id="formula_4">minimizing the reverse D KL [p Z (z, ?) p Z (z)] [27], wher? p Z (z, ?)</formula><p>is the model prediction and p Z (z) is a target density. The loss function for this objective is defined as</p><formula xml:id="formula_5">L(?) = Ep Z (z,?) [logp Z (z, ?) ? log p Z (z)] .<label>(5)</label></formula><p>If p Z (z) is distributed according to Section 3.1 MVG assumption, we can express (5) as a function of Mahalanobis distance M (z) using its definition from <ref type="formula" target="#formula_2">(3)</ref> as</p><formula xml:id="formula_6">L(?) = Ep Z (z,?) M 2 (z) ? E 2 (u) 2 + log | det J | det ? ?1/2 , (6) where E 2 (u) = u 2 2 is a squared Euclidean distance of a sample u ? N (0, I) (detailed proof in Appendix A).</formula><p>Then, the loss in (6) converges to zero when the likelihood contribution term | det J | of the model g(?) (normalized by det ? ?1/2 ) compensates the difference between a squared Mahalanobis distance for z from the target density and a squared Euclidean distance for u ? N (0, I).</p><p>This normalizing flow framework can estimate the exact likelihoods of any arbitrary distribution with p Z density, while Mahalanobis distance is limited to MVG distribution only. For example, CNNs trained with L 1 regularization would have Laplace prior <ref type="bibr" target="#b10">[11]</ref> or have no particular prior in the absence of regularization. Moreover, we introduce conditional normalizing flows in the next section and show that they are more compact in size and have fully-convolutional parallel architecture compared to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> models. We implement a feature extraction scheme with multiscale feature pyramid pooling similar to recent models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. We define the discriminatively-trained CNN feature extractor as an encoder h(?) in <ref type="figure" target="#fig_0">Figure 2</ref>. The CNN encoder maps image patches x into a feature vectors z that contain relevant semantic information about their content. CNNs accomplish this task efficiently due to their translationequivariant architecture with the shared kernel parameters.</p><p>In our experiments, we use ImageNet-pretrained encoder following Schirrmeister et al. <ref type="bibr" target="#b35">[34]</ref> who show that large natural-image datasets can serve as a representative distribution for pretraining. If a large application-domain unlabeled data is available, the self-supervised pretraining from <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b18">19]</ref> can be a viable option.</p><p>One important aspect of a CNN encoder is its effective receptive field <ref type="bibr" target="#b20">[21]</ref>. Since the effective receptive field is not strictly bounded, the size of encoded patches x cannot be exactly defined. At the same time, anomalies have various sizes and shapes, and, ideally, they have to be processed with the variable receptive fields. To address the ambiguity between CNN receptive fields and anomaly variability, we adopt common multi-scale feature pyramid pooling approach. <ref type="figure" target="#fig_0">Figure 2</ref> shows that the feature vectors</p><formula xml:id="formula_7">z k i ? R D k , i ? {H k ? W k } are extracted by K pooling lay- ers.</formula><p>Pyramid pooling captures both local and global patch information with small and large receptive fields in the first and last CNN layers, respectively. For convenience, we number pooling layers in the last to first layer order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CFLOW decoders for likelihood estimation</head><p>We use the general normalizing flow framework from Section 3.3 to estimate log-likelihoods of feature vectors z. Hence, our generative decoder model g(?) aims to fit true density p Z (z) by an estimated parameterized densit? p Z (z, ?) from (1). However, the feature vectors are as-sumed to be independent of their spatial location in the general framework. To increase efficacy of distribution modeling, we propose to incorporate spatial prior into g(?) model using conditional flow framework. In addition, we model p k Z (z, ?) densities using K independent decoder models g k (? k ) due to multi-scale feature pyramid pooling setup.</p><p>Our conditional normalizing flow (CFLOW) decoder architecture is presented in <ref type="figure" target="#fig_0">Figure 2</ref>. We generate a conditional vector c k i using a 2D form of conventional positional encoding (PE) <ref type="bibr" target="#b40">[39]</ref>. Each c k i ? R C k contains sin and cos harmonics that are unique to its spatial location (h k , w k ) i . We extend unconditional flow framework to CFLOW by concatenating the intermediate vectors inside decoder coupling layers with the conditional vectors c i as in <ref type="bibr" target="#b1">[2]</ref>.</p><p>Then, the kth CFLOW decoder contains a sequence of conventional coupling layers with the additional conditional input. Each coupling layer comprises of fully-connected layer with (D k +C k )?(D k +C k ) kernel, softplus activation and output vector permutations. Usually, the conditional extension does not increase model size since C k D k . For example, we use the fixed C k = 128 in all our experiments. Our CFLOW decoder has translation-equivariant architecture, because it slides along feature vectors extracted from the intermediate feature maps with kernel parameter sharing. As a result, both the encoder h(?) and decoders g k (? k ) have convolutional translation-equivariant architectures.</p><p>We train CFLOW-AD using a maximum likelihood objective, which is equivalent to minimizing loss defined by</p><formula xml:id="formula_8">L(?) = D KL [p Z (z) p Z (z, c, ?)] ? 1 N N i=1 u i 2 2 2 ? log |det J i | + const,<label>(7)</label></formula><p>where the random variable u i = g ?1 (z i , c i , ?), the Jacobian J i = ? z g ?1 (z i , c i , ?) for CFLOW decoder and an expectation operation in D KL is replaced by an empirical train dataset D train of size N . For brevity, we drop the kth scale notation. The derivation is given in Appendix B. <ref type="table">Table 1</ref>. Complexity estimates for SPADE <ref type="bibr" target="#b6">[7]</ref>, PaDiM <ref type="bibr" target="#b7">[8]</ref> and our CFLOW-AD. We compare train and test complexity as well as memory requirements. All models use the same encoder h(?) setup, but diverge in the post-processing. SPADE allocates memory for a train gallery G used in k-nearest-neighbors. PaDiM keeps large matrices (? k i ) ?1 , i ? {H k ?W k } for Mahalanobis distance. Our model employs trained decoders g k (? k ) for post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Train Test Memory</p><formula xml:id="formula_9">SPADE [7] G v?G v ? z i 2 2 ? + G PaDiM [8] ? ?1 i M (z i ) ? + ? ?1 i Ours L(?) logp Z (z i , c i , ?) ? + ?</formula><p>After training the decoders g k (? k ) for all K scales using <ref type="bibr" target="#b6">(7)</ref>, we estimate test dataset D test log-likelihoods as</p><formula xml:id="formula_10">logp Z (z i , c i ,?) = ? u i 2 2 + D log(2?) 2 + log |det J i | . (8) Next, we convert log-likelihoods to probabilities p k i = e logp Z (z k i ,c k i ,? k )</formula><p>for each kth scale using <ref type="bibr" target="#b7">(8)</ref> and normalize them to be in [0 : 1] range. Then, we upsample p k i to the input image resolution (H ?W ) using bilinear interpolation P k = b(p k ) ? R H?W . Finally, we calculate anomaly score maps S H?W by aggregating all upsampled probabilities as <ref type="table">Table 1</ref> analytically compares complexity of CFLOW-AD and recent state-of-the-art models with the same pyramid pooling setup i.e. z k?{1...K} = h(x, ?). SPADE <ref type="bibr" target="#b6">[7]</ref> performs k-nearest-neighbor clustering between each test point z k i and a gallery G of train data. Therefore, the method requires large memory allocation for gallery G and a clustering procedure that is typically slow compared to convolutional methods.</p><formula xml:id="formula_11">S = max K k=1 P k ? K k=1 P k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Complexity analysis</head><p>PaDiM <ref type="bibr" target="#b7">[8]</ref> estimates train-time statistics i.e. inverses of covariance matrices</p><formula xml:id="formula_12">(? k i ) ?1 ? R D k ?D k , i ? {H k ? W k } to calculate M (z k i ) at test-time.</formula><p>Hence, it has low computational complexity, but it stores in memory H k ? W k matrices of D k ? D k size for every kth pooling layer.</p><p>Our method optimizes generative decoders g k (? k ) using <ref type="formula" target="#formula_8">(7)</ref> during the train phase. At the test phase, CFLOW-AD simply infers data log-likelihoods logp Z (z i , c i ,?) using (8) in a fully-convolutional fashion. Decoder parameters ? k?{1...K} are relatively small as reported in <ref type="table" target="#tab_2">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setup</head><p>We conduct unsupervised anomaly detection (imagelevel) and localization (pixel-level segmentation) experiments using the MVTec <ref type="bibr" target="#b3">[4]</ref> dataset with factory defects and the STC <ref type="bibr" target="#b21">[22]</ref> dataset with surveillance camera videos. The code is in PyTorch <ref type="bibr" target="#b27">[28]</ref> with the FrEIA library <ref type="bibr" target="#b0">[1]</ref> used for generative normalizing flow modeling.</p><p>Industrial MVTec dataset comprises 15 classes with total of 3,629 images for training and 1,725 images for testing. The train dataset contains only anomaly-free images without any defects. The test dataset contains both images containing various types of defects and defect-free images. Five classes contain different types of textures (carpet, grid, leather, tile, wood), while the remaining 10 classes represent various types of objects. We resize MVTec images without cropping according to the specified image resolution (e.g. H ? W = 256 ? 256, 512 ? 512 etc.) and apply ?5 ? augmentation rotations during training phase only.</p><p>STC dataset contains 274,515 training and 42,883 testing frames extracted from surveillance camera videos and divided into 13 distinct university campus scenes. Because STC is significantly larger than MVTec, we experiment only with 256 ? 256 resolution and apply the same preprocessing and augmentation pipeline as for MVTec.</p><p>We compare CFLOW-AD with the models reviewed in Section 2 using MVTec and STC datasets. We use widely-used threshold-agnostic evaluation metrics for localization: area under the receiver operating characteristic curve (AUROC) and area under the per-region-overlap curve (AUPRO) <ref type="bibr" target="#b3">[4]</ref>. AUROC is skewed towards large-area anomalies, while AUPRO metric ensures that both large and small anomalies are equally important in localization. Image-level AD detection is reported by the AUROC only.</p><p>We run each CFLOW-AD experiment four times on the MVTec and report mean (?) of the evaluation metric and, if specified, its standard deviation (??). For the larger STC dataset, we conduct only a single experiment. As in other methods, we train a separate CFLOW-AD model for each MVTec class and each STC scene. All our models use the same training hyperparameters: Adam optimizer with 2e-4 learning rate, 100 train epochs, 32 mini-batch size for encoder and cosine learning rate annealing with 2 warm-up epochs. Since our decoders are agnostic to feature map dimensions and have low memory requirements, we train and test CFLOW-AD decoders with 8,192 (32?256) mini-batch size for feature vector processing. During the train phase 8,192 feature vectors are randomly sampled from 32 random feature maps. Similarly, <ref type="bibr" target="#b7">8,</ref><ref type="bibr">192</ref> feature vectors are sequentially sampled during the test phase. The feature pyramid pooling setup for ResNet-18 and WideResnet-50 encoder is identical to PaDiM <ref type="bibr" target="#b7">[8]</ref>. The effects of other architectural hyperparameters are studied in the ablation study. <ref type="table">Table 2</ref> presents a comprehensive study of various design choices for CFLOW-AD on the MVTec dataset using AUROC metric. In particular, we experiment with the input image resolution (H ? W ), encoder architecture (ResNet-  18 <ref type="bibr" target="#b12">[13]</ref>, WideResnet-50 <ref type="bibr" target="#b44">[43]</ref>, MobileNetV3L <ref type="bibr" target="#b13">[14]</ref>), type of normalizing flow (unconditional (UFLOW) or conditional (CFLOW)), number of flow coupling layers (# of CL) and pooling layers (# of PL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study</head><p>Our study shows that the increase in number of decoder's coupling layers from 4 to 8 gives on average 0.15% gain due to a more accurate distribution modeling. Even higher 1.4% AUROC improvement is achieved when processing 3-scale feature maps (layers 1, 2 and 3) compared 2-scale only (layers 2, 3). The additional feature map (layer 1) with larger scale (H 1 ? W 1 = 32 ? 32) provides more precise spatial semantic information. The conditional normalizing flow (CFLOW) is on average 0.5% better than the unconditional (UFLOW) due to effective encoding of spatial prior. Finally, larger WideResnet-50 outperforms smaller ResNet-18 by 0.81%. MobileNetV3L, however, could be a good design choice for both fast inference and high AUROC.</p><p>Importantly, we find that the optimal input resolution is not consistent among MVTec classes. The classes with macro objects e.g. cable or pill tend to benefit from the smaller-scale processing (256?256), which, effectively, translates to larger CNN receptive fields. Majority of classes perform better with 512?512 inputs i.e. smaller receptive fields. Finally, we discover that the transistor class has even higher AUROC with the resized to 128?128 images. Hence, we report results with the highest performing input resolution settings in the Section 5.3 comparisons. <ref type="table" target="#tab_1">Table 4</ref> summarizes average MVTec results for the best published models. CFLOW-AD with WideResNet-50 encoder outperforms state-of-the-art by 0.36% AUROC in detection, by 1.12% AUROC and 2.5% AUPRO in localization, respectively. <ref type="table">Table 3</ref> contains per-class comparison for the subset of models grouped by the task and type of encoder architecture. CFLOW-AD is on par or significantly exceeds the best models in per-class comparison with the same encoder setups. <ref type="table">Table 5</ref> presents high-level comparison of the best recently published models on the STC dataset. CFLOW-AD outperforms state-of-the-art SPADE <ref type="bibr" target="#b6">[7]</ref> by 0.73% AUROC in anomaly detection and PaDiM <ref type="bibr" target="#b7">[8]</ref> by 3.28% AUROC in anomaly localization tasks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Quantitative comparison</head><p>Note that our CFLOW-AD models in <ref type="table" target="#tab_1">Tables 3-4</ref> use variable input resolution as discussed in the ablation study: 512?512, 256?256 or 128?128 depending on the MVTec class. We used fixed 256?256 input resolution in <ref type="table">Table 5</ref> for the large STC dataset to decrease training time. Other reference hyperparameters in Tables 4-5 are set as: WideResnet-50 encoder with 3-scale pooling layers, conditional normalizing flow decoders with 8 coupling layers.  <ref type="figure" target="#fig_0">Figure 2</ref>. Finally, we show the predicted segmentation masks with the threshold selected to maximize F1-score. <ref type="figure" target="#fig_4">Figure 4</ref> presents an additional evidence that our CFLOW-AD model actually addresses the OOD task sketched in <ref type="figure">Figure 1</ref> toy example. We plot distribution of output anomaly scores for anomaly-free (green) and anomalous feature vectors (red). Then, CFLOW-AD is able to distinguish in-distribution and out-of-distribution feature vectors and separate them using a scalar threshold ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Complexity evaluations</head><p>In addition to analytical estimates in <ref type="table">Table 1</ref>, we present the actual complexity evaluations for the trained models using inference speed and model size metrics. Particularly, <ref type="table">Table 1</ref> compares CFLOW-AD with the models from Tables 4-5 that have been studied by Defard et al. <ref type="bibr" target="#b7">[8]</ref>.</p><p>The model size in <ref type="table" target="#tab_2">Table 6</ref> is measured as the size of all floating-point parameters in the corresponding model i.e. its encoder and decoder (post-processing) models. Because the encoder architectures are identical, only the post-processing models are different. Since CFLOW-AD decoders do not explicitly depend on the feature map dimensions (only on feature vector depths), our model is significantly smaller    <ref type="table" target="#tab_1">Table 4</ref>. Green density represent scores for the anomaly-free feature vectors, while region of red-color density shows scores for feature vectors with anomalies. The threshold ? is selected to optimize F1-score.  <ref type="table" target="#tab_2">Table 6</ref> is measured with INTEL I7 CPU for SPADE and PaDiM in Defard et al. <ref type="bibr" target="#b7">[8]</ref> study with 256?256 inputs. We deduce that this suboptimal CPU choice was made due to large memory requirements for these models in <ref type="table" target="#tab_2">Table 6</ref>. Thus, their GPU allocation for fast inference is infeasible. In contrast, our CFLOW-AD can be processed in real-time with 8? to 25? faster inference speed on 1080 8GB GPU with the same input resolution and feature extractor. In addition, MobileNetV3L encoder provides a good trade-off between accuracy, model size and inference speed for practical inspection systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We proposed to use conditional normalizing flow framework to estimate the exact data likelihoods which is infeasible in other generative models. Moreover, we analytically showed the relationship of this framework to previous distance-based models with multivariate Gaussian prior.</p><p>We introduced CFLOW-AD model that addresses the complexity limitations of existing unsupervised AD models by employing fully-convolutional translation-equivariant architecture. As a result, CFLOW-AD is faster and smaller by a factor of 10? than prior models with the same input resolution and feature extractor setup.</p><p>CFLOW-AD achieves new state-of-the-art for popular MVTec with 98.26% AUROC in detection, 98.62% AU-ROC and 94.60% AUPRO in localization. Our new stateof-the-art for STC dataset is 72.63% and 94.48% AUROC in detection and localization, respectively. Our ablation study analyzed design choices for practical real-time processing including feature extractor choice, multi-scale pyramid pooling setup and the flow model hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Relationship with the flow framework</head><p>The loss function for the reverse D KL [p Z (z, ?) p Z (z)] objective <ref type="bibr" target="#b26">[27]</ref>, wherep Z (z, ?) is the model prediction and p Z (z) is a target density, is defined as</p><formula xml:id="formula_13">L(?) = Ep Z (z,?) [logp Z (z, ?) ? log p Z (z)] .<label>(5)</label></formula><p>The first term in (5) can be written using (4) definition for a standard MVG prior (u ? N (0, I)) as logp Z (z, ?) = log(2?) ?D/2 ? E 2 (u)/2 + log |det J | , (5.1) where E 2 (u) = u 2 2 is a squared Euclidean distance of u. Similarly, the second term in (5) can be written for MVG density (2) using a square of Mahalanobis distance as log p Z (z) = log(2?) ?D/2 + log det ? ?1/2 ? M 2 (z)/2.</p><p>(5.2) By substituting (5.1-5.2) into (5), the constants log(2?) ?D/2 are eliminated and the loss is</p><formula xml:id="formula_14">L(?) = Ep Z (z,?) M 2 (z) ? E 2 (u) 2 + log | det J | det ? ?1/2 .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CFLOW decoders for likelihood estimation</head><p>We train CFLOW-AD using a maximum likelihood objective, which is equivalent to minimizing the forward D KL objective <ref type="bibr" target="#b26">[27]</ref> with the loss defined by L(?) = D KL [p Z (z) p Z (z, c, ?)],</p><p>wherep Z (z, c, ?) is a conditional normalizing flow (CFLOW) model with a condition vector c ? R C . The target density p Z (z) is usually replaced by a constant because the parameters ? do not depend on this density during gradient-based optimization. Then by analogy with unconditional flow (4), the loss <ref type="formula" target="#formula_8">(7)</ref>  In practice, the expectation operation in (7.1) is replaced by an empirical train dataset D train of size N . Using the definition of base distribution with p U (u), the final form of (7) can be expressed as</p><formula xml:id="formula_16">L(?) ? 1 N N i=1 u i 2 2 2</formula><p>? log |det J i | + const, <ref type="bibr">(7.2)</ref> where the random variable u i = g ?1 (z i , c i , ?) and the Jacobian J i = ? z g ?1 (z i , c i , ?) depend both on input features z i and conditional vector c i for CFLOW model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our CFLOW-AD with a fully-convolutional translation-equivariant architecture. Encoder h(?) is a CNN feature extractor with multi-scale pyramid pooling. Pyramid pooling captures both global and local semantic information with the growing from top to bottom receptive fields. Pooled feature vectors z k i are processed by a set of decoders g k (? k ) independently for each kth scale. Our decoder is a conditional normalizing flow network with a feature input z k i and a conditional input c k i with spatial information from a positional encoder (PE). The estimated multi-scale likelihoods p k i are upsampled to the input size and added up to produce anomaly map. 4. The proposed CFLOW-AD model 4.1. CFLOW encoder for feature extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>visually shows examples from the MVTec and the corresponding CFLOW-AD predictions. The top row shows ground truth masks from D test including examples with and without anomalies. Then, our model produces anomaly score maps (middle row) using the architecture from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Examples of the input images with ground truth anomaly masks (top row) for various classes of the MVTec. Our CFLOW-AD model fromTable 4estimates anomaly score maps (middle row) and generates segmentation masks (bottom row) for a threshold selected to maximize F1-score. The predicted segmentation mask should match the corresponding ground truth as close as possible.ProbabilityAnomaly score</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Distribution of anomaly scores for the cable class from the MVTec learned by CFLOW-AD model from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>forp Z (z, c, ?) can be written asL(?) = ?E p Z (z) [log p U (u) + log |det J |] + const. (7.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Ablation study of CFLOW-AD using localization AUROC metric on the MVTec<ref type="bibr" target="#b3">[4]</ref> dataset, %. We experiment with input image resolution (H ? W ), encoder architecture (ResNet-18 (R18), WideResnet-50 (WRN50) and MobileNetV3L (MNetV3)), type of normalizing flow (unconditional (UFLOW) and conditional (CFLOW)), number of coupling (# of CL) and pooling layers (# of PL). 28?0.03 97.24?0.03 98.76?0.01 98.98?0.01 98.83?0.01 98.47?0.03 98.64?0.01 98.74 98.92 Cable 95.71?0.01 96.17?0.07 97.64?0.04 97.12?0.06 95.29?0.04 96.75?0.04 96.07?0.06 97.62 97.49 Capsule 98.17?0.02 98.19?0.05 98.98?0.00 98.64?0.02 98.40?0.12 98.62?0.02 98.28?0.05 92?0.02 99.00?0.06 99.61?0.01 99.66?0.00 99.65?0.00 99.45?0.01 99.51?0.02 99.50 99.64 Metal Nut 96.72?0.03 96.72?0.06 98.56?0.03 98.25?0.04 98.16?0.03 97.59?0.05 96.42?0.03 98.36 98.78 Pill 98.46?0.02 98.46?0.01 98.95?0.00 98.52?0.05 98.20?0.08 98.34?0.02 97.80?0.05 98.69 98.44 Screw 94.98?0.06 95.28?0.06 98.10?0.05 98.86?0.02 98.78?0.01 97.38?0.03 98.40?0.03 98.04 99.09 Tile 95.52?0.02 95.66?0.06 97.71?0.02 98.01?0.01 97.98?0.02 95.10?0.02 95.80?0.10 96.07 96.48 Toothbrush 98.02?0.03 97.98?0.00 98.56?0.02 98.93?0.00 98.89?0.00 98.44?0.02 99.00?0.01 98.09 98.80 Transistor 93.09?0.28 94.05?0.11 93.28?0.40 80.52?0.13 76.28?0.14 92.71?0.23 83.34?0.46 97.79 95.22 Wood 90.65?0.10 90.59?0.07 94.49?0.03 96.65?0.01 96.56?0.02 93.51?0.03 95.00?0.04 92.24 94.96 Zipper 96.80?0.02 97.01?0.05 98.41?0.09 99.08?0.02 99.06?0.01 97.71?0.06 98.98?0.01 The detailed comparison of PaDiM<ref type="bibr" target="#b7">[8]</ref>, SPADE<ref type="bibr" target="#b6">[7]</ref>, CutPaste<ref type="bibr" target="#b18">[19]</ref> and our CFLOW-AD on the MVTec<ref type="bibr" target="#b3">[4]</ref> dataset for every class using AUROC or, if available, a tuple (AUROC, AUPRO) metric, %. CFLOW-AD model is with the best hyperparameters from Section 5.2 ablation study. For fair comparison, we group together results with the same encoder architectures such as ResNet-18 and WideResNet-50.</figDesc><table><row><cell>Encoder</cell><cell>WRN50</cell><cell>WRN50</cell><cell>WRN50</cell><cell cols="2">WRN50</cell><cell cols="2">WRN50</cell><cell>R18</cell><cell>R18</cell><cell>MNetV3 MNetV3</cell></row><row><cell># of CL</cell><cell cols="2">4 ?? 8</cell><cell>8</cell><cell>8</cell><cell></cell><cell>8</cell><cell></cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell></row><row><cell># of PL</cell><cell>2</cell><cell cols="2">2 ?? 3</cell><cell>3</cell><cell></cell><cell>3</cell><cell></cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>H?W</cell><cell>256</cell><cell>256</cell><cell cols="3">256 ?? 512</cell><cell>512</cell><cell></cell><cell cols="2">256 ?? 512</cell><cell>256 ?? 512</cell></row><row><cell>Type</cell><cell cols="10">CFLOW CFLOW CFLOW CFLOW ? UFLOW CFLOW CFLOW CFLOW CFLOW</cell></row><row><cell>Bottle</cell><cell cols="10">97.98.89</cell><cell>98.75</cell></row><row><cell>Carpet</cell><cell cols="9">98.50?0.01 98.55?0.01 99.23?0.01 99.25?0.01 99.24?0.00 99.00?0.01 99.29?0.00</cell><cell>98.64</cell><cell>99.00</cell></row><row><cell>Grid</cell><cell cols="9">93.77?0.05 93.88?0.16 96.89?0.02 98.99?0.02 98.74?0.00 93.95?0.04 98.53?0.01</cell><cell>94.75</cell><cell>98.81</cell></row><row><cell>Hazelnut</cell><cell cols="9">98.08?0.01 98.13?0.02 98.82?0.01 98.89?0.01 98.88?0.01 98.81?0.01 98.41?0.01</cell><cell>98.88</cell><cell>99.00</cell></row><row><cell>Leather</cell><cell cols="10">98.97.50</cell><cell>99.07</cell></row><row><cell>Average</cell><cell>96.31</cell><cell>96.46</cell><cell>97.87</cell><cell cols="2">97.36</cell><cell cols="2">96.86</cell><cell>97.06</cell><cell>96.90</cell><cell>97.59</cell><cell>98.16</cell></row><row><cell>Task</cell><cell cols="2">Localization</cell><cell></cell><cell cols="2">Detection</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Localization</cell></row><row><cell>Encoder</cell><cell></cell><cell cols="2">ResNet-18</cell><cell></cell><cell cols="2">EffNetB4</cell><cell></cell><cell cols="3">WideResNet-50</cell></row><row><cell cols="4">Class/Model CutPaste Ours CutPaste</cell><cell>Ours</cell><cell cols="2">CutPaste</cell><cell>Ours</cell><cell>SPADE</cell><cell cols="2">PaDiM</cell><cell>Ours</cell></row><row><cell>Bottle</cell><cell>97.6</cell><cell>98.64</cell><cell>98.3</cell><cell>100.00</cell><cell cols="2">100.0</cell><cell cols="4">100.0 (98.4, 95.5) (98.3, 94.8) (98.98, 96.80)</cell></row><row><cell>Cable</cell><cell>90.0</cell><cell>96.75</cell><cell>80.6</cell><cell>97.62</cell><cell cols="2">96.2</cell><cell cols="4">97.59 (97.2, 90.9) (96.7, 88.8) (97.64, 93.53)</cell></row><row><cell>Capsule</cell><cell>97.4</cell><cell>98.62</cell><cell>96.2</cell><cell>93.15</cell><cell cols="2">95.4</cell><cell cols="4">97.68 (99.0, 93.7) (98.5, 93.5) (98.98, 93.40)</cell></row><row><cell>Carpet</cell><cell>98.3</cell><cell>99.29</cell><cell>93.1</cell><cell>98.20</cell><cell cols="2">100.0</cell><cell cols="4">98.73 (97.5, 94.7) (99.1, 96.2) (99.25, 97.70)</cell></row><row><cell>Grid</cell><cell>97.5</cell><cell>98.53</cell><cell>99.9</cell><cell>98.97</cell><cell cols="2">99.1</cell><cell cols="4">99.60 (93.7, 86.7) (97.3, 94.6) (98.99, 96.08)</cell></row><row><cell>Hazelnut</cell><cell>97.3</cell><cell>98.81</cell><cell>97.3</cell><cell>99.91</cell><cell cols="2">99.9</cell><cell cols="4">99.98 (99.1, 95.4) (98.2, 92.6) (98.89, 96.68)</cell></row><row><cell>Leather</cell><cell>99.5</cell><cell>99.51</cell><cell>100.0</cell><cell>100.00</cell><cell cols="2">100.0</cell><cell cols="4">100.0 (97.6, 97.2) (98.9, 88.8) (99.66, 99.35)</cell></row><row><cell>Metal Nut</cell><cell>93.1</cell><cell>97.59</cell><cell>99.3</cell><cell>98.45</cell><cell cols="2">98.6</cell><cell cols="4">99.26 (98.1, 94.4) (97.2, 85.6) (98.56, 91.65)</cell></row><row><cell>Pill</cell><cell>95.7</cell><cell>98.34</cell><cell>92.4</cell><cell>93.02</cell><cell cols="2">93.3</cell><cell cols="4">96.82 (96.5, 94.6) (95.7, 92.7) (98.95, 95.39)</cell></row><row><cell>Screw</cell><cell>96.7</cell><cell>98.40</cell><cell>86.3</cell><cell>85.94</cell><cell cols="2">86.6</cell><cell cols="4">91.89 (98.9, 96.0) (98.5, 94.4) (98.86, 95.30)</cell></row><row><cell>Tile</cell><cell>90.5</cell><cell>95.80</cell><cell>93.4</cell><cell>98.40</cell><cell cols="2">99.8</cell><cell cols="4">99.88 (87.4, 75.9) (94.1, 86.0) (98.01, 94.34)</cell></row><row><cell>Toothbrush</cell><cell>98.1</cell><cell>99.00</cell><cell>98.3</cell><cell>99.86</cell><cell cols="2">90.7</cell><cell cols="4">99.65 (97.9, 93.5) (98.8, 93.1) (98.93, 95.06)</cell></row><row><cell>Transistor</cell><cell>93.0</cell><cell>97.69</cell><cell>95.5</cell><cell>93.04</cell><cell cols="2">97.5</cell><cell cols="4">95.21 (94.1, 87.4) (97.5, 84.5) (97.99, 81.40)</cell></row><row><cell>Wood</cell><cell>95.5</cell><cell>95.00</cell><cell>98.6</cell><cell>98.59</cell><cell cols="2">99.8</cell><cell cols="4">99.12 (88.5, 97.4) (94.9, 91.1) (96.65, 95.79)</cell></row><row><cell>Zipper</cell><cell>99.3</cell><cell>98.98</cell><cell>99.4</cell><cell>96.15</cell><cell cols="2">99.9</cell><cell cols="4">98.48 (96.5, 92.6) (98.5, 95.9) (99.08, 96.60)</cell></row><row><cell>Average</cell><cell>96.0</cell><cell>98.06</cell><cell>95.2</cell><cell>96.75</cell><cell cols="2">97.1</cell><cell cols="4">98.26 (96.0, 91.7) (97.5, 92.1) (98.62, 94.60)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Average AUROC and AUPRO on the MVTec<ref type="bibr" target="#b3">[4]</ref> dataset, %. Both the best detection and localization metrics are presented, if available. CFLOW-AD is with WideResNet-50 encoder.</figDesc><table><row><cell>Metric</cell><cell cols="2">AUROC</cell><cell>AUPRO</cell></row><row><cell>Model</cell><cell>Detection</cell><cell cols="2">Localization</cell></row><row><cell>DifferNet [30]</cell><cell>94.9</cell><cell>-</cell><cell>-</cell></row><row><cell>DFR [37]</cell><cell>-</cell><cell>95.0</cell><cell>91.0</cell></row><row><cell>SVDD [42]</cell><cell>92.1</cell><cell>95.7</cell><cell>-</cell></row><row><cell>SPADE [7]</cell><cell>85.5</cell><cell>96.0</cell><cell>91.7</cell></row><row><cell>CutPaste [19]</cell><cell>97.1</cell><cell>96.0</cell><cell>-</cell></row><row><cell>PaDiM [8]</cell><cell>97.9</cell><cell>97.5</cell><cell>92.1</cell></row><row><cell>CFLOW-AD (ours)</cell><cell>98.26</cell><cell>98.62</cell><cell>94.60</cell></row><row><cell cols="4">Table 5. Average AUROC on the STC [22] dataset, %. Both</cell></row><row><cell cols="4">the best available detection and localization metrics are showed.</cell></row><row><cell cols="3">CFLOW-AD is with WideResNet-50 encoder.</cell><cell></cell></row><row><cell>Metric</cell><cell></cell><cell>AUROC</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Detection Localization</cell></row><row><cell>CAVGA [40]</cell><cell>-</cell><cell cols="2">85.0</cell></row><row><cell>SPADE [7]</cell><cell>71.9</cell><cell cols="2">89.9</cell></row><row><cell>PaDiM [8]</cell><cell>-</cell><cell cols="2">91.2</cell></row><row><cell>CFLOW-AD (ours)</cell><cell>72.63</cell><cell cols="2">94.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6 .</head><label>6</label><figDesc>Complexity comparison in terms of inference speed (fps) and model size (MB). Inference speed for CFLOW-AD models fromTable 3is measured for (256?256) / (512?512) inputs.</figDesc><table><row><cell>Complexity metric</cell><cell>Inference</cell><cell cols="2">Model size, MB</cell></row><row><cell>and Model</cell><cell>speed, fps</cell><cell>STC</cell><cell>MVTec</cell></row><row><cell>R18 encoder only</cell><cell>80 / 62</cell><cell></cell><cell>45</cell></row><row><cell>PaDiM-R18 [8]</cell><cell>4.4</cell><cell>210</cell><cell>170</cell></row><row><cell>CFLOW-AD-R18</cell><cell>34 / 12</cell><cell></cell><cell>96</cell></row><row><cell>WRN50 encoder only</cell><cell>62 / 30</cell><cell cols="2">268</cell></row><row><cell>SPADE-WRN50 [7]</cell><cell>0.1</cell><cell>37,000</cell><cell>1,400</cell></row><row><cell>PaDiM-WRN50 [8]</cell><cell>1.1</cell><cell>5,200</cell><cell>3,800</cell></row><row><cell>CFLOW-AD-WRN50</cell><cell>27 / 9</cell><cell cols="2">947</cell></row><row><cell>MNetV3 encoder only</cell><cell>82 / 61</cell><cell></cell><cell>12</cell></row><row><cell>CFLOW-AD-MNetV3</cell><cell>35 / 12</cell><cell></cell><cell>25</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For comprehensive review of the existing AD methods we refer readers to Ruff et al.<ref type="bibr" target="#b31">[31]</ref> and Pang et al.<ref type="bibr" target="#b25">[26]</ref> surveys.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing inverse problems with invertible neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Lynton Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K?the</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynton</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>L?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>K?the</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02392</idno>
		<title level="m">Guided image generation with conditional invertible neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain MR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04488</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MVTec AD -a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<meeting>the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357v3</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PaDiM: a patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelique</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition (ICPR) Workshops</title>
		<meeting>the International Conference on Pattern Recognition (ICPR) Workshops</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching for MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Why normalizing flows fail to detect out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CutPaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-granularity tracking with modularlized components for unsupervised vehicles anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked RNN framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the generalized distance in statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanta</forename><surname>Chandra Mahalanobis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Institute of Sciences (Calcutta)</title>
		<meeting>the National Institute of Sciences (Calcutta)</meeting>
		<imprint>
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilan</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by CNN-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Normalizing flows for probabilistic modeling and inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<pubPlace>Alban</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autodiff workshop at Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Modeling the distribution of normal data in pre-trained deep features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><surname>Merhof</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14140</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Same same but DifferNet: Semi-supervised defect detection with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A unifying review of deep and shallow anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><forename type="middle">Reinhard</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc of the IEEE</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Objectcentric anomaly detection by attribute-based reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niousha</forename><surname>Sadjadi</surname></persName>
		</author>
		<editor>Soroosh Baselizadeh, Mohammad Hossein Rohban, and Hamid R</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiresolution knowledge distillation for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding anomaly detection with deep invertible networks through hierarchies of distributions and features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">Tibor</forename><surname>Schirrmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonio</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Erfurth. F-Anogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical image analysis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unsupervised anomaly segmentation via deep feature reconstruction. Neurocomputing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiquan</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vasilev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meissner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sgarlata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tomassini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<title level="m">q-space novelty detection with variational autoencoders. MICCAI 2019 International Workshop on Computational Diffusion MRI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention guided anomaly localization in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Rajat Vikram Singh, and Abhijit Mahalanobis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Student-teacher feature pyramid matching for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04257</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Patch SVDD: Patch-level SVDD for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Encoding structure-texture relation with P-Net for anomaly detection in retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
