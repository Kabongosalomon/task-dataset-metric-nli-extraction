<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-label Classification with Partial Annotations using Class-aware Selective Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-21">21 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
							<email>emanuel.benbaruch@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
							<email>tal.ridnik@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
							<email>itamar.friedman@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Ben-Cohen</surname></persName>
							<email>avi.bencohen@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
							<email>nadav.zamir@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
							<email>asaf.noy@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-label Classification with Partial Annotations using Class-aware Selective Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-21">21 Oct 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale multi-label classification datasets are commonly, and perhaps inevitably, partially annotated. That is, only a small subset of labels are annotated per sample. Different methods for handling the missing labels induce different properties on the model and impact its accuracy. In this work, we analyze the partial labeling problem, then propose a solution based on two key ideas. First, un-annotated labels should be treated selectively according to two probability quantities: the class distribution in the overall dataset and the specific label likelihood for a given data sample. We propose to estimate the class distribution using a dedicated temporary model, and we show its improved efficiency over a na?ve estimation computed using the dataset's partial annotations. Second, during the training of the target model, we emphasize the contribution of annotated labels over originally un-annotated labels by using a dedicated asymmetric loss. With our novel approach, we achieve state-of-the-art results on OpenImages dataset (e.g. reaching 87.3 mAP on V6). In addition, experiments conducted on LVIS and simulated-COCO demonstrate the effectiveness of our approach. Code is available at https://github.com/Alibaba-MIIL/PartialLabelingCSL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, a remarkable progress has been made in multilabel classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref>. Dedicated loss functions were proposed in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref>, as well as transformers based approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. In many common cases, such as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, as the amounts of samples and labels in the data increase, it becomes impractical to fully annotate each image. For example, OpenImages dataset <ref type="bibr" target="#b14">[15]</ref> consists of 9 million training images and having 9,600 classes. An exhaustive annotation process would require annotating more than 86 billion labels. As a result, partially labeled data is inevitable in realistic large-scale multi-label classification tasks. A partially labeled image is annotated with a subset <ref type="figure">Figure 1</ref>. Challenges in partial annotation. (1) "Lip" and "Yellow" are clearly present in the left image but were not annotated as positive labels. The middle and right images are annotated with "Yellow" and "Lip" respectively, while not being dominant labels in those images. <ref type="bibr" target="#b1">(2)</ref> The deficiency of positive annotations is a key challenge: classes that frequently appear in images (e.g. "Black", "Lip") may be annotated much less comparing with infrequent classes ("Flower", "Guitar") (3) Most labels are unannotated. How to exploit a temporary model's predictions for the un-annotated labels when training a target model? of positive labels and a subset of negative labels, where the rest un-annotated labels are considered as unknown ( <ref type="figure">Figure  1</ref>). Typically, the majority of the labels are un-annotated. For example, on average, a picture in OpenImages is annotated with only 7 labels. Thus, the question of how to treat the numerous un-annotated labels may have a considerable impact on the learning process.</p><p>The basic training mode for handling the un-annotated labels is simply to ignore their contribution in the loss function, as proposed in <ref type="bibr" target="#b5">[6]</ref>. We denote this mode as Ignore. While ignoring the un-annotated labels is a reasonable choice, it may lead to a poor decision boundary as it exploits only a fraction of the data, see <ref type="figure" target="#fig_0">Figure 2</ref>(b). Moreover, in a typical multi-label dataset, the probability of a label being negative is very high. Consequently, treating the un-annotated labels as negative may improve the discriminative power as it enables the exploitation of the entire data <ref type="bibr" target="#b13">[14]</ref>. However, this training mode, denoted as Neg- Negative mode treats all un-annotated labels as negatives. It may produce suboptimal decision boundary as it adds noise of un-annotated positive labels. Also, annotated and un-annotated negative samples contribute similarly to the optimization process. (d) Our approach aims at mitigating these drawbacks by predicting the probability of a label being present in the image. ative, has two main drawbacks: adding label noise to the training, and inducing a high imbalance between negative and positive samples <ref type="bibr" target="#b1">[2]</ref>. This mode is illustrated in <ref type="figure" target="#fig_0">Figure  2</ref>(c).</p><p>While treating the un-annotated labels as negative can be useful for many classes, it may significantly harm the learning of labels that tend to appear frequently in the images although not being sufficiently annotated. For example, color classes are labeled in only a small number of samples in OpenImages <ref type="bibr" target="#b14">[15]</ref>, e.g. class "Black" is annotated in 1,688 samples, which is only 0.02% of the samples, while they are probably present in most of the images (see an example in <ref type="figure">Figure 1</ref>). Consequently, such classes are trained with many wrong negative samples. Thus, it would be worthwhile to first identify the frequent classes in the data and treat them accordingly. While in fully annotated multi-label datasets (e.g. MS-COCO <ref type="bibr" target="#b17">[18]</ref>) the class frequencies can be directly inferred by counting the number of their annotations, in partially annotated datasets it is not straightforward. Counting the number of positive annotations per class is misleading as the numbers are usually not proportional to the true class frequencies. In OpenImages, assumably infrequent classes like "Boat" and "Snow" are annotated in more than 100,000 samples, while frequent classes as colors are annotated in only ?1,500 images. Therefore, the class distribution is required to be estimated from the data.</p><p>In this paper, we propose a Selective approach that aims at mitigating the weaknesses raised by the primary training modes ( <ref type="figure" target="#fig_0">Figure 2</ref>). In particular, we will select one of the primary mode (Ignore or Negative) for each label individually by utilizing two probabilistic conditions, termed as label likelihood and label prior. The label likelihood quantifies the probability of a label being present in a specific image. The label prior represents the probability of a label being present in the data. To acquire a reliable label prior, we propose a method for estimating the class distribution. To that end, we train a classification model using Ignore mode and evaluate it on a representative dataset. Then, when training the final model, to handle the high negative-positive imbalance, we adopt the asymmetric loss <ref type="bibr" target="#b1">[2]</ref>, which enables focusing on the hard samples, while at the same time controlling the impact from the positive and negative samples. We further suggest decoupling the focusing levels of the annotated and un-annotated terms in the loss to emphasize the contributions from the annotated negative samples.</p><p>Extensive experiments were conducted on three datasets: OpenImages <ref type="bibr" target="#b14">[15]</ref> (V3 and V6) and LVIS <ref type="bibr" target="#b7">[8]</ref> which are partially annotated datasets with 9,600 and 1,203 classes, respectively. In addition, we simulated partially annotated versions of the MS-COCO <ref type="bibr" target="#b17">[18]</ref> for exploring and verifying our approach. Results and comparisons demonstrate the effectiveness of our proposed scheme. Specifically, on Open-Images (V6) we achieve a state-of-the-art result of 87.34% mAP score. The contributions of the paper can be summarized as follows:</p><p>? Introducing a novel selective scheme for handling partially labeled data, that treat each un-annotated label separately based on two probabilistic quantities: label likelihood and label prior. Our approach outperforms previous methods in several partially labeled benchmarks. ? We identify a key challenge in partially labeled data, regarding the inaccuracy of calculating the class distribution using the annotations, and offer an effective approach for estimating the class distribution from the data. ? A partial asymmetric loss is proposed to dynamically control the impact of the annotated and un-annotated negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Several methods had been proposed to tackle the partial labeling challenge. <ref type="bibr" target="#b5">[6]</ref> offered a partial binary cross-entropy (CE) loss to weigh each sample according to the proportion of known labels, where the un-annotated labels are simply ignored in the loss computation. In <ref type="bibr" target="#b13">[14]</ref> they proposed to involve also the un-annotated labels in the loss, treating them as negative while smoothing their contribution by incorporating a temperature parameter in their sigmoid function. An interactive approach was presented in <ref type="bibr" target="#b10">[11]</ref> whose loss is composed of cross-entropy for the annotated labels and a smoothness term as a regularization. A curriculum learning strategy was also used in <ref type="bibr" target="#b5">[6]</ref> to complete the missing labels. Instead of using the same training mode for all classes, in this paper we propose adjusting the training mode, either as Ignore or Negative for each class individually, relying on probabilistic based conditions. Also, we introduce a key challenge in partial labeling, concerning the inability to infer the class distribution directly from the number of annotations, and suggest an estimation procedure to handle this.</p><p>Other methods were proposed in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref> to cope with partial labels, for example by a low-rank empirical risk minimization <ref type="bibr" target="#b29">[30]</ref> or by learning structured semantic correlations <ref type="bibr" target="#b27">[28]</ref>. However, they are not scalable to large datasets and their optimization procedures are not well adapted to deep neural networks.</p><p>Positive Unlabeled (PU) is also related to partial labeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>. The difference is that PU learning approaches use only positive and un-annotated labels without any negative annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning from Partial Annotations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Given a partially annotated multi-label dataset with C classes, each sample x ? X , corresponding to a specific image, and is annotated by a label vector y = {y c } C c=1 , where y c ? {?1, 0, 1} denotes whether the class c is present in the image ('1'), absent ('?1') or unknown ('0'). For a given image, we denote the sets of positive and negative labels as P x = {c|y c = 1}, and N x = {c|y c = ?1}, respectively. The set of un-annotated labels is denoted by</p><formula xml:id="formula_0">U x = {c|y c = 0}. Note that typically, |P x ? N x | ? |U x |.</formula><p>A general form of the partially annotated multi-label classification loss can be defined as follows,</p><formula xml:id="formula_1">L(x) = c?Px L + c (x) + c?Nx L ? c (x) + c?Ux L u c (x) (1) where L + c (x), L ? c (x) and L u c (x)</formula><p>are the loss terms of the positive, negative and un-annotated labels for sample x, respectively. Given a set of N labeled samples</p><formula xml:id="formula_2">{x i , y i } N i=1</formula><p>, our goal is to train a neural-network model f (x; ?), parametrized by ?, to predict the presence or absence of each class given an input image. We denote by p = {p c } C c=1 the class prediction vector, computed by the model: p c = ?(z c ) where ?(?) is the sigmoid function, and z c is the output logit corresponding to class c.</p><p>For example, applying the binary CE loss while considering only the annotated labels is defined by setting the loss terms as L + c (x) = log(p c ), L ? c (x) = log(1 ? p c ) and L u c (x) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">How to Treat the Un-annotated Labels?</head><p>Typically, the number of un-annotated labels is much higher than the annotated ones. Therefore, the question of how to treat the un-annotated labels may have a considerable impact on the learning process. Herein, we will first define the two primary training modes and detail their strengths and limitations. Then, in light of these insights, we will propose a class aware mechanism which may better handle the un-annotated labels.</p><p>Mode Ignore. The basic scheme for handling the unannotated labels is simply to ignore them, as suggested in <ref type="bibr" target="#b5">[6]</ref>. In this mode we set L u c (x) = 0. This way the training data is not contaminated with wrong annotations. However, its drawback is that it enables to use only a subset of the data. For example, in OpenImages dataset, the number of samples with either positive or negative annotations for the class "Cat" is only ? 0.9% of the training data. This may lead to a sub-optimal classification boundary when the annotated negative labels do not sufficiently cover the space of the negative class. See illustration in <ref type="figure" target="#fig_0">Figure 2</ref> Mode Negative. In typical multi-label datasets, the chance of a specific label to appear in an image is very low. For example, in the fully-annotated MS-COCO dataset <ref type="bibr" target="#b17">[18]</ref>, a label is annotated as negative with a probability of ? 0.96. Based on this prior assumption, a reasonable choice would be to treat the un-annotated labels as negative, i.e. setting L u c (x) = L ? c (x). This working mode was also suggested in <ref type="bibr" target="#b13">[14]</ref>. While this mode enables the utilization of the entire dataset, it suffers from two main limitations. First, it may wrongly annotate positive labels as negative annotations, adding label noise to the training. Secondly, this mode inherently triggers a high imbalance between negative and positive samples. Balancing them, for example by down-weighting the contribution of the negative samples, may diminish the impact of the valuable annotated negative samples. These weaknesses are illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> The question of which mode to choose has no unequivocal answer. It depends on various conditions and may have its origin in the annotation scheme used. In section 5.1, we will show that different partial annotation procedures can lead to favor different loss modes (See <ref type="figure">Figure 6</ref>). Moreover, as discussed in the next section, the used mode can influence each class differently, depends upon the class presence frequency in the data and the number of available annotations. <ref type="figure">Figure 3</ref>. Proposed approach. First, a class distribution estimation phase is performed to obtain a reliable label prior using a temporary network trained with the Ignore mode. Then, the target model is trained using a Selective approach which assigns a Negative or Ignore mode for each label based on its estimated prior and likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Class Distribution in Partial Annotation</head><p>As aforementioned, in multi-label datasets the majority of labels are present in only a small fraction of the data. For example, in MS-COCO, 89% of the classes appear in less than 5% of the data. Thus, treating all un-annotated labels as negative may improve the discriminative power for many classes, as more real negative samples are involved in the training, while the added label noise is negligible. However, this may significantly harm the learning of classes whose number of positive annotations in the dataset is much lower than the actual number of samples they appear in. Consider the case of the class "Person" in MS-COCO. It is present in 55% of the data (45,200 samples). Now, suppose that only a subset of 1,000 positive annotations are available, and the rest are switched to negative. It means that during the training, most of the prediction errors are due to wrong annotations. In this case, the optimization will be degraded and the network confidence will be decayed considerably. Hence, it will be beneficial to first identify the frequent labels and handle them differently in the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Positive Annotations Deficiency</head><p>To identify the frequent labels, we need to reliably acquire their distribution in the data. While in fully annotated datasets it can be easily obtained by counting the number of annotations per class and normalizing by the total number of samples, in partially annotated datasets it is not straightforward. While one may suggest counting the number of positive annotations for each class, the resulted numbers are misleading and are usually not proportional to the true class frequencies. For example, in OpenImages (V6), we found that many common and general classes which are frequently present in images are labeled with very few positive annotations. For example, general classes such as "Daytime", "Event" or "Design" are labeled in only 1,709, 1,517 and 1,394 images (out of 9M), respectively. Color classes which massively appear in images are also rarely annotated. "Black" and "White" classes are labeled in only 1,688 and 1,497 images, respectively. We may assume that classes such as "Daytime" or "White" are present in much more than 0.02% of the samples. Similarly, in LVIS dataset, the classes "Person" and "Shirt" are annotated in only 1,928 and 1,942 samples, respectively, while they practically appear in much more images (note that in MS-COCO, which shares the same images with LVIS, the class "Person" appears in 55% of the samples).</p><p>Note that the labels are not necessarily annotated according to their dominance in the image. In <ref type="figure">Figure 1</ref>, we show examples of three images and corresponding annotations of the classes "Lip" and "Yellow". As can be seen, the left image was not annotated with neither "Lip" nor "Yellow" although these labels are present and dominant in it. Also, "Lip" is annotated in only 1,121 images which is highly deficient in view of the fact that the class "Human face" is annotated in 327,899 images.</p><p>According to the above-mentioned observations, the number of positive annotations cannot be used to measure the class frequencies in partially labeled datasets. In section 4.2, we will propose a simple yet effective approach for estimating the class distribution from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Approach</head><p>In this section we will present our method which aims at mitigating the issues raised in training partially annotated data. An overview of the proposed approach is summarized in <ref type="figure">Figure 3</ref>.</p><p>To mitigate the high negative-positive imbalance problem, we adopt the asymmetric loss (ASL) proposed in <ref type="bibr" target="#b1">[2]</ref> as the base loss for the multi-label classification task. It enables to dynamically focus on the hard samples while at the same time controlling the contribution propagated from the positive and negative samples. First, let us denote the basic term of the focal loss <ref type="bibr" target="#b16">[17]</ref> for a given class c, by:</p><formula xml:id="formula_3">L F (p c , ?) = (1 ? p c ) ? log p c (2)</formula><p>where ? is the focusing parameter, which adjusts the decay rate of the easy samples. Then, we define the partially annotated loss as follows,</p><formula xml:id="formula_4">L(x) = c?Px L F (p c , ? + ) + c?Nx L F (1 ? p c , ? ? ) + c?Ux ? c L F (1 ? p c , ? u )<label>(3)</label></formula><p>where ? + , ? ? and ? u are the focusing parameters for the positive, negative and un-annotated labels, respectively. ? c is the selectivity parameter and it is introduced in section 4.1. We usually set ? + &lt; ? ? to decay the positive term with a lower rate than the negative one because the positive samples are infrequent compared to the negative samples. In addition, as for a given class, the negative annotated samples are verified ground-truth we are interested in preserving their contribution to the loss. Therefore, we suggest decoupling of the focusing parameter of the annotated negative labels from the un-annotated one, allowing us to set a lower decay rate for the annotated negative labels: ? ? &lt; ? u . This way, the impact of the annotated negative samples on establishing the classification boundary for each class is higher (see <ref type="figure" target="#fig_0">Figure 2(d)</ref>). We term this form of asymmetric loss as Partial-ASL (P-ASL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Class-aware Selective Loss</head><p>As described in section 3.1, both Ignore and Negative modes are supported by inadequate assumptions for the partial annotation problem. In this section, we propose a selective approach for adjusting the mode per individual class. The core idea is to examine the probability of each unannotated label being present in a given sample x. Unannotated labels that are suspected as positive will be ignored. The others will be treated as negative.</p><p>For that purpose, we define two probabilistic values: label likelihood and label prior, and detail their usage in the following section. These two quantities are complementary to each other. The label likelihood enables to dynamically ignore the loss contribution of a label in a given image by inspecting its visual content. The label prior extracts useful information of the estimated class frequencies in the data and uses it regardless of the specific image content. </p><p>It can be simply estimated by the network prediction {p c } c?Ux throughout the training. A high p c may imply that the un-annotated label c appears in the image, and treating it as negative may lead to an error. Accordingly, the label c should be ignored. In practice, we allow for K un-annotated labels with top prediction values to be ignored. i.e.</p><formula xml:id="formula_6">? L = c ? U x | c ? TopK({p c })<label>(5)</label></formula><p>where the TopK(?) operator returns the indices of the top K elements of the input vector. The algorithm scheme is illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>. Note that this implementation enables us to "walk" on a continuous scale between the Negative and Ignore modes. Setting K = 0 corresponds to Negative mode, as no un-annotated label is ignored. K = C equivalents to the Ignore mode, as all un-annotated labels are ignored. Label prior. Defined by the probability of a label c being present in an image. It can also be viewed as the actual label presence frequency in the data. We are interested in the label prior for the un-annotated labels, P (y c = 1); ?c ? U x .</p><p>According to section 3.3, the label prior should be estimated from the data, as the class distribution is hidden in partially annotated datasets. In the next section (4.2), we will introduce the scheme for estimating the label prior. Meanwhile, let us denote byP r (c) the label prior estimator for class c.</p><p>We are interested in disabling the loss contribution of labels with high prior values. These labels are formally defined by the following set,</p><formula xml:id="formula_8">? P = c ? U x |P r (c) &gt; ?<label>(7)</label></formula><p>where ? ? [0, 1] represents the minimum fraction of the data determining a label to be ignored. Finally, we denote the set of labels whose loss contribution are ignored, as the union of the two previously computed sets, ? Ignore = ? L ? ? P .</p><p>Accordingly, we set the parameter ? c in equation <ref type="formula" target="#formula_4">(3)</ref> as follows,</p><formula xml:id="formula_10">? c = 0 c ? ? Ignore 1 c / ? ? Ignore<label>(9)</label></formula><p>Note that we have explored other alternatives for implementing the label prior in the loss function. In particular, in appendix B we compare a soft method that integrates the label prior by setting ? c = exp(??P r (c)); ?c / ? ? L , and show that using a hard decision mechanism, as proposed in equation <ref type="formula" target="#formula_10">(9)</ref>, produces better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Estimating the Class Distribution</head><p>We aim at estimating the class distribution in a representative dataset X . For that, we first need to assess the presence of each class in every image in the data, i.e. we would like to first approximate the probability of a class c being present in an image x ? X : P (y c = 1|x). To that end, we propose training a model parametrized by ?, for predicting each class in a given image, i.e. P (y c = 1|x; ?). Afterwards, the model is applied on the sample set X (e.g. the training data). The label prior can then be estimated by calculating the expectation,</p><formula xml:id="formula_11">P (y c = 1; ?) = 1 |X | x?X P (y c = 1|x; ?).<label>(10)</label></formula><p>For estimating the label priors, we train the model in Ignore mode. While the discriminative power of the Negative mode may be stronger for majority of the labels, it may fail to provide a reliable prediction values for frequent classes with small number of positive annotations. Propagating abundance of gradient errors from wrong negative annotations will decay the expected returned prediction for those classes and will fail to approximate P (y c = 1|x). Consequently, our suggested estimation for the class distribution is given by, P r (c) = P (y c = 1; ? Ignore ),</p><p>where ? Ignore denotes the model parameters trained in Ignore mode. In section 5.2, we will empirically show the effectiveness of the Ignore mode in ranking the class frequencies and the inapplicability of the Negative mode to do that. To qualitatively show the estimation effectiveness, we present in <ref type="figure" target="#fig_5">Figure 5</ref> the top 20 frequent classes in OpenImages (V6) as estimated by our proposed procedure. Note that all the top classes are commonly present in images such as colors ("White", "Black", "Blue" etc.) or general classes such as "Photograph", "Light", "Daytime" or "Line". In appendix D, we show the next top 60 estimated classes. Also, in appendix E, we provides the top 20 estimated frequent classes for LVIS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Study</head><p>In this section, we will experimentally demonstrate the insights discussed in the previous sections. We will mainly utilize the fully annotated MS-COCO dataset <ref type="bibr" target="#b17">[18]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Impact of Annotation Schemes</head><p>As aforementioned in section 3.2, the scheme used for annotating the dataset can substantially induce the learning process. Specifically, the choice of how to treat the un-annotated labels is highly influenced by the annotation scheme. To demonstrate that, we simulate two partial annotation schemes on the original fully annotated MS-COCO dataset <ref type="bibr" target="#b17">[18]</ref>. MS-COCO includes 80 classes, 82,081 training samples, and 40,137 validation samples, following the 2014 split. The two simulated annotation schemes are detailed as follows: Fixed per class (FPC). For each class, we randomly sample a fixed number of positive annotations, denoted by N s , and the same number of negative annotations. The rest of the annotations are dropped. Random per annotation (RPA). We omit each annotation with probability p. Note that this simulation preserves the true class distribution of the data.</p><p>In <ref type="figure">Figure 6</ref>, we show results obtained using each one of the simulation schemes for each primary mode (Ignore and Negative) while varying N s and p values. As can be seen, while in RPA <ref type="figure">(Figure 6(a)</ref>), the Ignore mode consistently shows better results, in FPC ( <ref type="figure">Figure 6(b)</ref>), the Negative mode is superior. Note that as we keep more of the annotated labels (by either increasing N s or decreasing p), the gap between the two training modes is reduced, catching the maximal result. The phenomenons observed in the two case studies we simulated are also related in real practical procedures for partially annotating multi-label datasets. While in the FPC simulation, the class distribution is completely vanished and cannot be inferred by the number of positive annotations (N s for c = 1, ..., C), the RPA scheme preserves the class distribution.  <ref type="figure">Figure 6</ref>. Impact of annotation schemes. mAP results obtained using the RPA (a) and the FPC (b) simulation schemes for each primary mode. While in RPA, Ignore mode consistently shows better results, in FPC, the Negative mode is superior.  <ref type="figure">Figure 7</ref>. Spearman correlation between the true class distribution and the estimated distribution. Unlike the Negative mode, training a model using Ignore mode is well suited for estimating the class distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Estimating the Label Prior</head><p>To demonstrate the estimation quality of the class distribution obtained by the approach proposed in section 4.2, we follow the FPC simulation scheme applied on the MS-COCO dataset (as described in section 5.1), where a constant number of 1,000 annotations remained for each class. Because MS-COCO is a fully annotated dataset, we can compare the estimated class distribution (i.e. the label prior) to the true class distribution inferred by the original number of annotations. In particular, we measure the similarity between the original class frequencies and the estimated ones using the Spearman correlation test. In <ref type="figure">figure 7</ref>, we show the Spearman correlation scores while varying the number of top-ranked classes. We also show the results obtained with Negative mode as a reference. Specifically, the Spearman correlation computed over all the 80 classes, with the estimator obtained using the Ignore mode is 0.81, demonstrating the estimator's effectiveness. In the next section, we will show how it benefits the overall classification results. Also, in appendix C we present the top frequent classes measured by our estimator and compare them to those obtained by the original class frequencies in MS-COCO. Backbone mAP(C) mAP(O) OFA-595 <ref type="bibr" target="#b2">[3]</ref> 85.40 92.87 ResNet-50 <ref type="bibr" target="#b9">[10]</ref> 86.15 93.16 TResNet-M <ref type="bibr" target="#b22">[23]</ref> 86.72 93.57 TResNet-L <ref type="bibr" target="#b22">[23]</ref> 87.34 93.77 <ref type="table">Table 2</ref>. OpenImages (V6) results for different backbones. Using TResNet-L model we achieve top results on OpenImages V6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Benchmark Results</head><p>In this section, we will report our main results on the partially annotated multi-label datasets: OpenImages <ref type="bibr" target="#b14">[15]</ref>, and LVIS <ref type="bibr" target="#b7">[8]</ref>. The results on MS-COCO dataset are presented in appendix C. We will present a comparison to previous methods which handle partial annotations, among other baseline approaches in multi-label classification. The evaluation metric used in the experiments is the mean average precision (mAP). In particular, we report the standard perclass mAP denoted as mAP(C), and overall mAP denoted as mAP(O), which considers the number of samples in each class. The training details and the loss hyper-parameters used are provided in appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">OpenImages V6</head><p>Openimages V6 is a large-scale multi-label dataset <ref type="bibr" target="#b14">[15]</ref>, consists of 9 million training images, 41,620 validation samples, and 125,456 test samples. It is a partially annotated dataset, with 9,600 trainable classes. In <ref type="table">Table 1</ref>, we present the mAP results obtained by our proposed Selective method and compare them to other approaches. Interestingly, Ignore mode produces better results than Negative mode, as OpenImages contains many under-annotated frequent classes such as colors and other general classes (see <ref type="figure" target="#fig_5">Figure 5</ref>). Using Negative mode adds a massive label noise and harms the learning of many common classes.</p><p>In <ref type="table">Table 2</ref>, we present results for different network architectures. Specifically, using TResNet-L <ref type="bibr" target="#b22">[23]</ref>, we achieve state-of-the-art result of 87.34 mAP score.</p><p>To show the impact of decoupling the focusing parameters of the annotated and un-annotated loss terms in P-ASL  <ref type="table">Table 3</ref>. Results for OpenImages (V3). Comparing the mAP score obtained using our Selective approach to previous multi-label classification methods.  <ref type="figure">Figure 8</ref>. Impact of decoupling the focusing parameters. We set the un-annotated focusing to ?u = 7 and varied the annotated negative focusing ??. Likelihood + Prior Likelihood <ref type="figure">Figure 9</ref>. Ablation study of the Selective approach components. mAP results are shown for different numbers of top likelihood labels, K. We show results for the case of using only the likelihood condition ?L, and with both conditions ?L ? ?P .</p><p>as proposed in equation <ref type="formula" target="#formula_4">(3)</ref>, we varied the negative focusing parameter ? ? , while fixing ? u = 7. The results are presented in <ref type="figure">Figure 8</ref>. The case of ? ? = 7 represents the standard ASL <ref type="bibr" target="#b1">[2]</ref>. As can be seen, the mAP score increases as we lower ? ? , up to 2. It indicates that lowering the decay rate for the annotated negative term boosts their contribution to the loss.</p><p>In <ref type="figure">Figure 9</ref>, we show the mAP scores while varying the number of top likelihood classes, K as defined in equation <ref type="bibr" target="#b4">(5)</ref>. Note that setting K = 0 is equivalent to use Negative mode. Training with high enough K becomes similar to training using Ignore mode. The highest mAP results are obtained with both the likelihood and prior conditions.  <ref type="table">Table 4</ref>. Results for LVIS. The Selective approach with P-ASL improves both mAP(C) and mAP(O) scores. Also, it provides top result for the frequent class "Person".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">OpenImages V3</head><p>To be compatible with previously published results, we used the OpenImages V3 which contains 5,000 trainable classes. We follow the comparison setting described in <ref type="bibr" target="#b10">[11]</ref>. Also, for a fair comparison we used the ResNet-101 <ref type="bibr" target="#b9">[10]</ref> backbone, pre-trained on the ImageNet dataset. In <ref type="table">Table 3</ref>, we show the mAP score results obtained using previous approaches and compared them to our Selective method. As shown, our method significantly outperforms previous approaches that deal with partial annotation in a multi-label setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">LVIS</head><p>LVIS is a partially labeled dataset originally annotated for object detection and image segmentation, that was adopted as a multi-label classification benchmark. It consists of 100,170 images for training and 19,822 images for testing. It contains 1,203 classes. In <ref type="table">Table 4</ref>, we present a comparison between different approaches on the LVIS dataset. As can be seen, in this case, the Negative mode is better, compared to the Ignore mode. This can be related to the fact that most of the labels are related to specific objects which do no appear frequently in the images. The most frequent class is "Person". Therefore we also added its average precision to <ref type="table">Table 4</ref>. Note that the Ignore model better learns the class "Person" compared to the one trained with Negative mode. Using the P-ASL with the Selective mode, we were able to obtain superior mAP results as well as top average precision even for the most frequent class, "Person".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we presented a novel technique for handling partially labeled data in multi-label classification. We observed that ignoring the un-annotated labels in the loss or treating them as negative should be determined individually for each class. We proposed a selective mechanism that uses the label likelihood computed throughout the training, and the label prior which is obtained by estimating the class distribution from the data. The un-annotated labels are further softened via a partial asymmetric loss. Extensive experiments analysis shows that our proposed approach outperforms other previous methods on partially labeled datasets, including OpenImages, LVIS, and simulated-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Details</head><p>Unless stated otherwise, all experiments were conducted with the following training configuration. As a default, we used the TResNet-M model <ref type="bibr" target="#b22">[23]</ref>, pre-trained on ImageNet-21k dataset <ref type="bibr" target="#b21">[22]</ref>. The model was fine-tuned using Adam optimizer <ref type="bibr" target="#b12">[13]</ref> and 1-cycle cosine annealing policy <ref type="bibr" target="#b23">[24]</ref> with a maximal learning rate of 2e-4 for training OpenImages and MS-COCO, and 6e-4 for training LVIS. We used trueweight-decay <ref type="bibr" target="#b19">[20]</ref> of 3e-4 and standard ImageNet augmentations. For fair comparison to previously published results on OpenImages V3, we also trained a ResNet-101 model, pre-trained on ImageNet.</p><p>In the OpenImages experiments we used the following hyper-parameters: ? = 0.05, K = 200, ? u = 7, ? ? = 2 and ? + = 1. In LVIS we used: ? u = 1, ? ? = 0 and ? + = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Soft Label Prior</head><p>Herein, we will explore a soft alternative for integrating the label prior in the loss. We follow equation <ref type="formula" target="#formula_4">(3)</ref> and define the un-annotaetd weights by</p><formula xml:id="formula_13">? c = exp(??P r (c))<label>(12)</label></formula><p>where ? is the decay factor. In <ref type="table">Table 5</ref> we compare the soft label prior to the configuration used in section 4.1.</p><p>Method mAP(C) mAP(O) P-ASL, Selective 86.72 93.57 P-ASL, Selective (soft) 86.62 93.59 <ref type="table">Table 5</ref>. OpenImages (V6) results using soft label prior. We used ? = 10.</p><p>As the soft label prior provided with lower mAP(C) results, we did not use it in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on MS-COCO</head><p>In this section, we will present the results obtained on a partially annotated version of MS-COCO, based on the fixed per class (FPC) simulation scheme. Note that in this experiment, the class distribution measured by the number of annotations is no longer meaningful, as all classes have the same number of annotations. The mAP results, as well as the average precision (AP) scores for the class "Person", are presented in <ref type="figure">Figure 10</ref>. The Negative mode produces higher mAP (computed over all the classes) compared to the Ignore mode. However, as the frequent class "Person" is present in most of the images, the Negative mode is inferior, especially in the cases of a small number of annotations. Using the Selective approach, top results can be achieved for both mAP and the person AP. In <ref type="figure">Figure 11</ref>, we show the top 10 frequent classes obtained using our procedure for estimating the class distribution as described in section 4.2, and compared them to those obtained using the original class frequencies in MS-COCO. As can be seen, most of the frequent classes measured by the original distribution are also highly ranked by our estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Frequent classes in OpenImages</head><p>We add more results of the class distribution estimated by our approach (detailed in 4.2) for OpenImages dataset. See <ref type="figure" target="#fig_0">Figure 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Frequent classes in LVIS</head><p>In <ref type="figure">Figure 13</ref> we plot the top frequent classes in LVIS, obtained by our estimator detailed in section 4.2. Also in LVIS, it can be seen that the most estimated frequent classes are related to common objects as "Person", "Shirt", "Trousers", "Shoe", etc.  <ref type="figure">Figure 13</ref>. Estimating the calss distribution in LVIS. Top 20 frequent classes estimated by the Ignore model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of training modes for handling partial labeling. (a) In a partially labeled dataset, only a portion of the samples are annotated for a given class. (b) Ignore mode exploits only a subset of the samples which may lead to a limited decision boundary. (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Label likelihood. Defined by the conditional probability of an un-annotated label c of being positive given the image and the model parameters. i.e. P (y c = 1|x; ?); ?c ? U x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Using the label likelihood. Un-annotated labels with highest network confidence may be related to positive items in the image. Thus, we ignore them in the loss computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Estimating the class distribution in OpenImages. Top 20 frequent classes estimated by the Ignore model. Top classes are all related to common labels such as colors or general concepts. simulating partial annotation under specific case studies. The evaluation metric used in the experiments is the mean average precision (mAP). Training details are provided in appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>Results on MS-COCO (FPC). . Class frequency estimation in MS-COCO. Top frequent classes measured by (a) original class distribution and (b) estimated class distribution. The estimated top 10 frequent classes are included in the original top classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Estimating the calss distribution in OpenImages. Additional top 60 frequent classes as estimated by our approach.</figDesc><table><row><cell>Tan Property Grey Yellow Area Beige Snapshot Material Product Design Habitat Season Natural material Electric blue Style Colorfulness Material property Structure Turquoise (Color) Beauty</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pink Aqua Ecosystem Gas Carmine Orange (Color) Maroon Nature Dude Parallel Silver (Color) Magenta Weather Vertebrate Rectangle Phenomenon Lighting Purple Composite material Amber (Color)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Outerwear Person Summer Sense Violet Cobalt blue Natural environment Organism Event Skin Terrestrial plant Botany Morning Peach (Color) Lavender (Color) Teal Staff T-shirt Infrastructure System</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell cols="2">0.3 Label frequency 0.4 0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3 Label frequency 0.4 0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2 Label frequency 0.3 0.4</cell><cell>0.5</cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell></row><row><cell>Figure 12. 0.0 person handle pole shirt trousers hat crossbar strap tag jacket shoe bottle jean trash_can chair watch spectacles sunglasses flag jersey</cell><cell>0.1</cell><cell></cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell></cell><cell>0.5</cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Label frequency</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning from positive and unlabeled data: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessa</forename><surname>Bekker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
		<idno>abs/1811.04820</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14119</idno>
		<title level="m">Asymmetric loss for multi-label classification</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mltr: Multi-label classification with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hezheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglin</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep convnet for multi-label classification with partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01755</idno>
		<title level="m">Multi-label image recognition with multi-class attentional regions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning from positive and unlabeled data with arbitrary positive shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zayd</forename><surname>Hammoudeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive multi-label CNN learning with partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving positive unlabeled learning: Practical aul estimation and new training method for extremely imbalanced data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting weakly supervised visual patterns to learn from partial annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">General multi-label image classification with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context, 2014. 2, 3</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Query2label: A simple transformer way to multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Seeing through the human reporting bias: Visual classifiers from noisy human-centric labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Imagenet-21k pretraining for the masses</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tresnet: High performance gpu-dedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-label learning with missing labels using mixed dependency graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Distribution-balanced loss for multi-label classification in long-tailed datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving multi-label learning with missing labels by structured semantic correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-modality attention with semantic graph embedding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renchun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12709" to="12716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Large-scale multi-label learning with missing labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
