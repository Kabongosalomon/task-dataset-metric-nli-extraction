<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaobo</forename><surname>Li</surname></persName>
							<email>shli@insun.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<email>qun.liu@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Collecting supporting evidence from large corpora of text (e.g., Wikipedia) is of great challenge for open-domain Question Answering (QA). Especially, for multi-hop open-domain QA, scattered evidence pieces are required to be gathered together to support the answer extraction. In this paper, we propose a new retrieval target, hop, to collect the hidden reasoning evidence from Wikipedia for complex question answering. Specifically, the hop in this paper is defined as the combination of a hyperlink and the corresponding outbound link document. The hyperlink is encoded as the mention embedding which models the structured knowledge of how the outbound link entity is mentioned in the textual context, and the corresponding outbound link document is encoded as the document embedding representing the unstructured knowledge within it. Accordingly, we build HopRetriever which retrieves hops over Wikipedia to answer complex questions. Experiments on the HotpotQA dataset demonstrate that Ho-pRetriever outperforms previously published evidence retrieval methods by large margins. Moreover, our approach also yields quantifiable interpretations of the evidence collection process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-hop QA <ref type="bibr" target="#b23">(Yang et al. 2018</ref>) is the Question Answering (QA) task that requires reasoning over multiple supporting documents to extract the final answer. For the open-domain setting, a key part of Multi-hop QA is to retrieve an evidence path from the whole knowledge source (e.g., Wikipedia). Most of the recent works view multi-hop evidence collection as an iterative document retrieval problem <ref type="bibr" target="#b0">(Asai et al. 2020;</ref><ref type="bibr" target="#b10">Feldman and El-Yaniv 2019;</ref><ref type="bibr" target="#b4">Das et al. 2019a</ref>), which can be decomposed to several single-step document retrieval. In contrast, some others <ref type="bibr" target="#b7">(Dhingra et al. 2020;</ref><ref type="bibr" target="#b8">Ding et al. 2019)</ref> focus on mentioned entities and try to traverse textual data like a virtual structured Knowledge Base (KB). These two methods leverage two different kinds of knowledge for evidence collection respectively: (i) informative but unstructured facts inside the introductory documents of entities. (ii) the structured and implicit relations between entities themselves <ref type="bibr">1</ref> .</p><p>Two examples in <ref type="figure" target="#fig_0">Figure 1</ref> show that both of the above knowledge is needed for complex question answering. We consider the problem of based on what evidence can one jump to the second document for further retrieval. For question 1, the structured relation "directed by" implied by "...directed by Adriana Trigiani" in the first document matches the relation "director of" in the question, hence providing sufficient and convincing evidence that one can hop to the introductory document of Adriana Trigiani for further retrieval, even without pre-reading it. However, things become complicated for question 2, for three entities share the same relation "song of": On My Mind, Army, and Something in the Way You Move. In fact, only the entity On My Mind satisfies the condition "works with other writers" in the question, which makes the relation itself insufficient and indistinctive to make the choice among the three entities. The truth is that only if the unstructured facts about the entity On My Mind is browsed through, one can find the conclusive evidence.</p><p>As shown above, to collect sufficient supporting evidence within Wikipedia, it's necessary to consider both relational structures between entities and unstructured knowledge hidden inside the introductory document. When the answering process follows the pattern of "following the vine to get the melon", implicit entity-level relation makes retrieval efficient and effective. However, when the relation chain failed, those unstructured facts in the document mount the stage.</p><p>In this paper, we study how the structured and unstructured knowledge can be combined together and collaboratively contribute to the evidence collection. Accordingly, We define a hop as the combination of a hyperlink and a corresponding outbound link document. A hyperlink in Wikipedia implies how the introductory document of an entity mentions some other, while the outbound link document stores all the unstructured facts and events, which makes a hop contain both relational and factoid evidence for future retrieval.</p><p>One challenge is how to transform the binary (link or not) hyperlink in Wikipedia to distributed representations implying the implicit and complicated entity relation. One step towards this is the recent work on distributed relation learning <ref type="bibr" target="#b19">(Soares et al. 2019)</ref>, in which the relation representations are learned solely from the entity-linked text in an unsupervised way. With the powerful ability of BERT <ref type="bibr" target="#b6">(Devlin et al. 2019</ref>) for text encoding, <ref type="bibr" target="#b8">(Ding et al. 2019)</ref> and <ref type="bibr" target="#b7">(Dhingra et al. 2020</ref>) encodes entity spans into node representations to conduct relation-based reasoning. In this paper, we represent each hyperlink with the corresponding entity mention, with the currently described entity as the mention subject and the outbound link entity as the mention object.</p><p>Our contributions. To be more specific, this paper introduces HopRetriever, a method to automatically and adaptively leverage both the structured entity relation and unstructured introductory facts for evidence collection. For each entity mention within Wikipedia documents, we encode the textual context around it into mention embedding to represent the implicit structured knowledge. As for the representation of unstructured knowledge in documents, we use BERT to encode document text conditioned on the original question, as previous works do. For each step retrieval, the hop from one document(entity) to another one can gather evidence from two perspectives: (i) How the current document mentions the other one. (ii) What facts are hidden in the introductory document of the other entity. Experiments conclude that our retrieval method outperforms both entitycentric retrieval methods and document-wise retrieval ones.</p><p>Our prime contributions are as follows: ? We propose to retrieve hops over Wikipedia to answer complex questions, which adaptively and selectively collects evidence from both structured entity relation and unstructured facts within documents.</p><p>? We propose to represent hyperlinks in Wikipedia with mention embeddings, which we show can precisely capture the implicit relation between entities.</p><p>? Evaluated on HotpotQA <ref type="bibr" target="#b23">(Yang et al. 2018)</ref>, the proposed approach significantly outperforms previously published evidence retrieval methods. Additionally, we conduct further experimental analysis and demonstrate the good interpretability of our method.  <ref type="bibr" target="#b0">Asai et al. (2020)</ref> proposes the PathRetriever that retrieves documents paths along the outboundlink of text graph. With the graph structure of the documents, PathRetriever reduces the search space of documents during each step retrieval, which is much smaller than that of previous iterative retrievers. The biggest difference between PathRetriever and our method is that we additionally consider the structured and multi-valued relation between entities, while PathRetriever uses hyperlinks in a binary way: link or not link. Entity-centric reasoning. Considering that most factoid QA problems are entity-centric, some other works focus on the entity mention to collect reasoning evidence. Cognitive Graph <ref type="bibr" target="#b8">(Ding et al. 2019</ref>) trains a reading comprehension model to predict the next-hop spans, aiming to find the most evidential mentioned entity. Similarly, DrKIT (Dhingra et al. 2020) constructs large mounts of entity mentions from the corpus and proposes a method to reason over these mentions, softly following paths of latent relations. We've shown in <ref type="figure" target="#fig_0">Figure 1</ref> that when the question is not the case of "following the vine to get the melon", the mention itself fails to provide sufficient reasoning evidence for which entity to hop. Inspired by the idea of pseudo-relevance feedback <ref type="bibr" target="#b22">(Xu and Croft 2017)</ref>, <ref type="bibr" target="#b5">Das et al. (2019b)</ref> also leverages entitylink to find more supporting evidence. However, this method is still document-level, for the entity links are used not for relation representation, but document expansion. We empirically show significant improvement over the above methods.  <ref type="figure">Figure 2</ref>: Retrieving Hops over Wikipedia text graph. Documents are retrieved by selecting hops over them iteratively. Each directed arrow implies a mention m i,j , which reveals how e i mentions e j in the document d i . Hops between entities are indicated by curved arrows. If the mention m i,j exists between e i and e j , the hop hop i,j is represented based on both m i,j and the introductory document d j for retrieval or based on the d j solely if no mentions exist. troductory documents at the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>3.1 Overview Task definition. Our task is to obtain the answer a for an open-domain multi-hop question q. A retriever model Retriever is used to collect the multiple evidence pieces over a large-scale knowledge source K: D q = Retriever(q, K).</p><p>(1) D q should contain multiple documents that are necessary for answering the multi-hop question. All textual facts in D q and q are concatenated together and fed into a answer extraction model Reader to obtain the answer a: a = Reader(q, D q ).</p><p>(2) Our approach. In this paper, we propose HopRetriever to take the place of the retriever model Retriever while keeping the answer extraction model Read as standard <ref type="bibr" target="#b6">(Devlin et al. 2019</ref>). The knowledge source K is constructed from Wikipedia 2 . Each Wikipedia page corresponds to an entity e i , accompanied by an introductory document d i . Moreover, if there exists an anchor text in d i linked to e j , we denote it as a mention m i,j = e i di ? ? e j , which means e j is mentioned by e i via d i . Accordingly, the knowledge source is formulated as K = {D, E, M } that consists of an entity set E = {e i }, an introductory document set D = {d i }, and a mention set M = {m i,j }. D q is retrieved iteratively. At each retrieval step, a document is fetched by examining not only the unstructured facts contained in but also the mention of it in the latest selected document. To achieve that, we encode the unstructured textual facts and the mention respectively and then represented them together within a hop. HopRetriever uses hops as the matching objects when retrieving over Wikipedia. The overview of a retrieval process is shown in <ref type="figure">Figure 2</ref>. The details about the hop encoding and the iterative retrieval procedure are discussed in the following two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hop Encoding</head><p>HopRetriever considers retrieving a new document d j conditioning on the retrieval history as finding the proper hop from the current foothold entity e i to the entity e j . The representation of each hop consists of mention embedding m i,j that implies the entity relation from e i to e j , and the document embedding u j of the introductory document of entity e j .</p><p>Mention embedding. We consider the problem of how to encode a hop hop i,j into hop encoding hop i,j . The structured entity relation revealed by m i,j is encoded as mention embedding m i,j , based on the context around it. Inspired by <ref type="bibr" target="#b19">Soares et al. (2019)</ref>, two entity markers clipping the anchor text of each mentioned entity are introduced to obtain the mention embedding. An example is shown in <ref type="figure" target="#fig_2">Figure 3</ref> (from the second example in <ref type="figure" target="#fig_0">Figure 1</ref>), the document that contains the mention of On My Mind is fed into BERT with two additional [MARKER] tokens, and the output representation of the first [MARKER] token is used as the mention embedding vector.  If e j is not mentioned directly in the introductory document of e i , we represent the relation between them with a trainable uniformed vector m P , as shown below:</p><formula xml:id="formula_0">m i,j = BERT [M-j] (q; d i ), if m i,j ? M m P , otherwise<label>(3)</label></formula><p>where the BERT <ref type="bibr">[M-j]</ref> is the representation of the entity marker corresponding to entity e j .</p><p>Document embedding. The unstructured knowledge about the entity e j is encoded as document embedding u j by feeding the textual facts in d j (concatenated with q) into BERT, and the output representation of the [CLS] token is taken as the document embedding vector:</p><formula xml:id="formula_1">u j = BERT [CLS] (q; d j ).<label>(4)</label></formula><p>Knowledge fusion. The mention embedding m i,j and the document embedding u j are fused together as hop encoding hop i,j by the attention mechanism proposed in <ref type="bibr" target="#b20">Sukhbaatar et al. (2015)</ref>. The following fusion procedure allows HopRetriever to adaptively and selectively manage the two kinds of knowledge according to which truly matters:</p><formula xml:id="formula_2">a m = hW k m i,j a u = hW k u j {w m , w u } = softmax({a m , a u }) hop i,j = w m ? W v m i,j + w u ? W v u j ,<label>(5)</label></formula><p>where h is the vector that encodes the corresponding retrieval history, the W k projects the two embedding vectors (i.e. m i,j and u j ) into key vectors. The h acts as query vector that interacts with the key vectors to calculate the importance weight w m for the mention embedding m i,j and w u for the document embedding u j , then m i,j and u j are projected into value vectors by W v and fused as hop encoding with important weights.  <ref type="figure">Figure 4</ref>: The retrieval process of HopRetriever for three hops. hop s,i indicates a beginning jump from the start to e i is selected based on the initial hidden state h s . The selection of hop hop i,j retrieves the supporting document d j at the second step. hop e ends the retrieval process finally. <ref type="figure">Figure 4</ref> illustrates a three-step recurrent hop retrieval process. Generally, let e i denote the foothold entity selected at the previous t ? 1 step, the probability of retrieving the document d j at t step is calculated by the dot product of h t and hop encoding hop i,j (i.e. the Hop Selector in <ref type="figure">Figure 4</ref>),as formulated in the following equation:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Iterative Retrieval of Hops</head><formula xml:id="formula_3">p(d j ) = sigmoid(h t hop i,j ),<label>(6)</label></formula><p>where h t is the hidden state vector that encodes all the previously selected hops by a Recurrent Neural Network (RNN):</p><formula xml:id="formula_4">h t = h s , t = 1 RNN(h t?1 , hop k,i ), t ? 2 (7)</formula><p>where h s is the initial hidden state vector and hop k,i is the encoding of the hop selected at t ? 1 step. Specially, for t = 1, the hop hop s,j indicating jumping from the retrieving start to e j is introduced. Similarly, a special end hop hop e is used to mark the end of the retrieval process and it is encoded by m p and a virtual end document encoding u e . Let f denote the fusion function formulated as Equation <ref type="formula" target="#formula_2">(5)</ref>, the encodings of different hops are summarized in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation</head><p>Encoding Explanation</p><formula xml:id="formula_5">hop i,j f (mP, uj) ej is not mentioned in di f (mi,j, uj) ej is mentioned in di hop s,j f (mP, uj)</formula><p>Select dj at the beginning hop e f (mP, ue) Retrieval finish </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fine-Grained Sentence-Level Retrieval</head><p>A single supporting document can be split into multiple sentences and may not all these sentences are essential for answering the question. Pointing out the indispensable supporting sentences can illuminate the reasons why a document is required. In HopRetriever, the supporting sentence prediction is added as an auxiliary task along with the primary hop retrieval task. At step t, the probability p(s i,l ) that indicates the l-th sentence in the latest retrieved document d i is a supporting sentence is calculated by the following equations:</p><formula xml:id="formula_6">s i,l = BERT [SM-l] (q; d i ) (8) p(s i,l ) = sigmoid(h t W s s i,l ),<label>(9)</label></formula><p>where s i,l is the sentence embedding vector obtained by inserting a sentence marker <ref type="bibr">[SM-l]</ref> at the end of the l-th sentence in d i , which is similar to how the mention embedding is obtained. If p(s i,l ) &gt; 0.5, then the l-th sentence in document d i is identified as a supporting sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Objective Functions of HopRetriever</head><p>HopRetriever is a sequence prediction model with binary cross-entropy objective functions at each step. At the retrieval step t, the objective function of the primary hop retrieval task is</p><formula xml:id="formula_7">log p(d j ) + d j ?D,dj =dj log(1 ? p(d j )),<label>(10)</label></formula><p>where d j is the ground-truth document. For the auxiliary supporting sentence prediction task, the object function at step t is</p><formula xml:id="formula_8">l?Li log p(s i,l ) + l / ?Li log(1 ? p(s i,l )),<label>(11)</label></formula><p>where s i,l is the l-th sentence in d i , L i is the set of indices of the ground-truth supporting sentences in d i . The above two objective functions are maximized together in training.  In the official evaluation, the participant model is required to predict both the exact supporting sentences and the answer text.</p><p>Pipeline. The whole procedure follows a coarse-to-fine pipeline that contains three stages:</p><p>1. Preliminary retrieval: Only the top-500 documents are used to construct the initial candidate hops of HopRetriever, according to the TF-IDF scores of documents w.r.t. the input question.</p><p>2. Supporting documents retrieval and supporting sentence prediction: HopRetriever retrieves the supporting documents iteratively starting from the initial candidate hops, and also predicts supporting sentences from the retrieved documents.</p><p>3. Answer extraction: The answer within the retrieved supporting documents is extracted using BERT (large, whole word mask), following the conventional answer boundary prediction approach <ref type="bibr" target="#b6">(Devlin et al. 2019;</ref><ref type="bibr" target="#b18">Seo et al. 2017)</ref>, which is the same as PathRetriever <ref type="bibr" target="#b0">(Asai et al. 2020)</ref>.</p><p>Implementation details. The negative hop sequences used to train the proposed model are constructed by traversing through the entities in Wikipedia. And the top-40 TD-IDF scored documents w.r.t. the question and top-10 scored documents w.r.t. the ground-truth documents are used as the start points of the traverse.The length of negative hop sequences is fixed to 3. We restrict the maximum input sequence length of BERT to 384. In training, the batch size is set to 16, the learning rate is 3 ? 10 ?5 , and the number of training epochs is 3. We use beam search with beam size set to 8 at the inference time.</p><p>To achieve better performance, we introduce a neural ranker based on BERT-base <ref type="bibr" target="#b16">(Nogueira and Cho 2019)</ref> to produce more precise top-500 documents in the preliminary retrieval. And use ELECTRA <ref type="bibr" target="#b3">(Clark et al. 2019)</ref> to take the place of BERT, i.e., use the ELECTRA base in HopRetriever for document sequence retrieval and use ELECTRA large for answer extraction. The results of this enhanced pipeline are denoted as HopRetriever-plus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Evidence collection. The HopRetriever is first evaluated by measuring the coverage of ground-truth answers, supporting sentences, and supporting documents in the retrieved supporting documents, as shown in <ref type="table" target="#tab_4">Table 2</ref>. The metric Ans exists measures the percentage of the questions whose answers are extractable from the retrieved document sequence. Sent exists is the percentage of the supporting sentences that can be found. The percentage of the questions that have all ground-truth documents retrieved are showed as the All docs exist.</p><p>Three models that mainly focus on evidence collection over Wikipedia are evaluated as baselines on the development set:</p><p>? Cognitive Graph QA <ref type="bibr" target="#b8">(Ding et al. 2019</ref>) explicitly utilizes the structured knowledge in Wikipedia with a graph whose nodes are entities or answer spans. The representations of nodes are maintained by Graph Neural Network (GNN) <ref type="bibr" target="#b1">(Battaglia et al. 2018;</ref><ref type="bibr" target="#b12">Kipf et al. 2017</ref>).</p><p>? Semantic Retrieval <ref type="bibr" target="#b15">(Nie et al. 2019</ref>) is a multi-grained retrieval baseline that retrieves supporting documents and sentences together, focuses on the unstructured knowledge in Wikipedia.</p><p>? PathRetriever <ref type="bibr" target="#b0">(Asai et al. 2020</ref>) introduces a similar iterative retrieval framework, but only focuses on the unstructured knowledge provided in the introductory document at each retrieval step.</p><p>To be fairly compared with PathRetriever, which is the state-of-the-art published model, HopRetriever uses the same initial search space (i.e. top-500 documents based on TF-IDF scores) and pre-trained model (i.e. BERT-base) with PathRetriever. Notably, HopRetriever outperforms the PathRetriever by 5.93%, 6.36%, and 8.63% on the top-1 evidence collection metrics respectively, and also achieves sig-    <ref type="table" target="#tab_6">Table 3</ref>. We can observe that HopRetrieve works more effectively on the bridging questions. In the HotpotQA dataset, the ground-truth supporting documents of comparison questions may not be directly relevant to each other where no structured knowledge is available, which makes HopRetriever perform almost the same as PathRetriever. In contrast, the ground-truth supporting documents of the bridging questions are stringed with mentions that can provide informative structured knowledge, so HopRetriever performs better by leveraging mentions additionally.</p><p>Answer extraction and supporting sentence prediction. <ref type="table" target="#tab_7">Table 4</ref> shows the performance of different methods for the answer and supporting sentence prediction. Naturally, the answer extraction and supporting sentence prediction result benefit from the improvements of document retrieving. By providing more accurate supporting documents, HopRetriever outperforms all the aforementioned baseline models on the development set and also the other published models 3 on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>Detailed analysis of HopRetriever is carried out in this section, especially about how the structured and unstructured knowledge in Wikipedia contribute to evidence retrieval.</p><p>Embedding weights on different question types. At the t step in the retrieval procedure of HopRetriever, the decision whether to select a document d j depends on the hop encoding hop i,j , which contains a mention embedding and a document embedding assigned with learnable weights as formulated in Equation <ref type="formula" target="#formula_2">(5)</ref>. We analyze the weights and find that they provide intuitive explanation about which embedding is more important for different question types. <ref type="table">Table 5</ref> shows the average weight of mention embedding and document embedding on different question types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Type Mention Document</head><p>Bridging 89.53% 10.47% Comparison 4.61% 95.39% <ref type="table">Table 5</ref>: Weights of mention embedding and document embedding on bridging questions and comparison questions.</p><p>It can be seen that the mention embedding accounts for a large portion (89.53%) on the bridge questions. The bridge questions always require selecting a proper document along with a hyperlink and the mentions do provide helpful information for bridging the evidence pieces. Conversely, when processing the comparison questions, the weight of mention embedding is relatively small (4.61%) because there are no available mentions between the supporting documents.</p><p>Embedding weights in different cases. Three examples are presented in <ref type="figure" target="#fig_4">Figure 5</ref> to further inspect the learned weights in the hop encoding. In case 1, a strong clue that matches with the information "director of" in question occurs as the mention "directed by", so the weight of mention embedding is relatively high. In case 2, the entity "World    <ref type="table">Table 6</ref>: Ablation experiments of HopRetriever.</p><p>War I" and "World War II" are mentioned with the same context, which means they cannot be distinguished only based on the mention embedding, so more attention is paid to the document embedding which encodes the important fact "60 million". In case 3, no mentions exist in the latest selected document so the hop encoding almost completely depends on the document embedding. We can see that the embedding weights can bring intuitive interpretation about which embedding, or which types of knowledge, is more important for different questions when selecting a hop.</p><p>Probing task for the mention embedding. The structured entity relation is represented by the markers around the mentions, as described in Section 3.2. To explore what the mention embedding learns, we design a special probing task: distracted hop selection. That is, the ground-truth hop for bridging questions is shuffled with other hops that have the same mentioned entity but different mention context, and HopRetriever is required to select the right one from these distracting hops for each question. To make the right selection, one should understand more about how each entity is mentioned, but not the entity itself. The summary of this task is shown in <ref type="table">Table 7</ref>. The experiment result shows that although the distracting hops are not used as negative samples for training, the HopRetriever can retrieve ground-truth hops just based on learned mention embedding at high accuracy (96.42%), indicating that the mention embedding does learn the implicit relation between entities, but not the entities themselves.</p><p>Ablation study. As shown in <ref type="table">Table 6</ref>, ablation experiments are conducted to corroborate the effectiveness of Ho-  <ref type="table">Table 7</ref>: Summary of the mention embedding probing task.</p><p>pRetriever. In experiment 1, the structured knowledge in hops is removed (i.e. set the weight of mention embedding w m to 0 in Equation 5), the performance dropped significantly, which stresses the importance of structured knowledge in Wikipedia for multi-hop evidence retrieval. The performance also degraded in experiment 2 in which the weighting for the structured and unstructured knowledge in hops is disabled (i.e. set w m = w u = 1 in Equation 5), demonstrating that the fusion function improves the performance while providing interpretations. The auxiliary supporting sentence prediction task is removed in the experiment 3. The result shows that the auxiliary task has no sideeffect on the primary hop retrieval task. Additionally, the sentence representations are obtained by the sentence markers contained in the latest retrieved document which has been encoded already at the previous step. So the auxiliary task does not require much additional computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose the HopRetriever to collect reasoning evidence over Wikipedia for multi-hop question answering. Both the structured knowledge indicated by hyperlinks and the unstructured knowledge presented as introduc-tory documents in Wikipedia, are involved and leveraged together in HopRetriever to help the evidence collection. The experiment on the HotpotQA dataset shows that the performance of HopRetriever improved observably as a result of combining the structured knowledge with unstructured knowledge, and outperforms all the published models on the leaderboard. Moreover, by inspecting the proportion of the two kinds of knowledge in hops, which kind of knowledge leads the retrieving of each evidence piece can be observed directly, which also provides extra intuitive interpretations for the selection of each evidence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two examples showing that both structured relation and unstructured fact are needed for complex question answering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Question decomposition.<ref type="bibr" target="#b21">Wolfson et al. (2020)</ref>,<ref type="bibr" target="#b17">Perez et al. (2020), and</ref><ref type="bibr" target="#b14">Min et al. (2019)</ref> propose to decompose a complicated question into several simpler sub-questions and conduct single-hop QA at each step. The challenge for question decomposition is to ensure each sub-question collects the truly necessary evidence. As we know from example 2 inFigure 1, when the structured relation fails, one can not ask reasonable sub-question without exploring enough in-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Encoding the mention using entity markers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The weights of mention embedding and document embedding in different cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Types of hop encoding.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(Nie et al. 2019)  77.84 85.96 86.39 81.68 88.35 88.50 69.35 81.73  82.07 PathRetriever (Asai et al. 2020) 80.96 89.09 89.98 82.05 88.69 89.48 73.91 86.12 87.39 HopRetriever 86.89 91.11 91.80 88.41 92.78 93.20 82.54 88.60 89.09 HopRetriever-plus 89.25 93.07 93.64 91.29 95.44 95.70 86.94 93.25 93.72</figDesc><table><row><cell>Model</cell><cell>Ans exists</cell><cell></cell><cell></cell><cell>Sent exists</cell><cell></cell><cell cols="2">All docs exist</cell><cell></cell></row><row><cell cols="9">top-1 top-5 top-8 top-1 top-5 top-8 top-1 top-5 top-8</cell></row><row><cell>Cognitive Graph QA (Ding et al. 2019) 72.21</cell><cell>-</cell><cell>-</cell><cell>70.25</cell><cell>-</cell><cell>-</cell><cell>57.80</cell><cell>-</cell><cell>-</cell></row><row><cell>Semantic Retrieval</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Evidence collection result on the HotpotQA fullwiki development set. We compare the top-1, top-5, and top-8 output document sequences from different retrievers using respectively. Cognitive Graph QA produces one document sequence for each question. HopRetriever and PathRetriever output the top-8 document sequences by the adoption of beam search. Semantic Retrieval * ranks documents instead of document sequences, so we assemble the top-2, top-10, and top-16 output documents into the top-1, top-5, and top-8 document sequences respectively for better fairness.</figDesc><table><row><cell>4 Experiments</cell></row><row><cell>4.1 Setup</cell></row><row><cell>Dataset. HopRetriever is evaluated on the multi-hop ques-</cell></row><row><cell>tion answering dataset HotpotQA (Yang et al. 2018),</cell></row><row><cell>which includes 90,564 question-answer pairs with anno-</cell></row><row><cell>tated supporting documents and sentences for training, 7,405</cell></row><row><cell>question-answer pairs for development, and 7,405 questions</cell></row><row><cell>for testing. All the testing questions correspond to multi-</cell></row></table><note>ple supporting documents. We focus on the fullwiki setting of HotpotQA, in which the supporting documents for each question are scattered among almost 5M Wikipedia pages.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>81.17 82.25 88.33 90.36 90.62 86.42 89.58 90.38 HopRetriever (Comparison) 77.40 80.97 82.31 91.31 92.73 92.79 84.26 85.41 85.41 PathRetriever (Bridging) 81.95 91.08 91.92 80.56 88.29 89.20 70.77 85.25 86.63 HopRetriever (Bridging) 89.27 93.66 94.19 87.73 92.79 93.29 82.11 89.41 90.01</figDesc><table><row><cell>Model</cell><cell>Ans exists</cell><cell>Sent exists</cell><cell>All docs exist</cell></row><row><cell cols="4">Recall @ top-1 top-5 top-8 top-1 top-5 top-8 top-1 top-5 top-8</cell></row><row><cell>PathRetriever (Comparison)</cell><cell>77.00</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Evidence collection results on different types of questions.</figDesc><table><row><cell></cell><cell>Model</cell><cell>EM</cell><cell>Ans</cell><cell>F1</cell><cell>EM</cell><cell>Sup</cell><cell>F1</cell><cell cols="2">Joint EM</cell><cell>F1</cell></row><row><cell></cell><cell>Cognitive Graph QA (Ding et al. 2019)</cell><cell cols="8">37.55 49.40 23.11 58.52 12.18 35.28</cell></row><row><cell>dev</cell><cell>Semantic Retrieval (Nie et al. 2019) PathRetriever (Asai et al. 2020)</cell><cell cols="8">46.41 58.70 39.86 71.53 26.53 49.00 60.49 73.30 49.16 76.05 35.82 61.43</cell></row><row><cell></cell><cell>HopRetriever</cell><cell cols="8">62.07 75.18 52.53 78.92 37.81 64.50</cell></row><row><cell></cell><cell>HopRetriever-plus</cell><cell cols="8">66.56 79.21 56.02 81.81 42.01 68.97</cell></row><row><cell></cell><cell>DecompRC (Min et al. 2019)</cell><cell cols="3">30.00 40.65</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Cognitive Graph QA (Ding et al. 2019)</cell><cell cols="8">37.12 48.87 22.82 57.69 12.42 34.92</cell></row><row><cell></cell><cell>DrKIT (Dhingra et al. 2020)</cell><cell cols="7">42.13 51.72 37.05 59.84 24.69</cell><cell>42.8</cell></row><row><cell></cell><cell>Semantic Retrieval (Nie et al. 2019)</cell><cell cols="8">45.32 57.34 38.67 70.83 25.14 47.60</cell></row><row><cell>test</cell><cell>Transformer-XH (Zhao et al. 2019)</cell><cell cols="8">51.60 64.07 40.91 71.42 26.14 51.29</cell></row><row><cell></cell><cell>PathRetriever (Asai et al. 2020)</cell><cell cols="8">60.04 72.96 49.08 76.41 35.35 61.18</cell></row><row><cell></cell><cell cols="9">Semantic Retrieval + HGN (Fang et al. 2019) 59.74 71.41 51.03 77.37 37.92 62.26</cell></row><row><cell></cell><cell>HopRetriever</cell><cell cols="8">60.83 73.93 53.07 79.26 38.00 63.91</cell></row><row><cell></cell><cell>HopRetriever-plus</cell><cell cols="8">64.83 77.81 56.08 81.79 40.95 67.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Answer extraction and supporting sentence prediction result in the fullwiki setting of HotpotQA.</figDesc><table><row><cell>nificant improvement over Semantic Retriever and Cogni-</cell></row><row><cell>tive Graph QA, which further demonstrates the effectiveness</cell></row><row><cell>of HopRetriever.</cell></row><row><cell>A more detailed comparison with PathRetriever is shown</cell></row><row><cell>in</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, we view the entity relation as structured knowledge is because it directly connects two entities and can be applied to build a structured entity graph.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://en.wikipedia.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">By the submission time of this paper, recently published method on HotpotQA fullwiki leaderboard is PathRetriever.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJgVHkrYDH" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to Answer Open-Domain Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1171" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Barzilay, R.</editor>
		<editor>and Kan, M.</editor>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkfPSh05K7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-step Entity-centric Information Retrieval for Multi-Hop Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kavarthapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5816</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-5816" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</title>
		<meeting>the 2nd Workshop on Machine Reading for Question Answering<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-04" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Burstein, J.</editor>
		<editor>Doran, C.</editor>
		<editor>and Solorio, T.</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Differentiable Reasoning over a Virtual Knowledge Base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJxstlHFPH" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cognitive Graph for Multi-Hop Reading Comprehension at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1259</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1259" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>Korhonen, A.</editor>
		<editor>Traum, D. R.</editor>
		<editor>and M?rquez, L.</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2694" to="2703" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Hierarchical Graph Network for Multi-hop Question Answering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv arXiv-1911</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-Hop Paragraph Retrieval for Open-Domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1222</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1222" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>Korhonen, A.</editor>
		<editor>Traum, D. R.</editor>
		<editor>and M?rquez, L.</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2296" to="2309" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dense Passage Retrieval for Open-Domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<ptr target="CoRRabs/2004.04906.URLhttps://arxiv.org/abs/2004.04906" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latent Retrieval for Weakly Supervised Open Domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1612</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1612" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>Korhonen, A.</editor>
		<editor>Traum, D. R.</editor>
		<editor>and M?rquez, L.</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-hop Reading Comprehension through Question Decomposition and Rescoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6097" to="6109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Revealing the Importance of Semantic Retrieval for Machine Reading at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1258</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1258" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="2553" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised question decomposition for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09758</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BI-DIRECTIONAL ATTENTION FLOW FOR MA-CHINE COMPREHENSION</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Matching the Blanks: Distributional Similarity for Relation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1279</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1279" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>Korhonen, A.</editor>
		<editor>Traum, D. R.</editor>
		<editor>and M?rquez, L.</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-toend memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Break it down: A question understanding benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deutch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="183" to="198" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quary Expansion Using Local and Global Document Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/3130348.3130364</idno>
		<ptr target="https://doi.org/10.1145/3130348.3130364" />
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="175" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1259</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1259" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Riloff, E.</editor>
		<editor>Chiang, D.</editor>
		<editor>Hockenmaier, J.</editor>
		<editor>and Tsujii, J.</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
