<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Out-of-Distribution Generalization via Risk Extrapolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
						</author>
						<title level="a" type="main">Out-of-Distribution Generalization via Risk Extrapolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model's sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anticausal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution ("covariate shift"). By tradingoff robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While neural networks often exhibit super-human generalization on the training distribution, they can be extremely sensitive to distributional shift, presenting a major roadblock for their practical application <ref type="bibr">(Su et al., 2019;</ref><ref type="bibr" target="#b13">Engstrom et al., 2017;</ref><ref type="bibr">Recht et al., 2019;</ref><ref type="bibr" target="#b29">Hendrycks &amp; Dietterich, 2019)</ref>. This sensitivity is often caused by relying on "spurious" features unrelated to the core concept we are trying to learn <ref type="bibr" target="#b15">(Geirhos et al., 2018)</ref>. For instance, <ref type="bibr" target="#b4">Beery et al. (2018)</ref> give the example of an image recognition model failing to correctly classify cows on the beach, since it has learned to 1 Mila 2 University of Montreal 3 Vector 4 University of Toronto 5 McGill University 6 Facebook AI Research. Correspondence to: &lt;david.scott.krueger@gmail.com&gt;. make predictions based on the features of the background (e.g. a grassy field) instead of just the animal.</p><p>In this work, we consider out-of-distribution (OOD) generalization, also known as domain generalization, where a model must generalize appropriately to a new test domain for which it has neither labeled nor unlabeled training data. Following common practice <ref type="bibr" target="#b7">(Ben-Tal et al., 2009)</ref>, we formulate this as optimizing the worst-case performance over a perturbation set of possible test domains, F:</p><formula xml:id="formula_0">R OOD F (?) = max e?F R e (?)<label>(1)</label></formula><p>Since generalizing to arbitrary test domains is impossible, the choice of perturbation set encodes our assumptions about which test domains might be encountered. Instead of making such assumptions a priori, we assume access to data from multiple training domains, which can inform our choice of perturbation set. A classic approach for this setting is group distributionally robust optimization (DRO) <ref type="bibr">(Sagawa et al., 2019)</ref>, where F contains all mixtures of the training distributions. This is mathematically equivalent to considering convex combinations of the training risks.</p><p>However, we aim for a more ambitious form of OOD generalization, over a larger perturbation set. Our method minimax Risk Extrapolation (MM-REx) is an extension of DRO where F instead contains affine combinations of training risks, see <ref type="figure" target="#fig_0">Figure 1</ref>. Under specific circumstances, MM-REx can be thought of as DRO over a set of extrapolated domains. 1 But MM-REx also unlocks fundamental new generalization capabilities unavailable to DRO.</p><p>In particular, focusing on supervised learning, we show that Risk Extrapolation can uncover invariant relationships between inputs X and targets Y . Intuitively, an invariant relationship is a statistical relationship which is maintained across all domains in F. Returning to the cow-on-the-beach example, the relationship between the animal and the label is expected to be invariant, while the relationship between the background and the label is not. A model which bases its predictions on such an invariant relationship is said to perform invariant prediction. 2 <ref type="bibr">1</ref> We define "extrapolation" to mean "outside the convex hull", see Appendix B for more. <ref type="bibr">2</ref> Note this is different from learning an invariant representation arXiv:2003.00688v5 <ref type="bibr">[cs.</ref>LG] 25 Feb 2021 Many domain generalization methods assume P (Y |X) is an invariant relationship, limiting distributional shift to changes in P (X), which are known as covariate shift <ref type="bibr" target="#b6">(Ben-David et al., 2010b)</ref>. This assumption can easily be violated, however. For instance, when Y causes X, a more sensible assumption is that P (X|Y ) is fixed, with P (Y ) varying across domains <ref type="bibr">(Sch?lkopf et al., 2012;</ref><ref type="bibr">Lipton et al., 2018)</ref>. In general, invariant prediction may involve an aspect of causal discovery. Depending on the perturbation set, however, other, more predictive, invariant relationships may also exist <ref type="bibr">(Koyama &amp; Yamaguchi, 2020)</ref>.</p><formula xml:id="formula_1"># ? P 1 (X, Y ) # ? P 2 (X, Y ) e1 e2 e3 R R RI convex hull of training distributions # ? P 1 (X, Y ) # ? P 2 (X, Y )</formula><p>The first method for invariant prediction to be compatible with modern deep learning problems and techniques is Invariant Risk Minimization (IRM) , making it a natural point of comparison. Our work focuses on explaining how REx addresses OOD generalization, and highlighting differences (especially advantages) of REx compared with IRM and other domain generalization methods, see <ref type="table" target="#tab_0">Table 1</ref>. Broadly speaking, REx optimizes for robustness to the forms of distributional shift that have been observed to have the largest impact on performance in training domains. This can be a significant advantage over the more focused (but also limited) robustness that IRM targets. For instance, unlike IRM, REx can also encourage robustness to covariate shift (see Section 3 and <ref type="figure">Figure 3</ref>.2).</p><p>Our experiments show that REx significantly outperforms IRM in settings that involve covariate shift and require invariant prediction, including modified versions of CMNIST and simulated robotics tasks from the Deepmind control suite. On the other hand, because REx does not distinguish between underfitting and inherent noise, IRM has an advantage in settings where some domains are intrinsically harder than others. Our contributions include:</p><p>1. MM-REx, a novel domain generalization problem for- ; see Section 2.3. mulation suitable for invariant prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Demonstrating that REx solves invariant prediction</head><p>tasks where IRM fails due to covariate shift.</p><p>3. Proving that equality of risks can be a sufficient criteria for discovering causal structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background &amp; Related work</head><p>We consider multi-source domain generalization, where our goal is to find parameters ? that perform well on unseen domains, given a set of m training domains, E = {e 1 , .., e m }, sometimes also called environments. We assume the loss function, is fixed, and domains only differ in terms of their data distribution P e (X, Y ) and dataset D e . The risk function for a given domain/distribution e is:</p><p>R e (?) . = E (x,y)?Pe(X,Y ) (f ? (x), y)</p><p>We refer to members of the set {R e |e ? E} as the training risks or simply risks. Changes in P e (X, Y ) can be categorized as either changes in P (X) (covariate shift), changes in P (Y |X) (concept shift), or a combination. The standard approach to learning problems is Empirical Risk Minimization (ERM), which minimizes the average loss across all the training examples from all the domains:</p><formula xml:id="formula_3">R ERM (?) . = E (x,y)?? e?E De (f ? (x), y)<label>(3)</label></formula><p>= e |D e |E (x,y)?De (f ? (x), y) (4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Robust Optimization</head><p>An approach more taylored to OOD generalization is robust optimization <ref type="bibr" target="#b7">(Ben-Tal et al., 2009)</ref>, which aims to optimize a model's worst-case performance over some perturbation set of possible data distributions, F (see Eqn. 1). When only a single training domain is available (singlesource domain generalization), it is common to assume that P (Y |X) is fixed, and let F be all distributions within some f -divergence ball of the training P (X) <ref type="bibr">(Hu et al., 2016;</ref><ref type="bibr" target="#b3">Bagnell, 2005)</ref>. As another example, adversarial robustness can be seen as instead using a Wasserstein ball as a perturbation set <ref type="bibr">(Sinha et al., 2017)</ref>. The assumption that P (Y |X) is fixed is commonly called the "covariate shift assumption" <ref type="bibr" target="#b6">(Ben-David et al., 2010b)</ref>; however, we assume that covariate shift and concept shift can co-occur, and refer to this assumption as the fixed relationship assumption (FRA).</p><p>In  <ref type="bibr">(Hu et al., 2016)</ref>, DRO yields a much lower dimensional perturbation set, with at most one direction of variation per domain, regardless of the dimensionality of X and Y . It also does not rely on FRA, and can provide robustness to any form of shift in P (X, Y ) which occurs across training domains. Minimax-REx is an extension of this approach to affine combinations of training risks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Invariant representations vs. invariant predictors</head><p>An equipredictive representation, ?, is a function of X with the property that P e (Y |?) is equal, ?e ? F. In other words, the relationship between such a ? and Y is fixed across domains. Invariant relationships between X and Y are then exactly those that can be written as P (Y |?(x)) with ? an equipredictive representation. A modelP (Y |X = x) that learns such an invariant relationship is called an invariant predictor. Intuitively, an invariant predictor works equally well across all domains in F. The principle of risk extrapolation aims to achieve invariant prediction by enforcing such equality across training domains E, and does not rely on explicitly learning an equipredictive representation.</p><p>Koyama &amp; Yamaguchi (2020) prove that a maximal equipredictive representation -that is, one that maximizes mutual information with the targets, ? * . = argmax ? I(?, Y ) -solves the robust optimization problem (Eqn. 1) under fairly general assumptions. 3 When ? * is unique, we call the features it ignores spurious. The result of Koyama &amp; Yamaguchi (2020) provides a theoretical reason for favoring invariant prediction over the common approach of learning invariant representations (Pan et al., 2010), which make P e (?) or P e (?|Y ) equal ?e ? E. Popular methods here include adversarial domain adaptation (ADA)  and conditional ADA (C-ADA) <ref type="bibr" target="#b27">(Long et al., 2018)</ref>. Unlike invariant predictors, invariant representations can easily fail to generalize OOD: ADA forces the predictor to have the same marginal prediction? P (Y ), which is a mistake when P (Y ) in fact changes across domains <ref type="bibr" target="#b27">(Zhao et al., 2019)</ref>; C-ADA suffers from more subtle issues .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Invariance and causality</head><p>The relationship between cause and effect is a paradigmatic example of an invariant relationship. Here, we summarize definitions from causal modeling, and discuss causal approaches to domain generalization. We will refer to these definitions for the statements of our theorems in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definitions.</head><p>A causal graph is a directed acyclic graph (DAG), where nodes represent variables and edges point from causes to effects. In this work, we use Structural Causal Models (SCMs), which also specify how the value of a variable is computed given its parents. An SCM, C, is defined by specifying the mechanism, f Z : P a(Z) ? Training accuracies (left) and risks (right) on colored MNIST domains with varying P (Y = 0|color = red) after 500 epochs. Dots represent training risks, lines represent test risks on different domains. Increasing the V-REx penalty (?) leads to a flatter "risk plane" and more consistent performance across domains, as the model learns to ignore color in favor of shape-based invariant prediction. Note that ? = 100 gives the best worst-case risk across the 2 training domains, and so would be the solution preferred by <ref type="bibr">DRO (Sagawa et al., 2019)</ref>. This demonstrates that REx's counter-intuitive propensity to increase training risks can be necessary for good OOD performance. dom(Z) for each variable Z. 4 Mechanisms are deterministic; noise in Z is represented explicitly via a special noise variable N Z , and these noise variables are jointly independent. An intervention, ? is any modification to the mechanisms of one or more variables; an intervention can introduce new edges, so long as it does not introduce a cycle. do(X i = x) denotes an intervention which sets X i to the constant value x (removing all incoming edges). Data can be generated from an SCM, C, by sampling all of the noise variables, and then using the mechanisms to compute the value of every node whose parents' values are known. This sampling process defines an entailed distribution, P C (Z) over the nodes Z of C. We overload f Z , letting f Z (Z) refer to the conditional distribution P C (Z|Z \ {Z}).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">CAUSAL APPROACHES TO DOMAIN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GENERALIZATION</head><p>Instead of assuming P (Y |X) is fixed (FRA), works that take a causal approach to domain generalization often assume that the mechanism for Y is fixed; we call this the fixed mechanism assumption (FMA). Meanwhile, they assume X may be subject to different (e.g. arbitrary) interventions in different domains <ref type="bibr" target="#b8">(B?hlmann, 2018</ref> A more similar method to REx is Invariant Risk Minimization (IRM) , which shares properties (1) and (2) of the list above. Like REx, IRM also uses a weaker form of invariance than ICP; namely, they insist that the optimal linear classifier must match across domains. 5 Still, REx differs significantly from IRM. While IRM specifically aims for invariant prediction, REx seeks robustness to whichever forms of distributional shift are present. Thus, REx is more directly focused on the problem of OOD generalization, and can provide robustness to a wider variety of distributional shifts, inluding covariate shift. Also, unlike REx, IRM seeks to match E(Y |?(X)) across domains, not the full P (Y |?(X)). This, combined with IRM's indifference to covariate shift, make it more effective in cases where different domains or examples are inherently more noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Fairness</head><p>Equalizing risk across different groups (e.g. male vs. female) has been proposed as a definition of fairness <ref type="bibr" target="#b12">(Donini et al., 2018)</ref>, generalizing the equal opportunity definition of fairness <ref type="bibr" target="#b21">(Hardt et al., 2016)</ref>. <ref type="bibr">Williamson &amp; Menon (2019)</ref> propose using the absolute difference of risks to measure deviation from this notion of fairness; this corresponds to our MM-REx, in the case of only two domains, and is similar to V-REx, which uses the variance of risks. However, in the context of fairness, equalizing the risk of training groups is the goal. Our work goes beyond this by showing that it can serve as a method for OOD generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Risk Extrapolation</head><p>Before discussing algorithms for REx and theoretical results, we first expand on our high-level explanations of what REx does, what kind of OOD generalization it promotes, and how. The principle of Risk Extrapolation (REx) has two aims:</p><p>1. Reducing training risks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Increasing similarity of training risks</head><p>In general, these goals can be at odds with each other; decreasing the risk in the domain with the lowest risk also decreases the overall similarity of training risks. Thus methods for REx may seek to increase risk on the best performing domains. While this is counter-intuitive, it can be necessary to achieve good OOD generalization, as <ref type="figure">Figure 2</ref> demonstrates. From a geometric point of view, encouraging equality of risks flattens the "risk plane" (the affine span of the training risks, considered as a function of the data distribution, see <ref type="figure" target="#fig_0">Figures 1</ref> and 2). While this can result in higher training risks, it also means that the risk changes less if the distributional shifts between training domains are magnified at test time. <ref type="figure">Figure 2</ref> illustrates how flattening the risk plane can promote OOD generalization on real data, using the Colored MNIST (CMNIST) task as an example . In the CMNIST training domains, the color of a digit is more predictive of the label than the shape is. But because the correlation between color and label is not invariant, predictors that use the color feature achieve different risk on different domains. By enforcing equality of risks, REx prevents the model from using the color feature enabling successful generalization to the test domain where the correlation between color and label is reversed.</p><p>Probabilities vs. Risks. <ref type="figure">Figure 3</ref> depicts how the extrapolated risks considered in MM-REx can be translated into a corresponding change in P (X, Y ), using an example of pure covariate shift. Training distributions can be thought of as points in an affine space with a dimension for every possible value of (X, Y ); see Appendix C.1 for an example. Because the risk is linear w.r.t. P (x, y), a convex combination of risks from different domains is equivalent to the risk on a domain given by the mixture of their distributions. The same holds for the affine combinations used in MM-REx, with the caveat that the negative coefficients may lead to negative probabilities, making the resulting P (X, Y ) a quasiprobability distribution, i.e. a signed measure with integral 1. We explore the theoretical implications of this in Appendix E.</p><p>x P(x)</p><formula xml:id="formula_4">Pe 1 (x) Pe 2 (x) x P(x)</formula><p>interpolation extrapolation <ref type="figure">Figure 3</ref>. Extrapolation can yield a distribution with negative P (x) for some x. Left: P (x) for domains e1 and e2. Right: Point-wise interpolation/extrapolation of P e 1 (x) and P e 2 (x). Since MM-REx target worst-case robustness across extrapolated domains, it can provide robustness to such shifts in P(X) (covariate shift).</p><p>Covariate Shift. When only P (X) differs across domains (i.e. FRA holds), as in <ref type="figure">Figure 3</ref>, then ?(x) = x is already an equipredictive representation, and so any predictor is an invariant predictor. Thus methods which only promote invariant prediction -such as IRM -are not expected to improve OOD generalization (compared with ERM). Indeed, <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Methods of Risk Extrapolation</head><p>We now formally describe the Minimax REx (MM-REx) and Variance-REx (V-REx) techniques for risk extrapolation. Minimax-REx performs robust learning over a perturbation set of affine combinations of training risks with bounded coefficients:</p><formula xml:id="formula_5">R MM-REx (?) . = max ?e?e=1 ?e??min m e=1 ? e R e (?) (6) = (1 ? m? min ) max e R e (?) + ? min m e=1 R e (?) ,<label>(7)</label></formula><p>where m is the number of domains, and the hyperparameter ? min controls how much we extrapolate. For negative values of ? min , MM-REx places negative weights on the risk of all but the worst-case domain, and as ? min ? ??, this criterion enforces strict equality between training risks; ? min = 0 recovers risk interpolation (RI). Thus, like RI, MM-REx aims to be robust in the direction of variations in P (X, Y ) between test domains. However, negative coefficients allow us to extrapolate to more extreme variations. Geometrically, larger values of ? min expand the perturbation set farther away from the convex hull of the training risks, encouraging a flatter "risk-plane" (see <ref type="figure">Figure 2</ref>).</p><p>While MM-REx makes the relationship to RI/RO clear, we found using the variance of risks as a regularizer (V-REx) simpler, stabler, and more effective:</p><formula xml:id="formula_6">R V-REx (?) . = ? Var({R 1 (?), ..., R m (?)}) + m e=1 R e (?)<label>(8)</label></formula><p>Here ? ? [0, ?) controls the balance between reducing average risk and enforcing equality of risks, with ? = 0 recovering ERM, and ? ? ? leading V-REx to focus entirely on making the risks equal. See Appendix for the relationship between V-REx and MM-REx and their gradient vector fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Theoretical Conditions for REx to Perform Causal</head><p>Discovery We now prove that exactly equalizing training risks (as incentivized by REx) leads a model to learn the causal mechanism of Y under assumptions similar to those of Peters et al. <ref type="bibr">(2016)</ref>, namely:</p><p>1. The causes of Y are observed, i.e. P a(Y ) ? X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Domains correspond to interventions on X.</head><p>3. Homoskedasticity (a slight generalization of the additive noise setting assumed by Peters et al. <ref type="formula" target="#formula_0">(2016)</ref>). We say an SEM C is homoskedastic (with respect to a loss function ), if the Bayes error rate of (f</p><formula xml:id="formula_7">Y (x), f Y (x))</formula><p>is the same for all x ? X . 6</p><p>The contribution of our theory (vs. ICP) is to prove that equalizing risks is sufficient to learn the causes of Y . In contrast, they insist that the entire distribution of error residuals (in predicting Y ) be the same across domains. We provide proof sketches here and complete proofs in the appendix.</p><p>Theorem 1 demonstrates a practical result: we can identify a linear SEM model using REx with a number of domains linear in the dimensionality of X. <ref type="bibr">6</ref> Note that our definitions of homoskedastic/heteroskedastic do not correspond to the types of domains constructed in <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref>, Section 5.1, but rather are a generalization of the definitions of these terms as commonly used in statistics. Specifically, for us, heteroskedasticity means that the "predicatability" (e.g. variance) of Y differs across inputs x, whereas for <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref>, it means the predicatability of Y at a given input varies across domains; we refer to this second type as domainhomo/heteroskedasticity for clarity. Theorem 1. Given a Linear SEM,</p><formula xml:id="formula_8">X i ? j =i ? (i,j) X j + ? i , with Y .</formula><p>= X 0 , and a predictor f ? (X) . = j:j&gt;0 ? j X j + ? j that satisfies REx (with mean-squared error) over a perturbation set of domains that contains 3 distinct do() interventions for each X i : i &gt; 0. Then ? j = ? 0,j , ?j.</p><p>Proof Sketch. We adapt the proof of Theorem 4i from <ref type="bibr">Peters et al. (2016)</ref>. They show that matching the residual errors across observational and interventional domains forces the model to learn f Y . We use the weaker condition of matching risks to derive a quadratic equation that the do() interventions must satisfy for any model other than f Y . Since there are at most 2 solutions to a quadratic equation, insisting on equality of risks across 3 distinct do() interventions forces the model to learn f Y .</p><p>Given the assumption that a predictor satisfies REx over all interventions that do not change the mechanism of Y , we can prove a much more general result. We now consider an arbitrary SCM, C, generating Y and X, and let E I be the set of domains corresponding to arbitrary interventions on X, similarly to <ref type="bibr">Peters et al. (2016)</ref>.</p><p>Theorem 2. Suppose is a (strictly) proper scoring rule. Then a predictor that satisfies REx for a over E I uses f Y (x) as its predictive distribution on input x for all x ? X .</p><p>Proof Sketch. Since the distribution of Y given its parents doesn't depend on the domain, f Y can make reliable point-wise predictions across domains. This translates into equality of risk across domains when the overall difficulty of the examples is held constant across domains, e.g. by assuming homoskedasticity. 7 While a different predictor might do a better job on some domains, we can always find an domain where it does worse than f Y , and so f Y is both unique and optimal.</p><p>Remark. Theorem 2 is only meant to provide insight into how the REx principle relates to causal invariance; the perturbation set in this theorem is uncountably infinite. Note, however, that even in this setting, the ERM principle does not, in general, recover the causal mechanism for Y . Rather, the ERM solution depends on the distribution over domains. For instance, if all but an ? 0 fraction of the data comes from the CMNIST training domains, then ERM will learn to use the color feature, just as in original the CMNIST task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate REx and compare with IRM on a range of tasks requiring OOD generalization. REx provides generalization benefits and outperforms IRM on a wide range of tasks, including: i) variants of the Colored MNIST (CM-NIST) dataset  with covariate shift, ii) continuous control tasks with partial observability and</p><formula xml:id="formula_9">0.0 0.1 0.2 0.3 0.4 0.5 p = P(shape(x) {0,1,2,3,4}) 0.2 0.3 0.4 0.5 0.6 Test Accuracy Random Guessing V-REx IRMv1 0.0 0.1 0.2 0.3 0.4 0.5 p = P(shape(x) {1,2} {6,7}) 0.2 0.3 0.4 0.5 0.6 Test Accuracy Random Guessing V-REx IRMv1 0.0 0.1 0.2 0.3 0.4 0.5 p = P(R1|Red) = P(G1|Green) 0.1 0.2 0.3 0.4 0.5 0.6</formula><p>Test Accuracy Random Guessing V-REx IRMv1 <ref type="figure">Figure 4</ref>. REx outperforms IRM on Colored MNIST variants that include covariate shift. The x-axis indexes increasing amount of shift between training distributions, with p = 0 corresponding to disjoint supports. Left: class imbalance, Center: shape imbalance, Right: color imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>train acc test acc V-REx (ours) 71.5 ? 1.0 68.7 ? 0.9 IRM 70.8 ? 0.9 66.9 ? 2.5 MM-REx (ours)</p><p>72.4 ? 1.8 66. spurious features, iii) domain generalization tasks from the DomainBed suite <ref type="bibr" target="#b18">(Gulrajani &amp; Lopez-Paz, 2020</ref>). On the other hand, when the inherent noise in Y varies across environments, IRM succeeds and REx performs poorly. <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref> construct a binary classification problem (with 0-4 and 5-9 each collapsed into a single class) based on the MNIST dataset, using color as a spurious feature. Specifically, digits are either colored red or green, and there is a strong correlation between color and label, which is reversed at test time. The goal is to learn the causal "digit shape" feature and ignore the anti-causal "digit color" feature. The learner has access to three domains:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Colored MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">A training domain where green digits have a 80%</head><p>chance of belonging to class 1 (digits 5-9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A training domain where green digits have a 90% chance of belonging to class 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>A test domain where green digits have a 10% chance of belonging to class 1.</p><p>We use the exact same hyperparameters as <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref>, only replacing the IRMv1 penalty with MM-REx or V-REx penalty. 8 These methods all achieve similar perfor-8 When there are only 2 domains, MM-REx is equivalent to a mance, see <ref type="table">Table 2</ref>.</p><p>CMNIST with covariate shift. To test our hypothesis that REx should outperform IRM under covariate shift, we construct 3 variants of the CMNIST dataset. Each variant represents a different way of inducing covariate shift to ensure differences across methods are consistent. These experiments combine covariate shift with interventional shift, since P (Green|Y = 1) still differs across training domains as in the original CMNIST.</p><p>1. Class imbalance: varying p = P (shape(x) ? {0, 1, 2, 3, 4}); as in Wu et al. (2020).</p><p>2. Digit imbalance: varying p = P (shape(x) ? {1, 2} ? {6, 7}); digits 0 and 5 are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Color imbalance:</head><p>We use 2 versions of each color, for 4 total channels: R 1 , R 2 , G 1 , G 2 . We vary p = P (R 1 |Red) = P (G 1 |Green).</p><p>While (1) also induces change in P (Y ), <ref type="formula" target="#formula_2">(2)</ref> and <ref type="formula" target="#formula_3">(3)</ref> induce only covariate shift in the causal shape and anti-causal color features (respectively). We compare across several levels of imbalance, p ? [0, 0.5], using the same hyperparameters from <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref>, and plot the mean and standard error over 3 trials.</p><p>V-REx significantly outperforms IRM in every case, see <ref type="figure">Figure 3</ref>.2. In order to verify that these results are not due to bad hyperparameters for IRM, we perform a random search that samples 340 unique hyperparameter combinations for each value of p, and compare the the number of times each method achieves better than chance-level (50% accuracy). Again, V-REx outperforms IRM; in particular, for small values of p, IRM never achieves better than random chance performance, while REx does better than random in 4.4%/23.7%/2.0% of trials, respectively, in the class/digit/color imbalance scenarios for p = 0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm ColoredMNIST</head><formula xml:id="formula_10">VLCS PACS OfficeHome ERM 52.0 ? 0.1 77.4 ? 0.3 85.7 ? 0.5 67.5 ? 0.5 IRM 51.8 ? 0.1 78.1 ? 0.0 84.4 ? 1.1 66.6 ? 1.0 V-REx 52.1 ? 0.1</formula><p>77.9 ? 0.5 85.8 ? 0.6 66.7 ? 0.5 <ref type="table">Table 3</ref>. REx, IRM, and ERM all perform comparably on a set of domain generalization benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Toy Structural Equation Models (SEMs)</head><p>REx's sensitivity to covariate shift can also be a weakness when reallocating capacity towards domains with higher risk does not help the model reduce their risk, e.g. due to irreducible noise. We illustrate this using the linear-Gaussian structural equation model (SEM) tasks introduced by Arjovsky et al. <ref type="bibr">(2019)</ref>. Like CMNIST, these SEMs include spurious features by construction. They also introduce 1) heteroskedasticity, 2) hidden confounders, and/or 3) elements of X that contain a mixture of causes and effects of Y . These three properties highlight advantages of IRM over ICP <ref type="bibr">(Peters et al., 2016)</ref>, as demonstrated empirically by <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref>. REx is also able to handle <ref type="formula" target="#formula_2">(2)</ref> and <ref type="formula" target="#formula_3">(3)</ref>, but it performs poorly in the heteroskedastic tasks. See Appendix G.2 for details and  <ref type="figure" target="#fig_2">Figure 5</ref>. We average over 10 runs on finger_spin and walker_walk, using hyperparameters tuned on cartpole_swingup (to avoid overfitting). See Appendix for details and further results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have demonstrated that REx, a method for robust optimization, can provide robustness and hence out-ofdistribution generalization in the challenging case where X contains both causes and effects of Y . In particular, like IRM, REx can perform causal identification, but REx can also perform more robustly in the presence of covariate shift. Covariate shift is known to be problematic when models are misspecified, when training data is limited, or does not cover areas of the test distribution. As such situations are inevitable in practice, REx's ability to outperform IRM in scenarios involving a combination of covariate shift and interventional shift makes it a powerful approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix Overview</head><p>Our code is available online at: https://anonymous.4open.science/r/12747e81-8505-43cb-b54e-e75e2344a397/. The sections of our appendix are as follows: </p><formula xml:id="formula_11">A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Definition and discussion of extrapolation in machine learning</head><p>We define interpolation and extrapolation as follows: interpolation refers to making decisions or predictions about points within the convex hull of the training examples and extrapolation refers to making decisions or predictions about points outside their convex hull. 9 This generalizes the familiar sense of these terms for one-dimensional functions. An interesting consequence of this definition is: for data of high intrinsic dimension, generalization requires extrapolation <ref type="bibr" target="#b22">(Hastie et al., 2009)</ref>, even in the i.i.d. setting. This is because the volume of high-dimensional manifolds concentrates near their boundary; see <ref type="figure">Figure 6</ref>.</p><p>Extrapolation in the space of risk functions. The same geometric considerations apply to extrapolating to new domains. Domains can be highly diverse, varying according to high dimensional attributes, and thus requiring extrapolation to generalize across. Thus Risk Extrapolation might often do a better job of including possible test domains in its perturbation set than Risk Interpolation does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training points</head><p>Test point <ref type="figure">Figure 6</ref>. Illustration of the importance of extrapolation for generalizing in high dimensional space. In high dimensional spaces, mass concentrates near the boundary of objects. For instance, the uniform distribution over a ball in N + 1-dimensional space can be approximated by the uniform distribution over the N -dimensional hypersphere. We illustrate this in 2 dimensions, using the 1-sphere (i.e. the unit circle). Dots represent a finite training sample, and the shaded region represents the convex hull of all but one member of the sample. Even in 2 dimensions, we can see why any point from a finite sample from such a distribution remains outside the convex hull of the other samples, with probability 1. The only exception would be if two points in the sample coincide exactly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Illustrative examples of how REx works in toy settings</head><p>Here, we work through two examples to illustrate:</p><p>1. How to understand extrapolation in the space of probability density/mass functions (PDF/PMFs) 2. How REx encourages robustness to covariate shift via distributing capacity more evenly across possible input distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. 6D example of REx</head><p>Here we provide a simple example illustrating how to understand extrapolations of probability distributions. Suppose X ? {0, 1, 2} and Y ? {0, 1}, so there are a total of 6 possible types of examples, and we can represent their distributions in a particular domain as a point in 6D space: (P (0, 0), P (0, 1), P (1, 0), P (1, 1), P (2, 0), P (2, 1)). Now, consider three domains e 1 , e 2 , e 3 given by</p><formula xml:id="formula_12">1. (a, b, c, d, e, f ) 2. (a, b, c, d, e ? k, f + k) 3. (2a, 2b, c(1 ? a+b c+d ), d(1 ? a+b c+d ), e, f )</formula><p>The difference between e 1 and e 2 corresponds to a shift in P (Y |X = 2), and suggests that Y cannot be reliably predicted across different domains when X = 2. Meanwhile, the difference between e 1 and e 3 tells us that the relative probability of X = 0 vs. X = 1 can change, and so we might want our model to be robust to these sorts of covariate shifts. Extrapolating risks across these 3 domains effectively tells the model: "don't bother trying to predict Y when X = 2 (i.e. aim for P (Y = 1|X = 2) = .5), and split your capacity equally across the X = 0 and X = 1 cases". By way of comparison, IRM would also aim forP (Y = 1|X = 2) = .5, whereas ERM would aim forP (Y = 1|X = 2) = 3f +k 3e+3f (assuming |D 1 | = |D 2 | = |D 3 |). And unlike REx, both ERM and IRM would split capacity between X = 0/1/2 cases according to their empirical frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Covariate shift example</head><p>We now give an example to show how REx provides robustness to covariate shift. Covariate shift is an issue when a model has limited capacity or limited data.</p><p>Viewing REx as robust learning over the affine span of the training distributions reveals its potential to improve robustness to distribution shifts. Consider a situation in which a model encounters two types of inputs: COSTLY inputs with probability q and CHEAP inputs with probability 1 ? q. The model tries to predicts the input -it outputs COSTLY with probability p and CHEAP with probability 1 ? p. If the model predicts right its risk is 0, but if it predicts COSTLY instead of CHEAP it gets a risk u = 2, and if it predicts CHEAP instead of COSTLY it gets a risk v = 4. The risk has expectation R q (p) = (1 ? p)(1 ? q)u + pqv. We have access to two domains with different input probabilities q 1 &lt; q 2 . This is an example of pure covariate shift.</p><p>We want to guarantee the minimal risk over the set of all possible domains:</p><formula xml:id="formula_13">min p?[0,1] max q?[0,1] R q (p) = (1 ? p)(1 ? q)u + pqv</formula><p>as illustrated in <ref type="figure">Figure 7</ref>. The saddle point solution of this problem is p = ? = u /u+v and R q (p) = uv /u+v, ?q. From the figure we see that R q1 (p) = R q2 (p) can only happen for p = ?, so the risk extrapolation principle will return the minimax optimal solution.</p><p>If we use ERM to minimize the risk, we will pool together the domains into a new domain with COSTLY input probabilit? q = (q 1 + q 2 )/2. ERM will return p = 0 ifq &gt; ? and p = 1 otherwise. Risk interpolation (RI) min p max q?{q1,q2} R q (p) will predict p = 0 if q 1 , q 2 &gt; ?, p = 1 if q 1 , q 2 &lt; ? and p = ? if q 1 &lt; ? &lt; q 2 . We see that only REx finds the minimax optimum for arbitrary values of q 1 and q 2 . </p><formula xml:id="formula_14">4.0 R q (p) R (p) max q R q (p) ( , R( )) Figure 7.</formula><p>Each grey line is a risk Rq(p) as functions of p for a specific value of q. The blue line is when q = ?. We highlight in red the curve maxq Rq(p) whose minimum is the saddle point marked by a purple star in p = ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. A summary of different types of causal models</head><p>Here, we briefly summarize the differences between 3 different types of causal models, see <ref type="table" target="#tab_8">Table 4</ref>. Our definitions and notation follow Elements of Causal Inference: Foundations and Learning Algorithms (Peters et al., 2017).</p><p>A Causal Graph is a directed acyclic graph (DAG) over a set of nodes corresponding to random variables Z, where edges point from causes (including noise variables) to effects. A Structural Causal Model (SCM), C, additionally specifies a deterministic mapping f Z for every node Z, which computes the value of that node given the values of its parents, which include a special noise variable N Z , which is sampled independently from all other nodes. This f Z is called the mechanism, structural equation, or structural assignment for Z. Given an SCM, C, the entailed distribution of C, P C (Z) is defined via ancestral sampling. Thus for any Z ? Z, we have that the marginal distribution P C (Z|Z \ Z) = P C (Z|P a(Z)).</p><p>A Causal Graphical Model (CGM) can be thought of as specifying these marginal distributions without explicitly representing noise variables N Z . We can draw rough analogies with (non-causal) statistical models. Roughly speaking, Causal Graphs are analogous to Graphical Models, whereas SCMs and CGMs are analogous to joint distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Independences Distributions Interventions Counterfactuals</head><p>Graphical Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Distribution</head><p>Causal Graph</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal Graphical Model</head><p>Structural Causal Model In practice, it may be advantageous to trade-off these two objectives, using a hyperparameter (e.g. ? for V-REx or ? min for MM-REx). However, in this section, we assume the 2nd criteria takes priority; i.e. we define "satisfying" the REx principle as selecting a minimal risk predictor among those that achieve exact equality of risks across all the domains in a set E.</p><p>Recall our assumptions from Section 3.2 of the main text:</p><p>1. The causes of Y are observed, i.e. P a(Y ) ? X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Domains correspond to interventions on X.</head><p>3. Homoskedasticity (a slight generalization of the additive noise setting assumed by Peters et al. <ref type="formula" target="#formula_0">(2016)</ref>). We say an SEM C is homoskedastic (with respect to a loss function ), if the Bayes error rate of (f Y (x), f Y (x)) is the same for all x ? X .</p><p>And see Section 2.3 for relevant definitions and notation.</p><p>We begin with a theorem based on the setting explored by Peters et al. <ref type="bibr">(2016)</ref>. Here, ? i . = N i are assumed to be normally distributed.</p><p>Theorem 1. Given a Linear SEM, X i ? j =i ? (i,j) X j + ? i , with Y . = X 0 , and a predictor f ? (X) . = j:j&gt;0 ? j X j + ? j that satisfies REx (with mean-squared error) over a perturbation set of domains that contains 3 distinct do() interventions for each X i : i &gt; 0. Then ? j = ? 0,j , ?j.</p><p>Proof. We adapt the proof of Theorem 4i from <ref type="bibr">Peters et al. (2016)</ref> to show that REx will learn the correct model under similar assumptions. Let Y ? ?X + ? be the mechanism for Y , assumed to be fixed across all domains, and let? = ?X be our predictor. Then the residual is R(?) = (? ? ?)X + ?. Define ? i . = ? i ? ? i , and consider an intervention do(X j = x) on the youngest node X j with ? j = 0. Then as in eqn 36/37 of Peters et al. <ref type="formula" target="#formula_0">(2016)</ref>, we compare the residuals R of this intervention and of the observational distribution:</p><formula xml:id="formula_15">R obs (?) = ? j X j + i =j ? i X i + ? R do(Xj =x) (?) = ? j x + i =j ? i X i + ?<label>(9)</label></formula><p>We now compute the MSE risk for both domains, set them equal, and simplify to find a quadratic formula for x:</p><formula xml:id="formula_16">E ? ? (? j X j + i =j ? i X i + ?) 2 ? ? = E ? ? (? j x + i =j ? i X i + ?) 2 ? ? (10) 0 = ? 2 j x 2 + 2? j E[ i =j ? i X i + ?]x ? E ? ? (? j X j ) 2 ? 2? j X j ( i =j ? i X i + ?) ? ?<label>(11)</label></formula><p>Since there are at most two values of x that satisfy this equation, any other value leads to a violation of REx, so that ? j needs to be zero -contradiction. In particular having domains with 3 different do-interventions on every X i guarantees that the risks are not equal across all domains.</p><p>Given the assumption that a predictor satisfies REx over all interventions that do not change the mechanism of Y , we can prove a much more general result. We now consider an arbitrary SCM, C, generating Y and X, and let E I be the set of domains corresponding to arbitrary interventions on X, similarly to Peters et al. <ref type="bibr">(2016)</ref>.</p><p>We emphasize that the predictor is not restricted to any particular class of models, and is a generic function f : X ? P(Y ), where P(Y ) is the set of distributions over Y . Hence, we drop ? from the below discussion and simply use f to represent the predictor, and R(f ) its risk. Theorem 2. Suppose is a (strictly) proper scoring rule. Then a predictor that satisfies REx for a over E I uses f Y (x) as its predictive distribution on input x for all x ? X .</p><p>Proof. Let R e (f, x) be the loss of predictor f on point x in domain e, and R e (f ) = P e (x) R e (f, x) be the risk of f in e.</p><p>Define ?(x) as the domain given by the intervention do(X = x), and note that R ?(x) (f ) = R ?(x) (f, x). We additionally define X 1 . = P ar(Y ).</p><p>The causal mechanism, f Y , satisfies the REx principle over</p><formula xml:id="formula_17">E I . For every x ? X , f Y (x) = P (Y |do(X = x)) = P (Y |do(X 1 = x 1 )) = P (Y |X 1 = x 1 ) is invariant (meaning 'independent of domain') by definition; P (Y |do(X = x)) = P (Y |do(X 1 = x 1 )) = P (Y |X 1 = x 1 )</formula><p>follows from the semantics of SEM/SCMs, and the fact that we don't allow f Y to change across domains. Specifically Y is always generated by the same ancestral sampling process that only depends on X 1 and N Y . Thus the risk of the predictor f</p><formula xml:id="formula_18">Y (x) at point x, R e (f Y , x) = (f Y (x), f Y (x)) is also invariant, soit R(f Y , x). Thus R e (f Y ) = P e (x) R e (f Y , x) = P e (x) R(f Y , x) is invariant whenever R(f Y , x)</formula><p>does not depend on x, and the homoskedasticity assumption ensures that this is the case. This establishes that setting f = f Y will produce equal risk across domains.</p><p>No other predictor satisfies the REx principle over E I . We show that any other g achieves higher risk than f Y for at least one domain. This demonstrates both that f Y achieves minimal risk (thus satisfying REx), and that it is the unique predictor which does so (and thus no other predictors satisfy REx). We suppose such a g exists and construct an domain where it achieves higher risk than f Y . Specifically, if g = f Y then let x ? X be a point such that g(x) = f Y (x). And since is a strictly proper scoring rule, this implies that</p><formula xml:id="formula_19">(g(x), f Y (x)) &gt; (f Y (x), f Y (x)). But (g(x), f Y (x))</formula><p>is exactly the risk of g on the domain ?(do(X = x)), and thus g achieves higher risk than f Y in ?(do(X = x)), a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. REx as DRO</head><p>We note that MM-REx is also performing robust optimization over a convex hull, see <ref type="figure" target="#fig_0">Figure 1</ref>. The corners of this convex hull correspond to "extrapolated domains" with coefficients (? min , ? min , ..., (1 ? (m ? 1)? min )) (up to some permutation). However, these domains do not necessarily correspond to valid probability distributions; in general, they are quasidistributions, which can assign negative probabilities to some examples. This means that, even if the original risk functions were convex, the extrapolated risks need not be. However, in the case where they are convex, then existing theorems, such as the convergence rate result of <ref type="bibr">(Sagawa et al., 2019)</ref>. This raises several important questions:</p><p>1. When is the affine combination of risks convex?</p><p>2. What are the effects of negative probabilities on the optimization problem REx faces, and the solutions ultimately found?</p><p>Negative probabilities: <ref type="figure" target="#fig_5">Figure 8</ref> illustrates this for a case where X = Z 2 2 , i.e. x is a binary vector of length 2. Suppose x 1 , x 2 are independent in our training domains, and represent the distribution for a particular domain by the point (P (X 1 = 1), P (X 2 = 1)). And suppose our 4 training distributions have (P (X 1 = 1), P (X 2 = 1)) equal to {(.4, .1), (.4, .9), (.6, .1), (.6, .9)}, with P (Y |X) fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. The relationship between MM-REx vs. V-REx, and the role each plays in our work</head><p>The MM-REx and V-REx methods play different roles in our work:</p><p>? We use MM-REx to illustrate that REx can be instantiated as a variant of robust optimization, specifically a generalization of the common Risk Interpolation approach. We also find MM-REx provides a useful geometric intuition, since we can visualize its perturbation set as an expansion of the convex hull of the training risks or distributions. ? We expect V-REx to be the more practical algorithm. It is simple to implement. And it performed better in our CMNIST experiments; we believe this may be due to V-REx providing a smoother gradient vector field, and thus more stable optimization, see <ref type="figure">Figure F</ref>.</p><p>Either method recovers the REx principle as a limiting case, as we prove in Section F.1. We also provide a sequence of mathematical derivations that sheds light on the relationship between MM-REx and V-REx in Section F.2 we can view these as a progression of steps for moving from the robust optimization formulation of MM-REx to the penalty term of V-REx:</p><p>1. From minimax to closed form: We show how to arrive at the closed-form version of MM-REx provided in Eqn. 7.</p><p>2. Closed form as mean absolute error: The closed form of MM-REx is equivalent to a mean absolute error (MAE) penalty term when there are only two training domains.</p><p>3. V-REx as mean squared error: V-REx is exactly equivalent to a mean squared error penalty term (always). Thus in the case of only two training domains, the difference between MM-REx and V-REx is just a different choice of norm. <ref type="figure">Figure 9</ref>. Vector fields of the gradient evaluated at different values of training risks R1(?), R2(?). We compare the gradients for RMM-REx (left) and RV-REx (right). Note that for RV-REx, the gradient vectors curve smoothly towards the direction of the origin, as they approach the diagonal (where training risks are equal); this leads to a smoother optimization landscape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. V-REx and MM-REx enforce the REx principle in the limit</head><p>We prove that both MM-REx and V-REx recover the constraint of perfect equality between risks in the limit of ? min ? ?? or ? ? ?, respectively. For both proofs, we assume all training risks are finite.</p><p>Proposition 1. The MM-REx risk of predictor f ? , R MM?REx (?) ? ? as ? min ? ?? unless R d = R e for all training domains d, e.</p><p>Proof. Suppose the risk is not equal across domains, and let the largest difference between any two training risks be &gt; 0. Then</p><formula xml:id="formula_20">R MM?REx (?) = (1 ? m? min ) max e R e (?) + ? min m i=1 R i (?) = max e R e (?) ? m? min max e R e (?) + ? min m i=1 R i (?) ? max e R e<label>(</label></formula><p>?) ? ? min , with the inequality resulting from matching up the m copies of ? min max e R e with the terms in the sum and noticing that each pair has a non-negative value (since R i ? max e R e is non-positive and ? min is negative), and at least one pair has the value ?? min . Thus sending ? ? ?? sends this lower bound on R MM?REx to ? and hence R MM?REx ? ? as well.</p><formula xml:id="formula_21">Proposition 2. The V-REx risk of predictor f ? , R V?REx (?) ? ? as ? ? ? unless R d = R e for all training domains d, e.</formula><p>Proof. Again, let &gt; 0 be the largest difference in training risks, and let ? be the mean of the training risks. Then there must exist an e such that |R e ? ?| ? /2. And thus V ar i (R i (?)) = i (R i ? ?) 2 ? ( /2) 2 , since all other terms in the sum are non-negative. Since &gt; 0 by assumption, the penalty term is positive and thus R V?REx (?) . = i R i (?) + ?V ar i (R i (?)) goes to infinity as ? ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Connecting MM-REx to V-REx</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.1. CLOSED FORM SOLUTIONS TO RISK INTERPOLATION AND MINIMAX-REX</head><p>Here, we show that risk interpolation is equivalent to the robust optimization objective of Eqn. 5. Without loss of generality, let R 1 be the largest risk, so R e ? R 1 , for all e. Thus we can express R e = R 1 ? d e for some non-negative d e , with d 1 = 0 ? d e for all e. And thus we can write the weighted sum of Eqn. 7 as:</p><formula xml:id="formula_22">R MM (?) . = max ?e?e=1 ?e??min m e=1 ? e R e (?)<label>(12)</label></formula><formula xml:id="formula_23">= max ?e?e=1 ?e??min m e=1 ? e (R 1 (?) ? d e )<label>(13)</label></formula><formula xml:id="formula_24">= R 1 (?) + max ?e?e=2 ?e??min m e=1 ?? e (d e )<label>(14)</label></formula><p>Now, since d e are non-negative, ?d e is non-positive, and the maximal value of this sum is achieved when ? e = ? min for all e ? 2, which also implies that ? 1 = 1 ? (m ? 1)? min . This yields the closed form solution provided in Eqn. 7. The special case of Risk Interpolation, where ? min = 0, yields Eqn. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.2. MINIMAX-REX AND MEAN ABSOLUTE ERROR REX</head><p>In the case of only two training risks, MM-REx is equivalent to using a penalty on the mean absolute error (MAE) between training risks. However, penalizing the pairwise absolute errors is not equivalent when there are m &gt; 2 training risks, as we show below. Without loss of generality, assume that R 1 &lt; R 2 &lt; ... &lt; R m . Then (1/2 of) the R MAE penalty term is:</p><formula xml:id="formula_26">i j?i (R i ? R j ) = mR m ? j?m R j + (m ? 1)R m?1 ? j?m?1 R j . . . (16) = j jR j ? j i?j R i (17) = j jR j ? j (m ? j + 1)R j (18) = j (2j ? m ? 1)R j<label>(19)</label></formula><p>For m = 2, we have 1/2R MAE = (2 * 1 ? 2 ? 1)R 1 + (2 * 2 ? 2 ? 1)R 2 = R 2 ? R 1 . Now, adding this penalty term with some coefficient ? MAE to the ERM term yields:</p><formula xml:id="formula_27">R MAE . = R 1 + R 2 + ? MAE (R 2 ? R 1 ) = (1 ? ? MAE )R 1 + (1 + ? MAE )R 2<label>(20)</label></formula><p>We wish to show that this is equal to R MM for an appropriate choice of learning rate ? MAE and hyperparameter ? MAE . Still assuming that R 1 &lt; R 2 , we have that:</p><formula xml:id="formula_29">R MM . = (1 ? ? min )R 2 + ? min R 1<label>(22)</label></formula><p>Choosing ? MAE = 1/2? MM is equivalent to multiplying R MM by 2, yielding:</p><formula xml:id="formula_30">2R MM . = 2(1 ? ? min )R 2 + 2? min R 1<label>(23)</label></formula><p>Now, in order for R MAE = 2R MM , we need that:</p><formula xml:id="formula_31">2 ? 2? min = 1 + ? MAE (24) 2? min = 1 ? ? MAE (25)<label>(26)</label></formula><p>And this holds whenever ? MAE = 1 ? 2? min . When m &gt; 2, however, these are not equivalent, since R MM puts equal weight on all but the highest risk, whereas R MAE assigns a different weight to each risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.3. PENALIZING PAIRWISE MEAN SQUARED ERROR (MSE) YIELDS V-REX</head><p>The V-REx penalty (Eqn. 8) is equivalent to the average pairwise mean squared error between all training risks (up to a constant factor of 2). Recall that R i denotes the risk on domain i. We have:</p><formula xml:id="formula_32">1 2n 2 i j (R i ? R j ) 2 = 1 2n 2 i j R 2 i + R 2 j ? 2R i R j (27) = 1 2n i R 2 i + 1 2n j R 2 j ? 1 n 2 i j R i R j (28) = 1 n i R 2 i ? 1 n i R i 2 (29) = Var(R) .<label>(30)</label></formula><p>G. Further results and details for experiments mentioned in main text G.1. CMNIST with covariate shift</p><p>Here we present the following additional results:</p><p>1. <ref type="figure" target="#fig_0">Figure 1</ref> of the main text with additional results using MM-REx, see G.1. These results used the "default" parameters from the code of <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A plot with results on these same tasks after performing a random search over hyperparameter values similar to that performed by <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref>.</p><p>3. A plot with the percentage of the randomly sampled hyperparameter combinations that have satisfactory (&gt; 50%) accuracy, which we count as "success" since this is better than random chance performance.</p><p>These results show that REx is able to handle greater covariate shift than IRM, given appropriate hyperparameters. Furthermore, when appropriately tuned, REx can outperform IRM in situations with covariate shift. The lower success rate of REx for high values of p is because it produces degenerate results (where training accuracy is less than test accuracy) more often.</p><p>The hyperparameter search consisted of a uniformly random search of 340 samples over the following intervals of the hyperparameters:</p><p>1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Accuracy</head><p>Random Guessing REx IRM <ref type="figure" target="#fig_0">Figure 11</ref>. This is <ref type="figure">Figure 3</ref>.2 of main text (class imbalance, digit imbalance, and color imbalance from left to right as described in "CMNIST with covariate shift" subsubsection of Section 4.1 in main text), but with hyperparameters of REx and IRM each tuned to perform as well as possible for each value of p for each covariate shift type.  <ref type="figure" target="#fig_0">Figure 12</ref>. This also corresponds to class imbalance, digit imbalance, and color imbalance from left to right as described in "CMNIST with covariate shift" subsubsection of Section 4.1 in main text; but now the y-axis refers to what percentage of the randomly sampled hyperparameter combinations we deemed to to be satisfactory. We define satisfactory as simultaneously being better than random guessing and having train accuracy greater than test accuracy. For p less than .5, a larger percentage of hyperparameter combinations are often satisfactory for REx than for IRM; for p greater than .5, a larger percentage of hyperparameter combinations are often satisfactory for IRM than for REx because train accuracy is greater than test accuracy for more hyperparameter combinations for IRM. We stipulate that train accuracy must be greater than test accuracy because test accuracy being greater than train accuracy usually means the model has learned a degenerate prediction rule such as "not color".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. SEMs from "Invariant Risk Minimization"</head><p>Here we present experiments on the (linear) structural equation model (SEM) tasks introduced by <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref>. <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref> construct several varieties of SEM where the task is to predict targets Y from inputs X 1 , X 2 , where X 1 are (non-anti-causal) causes of Y , and X 2 are (anti-causal) effects of Y . We refer the reader to Section 5.1 and <ref type="figure">Figure 3</ref> of <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref> for more details. We use the same experimental settings as <ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref> (except we only run 7 trials), and report results in <ref type="table" target="#tab_6">Table 5</ref>.</p><p>These experiments include several variants of a simple SEM, given by:</p><formula xml:id="formula_33">X 1 = N 1 Y = W 1?Y X 1 + N Y X 2 = W Y ?2 Y + N 2</formula><p>Where N 1 , N Y , N 2 are all sampled i.i.d. from normal distributions. The variance of these distributions may vary across domains.</p><p>While REx achieves good performance in the domain-homoskedastic case, it performs poorly in the domainheteroskedastic case, where the amount of intrinsic noise, ? 2 y in the target changes across domains. 10 Intuitively, this is because the irreducible error varies across domains in these tasks, meaning that the risk will be larger on some domains than others, even if the model's predictions match the expectation E(Y |P a(Y )). We tried using a "baseline" (see Eqn. 5) of r e = V ar(Y e ) <ref type="bibr">(Meinshausen et al., 2015)</ref> to account for the different noise levels in Y , but this did not work.</p><p>We include a mathematical analysis of the simple SEM given above in order to better understand why REx succeeds in the domain-homoskedastic, but not the domain-heteroskedastic case. Assuming that Y, X 1 , X 2 are scalars, this SEM becomes</p><formula xml:id="formula_34">X 1 = N 1 Y = w 1?y N 1 + N Y X 2 = w y?2 w 1?y N 1 + w y?2 N Y + N 2</formula><p>We consider learning a model? = ?X 1 + ?X 2 . Then the residual is:</p><formula xml:id="formula_35">Y ? Y = (? + w 1?y (?w y?2 ? 1))N 1 + (?w y?2 ? 1)N Y + ?N 2</formula><p>Since all random variables have zero mean, the MSE loss is the variance of the residual. Using the fact that the noise N 1 , N Y , N 2 are independent, this equals:</p><formula xml:id="formula_36">E[(? ? Y ) 2 ] = (? + w 1?y (?w y?2 ? 1)) 2 ? 2 1 + (?w y?2 ? 1) 2 ? 2 Y + ? 2 ? 2 2</formula><p>10 See Footnote 6. Thus when (only) ? 2 changes, the only way to keep the loss unchanged is to set the coefficient in front of ? 2 to 0, meaning ? = 0. By minimizing the loss, we then recover ? = w 1?y ; i.e. in the domain-homoskedastic setting, the loss equality constraint of REx yields the causal model. On the other hand, if (only) ? Y changes, then REx enforces ? = 1/w y?2 , which then induces ? = 0, recovering the anticausal model.</p><p>While <ref type="bibr">REx (like ICP (Peters et al., 2016)</ref>) assumes the mechanism for Y is fixed across domains (meaning P (Y |P a(Y )) is independent of the domain, e), IRM makes the somewhat weaker assumption that E(Y |P a(Y )) is independent of domain. While it is plausible that an appropriately designed variant of REx could work under this weaker assumption, we believe forbidding interventions on Y is not overly restrictive, and such an extension for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Reinforcement Learning Experiments</head><p>Here we provide details and further results on the experiments in Section 4.1. We take tasks from the Deepmind Control Suite <ref type="bibr">(Tassa et al., 2018)</ref> and modify the original state, s, to produce observation, o = (s + , ?s ) including noise and spurious features ?s , where s contains 1 or 2 dimensions of s. The scaling factor takes values ? = 1/2/3 for the two training and test domains, respectively. The agent takes o as input and learns a representation using Soft Actor-Critic <ref type="bibr" target="#b19">(Haarnoja et al., 2018)</ref> and an auxiliary reward predictor, which is trained to predict the next 3 rewards conditioned on the next 3 actions. Since the spurious features are copied from the state before the noise is added, they are more informative for the reward prediction task, but they do not have an invariant relationship with the reward because of the domain-dependent ?.</p><p>The hyperparameters used for training Soft Actor-Critic can be found in <ref type="table">Table 6</ref>. We used cartpole_swingup as a development task to tune the hyperparameters of penalty weight (chosen from [0.01, 0.1, 1, 10]) and number of iterations before the penalty is turned up (chosen from [5000, 10000, 20000]), both for REx and IRM. The plots with the hyperparameter sweep are in <ref type="figure" target="#fig_0">Figure 13</ref>.  <ref type="figure" target="#fig_0">Figure 13</ref>. Hyperparameter sweep for IRM and REx on cartpole_swingup. Green, blue, and orange curves correspond to REx, ERM, and IRM, respectively. The subfigure titles state the penalty strength ("penalty") and after how many iterations the penalty strength was increased ("iters"). We chose a penalty factor of 1 and 10k iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Experiments not mentioned in main text</head><p>We include several other experiments which do not contribute directly to the core message of our paper. Here is a summary of the take-aways from these experiments:</p><p>1. Our experiments in the CMNIST domain suggest that the IRM/V-REx penalty terms should be amplified exactly when the model starts overfitting training distributions.</p><p>2. Our financial indicators experiments suggest that IRM and REx often perform remarkably similarly in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1. A possible approach to scheduling IRM/REx penalties</head><p>We've found that REx and IRM are quite sensitive to the choice of hyperparameters. In particular, hyperparameters controlling the scheduling of the IRM/V-REx penalty terms are of critical importance. For the best performance, the penalty should be increased the relative weight of the penalty term after approximately 100 epochs of training (using a so-called "waterfall" schedule <ref type="bibr" target="#b11">(Desjardins et al., 2015)</ref>). See <ref type="figure" target="#fig_0">Figure 14</ref>(b) for a comparison. We also tried an exponential decay schedule instead of the waterfall and found the results (not reported) were significantly worse, although still above 50% accuracy.</p><p>Given the methodological constraints of out-of-distribution generalization mentioned in <ref type="bibr" target="#b18">(Gulrajani &amp; Lopez-Paz, 2020)</ref>, this could be a significant practical issue for applying these algorithms. We aim to address this limitation by providing a guideline for when to increase the penalty weight, based only on the training domains. We hypothesize that successful learning of causal features using REx or IRM should proceed in two stages:</p><p>1. In the first stage, predictive features are learned.</p><p>2. In the second stage, causal features are selected and/or predictive features are fine-tuned for stability.</p><p>This viewpoint suggests that we could use overfitting on the training tasks as an indicator for when to apply (or increase) the IRM or REx penalty.</p><p>The experiments presented in this section provide observational evidence consistent with this hypothesis. However, since the hypothesis was developed by observing patterns in the CMNIST training runs, it requires further experimental validation on a different task, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1.1. RESULTS AND INTERPRETATION</head><p>In <ref type="figure" target="#fig_0">Figure 14</ref>, we demonstrate that the optimal point to apply the waterfall in the CMNIST task is after predictive features have been learned, but before the model starts to memorize training examples. Before predictive features are available, the penalty terms push the model to learn a constant predictor, impeding further learning. And after the model starts to memorize, it become difficult to distinguish anti-causal and causal features. This second effect is because neural networks often have the capacity to memorize all training examples given sufficient training time, achieving and near-0 loss <ref type="bibr">(Zhang et al., 2016)</ref>. In the limits of this memorization regime, the differences between losses become small, and gradients of the loss typically do as well, and so the REx and IRMv1 penalties no longer provide a strong or meaningful training signal, see <ref type="figure" target="#fig_0">Figure 15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2. Domain Generalization: VLCS and PACS</head><p>Here we provide earlier experiments on the VLCS and PACS dataset. We removed these experiments from the main text of our paper in favor of the more complete DomainBed results.  <ref type="figure" target="#fig_0">Figure 14</ref>. Stability penalties should be applied around when traditional overfitting begins, to ensure that the model has learned predictive features, and that penalties still give meaningful training signals. Top: Test accuracy as a function of epoch at which penalty term weight is increased (learning rate is simultaneously decreased proportionally). Choosing this hyperparameter correctly is essential for good performance. Middle: Generalization gap on a validation set with 85% correlation between color and label (the same as the average training correlation). The best test accuracy is achieved by increasing the penalty when the generalization gap begins to increase. The increase clearly indicates memorization because color and shape are only 85%/75% correlated with the label, and so cannot be used to make predictions with higher than 85% accuracy. Bottom: Accuracy on training/test sets, as well as an auxilliary grayscale set. Training/test performance reach 85%/15% after a few epochs of training, but grayscale performance improves, showing that meaningful features are still being learned.</p><p>tuning on test distributions, we use VLCS to tune hyperparameters and then apply these exact same settings to PACS and report the final average over 10 runs on each domain.</p><p>We use the same architecture, training procedure and data augmentation strategy as the (formerly) state-of-the-art Jigsaw Puzzle approach <ref type="bibr" target="#b9">(Carlucci et al., 2019</ref>) (except with IRM or V-REx intead of JigSaw as auxilliary loss) for all three methods. As runs are very noisy, we ran each experiment 10 times, and report average test accuracies extracted at the time of the . This is because the model (a deep network) has sufficient capacity to fit the training sets almost perfectly. This prevents these penalties from having the intended effect, once the model has started to overfit. The y-axis is in log-scale. highest validation accuracy on each run. Results on PACS are in <ref type="table">Table 8</ref>. On PACS we found that REx outperforms IRM and IRM outperforms ERM on average, while all are worse than the state-of-the-art Jigsaw method.</p><p>We use all hyperparameters from the original Jigsaw codebase. <ref type="bibr">11</ref> We use Imagenet pre-trained AlexNet features and chose batch-size, learning rate, as well as penalty weights based on performance on the VLCS dataset where test performance on the holdout domain was used for the set of parameters producing the highest validation accuracy. The best performing parameters on VLCS were then applied to the PACS dataset without further changes. We searched over batch-sizes in {128, 384}, over penalty strengths in {0.0001, 0.001, 0.01, 0.1, 1, 10}, learning rates in {0.001, 0.01} and used average performance over all 4 VLCS domains to pick the best performing hyperparameters. <ref type="table" target="#tab_12">Table 7</ref> shows results on VLCS with the best performing hyperparameters.</p><p>The final parameters for all methods on PACS were a batch size of 384 with 30 epochs of training with Adam, using a learning rate of 0.001, and multiplying it by 0.1 after 24 epochs (this step schedule was taken from the Jigsaw repo).The penalty weight chosen for Jigsaw was 0.9; for IRM and REx it was 0.1.We used the same data-augmentation pipeline as the original Jigsaw code for ERM, IRM, Jigsaw and REx to allow for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VLCS</head><p>CALTECH SUN PASCAL LABELME Average  <ref type="table">Table 8</ref>. Accuracy (percent) of different methods on the PACS task. Results are test accuracy at the time of the highest validation accuracy, averaged over 10 runs. REx outperforms ERM on average, and performs similar to IRM and Jigsaw (the state-of-the-art). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3. Financial indicators</head><p>We find that IRM and REx seem to perform similarly across different splits of the data in a prediction task using financial data. The dataset is split into five years, 2014-18, containing 37 publicly reported financial indicators of several thousand publicly listed companies each. The task is to predict if a company's value will increase or decrease in the following year (see Appendix for dataset details.) We consider each year a different domain, and create 20 different tasks by selecting all possible combinations of domains where three domains represent the training sets, one domain the validation set, and another one the test set. We train an MLP using the validation set to determine an early stopping point, with ? = 10 4 . The per-task results summarized in <ref type="figure" target="#fig_0">fig. 16</ref> indicate substantial differences between ERM and IRM, and ERM and REx. The predictions produced by IRM and REx, however, only differ insignificantly, highlighting the similarity of IRM and REx. While performance on specific tasks differs significantly between ERM and IRM/REx, performance averaged over tasks is not significantly different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3.1. EXPERIMENT DETAILS</head><p>We use v1 of the dataset published on 12 and prepare the data as described in. <ref type="bibr">13</ref> We further remove all the variables that are not shared across all 5 years, leaving us with 37 features, and whiten the data through centering and normalizing by the standard deviation.</p><p>On each subtask, we train an MLP with two hidden layers of size 128 with tanh activations and dropout (p=0.5) after each layer. We optimize the binary cross-entropy loss using Adam (learning rate 0.001, ? 1 = 0.9, ? 2 = 0.999, = 10 ?8 ), and an L2 penalty (weight 0.001). In the IRM/REx experiments, the respective penalty is added to the loss (? = 1) and the original loss is scaled by a factor 10 ?4 after 1000 iterations. Experiments are run for a maximum of 9000 training iterations with early stopping based on the validation performance. All results are averaged over 3 trials. The overall performance of the different models, averaged over all tasks, is summarized in Tab. 9. The difference in average performance between ERM, IRM, and REx is not statistically significant, as the error bars are very large.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Left: Robust optimization optimizes worst-case performance over the convex hull of training distributions. Right: By extrapolating risks, REx encourages robustness to larger shifts. Here e1, e2, and e3 represent training distributions, and # ? P 1 (X, Y ), # ? P 2 (X, Y ) represent some particular directions of variation in the affine space of quasiprobability distributions over (X, Y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2. Training accuracies (left) and risks (right) on colored MNIST domains with varying P (Y = 0|color = red) after 500 epochs. Dots represent training risks, lines represent test risks on different domains. Increasing the V-REx penalty (?) leads to a flatter "risk plane" and more consistent performance across domains, as the model learns to ignore color in favor of shape-based invariant prediction. Note that ? = 100 gives the best worst-case risk across the 2 training domains, and so would be the solution preferred by DRO (Sagawa et al., 2019). This demonstrates that REx's counter-intuitive propensity to increase training risks can be necessary for good OOD performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Performance and standard error on walker_walk (top), finger_spin (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>) Appendix Overview B) Definition and discussion of extrapolation in machine learning C) Illustrative examples of how REx works in toy settings D) A summary of different types of causal model E) Theory F) The relationship between MM-REx vs. V-REx, and the role each plays in our work G) Further results and details for experiments mentioned in main text H) Experiments not mentioned in main text I) Overview of other topics related to OOD generalization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>The perturbation set for MM-REx can include "distributions" which assign invalid (e.g. negative) probabilities to some data-points. The range of valid distributions P (X) is shown in grey, and P (X) for 4 different training domains are shown as red points. The interior of the dashed line shows the perturbation set for ?min = ?1/2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>. HiddenDim = [2**7, 2**12] 2. L2RegularizerWeight = [10**-2, 10**-4] 3. Lr = [10**-2.8, 10**-4This isFigure 3.2 of main text with additional results using MM-REx. For each covariate shift variant (class imbalance, digit imbalance, and color imbalance from left to right as described in "CMNIST with covariate shift" subsubsection of Section 4.1 in main text) of CMNIST, the standard error (the vertical bars in plots) is higher for MM-REx than for V-REx.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 .</head><label>15</label><figDesc>Given sufficient training time, empirical risk minimization (ERM) minimizes both REx and IRMv1 penalty terms on Colored MNIST (without including either term in the loss function)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 16 .</head><label>16</label><figDesc>Financial indicators tasks. The left panel indicates the set of training domains; the middle and right panels show the test accuracy on the respective domains relative to ERM (a black dot corresponds to a training domain; a colored patch indicates the test accuracy on the respective domain.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>MethodInvariant Prediction Cov. Shift Robustness Suitable for Deep Learning A comparison of approaches for OOD generalization.</figDesc><table><row><cell>DRO</cell></row><row><cell>(C-)ADA</cell></row><row><cell>ICP</cell></row><row><cell>IRM</cell></row><row><cell>REx</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). We call changes in P (X, Y ) resulting from interventions on X interventional shift. Interventional shift can involve both covariate shift and/or concept shift. In their seminal work on Invariant Causal Prediction (ICP), Peters et al. (2016) leverage this invariance to learn which elements of X cause Y . ICP and its nonlinear extension (Heinze-Deml et al., 2018) use statistical tests to detect whether the residuals of a linear model are equal across domains. Our work differs from ICP</figDesc><table><row><cell>2. Our goal is OOD generalization, not causal inference.</cell></row><row><cell>These are not identical: invariant prediction can some-</cell></row><row><cell>times make use of non-causal relationships, but when</cell></row><row><cell>deciding which interventions to perform, a truly causal</cell></row><row><cell>model is called for.</cell></row><row><cell>in that:</cell></row><row><cell>1. Our method is model agnostic and scales to deep net-</cell></row><row><cell>works.</cell></row></table><note>3. Our learning principle only requires invariance of risks, not residuals. Nonetheless, we prove that this can ensure invariant causal prediction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>REx and IRM learn to ignore the spurious color feature. Strikethrough results achieved via tuning on the test set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>1 ? 1.5</cell><cell></cell></row><row><cell>RI</cell><cell></cell><cell>88.9 ? 0.3</cell><cell>22.3 ? 4.6</cell><cell></cell></row><row><cell>ERM</cell><cell></cell><cell>87.4 ? 0.2</cell><cell>17.1 ? 0.6</cell><cell></cell></row><row><cell cols="3">Grayscale oracle 73.5 ? 0.2</cell><cell>73.0 ? 0.4</cell><cell></cell></row><row><cell cols="2">Optimum</cell><cell>75</cell><cell>75</cell><cell></cell></row><row><cell cols="2">Chance</cell><cell>50</cell><cell>50</cell><cell></cell></row><row><cell>Table 2.</cell><cell>Accuracy</cell><cell>(percent)</cell><cell>on</cell><cell>Colored</cell></row><row><cell>MNIST.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>for results.4.3. Domain Generalization in the DomainBed SuiteMethodologically, it is inappropriate to assume access to the test environment in domain generalization settings, as the goal is to find methods which generalize to unknown test distributions.<ref type="bibr" target="#b18">Gulrajani &amp; Lopez-Paz (2020)</ref> introduced the DomainBed evaluation suite to rigorously compare existing approaches to domain generalization, and found that no method reliably outperformed ERM. We evaluate V-REx on DomainBed using the most commonly used trainingdomain validation set method for model selection. Due to limited computational resources, we limited ourselves to the 4 cheapest datasets. Results of baseline are taken from<ref type="bibr" target="#b18">Gulrajani &amp; Lopez-Paz (2020)</ref>, who compare with more methods. Results inTable 3give the average over 3 different train/valid splits.Finally, we turn to reinforcement learning, where covariate shift (potentially favoring REx) and heteroskedasticity (favoring IRM) both occur naturally as a result of randomness in the environment and policy. In order to show the benefits of invariant prediction, we modify tasks from the Deepmind Control Suite(Tassa et al., 2018)  to include spurious features in the observation, and train a Soft Actor-Critic<ref type="bibr" target="#b19">(Haarnoja et al., 2018)</ref> agent. REx outperforms both IRM and ERM, suggesting that REx's robustness to covariate shift outweighs the challenges it faces with heteroskedasticity in this setting, see</figDesc><table><row><cell>4.4. Reinforcement Learning with partial observability</cell></row><row><cell>and spurious features</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Hendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mazeika, M., and Dietterich, T. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606, 2018. Yang, Y., Song, Y.-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pp. 5542-5550, 2017. Li, Y., Tian, X., Gong, M., Liu, Y., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624-639, 2018. Lipton, Z. C., Wang, Y.-X., and Smola, A. Detecting and correcting for label shift with black box predictors. arXiv preprint arXiv:1802.03916, 2018. Meinshausen, N., B?hlmann, P., et al. Maximin effects in inhomogeneous large-scale data. The Annals of Statistics, 43(4):1801-1830, 2015. Pan, S. J., Tsang, I. W., Kwok, J. T., and Yang, Q. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks, 22(2):199-210, 2010. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adversarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167-7176, 2017. van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding, 2018. Wang, H., He, Z., Lipton, Z. C., and Xing, E. P. Learning robust representations by projecting superficial statistics out. arXiv preprint arXiv:1903.06256, 2019.</figDesc><table><row><cell>Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Using self-supervised learning can improve model robust-ness and uncertainty, 2019a. Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., and Lakshminarayanan, B. Augmix: A simple data processing method to improve robustness and uncertainty, 2019b. Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. Learning deep representations by mutual information estimation and maximization, 2018. Hu, W., Niu, G., Sato, I., and Sugiyama, M. Does distribu-tionally robust supervised learning give robust classifiers?, 2016. Ilse, M., Tomczak, J. M., and Forr?, P. Designing data augmentation for simulating interventions. arXiv preprint arXiv:2005.01856, 2020. Johansson, F. D., Sontag, D., and Ranganath, R. Support and invertibility in domain-invariant representations, 2019. Koyama, M. and Yamaguchi, S. Out-of-distribution gener-alization with maximal invariant predictor, 2020. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097-1105, 2012. Williamson, R. C. and Menon, A. K. Fairness risk measures, 2019. Wu, X., Guo, Y., Chen, J., Liang, Y., Jha, S., and Chalasani, P. Representation bayesian risk decompositions and multi-Li, D., Peters, J., B?hlmann, P., and Meinshausen, N. Causal in-ference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Soci-ety: Series B (Statistical Methodology), 78(5):947-1012, 2016. Peters, J., Janzing, D., and Sch?lkopf, B. Elements of causal inference: foundations and learning algorithms. 2017. agenet classifiers generalize to imagenet? arXiv preprint arXiv:1902.10811, 2019. Cer-Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do im-source domain adaptation, 2020.</cell></row><row><cell>tifying some distributional robustness with principled</cell></row><row><cell>adversarial training, 2017.</cell></row><row><cell>Su, J., Vargas, D. V., and Sakurai, K. One pixel attack</cell></row><row><cell>for fooling deep neural networks. IEEE Transactions on</cell></row><row><cell>Evolutionary Computation, 23(5):828-841, 2019.</cell></row><row><cell>Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y.,</cell></row><row><cell>de Las Casas, D., Budden, D., Abdolmaleki, A., Merel, J.,</cell></row><row><cell>Lefrancq, A., Lillicrap, T., and Riedmiller, M. DeepMind</cell></row><row><cell>control suite. Technical report, DeepMind, January 2018.</cell></row></table><note>Long, M., Cao, Z., Wang, J., and Jordan, M. I. Conditional adversarial domain adaptation. In Advances in Neural Information Processing Systems, pp. 1640-1650, 2018.Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case gener- alization, 2019. Sahoo, S. S., Lampert, C. H., and Martius, G. Learning equations for extrapolation and control, 2018. Sch?lkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., and Mooij, J. On causal and anticausal learning. In Proceedings of the 29th International Coference on Inter- national Conference on Machine Learning, ICML'12, pp. 459-466, Madison, WI, USA, 2012. Omnipress. ISBN 9781450312851. Shorten, C. and Khoshgoftaar, T. M. A survey on image data augmentation for deep learning. Journal of Big Data, 6(1):60, 2019. Sinha, A., Namkoong, H., Volpi, R., and Duchi, J.Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding, 2019. Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In CVPR 2011, pp. 1521-1528. IEEE, 2011.Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking general- ization. arXiv preprint arXiv:1611.03530, 2016. Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D.mixup: Beyond empirical risk minimization, 2017. Zhao, H., des Combes, R. T., Zhang, K., and Gordon, G. J. On learning invariant representation for domain adapta- tion, 2019.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>A comparison of causal and non-causal models.</figDesc><table><row><cell>E. Theory</cell></row><row><cell>E.1. Proofs of theorems 1 and 2</cell></row><row><cell>The REx principle (Section 3) has two goals:</cell></row><row><cell>1. Reducing training risks</cell></row><row><cell>2. Increasing similarity of training risks.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>Average mean-squared error between true and estimated weights on causal (X1) and non-causal (X2) variables. Top 2: When the level of noise in the anti-causal features varies across domains, REx performs well (FOU, FOS, POU, POS). Bottom 2: When the level of noise in the targets varies instead, REx performs poorly (FEU, FES, PEU, PES). Using the baselines re = V(Y ) does not solve the problem, and indeed, hurts performance on the homoskedastic domains.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Table 6. A complete overview of hyperparameters used for reinforcement learning experiments.</figDesc><table><row><cell></cell><cell>Value</cell></row><row><cell>Replay buffer capacity</cell><cell>1000000</cell></row><row><cell>Batch size</cell><cell>1024</cell></row><row><cell>Discount ?</cell><cell>0.99</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>Critic learning rate</cell><cell>10 ?5</cell></row><row><cell>Critic target update frequency</cell><cell>2</cell></row><row><cell>Critic Q-function soft-update rate ? Q</cell><cell>0.005</cell></row><row><cell>Critic encoder soft-update rate ? enc</cell><cell>0.005</cell></row><row><cell>Actor learning rate</cell><cell>10 ?5</cell></row><row><cell>Actor update frequency</cell><cell>2</cell></row><row><cell>Actor log stddev bounds</cell><cell>[?5, 2]</cell></row><row><cell>Encoder learning rate</cell><cell>10 ?5</cell></row><row><cell>Decoder learning rate</cell><cell>10 ?5</cell></row><row><cell>Decoder weight decay</cell><cell>10 ?7</cell></row><row><cell>L1 regularization weight</cell><cell>10 ?5</cell></row><row><cell>Temperature learning rate</cell><cell>10 ?4</cell></row><row><cell>Temperature Adam's ? 1</cell><cell>0.9</cell></row><row><cell>Init temperature</cell><cell>0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>To test whether REx provides a benefit on more realistic domain generalization tasks, we compared REx, IRM and ERM performance on the VLCS (Torralba &amp; Efros, 2011) and PACS (Li et al., 2017) image datasets. Both datasets are commonlyused for multi-source domain generalization. The task is to train on three domains and generalize to a fourth one at test time.Since every domain in PACS is used as a test set when training on the other three domains, it is not possible to perform a methodologically sound evaluation on PACS after examining results on any of the data. Thus to avoid performing any</figDesc><table><row><cell>accuracy (%)</cell><cell cols="2">0.50 0.55 0.60 0.65 0.70</cell><cell></cell><cell></cell><cell></cell><cell>IRM REx</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell></row><row><cell>accuracy (%)</cell><cell cols="2">0.02 0.04 0.06</cell><cell></cell><cell cols="3">train generalization gap (%) overfitting</cell></row><row><cell></cell><cell cols="2">0.00</cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell></row><row><cell cols="2">accuracy (%)</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell>test train grayscale</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>100</cell><cell>200 epoch</cell><cell>300</cell><cell>400</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 .</head><label>7</label><figDesc>Accuracy (percent) of different methods on the VLCS task. Results are test accuracy at the time of the highest validation accuracy, averaged over 10 runs. On VLCS REx outperforms all other methods. Numbers are shown in strike-through because we selected our hyperparameters based on highest test set performance; the goal of this experiment was to find suitable hyperparameters for the PACS experiment. 8?0.28 59.57?0.78 89.60?0.12 71.07 IRM 66.46?0.31 68.60?0.40 58.66?0.73 89.94?0.13 70.91 ERM 66.01?0.22 68.62?0.36 58.38?0.60 89.40?0.18 70.60 Jigsaw (SOTA) 66.96?0.39 66.67?0.41 61.27?0.73 89.54?0.19 71.11</figDesc><table><row><cell>REx (ours)</cell><cell>96.72 63.68</cell><cell>72.41</cell><cell>60.40</cell><cell>73.30</cell></row><row><cell>IRM</cell><cell>95.99 62.85</cell><cell>71.71</cell><cell>59.61</cell><cell>72.54</cell></row><row><cell>ERM</cell><cell>94.76 61.92</cell><cell>69.03</cell><cell>60.55</cell><cell>71.56</cell></row><row><cell>Jigsaw (SOTA)</cell><cell>96.46 63.84</cell><cell>70.49</cell><cell>60.06</cell><cell>72.71</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The first formal definition of an equipredictive representation we found was by Koyama &amp; Yamaguchi (2020), who use the term "(maximal) invariant predictor". We prefer our terminology since: 1) it is more consistent with<ref type="bibr" target="#b1">Arjovsky et al. (2019)</ref>, and 2) ? is a representation, not a predictor.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Our definitions follow Elements of Causal Inference (Peters et al., 2017); our notation mostly does as well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In practice, IRMv1 replaces this bilevel optimization problem with a gradient penalty on classifier weights.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Note we could also assume no covariate shift in order to fix the difficulty, but this seems hard to motivate in the context of interventions on X, which can change P (X).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Surprisingly, we were not able to find any existing definition of these terms in the machine learning literature. They have been used in this sense<ref type="bibr" target="#b22">(Hastie et al., 2009;</ref><ref type="bibr" target="#b20">Haffner, 2002)</ref>, but also to refer to strong generalization capabilities more generally(Sahoo et al., 2018).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://github.com/fmcarlucci/JigenDG</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018 13 https://www.kaggle.com/cnic92/explore-and-clean-financial-indicators-dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">&amp; Khoshgoftaar, 2019; Hendrycks et al., 2019a;<ref type="bibr" target="#b9">Carlucci et al., 2019)</ref>. These methods can also been combined effectively in various ways(Tian et al., 2019;<ref type="bibr" target="#b2">Bachman et al., 2019;</ref><ref type="bibr" target="#b17">Gowal et al., 2019)</ref>. Data augmentation and self-supervised learning methods typically use prior knowledge such as 2D image structure. Several recent works also use prior knowledge to design augmentation strategies for invariance to superficial features that may be spuriously correlated with labels in object recognition tasks<ref type="bibr" target="#b23">(He et al., 2019;</ref> Wang et al., 2019;<ref type="bibr" target="#b17">Gowal et al., 2019;</ref> Ilse et al., 2020). In contrast, REx can discover which features have invariant relationships with the label without such prior knowledge.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FOU(c)</head><p>FOU <ref type="formula">(</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improving out-of-distribution generalization via multitask self-supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">D. Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th National Conference on Artificial Intelligence</title>
		<meeting>the 20th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="714" to="719" />
		</imprint>
	</monogr>
	<note>AAAI&apos;05</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>1611-3349</idno>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="472" to="489" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Impossibility theorems for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>P?l</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Robust optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>El Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Princeton University Press</publisher>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>B?hlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Invariance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robustness</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2071" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Empirical risk minimization under fairness constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exploring the landscape of spatial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02779</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Achieving robustness in the wild via adversarial mixing with disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03192</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">search of lost domain generalization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Escaping the convex hull with extrapolated vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Dietterich, T. G., Becker, S., and Ghahramani, Z.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="753" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference, and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards non-i.i.d. image classification: A dataset and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Invariant causal prediction for nonlinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heinze-Deml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<idno type="DOI">10.1515/jci-2017-0016</idno>
		<ptr target="http://dx.doi.org/10.1515/jci-2017-0016" />
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Test accuracy of models trained on the financial domain dataset, averaged over all 20 tasks, as well as min</title>
	</analytic>
	<monogr>
		<title level="m">Table 9</title>
		<imprint/>
	</monogr>
	<note>/max. accuracy across the tasks</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2010a) shares the goal of generalizing to new distributions at test time, but allows some access to the test distribution. A common approach is to make different domains have a similar distribution of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Overview of other topics related to OOD generalization Domain adaptation</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>which seeks a &quot;invariant representation&quot; of the inputs, i.e. one whose distribution is domain-independent. Recent works have identified fundamental shortcomings with this approach, however</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Complementary to the goal of domain generalization is out-of-distribution detection</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Hendrycks &amp; Gimpel</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Three common deep learning techniques that can improve OOD generalization are adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendrycks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>), self-supervised learning</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">2020) and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albuquerque</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Shorten</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
