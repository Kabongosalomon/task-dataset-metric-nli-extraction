<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung</forename><forename type="middle">Jin</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ale?</forename><surname>Leonardis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Birmingham</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we focus on category-level 6D pose and size estimation from monocular RGB-D image. Previous methods suffer from inefficient category-level pose feature extraction which leads to low accuracy and inference speed. To tackle this problem, we propose a fast shape-based network (FS-Net) with efficient category-level feature extraction for 6D pose estimation. First, we design an orientation aware autoencoder with 3D graph convolution for latent feature extraction. The learned latent feature is insensitive to point shift and object size thanks to the shift and scale-invariance properties of the 3D graph convolution. Then, to efficiently decode category-level rotation information from the latent feature, we propose a novel decoupled rotation mechanism that employs two decoders to complementarily access the rotation information. Meanwhile, we estimate translation and size by two residuals, which are the difference between the mean of object points and ground truth translation, and the difference between the mean size of the category and ground truth size, respectively. Finally, to increase the generalization ability of FS-Net, we propose an online box-cage based 3D deformation mechanism to augment the training data. Extensive experiments on two benchmark datasets show that the proposed method achieves state-of-the-art performance in both category-and instance-level 6D object pose estimation. Especially in category-level pose estimation, without extra synthetic data, our method outperforms existing methods by 6.3% on the NOCS-REAL dataset 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating 6D object pose plays an essential role in many computer vision tasks such as augmented reality <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b22">21]</ref>, virtual reality <ref type="bibr" target="#b1">[2]</ref>, and smart robotic arm <ref type="bibr" target="#b48">[47,</ref><ref type="bibr" target="#b37">36]</ref>. <ref type="bibr" target="#b0">1</ref> Paper code https://github.com/DC1991/FS-Net . Semantic illustration of FS-Net. We use different networks for different tasks. The RGB-based network is used for 2D object detection, and the shape-based 3D graph convolution autoencoder is used for 3D segmentation and rotation estimation. The residual-based network is used for translation and size estimation with segmented points.</p><p>For instance-level 6D pose estimation, in which training set and test set contain the same objects, huge progress has been made in recent years <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b10">10]</ref>. However, category-level 6D pose estimation remains challenging as the object shape and color are various in the same category. Existing methods addressed this problem by mapping the different objects in the same category into a uniform model via RGB feature or RGB-D fusion feature. For example, Wang et al. <ref type="bibr" target="#b42">[41]</ref> trained a modified Mask R-CNN <ref type="bibr" target="#b9">[9]</ref> to predict the normalized object coordinate space (NOCS) map of different objects based on RGB feature, and then computed the pose with observed depth and NOCS map by Umeyama algorithm <ref type="bibr" target="#b38">[37]</ref>. Chen et al. <ref type="bibr" target="#b3">[4]</ref> proposed to learn a canonical shape space (CASS) to tackle intra-class shape variations with RGB-D fusion feature <ref type="bibr" target="#b41">[40]</ref>. Tian et al. <ref type="bibr" target="#b36">[35]</ref> trained a network to predict the NOCS map of different objects, with the uniform shape prior learned from a shape collection, and RGB-D fusion feature <ref type="bibr" target="#b41">[40]</ref>.</p><p>Although these methods achieved state-of-the-art performance, there are still two issues. Firstly, the benefits of using RGB feature or RGB-D fusion feature for category-level pose estimation are still questionable. In <ref type="bibr" target="#b39">[38]</ref>, Vlach et al. showed that people focus more on shape than color when categorizing objects, as different objects in the same category have very different colors but stable shapes (shown in <ref type="figure">Figure 3</ref>). Thereby the use of RGB feature for categorylevel pose estimation may lead to low performance due to huge color variation in the test scene. For this issue, to alleviate the color variation, we merely use the RGB feature for 2D detection, while using the shape feature learned with point cloud extracted from depth image for category-level pose estimation.</p><p>Secondly, learning a representative uniform shape requires a large amount of training data; therefore, the performance of these methods is not guaranteed with limited training examples. To overcome this issue, we propose a 3D graph convolution (3DGC) autoencoder <ref type="bibr" target="#b20">[19]</ref> to effectively learn the category-level pose feature via observed points reconstruction of different objects instead of uniform shape mapping. We further propose an online box-cage based 3D data augmentation mechanism to reduce the dependencies of labeled data.</p><p>In this paper, the newly proposed FS-Net consists of three parts: 2D detection, 3D segmentation &amp; rotation estimation, and translation &amp; size estimation. In 2D detection part, we use the YOLOv3 <ref type="bibr" target="#b32">[31]</ref> to detect the object bounding box for coarse object points obtainment <ref type="bibr" target="#b5">[6]</ref>. Then in the 3D segmentation &amp; rotation estimation part, we design a 3DGC autoencoder to perform segmentation and observed points reconstruction jointly. The autoencoder encodes orientation information in the latent feature. Then we propose the decoupled rotation mechanism that uses two decoders to decode the category-level rotation information. For translation and size estimation, since they are all point coordinates related, we design a coordinate residual estimation network based on PointNet <ref type="bibr" target="#b28">[27]</ref> to estimate the translation residual and size residuals. To further increase the generalization ability of FS-Net, we use the proposed online 3D deformation for data augmentation. To summarize, the main contributions of this paper are as follows:</p><p>? We propose a fast shape-based network to estimate category-level 6D object size and pose. Due to the efficient category-level pose feature extraction, the framework runs at 20 FPS on a GTX 1080 Ti GPU.</p><p>? We propose a 3DGC autoencoder to reconstruct the observed points for latent orientation feature learning. Then we design a decoupled rotation mechanism to fully decode the orientation information. This decoupled mechanism allows us to naturally handle the circle symmetry object (in Section 3.3).</p><p>? Based-on the shape similarity of intra-class objects, we propose a novel box-cage based 3D deformation mech-anism to augment the training data. With this mechanism, the pose accuracy of FS-Net is improved by 7.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Instance-Level Pose Estimation</head><p>In instance-level pose estimation, a known 3D object model is usually available for training and testing. Based on the 3D model, instance-level pose estimation can be roughly divided into three types: template matching based, correspondences-based, and voting-based methods. Template matching methods <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b23">22]</ref> aligned the template to the observed image or depth map via hand-crafted or deep learning feature descriptors. As they need the 3D object model to generate the template pool, their applications in category-level 6D pose estimation are limited. Correspondences-based methods trained their model to establish 2D-3D correspondences <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b25">24]</ref> or 3D-3D correspondences <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>. Then they solved perspective-n-point and SVD problem with 2D-3D and 3D-3D correspondences <ref type="bibr" target="#b14">[14]</ref>, respectively. Some methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1]</ref> also used these correspondences to generate voting candidates, and then used RANSAC <ref type="bibr" target="#b8">[8]</ref> algorithm for selecting the best candidate. However, the generation of canonical 3D keypoints is based on the known 3D object model that is not available when predicting the category-level pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Category-Level Pose Estimation</head><p>Compared to instance-level, the major challenge of category-level pose estimation is the intra-class object variation, including shape and color variation. To handle the object variation problem, <ref type="bibr" target="#b42">[41]</ref> proposed to map the different objects in the same category to a NOCS map. Then they used semantic segmentation to access the observed points cloud with known camera parameters. The 6D pose and size are calculated by the Umeyama algorithm <ref type="bibr" target="#b38">[37]</ref> with the NOCS map and the observed points. Shape-Prior <ref type="bibr" target="#b36">[35]</ref> adopted similar method with <ref type="bibr" target="#b42">[41]</ref>, but both extra shape prior knowledge and dense-fusion feature <ref type="bibr" target="#b41">[40]</ref>, instead of RGB feature, are used. CASS <ref type="bibr" target="#b3">[4]</ref> estimated the 6D pose via the learning of a canonical shape space with dense-fusion feature <ref type="bibr" target="#b41">[40]</ref>. Since the RGB feature is sensitive to color variation, the performance of their methods in category-level pose estimation is limited. In contrast, our method is shape feature-based which is robust for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">3D Data Augmentation</head><p>In 3D object detection tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b4">5]</ref>, online data augmentation techniques such as translation, random flipping, shifting, scaling, and rotation are applied to original point clouds for training data augmentation. However, these  <ref type="figure">Figure 2</ref>. Architecture of FS-Net. The input of FS-Net is an RGB-D image. For RGB channels, we use a 2D detector to detect the object 2D location, category label 'C' (used as a one-hot feature for next tasks), and class probability map (cpm) (generate the 3D sphere center via maximum probability location and camera parameters). With this information and depth channel, the points in a compact 3D sphere are generated. Given the points in the 3D sphere, we first use the proposed 3D deformation mechanism for data augmentation. After that, we use a shape-based 3DGC autoencoder to perform observed points reconstruction (OPR), as well as point cloud segmentation, for orientation latent feature learning. Then we decode the rotation information into two perpendicular vectors from the latent feature. Finally, we use a residual estimation network to predict the translation and size residuals. 'cate-sizes' denotes the pre-calculated average sizes of different categories, 'k' is the rotation vector dimension, and the hollow '+' means feature concatenation. operations cannot change the shape property of the object. Simply adopting these operations on point clouds is not able to handle the shape variation problem in the 3D task. To address this, <ref type="bibr" target="#b7">[7]</ref> proposed part-aware augmentation which operates on the semantic parts of the 3D object with five manipulations: dropout, swap, mix, sparing, and noise injection. However, how to decide the semantic parts are ambiguous. In contrast, we propose a box-cage based 3D data augmentation mechanism which can generate the various shape variants (shown in <ref type="figure" target="#fig_2">Figure 5</ref>) and avoid semantic parts decision procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we describe the detailed architecture of FS-Net shown in <ref type="figure">Figure 2</ref>. Firstly, we use the YOLOv3 to detect the object location with RGB input. Secondly, we use 3DGC autoencoder to perform 3D segmentation and observed points reconstruction, the latent feature can learn orientation information through the process. Then we propose a novel decoupled rotation mechanism for decoding orientation information. Thirdly, we use PointNet <ref type="bibr" target="#b28">[27]</ref> to estimate the translation and object size. Finally, to increase the generalization ability of FS-Net, we propose the box-cage based 3D deformation mechanism. <ref type="bibr">Figure 3</ref>. Stable shape and various color. Top row: three bowl instances randomly chosen from the NOCS-REAL dataset. Bottom row: three bowl instances randomly cropped from the internet image search results (using the keyword 'bowl'). The color is varied, while the shape is relatively stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Object Detection</head><p>Following <ref type="bibr" target="#b5">[6]</ref>, we train a YOLOv3 <ref type="bibr" target="#b32">[31]</ref> to fast detect the object bounding box in RGB images, and output class (category) labels. Then we adopt the 3D sphere to locate the point cloud of the target object quickly. With these techniques, the 2D detection part provides a compact 3D learning space for the following tasks. Different from other category-level 6D object pose estimation methods that need semantic segmentation masks, we only need object bounding boxes. Since object detection is faster than semantic segmentation <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b9">9]</ref>, the detection speed of our method is faster than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Shape-Based Network</head><p>The output points of object detection contain both object and background points. To access the points that belong to the target object and calculate the rotation of the object, we need a network that performs two tasks: 3D segmentation and rotation estimation.</p><p>Although there are many network architectures that directly process point cloud <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b47">46]</ref>, most of the architectures calculate on point coordinates, which means their networks are sensitive to point clouds shift and size variation <ref type="bibr" target="#b20">[19]</ref>. This decreases the pose estimation accuracy.</p><p>To tackle the point clouds shift, Frustum-PointNet <ref type="bibr" target="#b27">[26]</ref> and G2L-Net <ref type="bibr" target="#b5">[6]</ref> employed the estimated translation to align the segmented point clouds to local coordinate space. However, their methods cannot handle the intra-class size variation.</p><p>To solve the point clouds shift and size variation problem, in this paper, we propose a 3DGC autoencoder to extract the point cloud shape feature for segmentation and rotation estimation. 3DGC is designed for point cloud classification and object part segmentation; our work shows that 3DGC can also be used for category-level 6D pose estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">3D Graph Convolution</head><p>3DGC kernel consists of m unit vectors. The m kernel vectors are applied to the n vectors generated by the center point with its n-nearest neighbors. Then, the convolution value is the sum of cosine similarity between kernel vectors and the n-nearest vectors. In a 2D convolution network, the trained network learned a weighted kernel, which has a higher response with a matched RGB value, while the 3DGC network learned the orientations of the m vectors in the kernel. The weighted 3DGC kernel has a higher response with a matched 3D pattern which is defined by the center point with its n-nearest neighbors. For more details, please refer to <ref type="bibr" target="#b20">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Rotation-Aware Autoencoder</head><p>Based on the 3DGC, we design an autoencoder for the estimation of category-level object rotation. To extract the latent rotation feature, we train the autoencoder to reconstruct the observed points transformed from the observed depth map of the object. There are several advantages to this strategy: 1) the reconstruction of observed points is view-based and symmetry invariant <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b35">34]</ref>, 2) the reconstruction of observed points is easier than that of a complete object model (shown in <ref type="table" target="#tab_2">Table 2</ref>), and 3) more representative orientation feature can be learned (shown in <ref type="table">Table 1</ref>).</p><p>In <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b35">34]</ref>, the authors also reconstructed the input images to observed views. However, the input and output of their models are 2D images that are different to our 3D point cloud input and output. Furthermore, our network architecture is also different from theirs.</p><p>We utilize Chamfer Distance to train the autoencoder, the reconstruction loss function L rec is defined as</p><formula xml:id="formula_0">L rec = xi?Mc min xi?Mc x i ?x i 2 2 + xi?Mc min xi?Mc x i ?x i 2 2 ,<label>(1)</label></formula><p>where M c andM c denote the ground truth point cloud and reconstructed point cloud, respectively. x i andx i are the points in M c andM c . With the help of 3D segmentation mask, we only use the features extracted from the observed object points for reconstruction.</p><p>After the network convergence, the encoder learned the rotation-aware latent feature. Since the 3DGC is scale and shift invariant, the observed points reconstruction enforces the autoencoder to learn the scale and shift invariant orientation feature under corresponding rotation. In the next subsection, we will describe how we decode rotation information from this latent feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoupled Rotation Estimation</head><p>Given the latent feature which contains rotation information, our task is to decode the category-level rotation feature. To achieve this, we utilize two decoders to extract the rotation information in a decoupled fashion. The two decoders decode the rotation information into two perpendicular vectors under corresponding rotation. These two vectors can represent rotation information completely (shown in <ref type="figure" target="#fig_1">Figure 4</ref>).</p><p>Since the two vectors are orthogonal, the decoded rotation information related to them is independent; we can use one of them to recover part rotation information of the object. For example, in <ref type="figure" target="#fig_4">Figure 8</ref>, we use the green vector axis to recover the pose. We can see that the green boxes and blue boxes are aligned well in the recovered axis.</p><p>Each decoder only needs to extract the orientation information along corresponding vector which is easier than the estimation of the complete rotation. The loss function is based on cosine similarity that defined as</p><formula xml:id="formula_1">L rot = v 1 , v 1 v 1 v 1 + ? r v 2 , v 2 v 2 v 2 ,<label>(2)</label></formula><p>wherev 1 andv 2 are the predicted vectors. v 1 and v 2 are the ground truth, and ? r is the balance parameter. The balance parameter ? r makes our network easy to handle circular symmetry object such as bottle, and for such circular symmetry object, the red vector is not necessary (shown in <ref type="figure" target="#fig_1">Figure 4</ref>). Without loss of generality, we assume that the green vector is along the symmetry axis; then, we set ? r as zero to handle the circular symmetry objects. For other types of symmetric objects, we can employ the rotation mapping function used in <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b36">35]</ref> to map the relevant rotation matrices to a unique one.</p><p>Please note that our decoupled rotation is different to the rotation representation proposed in <ref type="bibr" target="#b46">[45]</ref>. They took the first two columns from a rotation matrix as the new representation, which has no geometric meaning. In contrast, our representation is defined based on the shape of the target object, and our representation can avoid the discontinuity issue mentioned in <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b26">25]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Residual Prediction Network</head><p>As both translation and object size are related to points coordinates, inspired by <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b5">6]</ref>, we train a tiny PointNet <ref type="bibr" target="#b28">[27]</ref> that takes segmented point cloud as input. More concretely, the PointNet performs two related tasks: 1) estimating the residual between the translation ground truth and the mean value of the segmented point cloud; 2) estimating the residual between object size and the mean category size.</p><p>For size residual, we pre-calculate the mean size [x, y, z] T of each category by</p><formula xml:id="formula_2">? ? x y z ? ? = 1 N N i=1 [x i , y i , z i ] T ,<label>(3)</label></formula><p>where N is the amount of the object in that category. Then for object o in that category the ground truth [? o x , ? o y , ? o z ] T of the size residual estimation is calculated as</p><formula xml:id="formula_3">[? o x , ? o y , ? o z ] T = [x o , y o , z o ] T ? [x, y, z] T .<label>(4)</label></formula><p>We use mean square error (MSE) loss to predict both the translation and size residual. The total loss function L res is defined as:</p><formula xml:id="formula_4">L res = L tra + L size ,<label>(5)</label></formula><p>where L tra and L size are sub-loss for translation residual and size residual, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">3D Deformation Mechanism</head><p>One major problem in category-level 6D pose estimation is the intra-class shape variation. The existing methods employed two large synthetic datasets, i.e. CAMERA <ref type="bibr" target="#b42">[41]</ref> and 3D model dataset <ref type="bibr" target="#b2">[3]</ref> to learn this variation. However, this strategy not only needs extra hardware resources to store these big synthetic datasets but also increases the (pre-)training time.</p><p>To alleviate the shape variation issue, based on the fact that the shapes of most objects in the same category are similar <ref type="bibr" target="#b39">[38]</ref> (shown in <ref type="figure">Figure 3</ref>), we propose an online boxcage based 3D deformation mechanism for training data augmentation. We pre-define a box-cage for each rigid object (shown in <ref type="figure" target="#fig_2">Figure 5</ref>). Each point is assigned to its nearest surface of the cage; when we deform the surface, the corresponding points move as well.</p><p>Though box-cage can be designed more refined, in experiments, we find that with a simple box cage, i.e. 3D bounding box of the object, the generalization ability of the proposed method is considerably improved ( <ref type="table">Table 1)</ref>. Different to <ref type="bibr" target="#b44">[43]</ref>, we do not need the extra training process to obtain the box-cage of the object, and we do not need target shape to learn the deformation operation either. Our mechanism is totally online, which saves training time and storage space.</p><p>To make the deformation operation easier, we first transfer the points to the canonical coordinate system and then perform 3D deformation. Finally we transform them to global scene:</p><formula xml:id="formula_5">{P 1 , P 2 , ? ? ? , P n } = R(F 3D (R T (P ? T ))) + T, (6)</formula><p>where P is the points generated after the 2D detection step. R, T are the pose ground truth. {P 1 , P 2 , ? ? ? , P n } are the new generated training examples. F 3D is 3D deformation which includes cage enlarging, shrinking, changing the area of some surfaces. <ref type="bibr" target="#b42">[41]</ref> is the first real-world dataset for category-level 6D object pose estimation. The training set has 4300 real images of 7 scenes with 6 categories. For each category, there are 3 unique instances. In the testing set, there are 2750 real images spread in 6 scenes of the same 6 categories as the training set. In each test scene, there are about 5 objects which makes the dataset clutter and challenging. LINEMOD <ref type="bibr" target="#b12">[12]</ref> is a widely used instance-level 6D object pose estimation dataset which consists of 13 different objects with significant shape variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NOCS-REAL</head><p>We use the automatic point-wise labeling techniques proposed in <ref type="bibr" target="#b4">[5]</ref> to access the label of each point in both training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use Pytorch <ref type="bibr" target="#b24">[23]</ref> to implement our pipeline. All experiments are deployed on a PC with i7-4930K 3.4GHz CPU and GTX 1080Ti GPU.</p><p>First, to locate the object in RGB images, we fine-tune the YOLOv3 pre-trained on COCO dataset <ref type="bibr" target="#b18">[18]</ref> with the training dataset. Then we jointly train the 3DGC autoencoder and residual estimation network. The total loss function is defined as L Shape = ? seg L seg +? rec L rec +? rot L rot +? res L res , <ref type="bibr" target="#b7">(7)</ref> where ?s are the balance parameters. We empirically set them as 0.001, 1, 0.001, and 1 to keep different loss values at the same magnitude. We use cross entropy for 3D segmentation loss function L seg .</p><p>We adopt Adam <ref type="bibr" target="#b15">[15]</ref> to optimize the FS-Net. The initial learning rate is 0.001, and we halve it every 10 epochs. The maximum epoch is 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>For category-level pose estimation, we adopt the same metrics used in <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">35]</ref>:</p><p>? IoU X is Intersection-over-Union (IoU) accuracy for 3D object detection under different overlap thresholds. The overlap ratio larger than the threshold X is accepted.</p><p>? n ? m cm represents pose estimation error of rotation and translation. The rotation error less than n ? and the translation error less than m cm is accepted.</p><p>For instance-level pose estimation, we compare the performance of FS-Net with other state-of-the-art instancelevel methods using the ADD-(S) metric <ref type="bibr" target="#b12">[12]</ref>. <ref type="table">Table 1</ref>. Ablation studies on NOCS-REAL dataset. We use two different metrics to measure performance. '3DGC' means the 3D graph convolution. 'OPR' means observed points reconstruction. 'DR' represents the decoupled rotation mechanism. 'DEF' denotes the online 3D deformation. In the last row, the values in the bracket are the performance for the reconstruction of the complete object model transformed by the corresponding pose. Please note, for the sake of ablation study, we provide the ground truth 2D bounding box for different methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>We use the G2L-Net <ref type="bibr" target="#b5">[6]</ref> as the baseline method which extracted the latent feature for rotation estimation via pointwise orientated vector regression, and the ground truth of rotation is the eight corners of 3D bounding box with corresponding rotation. The loss function for rotation estimation is the mean square error between predicted 3D coordinates and ground truth. Compared to baseline, our proposed work has three novelties: a) view-based 3DGC autoencoder for observed point cloud reconstruction; b) rotation decoupled mechanism; c) online 3D deformation mechanism.</p><p>In <ref type="table">Table 1</ref>, we report the experimental results of three novelties on the NOCS-REAL dataset. Comparing Med3 and Med5, we find that reconstruction of the observed point cloud can learn better pose feature. The performance of Med2(Med1, G2L) and Med5(Med3, G2L+DR) shows that the proposed decoupled rotation mechanism can effectively extract the rotation information. The results of Med4 and Med5 demonstrate the effectiveness of the 3D deformation mechanism, which increases the pose accuracy by 7.7% in terms of 10 ? 10 cm metric. We also compare the different reconstruction choices: the reconstruction of observed points and the complete object model with corresponding rotation. From the last row of <ref type="table">Table 1</ref>, we can see that the observed points reconstruction can learn better rotation feature. Overall, <ref type="table">Table 1</ref> shows that the proposed novelties can improve the accuracy significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Generalization Performance</head><p>NOCS-REAL dataset provides 4.3k real images that covers various poses of different objects in different categories for training. That means the category-level pose information is rich in the training set. Thanks to the effectively pose feature extraction, FS-Net achieves state-of-the-art performance even with part of the real-world training data. We randomly choose different percentages of the training set to train FS-Net and test it on the whole testing set. <ref type="figure">Figure   6</ref> shows that: 1) FS-Net is robust to the size of the training dataset, and has good category-level feature extraction ability. Even with 20% of the training dataset, the FS-Net can still achieve state-of-the-art performance; 2) the 3D deformation mechanism significantly improves the robustness and performance of FS-Net. <ref type="figure">Figure 6</ref>. Generalization performance. With the given 2D bounding box and a randomly chosen 3D sphere center, we show how the training set size affects the pose estimation performance. 'w/o DEF' means no 3D deformation mechanism is adopted during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Evaluation of Reconstruction</head><p>Point cloud reconstruction has a close relationship with pose estimation performance. We compute the Chamfer Distance of the reconstructed point cloud with the ground truth point cloud and compared it with other reconstruction types used by other methods. From <ref type="table" target="#tab_2">Table 2</ref>, we can see that the average reconstruction error of our method is 0.86, which is 72.9% and 18.9% lower than that of Shape-Prior <ref type="bibr" target="#b36">[35]</ref> and CASS <ref type="bibr" target="#b3">[4]</ref>, respectively. It shows that our method achieves better pose estimation results via a simpler reconstruction task, i.e. observed points reconstruction rather than complete object model reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Comparison with State-of-the-Arts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1">Category-Level Pose Estimation</head><p>We compare FS-Net with NOCS <ref type="bibr" target="#b42">[41]</ref>, CASS <ref type="bibr" target="#b3">[4]</ref>, Shape-Prior <ref type="bibr" target="#b36">[35]</ref>, and 6D-PACK <ref type="bibr" target="#b40">[39]</ref> on NOCS-REAL dataset in <ref type="table">Table 4</ref>. We can see that our proposed method outperforms the other state-of-the-art methods on both accuracy and speed. Specifically, on 3D detection metric IOU 50 , our FS-Net outperforms the previous best method, NOCS, by 11.7% and the running speed is 4 times faster. In terms of 6D pose metric 5 ? 5cm and 10 ? 10 cm, FS-Net outperforms the CASS by the margins of 4.7% and 6.3%, respectively. FS-Net even outperforms 6D-PACK under 3D detection metric IOU 50 , which is a 6D tracker and needs an initial 6D pose and object size to start. See <ref type="figure" target="#fig_3">Figure 7</ref> for more quantitative details. The qualitative results are shown in  <ref type="figure" target="#fig_4">Figure 8</ref>. Please note, we only use real-world data (NOCS-REAL) to train our pose estimation part. Other methods use both synthetic dataset (CAMERA) <ref type="bibr" target="#b42">[41]</ref> and real-world data for training. The number of training examples in CAM-ERA is 275K, which is more than 60 times that of NOCS-REAL (4.3K). It shows that FS-Net can efficiently extract the category-level pose feature with fewer data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2">Instance-Level Pose Estimation</head><p>We compare the instance-level pose estimation results of FS-Net on the LINEMOD dataset with other state-of-thearts instance-level methods. From <ref type="table" target="#tab_3">Table 3</ref>, we can see that FS-Net achieves comparable results on both accuracy and speed. It shows that our method can effectively extract both category-level and instance-level pose features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Running Time</head><p>Given a 640? 480 RGB-D image, our method runs at 20 FPS with Intel i7-4930K CPU and 1080Ti GPU, which is 2 times faster than the previous fastest method 6D-PACK <ref type="bibr" target="#b40">[39]</ref>. Specifically, the 2D detection takes about 10ms to proceed. The pose and size estimation takes about 40ms. <ref type="table">Table 4</ref>. Category-level performance on NOCS-REAL dataset with different metrics. We summarize the pose estimation results reported in the origin papers on the NOCS-REAL dataset. '-' means no results are reported under this metric. The values in the bracket are the performance for synthetic NOCS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>IoU 25 IoU 50 IoU 75 5 ? 5cm 10 ? 5 cm 10 ? 10 cm Speed(FPS) NOCS <ref type="bibr" target="#b42">[41]</ref> 84   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a fast category-level pose estimation method that runs at 20 FPS which is fast enough for real-time applications. The proposed method first extracts the latent feature by the observed points reconstruction with a shape-based 3DGC autoencoder. Then the category-level orientation feature is decoded by the effective decoupled rotation mechanism. Finally, for translation and object size estimation, we use the residual network to estimate them based on residuals estimation. In addition, to increase the generalization ability of FS-Net and save the hardware source, we design an online 3D deformation mechanism for training set augmentation. Extensive experimental results demonstrate that FS-Net is less data-dependent, and can achieve state-of-the-art performance on category-and instance-level pose estimation in both accuracy and speed. Please note, our 3D deformation mechanism and decoupled rotation scheme are model-free, which can be applied to other pose estimation methods to boost the performance.</p><p>Although FS-Net achieves state-of-the-art performance, it relies on a robust 2D detector to detect the region of interest. In future work, we plan to adopt 3D object detection techniques to directly detect the objects from point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>This section provides more details about our FS-Net. Section 6.1 describes the details of the 3D deformation mechanism and deformed examples. Section 6.2 provides more quantitative results of the FS-Net on NOCS-REAL <ref type="bibr" target="#b42">[41]</ref> dataset and comparison with state-of-the-art method. Section 6.3 demonstrates that the proposed vectors-based rotation representation can be easily extended to handle other symmetric types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">3D Deformation Mechanism</head><p>As stated in Section 3.5 of the paper, the 3D deformation mechanism is box-cage based and the deformations are applied in a canonical space. In the canonical coordinate system, every box edge is parallel to an axis (shown in <ref type="figure" target="#fig_6">Figure 9</ref>). This property makes the 3D deformation calculation easier. For example, when we need to elongate/shrink the mug along Y axis by n times. We enlarge the distance between surface S 1,2,3,4 and surface S 5,6,7,8 by n times. Since these two surfaces are parallel to the XZ-plane, the x and z coordinates are unchanged. Then points coordinates are changed from [x, y, z] to [x, ny, z]. The calculations are similar when we need to elongate/shrink the mug along X or Z axis by n times:</p><formula xml:id="formula_6">[x, ny, z] = F x ([x, y, z]),<label>(8)</label></formula><formula xml:id="formula_7">[nx, y, z] = F y ([x, y, z]),<label>(9)</label></formula><formula xml:id="formula_8">[x, y, nz] = F z ([x, y, z]),<label>(10)</label></formula><p>where F x,y,z is the elongate/shrink operation along corresponding axis. Further, if the object is the mug or bowl, we may need to change the top or bottom size to generate new shapes (shown in <ref type="figure" target="#fig_0">Figure 10</ref>). In this case, assuming we enlarge the bottom along X axis by n times, then from bottom to top, the coordinates are changed as:</p><formula xml:id="formula_9">x new = (1 + (n ? 1) l L )x,<label>(11)</label></formula><p>where l is the distance from a point to the top surface, i.e. S 1,2,3,4 in <ref type="figure" target="#fig_6">Figure 9</ref>. L is the height of the object. Please note, all the edges are keep straight while deformation.  <ref type="figure" target="#fig_6">Figure 9</ref>. 3D object model. We assume that the center of 3D bounding box is the origin point of the coordinate. The surface is represented by its four corners. For example, the top surface is represented by S1,2,3,4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Detailed Results</head><p>We report the specific category pose estimation results under different metrics in <ref type="table" target="#tab_5">Table 5</ref>. We also provide the rotation recovered by one/two vectors in <ref type="figure" target="#fig_0">Figure 11</ref>. We can see that the bounding boxes are well aligned in the recovered vector direction. We compare FS-Net with the state-of-the-art method Shape-Prior <ref type="bibr" target="#b36">[35]</ref>, which utilized point cloud for category-level 6D object pose estimation. Shape-Prior <ref type="bibr" target="#b36">[35]</ref> estimated the object size and 6D pose from dense-fusion feature <ref type="bibr" target="#b41">[40]</ref>, while we estimate the pose from point cloud feature. <ref type="figure" target="#fig_0">Figure 12</ref> shows that our FS-Net is robust to color and shape variation, and can handle some failure cases of Shape-Prior. For Shape-Prior, we use the predicted results provided on their website: https://github.com/mentian/ object-deformnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enlarge bottom Enlarge top</head><p>Shrink along Z Shrink along X Shrink along XZ <ref type="figure" target="#fig_0">Figure 10</ref>. Examples of different deformations. We assume that the XY Z axis are the same as   <ref type="figure" target="#fig_1">Figure 4</ref> in the paper), respectively. For better illustration, we use ground truth object size to calculate the final 3D bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Rotation Representation for Symmetry Object</head><p>The vector based rotation representation proposed in the paper can only handle the symmetry objects like bottle, however, in real-world the symmetric types are various (see <ref type="figure" target="#fig_0">Figure 13</ref>). In this section, we will show how to extend the vector based rotation representation for different symmetric types. Our strategy is inspired by the rotation mapping operation proposed in <ref type="bibr" target="#b26">[25]</ref>. In the following, we will show how to find the rotation group (termed proper symmetries in <ref type="bibr" target="#b26">[25]</ref>) of a single rotation for common symmetric objects.</p><p>Our basic idea is list all the ambiguous rotations of a single rotation and choose the rotations that has the closest distance with the identity matrix:</p><formula xml:id="formula_10">R * = argmin R?G(Ri) D(R, R I ),<label>(12)</label></formula><p>where D(?, ?) is the distance between two rotation matrix, G(R i ) is a group of rotation that can provide the same visual appearance of a given object as rotation R i . Our goal is to find a rotation R * that can minimize the rotation distance. For symmetric object like bottle, we can avoid the rotation ambiguity by only using the green vector to represent the rotation (see <ref type="figure" target="#fig_1">Figure 4)</ref>, however, the case is non-trivial for other symmetric type. In the following, we describe how we find symmetry rotation group for different symmetric types</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Symmetry with two Axes</head><p>For this kind symmetric objects, in canonical space, when we rotate the object around one axis 180 ? , we can get the same appearance (see <ref type="figure">Figure for</ref> illustration). Assume that axis is Z axis, for arbitrary rotation R, the appearance A:</p><formula xml:id="formula_11">A R Z + 180 O = A O ,<label>(13)</label></formula><p>where R Z + 180 means rotation the object around Z 180 ? in clockwise, O denotes the object. That means we can find the rotation group of each rotation by right multiplication operation R Z + 180 . Then we use Equation 12 to find the representative rotation in the rotation group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Symmetry with N Axes</head><p>The idea can be easily extend to object with N symmetries around a single axis Z. For this kind of symmetric objects, when we rotate the object around axis Z by K 360 N ? (K = 1, 2, ? ? ? , N ) in canonical space, the appearance A of the object is unchanged:</p><formula xml:id="formula_12">A R Z + K 360 N ? O = A O .<label>(14)</label></formula><p>Then, the symmetric rotation group G(R) of rotation R is: <ref type="figure" target="#fig_0">(K=0,1,2,??? ,N )</ref> . We find the representative rotation in G(R) with Equation 12.</p><formula xml:id="formula_13">RR K 360 N ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">General Case</head><p>Most symmetric types are included in the description of Section 6.3.1 and 6.3.2. For any other symmetric object, the key idea here is to find the rotation operation that can produce the same appearance of the object. Then use Equation 12 to find the representative rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">Decoupled Rotation Representation</head><p>Given the representative rotation R * of ambiguous rotation, we generate its corresponding vector-based representation V by:</p><formula xml:id="formula_14">V = R * [v 1 , v 2 ],<label>(15)</label></formula><p>where v 1 is the vector along with the axis Z mentioned in Section 6.3.1 and 6.3.2, v 2 is the vectors orthogonal with v 1 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1. Semantic illustration of FS-Net. We use different networks for different tasks. The RGB-based network is used for 2D object detection, and the shape-based 3D graph convolution autoencoder is used for 3D segmentation and rotation estimation. The residual-based network is used for translation and size estimation with segmented points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Rotation represented by vectors. Left: The object rotation can be represented by two perpendicular vectors (green vector and red vector); Right: For circular symmetry object like the bottle, only the green vector matters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>3D deformed examples. The new training examples can be generated by enlarging, shrinking, or changing the area of some surfaces of the box-cages. The left one is the original point could with original 3D box-cage, i.e. 3D bounding box. The right three ones are the deformed point clouds with deformed boxcages (shown in yellow color). The green boxes are the original 3D bounding boxes before deformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Result on NOCS-REAL. The average precision of different thresholds tested on NOCS-REAL dataset with 3D IoU, rotation, and translation error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results on NOCS-REAL dataset. The first row is the pose and size estimation results. White 3D bounding boxes denote ground truth. Blue boxes are the poses recovered from two estimated rotation vectors. The green boxes are the poses recovered from one estimated rotation vector. Our results match ground truth well in both pose and size. The second row is the reconstructed observed points under corresponding poses, although the reconstructed points are not perfectly in line with the target points, the basic orientation information is kept. The third row is the ground truth of the observed points transformed from the observed depth map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>The upper right corner is the original point cloud with corresponding box-cage. The rest are the deformed box-cages and point clouds. The deformation operations are described on the top or bottom of the pictures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Rotation recovered by different vectors. The white boxes are the ground truth. Blue boxes are the rotation recovered by two estimated vectors. The green and red boxes are the rotation recovered by estimated green vector and estimated red vector (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Qualitative comparison with Shape-Prior. The white boxes are the ground truth. Blue boxes are our results. Red boxes are the poses predicted by Shape-Prior<ref type="bibr" target="#b36">[35]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Reconstruction type comparison. The comparison is on the NOCS-REAL dataset with the Chamfer Distance metric (?10 ?3 ). 'Complete' means the reconstruction of the complete 3D model. 'Observed' denotes only the reconstruction of the observed points.</figDesc><table><row><cell cols="3">Methods CASS [4] Shape-Prior [35]</cell><cell>Ours</cell></row><row><cell></cell><cell>Complete</cell><cell>Complete</cell><cell>Observed</cell></row><row><cell>Bottle</cell><cell>0.75</cell><cell>3.44</cell><cell>1.2</cell></row><row><cell>Bowl</cell><cell>0.38</cell><cell>1.21</cell><cell>0.39</cell></row><row><cell>Camera</cell><cell>0.77</cell><cell>8.89</cell><cell>0.44</cell></row><row><cell>Can</cell><cell>0.42</cell><cell>1.56</cell><cell>0.62</cell></row><row><cell>Laptop</cell><cell>3.73</cell><cell>2.91</cell><cell>2.23</cell></row><row><cell>Mug</cell><cell>0.32</cell><cell>1.02</cell><cell>0.29</cell></row><row><cell>Average</cell><cell>1.06</cell><cell>3.17</cell><cell>0.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Instance-level comparison on LINEMOD dataset. Our method achieves a comparable performance with the state-of-theart in both speed and accuracy.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell cols="2">ADD-(S) Speed(FPS)</cell></row><row><cell>PVNet [24]</cell><cell>RGB</cell><cell>86.3%</cell><cell>25</cell></row><row><cell>CDPN [17]</cell><cell>RGB</cell><cell>89.9%</cell><cell>33</cell></row><row><cell>DPOD [44]</cell><cell>RGB</cell><cell>95.2%</cell><cell>33</cell></row><row><cell>G2L-Net [6]</cell><cell>RGBD</cell><cell>98.7%</cell><cell>23</cell></row><row><cell cols="2">Densefusion[40] RGBD</cell><cell>94.3%</cell><cell>16</cell></row><row><cell>PVN3D [10]</cell><cell>RGBD</cell><cell>99.4%</cell><cell>5</cell></row><row><cell>Ours</cell><cell>RGBD</cell><cell>97.6%</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Category-Level results. Object-wise experiments with different metrics.Category IoU 75 5 ? 5 cm 10 ? 5 cm 10 ? 10 cm</figDesc><table><row><cell>Bottle</cell><cell>0.4710 0.4219</cell><cell>0.8134</cell><cell>0.8755</cell></row><row><cell>Bowl</cell><cell>0.9810 0.5916</cell><cell>0.9793</cell><cell>0.9793</cell></row><row><cell>Camera</cell><cell>0.5882 0.0176</cell><cell>0.1457</cell><cell>0.1480</cell></row><row><cell>Can</cell><cell>0.6334 0.4055</cell><cell>0.7820</cell><cell>0.8141</cell></row><row><cell>Laptop</cell><cell>0.3805 0.1659</cell><cell>0.5570</cell><cell>0.6859</cell></row><row><cell>Mug</cell><cell>0.7534 0.0874</cell><cell>0.3698</cell><cell>0.3706</cell></row><row><cell>Average</cell><cell>0.6345 0.2816</cell><cell>0.6078</cell><cell>0.6455</cell></row><row><cell cols="3">6.2.2 Comparison with State-of-The-Art</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3364" to="3372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Virtual reality technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grigore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Burdea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coiffet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning canonical shape space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengsheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11973" to="11982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ponitposenet: Point pose network for robust 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Basevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020-03" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Hyung Jin Chang, and Ales Leonardis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">G2l-net: Global to local network for realtime 6d pose estimation with embedding vector features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Hyung Jin Chang, Jinming Duan, and Ales Leonardis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Different symmetry types. 30 industry-relevant objects in T-LESS dataset [13</title>
		<imprint/>
	</monogr>
	<note>Figure 13. Object 1, 2, 3, 4 are circular symmetry, object 7, 8, 9, 10 have two symmetry axes, while object 27, 28 have four symmetry axes</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Part-aware data augmentation for 3d object detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeseok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13373,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient response maps for real-time detection of textureless objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="876" to="888" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="548" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Manolis Lourakis, and Xenophon Zabulis. T-less: An rgbd dataset for 6d pose estimation of texture-less objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Haluza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?tep?n</forename><surname>Obdr??lek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A solution for the best rotation to relate two sets of vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Kabsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="922" to="923" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified framework for multi-view multi-class object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7678" to="7687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolution in the cloud: Learning deformable kernels in 3d graph convolution networks for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pose estimation for augmented reality: a hands-on survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Spindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2633" to="2651" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Project tango</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Marder-Eppstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2016 Real-Time Live!</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="119" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11788</idno>
		<title level="m">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On object symmetries and 6d pose estimation from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgia</forename><surname>Pitteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Ramamonjisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bb8: a scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature mapping for learning fast and accurate 3d pose inference from synthetic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4663" to="4672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-path learning for object pose estimation across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltan-Csaba</forename><surname>Puang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narunas</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Vaskevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13916" to="13925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Shape prior deformation for categorical 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08454</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakumar</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10790</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="376" to="380" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How we categorize objects is related to how we remember them: the shape bias as a memory bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vlach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental child psychology</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">6-pack: Category-level 6d pose tracker with anchor-based keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="10059" to="10066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Martin-Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural cages for detail-preserving 3d deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yifan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Aigerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="75" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dpod: 6d pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1941" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Single image 3d object detection and pose estimation for grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mabel</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3936" to="3943" />
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
