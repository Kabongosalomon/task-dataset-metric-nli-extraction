<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Logo Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Bianco</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DISCo -Universit? degli Studi di Milano-Bicocca</orgName>
								<address>
									<postCode>20126</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Buzzelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DISCo -Universit? degli Studi di Milano-Bicocca</orgName>
								<address>
									<postCode>20126</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Mazzini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DISCo -Universit? degli Studi di Milano-Bicocca</orgName>
								<address>
									<postCode>20126</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DISCo -Universit? degli Studi di Milano-Bicocca</orgName>
								<address>
									<postCode>20126</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Logo Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Logo recognition</term>
					<term>Deep Learning</term>
					<term>Convolutional Neural Network</term>
					<term>Data augmentation</term>
					<term>FlickrLogos-32</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a method for logo recognition using deep learning. Our recognition pipeline is composed of a logo region proposal followed by a Convolutional Neural Network (CNN) specifically trained for logo classification, even if they are not precisely localized. Experiments are carried out on the FlickrLogos-32 database, and we evaluate the effect on recognition performance of synthetic versus real data augmentation, and image pre-processing. Moreover, we systematically investigate the benefits of different training choices such as class-balancing, sample-weighting and explicit modeling the background class (i.e. no-logo regions). Experimental results confirm the feasibility of the proposed method, that outperforms the methods in the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Logo recognition in images and videos is the key problem in a wide range of applications, such as copyright infringement detection, contextual advertise placement, vehicle logo for intelligent traffic-control systems <ref type="bibr" target="#b0">[1]</ref>, automated computation of brand-related statistics on social media <ref type="bibr" target="#b1">[2]</ref>, augmented reality <ref type="bibr" target="#b2">[3]</ref>, etc.</p><p>Traditionally, logo recognition has been addressed with keypoint-based detectors and descriptors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. For example Romberg and Lienhart <ref type="bibr" target="#b7">[8]</ref> presented a scalable logo recognition technique based on feature bundling, where individual local features are aggregated with features from their spatial neighborhood into Bag of Words (BoW). Romberg et al. <ref type="bibr" target="#b8">[9]</ref> exploited a method for encoding and indexing the relative spatial layout of local features detected in the logo images. Based on the analysis of the local features and the composition of basic spatial structures, such as edges and triangles, they derived a quantized representation of the regions in the logos. Revaud et al. <ref type="bibr" target="#b9">[10]</ref> introduced a technique to down-weight the score of those noisy logo detections by learning a dedicated burstiness model for the input logo. Boia et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> proposed a smart method to perform both logo localization and recognition using homographic class graphs. They also exploited inverted secondary models to handle inverted colors instances. Recently some works investigating the use of deep learning for logo recognition appeared <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Bianco et al. <ref type="bibr" target="#b12">[13]</ref> and Eggert et al. <ref type="bibr" target="#b13">[14]</ref> investigated the use of pretrained Convolutional Neural Networks (CNN) and synthetically generated data for logo recognition, trying different techniques to deal with the limited amount of training data. Also Iandola et al. <ref type="bibr" target="#b14">[15]</ref> investigated a similar approach, proposing and evaluating several network architectures. Oliveira et al. <ref type="bibr" target="#b15">[16]</ref> exploited pretrained CNN models and used them as part of a Fast Region-Based Convolutional Networks recognition pipeline. Given the limited amount of training data available for the logo recognition task, all these methods work on networks pretrained on different tasks.</p><p>In this paper we propose a method for logo recognition exploiting deep learning. The recognition pipeline is composed by a recall-oriented logo region proposal <ref type="bibr" target="#b16">[17]</ref>, followed by a Convolutional Neural Network (CNN) specifically trained for logo classification, even if they are not precisely localized. Within this pipeline, we investigate the benefit on the recognition performance of the application of different machine learning techniques in training, such as image pre-processing, class-balancing, sample weighting, and synthetic data augmentation. Furthermore we prove the benefit of adding as positive examples candidate regions coming from the object proposal to the ground truth logos, and the benefit of enlarging the size of the actual dataset with real data augmentation and the use of a background class (i.e. no-logo regions) in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Method</head><p>The proposed classification pipeline is illustrated in Figure 1. Since logos may appear in any image location with any orientation and scale, and more logos can coexist in the same image, for each image we generate different object proposals, that are regions which are more likely to contain a logo. These proposal are then cropped to a common size to match the input dimensions of the neural network and are propagated through a CNN specifically trained for logo recognition.</p><p>In order to have performance as high as possible within this pipeline, we use an object proposal that is highly recall-oriented. For this reason, the CNN classifier should be designed and trained to take into account that the logo regions proposed may contain many false positives or only parts of actual logos. To address these problems we propose here a training framework and investigate the influence on the final recognition performance of different implementation choices.</p><p>In more detail, the training framework is reported in <ref type="figure">Figure 2</ref>. The training data preparation is composed by two main parts:</p><p>-Precise ground-truth logo annotations: Given a set of training images and associated ground-truth specifying logo position and class, we first crop logo regions and annotate them with the ground-truth class. These regions are rectangular crops that completely contain logos but, due to the prospective of the image or the logo particular shape, may also contain part of the background.</p><p>-Object-proposal logo annotations: Since we must automatically localize regions that may contain a logo, an object proposal algorithm is employed in the whole pipeline as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. This algorithm is not applied only to the test images, but it is also run on the training images to extract regions that are more likely to contain a logo. Details about the particular algorithm used are given in the next subsection. Each object proposal in the training images is then labeled on the basis of its content: if it overlaps with a ground-truth logo region, it is annotated with the corresponding class and with the Intersection-over-Union (IoU) overlap ratio, otherwise it is labeld as background.</p><p>Within our training framework we investigate both the use of the precise ground-truth logo annotations alone or coupled with the object-proposal logo annotations. All positive instances, i.e. labeled logos and eventually object proposals that overlap with them by a significant amount (i.e. IoU? 0.5), are used to train a Convolutional Neural Network whose architecture is given below. Different training choices are investigated within our framework in <ref type="figure">Figure 2</ref>: -Contrast normalization: Images are contrastnormalized by subtracting the mean and dividing by the standard deviation, which are extracted from the whole training set. The hypothesis is that this should make the CNN more robust to changes in the lighting and imaging conditions.</p><formula xml:id="formula_0">-</formula><p>-Sample weighting: Positive instances are weighted on the basis of their overlap with ground-truth logo regions. The hypothesis is that this should make the CNN more confident on proposals highly overlapping with the ground truth logos.</p><p>-Background class: A background class is considered together with the logo classes. Background examples are not randomly selected, but are composed by the candidate regions generated by the object proposal algorithm on training images and that do not overlap with any logo. The hypothesis is that this should make the CNN more precise in discriminating logos and background class.</p><p>The actual contribution to the performance of each training choice considered will be discussed in Section 4. After the CNN is trained, a threshold is learned on top of the CNN predictions. If the CNN prediction with the highest confidence is below this threshold, the candidate region is labeled as not being a logo, otherwise CNN prediction is left unchanged.</p><p>The testing framework is reported in <ref type="figure">Figure 3</ref>. Given a test image, we extract the object proposals with the same algorithm used for training. We then perform contrastnormalization over each proposal (if enabled at training time), and feed them to the CNN. The CNN predictions on the proposals are max-pooled and the class identified with highest confidence (eventually including the background class) is selected. If the CNN confidence for a logo class is above the threshold that has been learned in training, the corresponding logo class is assigned to the image, otherwise the image is labeled as not containing any logo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object proposal</head><p>For object proposal we exploit a Selective Search algorithm originally introduced by van de Sande et al. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. The goal of Selective Search is to provide a set of regions likely to contain an instance of the object of interest, i.e. logos in our case. They can appear in any position and scale, and may have been acquired under different lighting conditions, and from slightly different point of views. The algorithm is designed to be highly recall-oriented; this implies that very few logos are not segmented, but also implies that a great number of false positive candidates are generated. The proposed regions will be disambiguated by the neural network that comes afterward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network Architecture</head><p>The architecture used for the experiments in the following sections is a tiny deep neural network. We opted for a Test image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image regions Prediction</head><p>Object proposal  tiny network because it is fast at test time and it can be trained on cheap GPUs in very short time. It also allows us to train the network without using any form of regularization like dropout <ref type="bibr" target="#b19">[20]</ref>, dropconnect <ref type="bibr" target="#b20">[21]</ref>, etc. decreasing even more the time needed for training and validating the network. The same network structure was used by Krizhevsky in <ref type="bibr" target="#b21">[22]</ref> on the CIFAR-10 dataset, where it was proven to be an high-performance network for the task of object recognition on tiny RGB images. It has three convolutional layers interleaved by ReLU nonlinearities and Pooling layers. All the pooling layers make the data dimensions halve after every Pooling block. The last part of the network (farthest from the input) consists in two Fully-connected layers with a final Softmax classifier. The whole net structure is presented in <ref type="table" target="#tab_1">Table 1</ref>.</p><formula xml:id="formula_1">CNN-based classification ? ? ? FedEx ? Google ? Heineken ? ? ? No logo</formula><p>To give an idea of the network size, our network has 1.5 ? 10 5 parameters whereas AlexNet (used in <ref type="bibr" target="#b13">[14]</ref>) and GoogLeNet (a similar structure is used in <ref type="bibr" target="#b14">[15]</ref>) have respectively 6 ? 10 7 and 1.3 ? 10 7 parameters. Therefore our network is less likely to overfit, even when the size of the training set is not large. Conv 32 filters of 5x5 5 <ref type="bibr">Relu 6</ref> Pool (average) with stride 2 7</p><p>Conv 64 filters of 5x5 8 Relu 9</p><p>Pool (average) with stride 2 10 Fully Connected of size 64 11 Fully Connected of size 33 12 Softmax</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Logos datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">FlickrLogos-32 Dataset</head><p>FlickrLogos-32 dataset <ref type="bibr" target="#b8">[9]</ref> is a publicly-available collection of photos showing 32 different logo brands. It is meant for the evaluation of logo retrieval and multi-class logo detection/recognition systems on real-world images. All logos have an approximately planar or cylindrical surface. For each class, the dataset offers 10 training images, 30 validation images, and 30 test images. An example image for each of the 32 classes of the FlickrLogos-32 dataset is reported in <ref type="figure" target="#fig_2">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Logos-32plus Dataset</head><p>Logos-32plus dataset is an expansion of the trainset of FlickrLogos-32. It has the same classes of objects as its counterpart but a larger cardinality (12312 instances). We collected this new dataset for three main reasons: first, since we want to test a deep learning approach, we needed a suitable dataset size. Second, we believe that Logos-32 dataset is not very representative of a data distribution for most real-world problems. Third, we hypothesize that synthetic data augmentation is not enough to model actual logo appearance variability. The Logos-32 dataset was collected with the aim to train keypoint-based approaches. Therefore the selection of images followed some implicit guidelines, such as: most of the images are on focus, no blurry or noisy images, and usually images with highly saturated colors. As a result, the variability of this dataset mainly resides on the amount of intraclass affine transformations which can be handled very well by keypointbased detection methods. We collected this new dataset with the aim of taking into account a larger set of real imaging conditions and transformations that may occur in uncontrolled acquisitions.</p><p>We built the Logos-32plus dataset with images retrieved from both Flickr and Google image search. In particular, to increase the variability of data distribution we performed multiple queries for each logo. The dendrogram scheme in <ref type="figure">Figure 5</ref> shows the tags used to compose the search queries used. To compose a single query we concatenate one leaf (a single logo) with a single tag of an ancestor node. The whole set of queries for each logo can be obtained by concatenating the logo name (leaf) with each tag contained in all the ancestors nodes. For example, all the queries used to search for the "Becks" logo are: "logo Becks", "merchandising Becks", "events Becks", "drink Becks", "bottle Becks", "can Becks", "beer Becks", "bier Becks" etc.</p><p>The dataset contains on average 400 examples per class, with each image including one or multiple instances of the same class. The detailed distribution of classes is shown in <ref type="figure" target="#fig_4">Figure 7</ref> and a comparison between the FlickrLogos-32 and the Logos-32plus datasets is presented in Table 2. The dataset is made available for research purposes at http://www.ivl.disco.unimib.it/activities/logorecognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Duplicates Removal</head><p>To ensure a high variability of the new dataset and to avoid any overlap with the existing one, we performed a semi-automatic check for duplicate images within the Logos-32plus dataset itself and with the FlickrLogos-32 dataset. The process has been carried out in two steps. First, we automatically found and discarded image duplicates using the SSIM measure <ref type="bibr" target="#b22">[23]</ref>: we checked for similarity every pair of images within the Logos-32plus dataset itself and with the FlickrLogos-32 dataset using the SSIM measure. Images with SSIM measure over 0.9 have been discarded.</p><p>As a second step, we removed near duplicates in a semiautomatic manner. We say that two images are near duplicates if they depict the same scene with small differences in appearance with a particular focus on the portion of the image containing the logo. Examples of near duplicates are different overlapping crops of the same photo or images of the same scene from a different point of view. An interesting example of near duplicates is shown in <ref type="figure">Figure 6</ref>. The two images depict the same gas station from a very similar point of view. The girls in the photo are in different poses but the appearance of the Esso logo in the two images is basically the same. In detail, to remove near duplicates we used the following procedure: -we trained our CNN (structure in <ref type="table" target="#tab_1">Table 1</ref>) from scratch on Logos-32plus dataset. To accomplish this task we fed the network with crops extracted from GT annotations and Object-proposals regions.</p><p>-We truncated the learned network leaving out the last two layers (softmax and last fully-connected). This network surgery operation let us use our network as a feature extractor exploiting the robust features learned by a deep neural network. We used this truncated network to extract features from every image crop that contains a tagged logo.</p><p>-We trained a k-NN classifier on top of the extracted features (using Logos-32plus as training set) and used it to retrieve from Logos-32plus and FlickrLogos-32 the nearest five results.</p><p>-Finally we manually checked for near duplicates among the five nearest results retrieved by the classifier. All the near duplicates have been discarded from the final dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup and Results</head><p>Experiments are performed considering the different training choices described in Section 2. These include class balancing, data augmentation, image contrast normalization, sample weighting, addition of a background class, and addition of positive examples actually generated by the object proposal algorithm. Each change to the training procedure is introduced one at a time, in order to assess its individual contribution, and the corrisponding value is underlined in <ref type="table" target="#tab_3">Table 3</ref> for better readability. All these configurations are trained using real data augmentation, i.e. with our extended Logos-32plus dataset in addition to FlickrLogos-32 training and validation sets. Results are reported in <ref type="table" target="#tab_3">Table 3</ref> in terms of both F1-measure and Accuracy on FlickrLogos-32 test set. With reference to <ref type="figure">Figure 3</ref>, the threshold on CNN predictions is automatically chosen to maximize the accuracy on FlickrLogos-32 training and validation sets. The best configuration is then compared to other state of the art methods in <ref type="table" target="#tab_5">Table 4</ref>. As further investigation we quantify the contribution given from real data augmentation, by training the same solution on the original FlickrLogos-32 training set only. Finally, we assess the impact of the object proposal algorithm to the overall performance. To do this we add all the ground truth locations to the test set, instead of relying on the object proposal only.</p><p>From the results reported in <ref type="table" target="#tab_3">Table 3</ref> it is possible to see that with respect to a straightforward application of deep learning to the logo recognition task   file:///I:/Documenti/Paper Loghi 2/dendrog <ref type="figure">Figure 5</ref>: Dendrogram representing the queries composition used to download the Logos-32plus dataset. To retrieve images of becks logos we used for instance: "logo Becks", "merchandising Becks", "drink Becks", "bottle Becks", "beer Becks" etc. <ref type="figure">Figure 6</ref>: Example of near duplicates. The two images depict the same scene from a similar point of view. The appearance of the Esso logo in the two images is basically the same. We removed one of the two images from our Logos-32 plus dataset because the other one is included in the FlickrLogos-32 test set.</p><p>show an improvement in F1-measure and accuracy of 31.8% and 52.4% with respect to TC -I.</p><p>-A second jump is obtained by including object proposals coming from Selective Search as additional training examples. This configuration is named TC -III and improves the F1-measure and accuracy by 11.3% and 12.4% with respect to TC -II.</p><p>-A third jump in performance is obtained by augmenting the cardinality of object proposals coming from Selective Search by perturbing them with random translations (i.e. synthetic data augmentation). This configuration is named TC -IV and improves the F1-measure and accuracy by 11.7% and 20.9% with respect to TC -III.</p><p>-A further, smaller, improvement in performance is obtained by considering class balancing to account for different cardinalities, with "Epoch" balancing giving consistently better performance than the "Batch" counterpart (named TC -V and TC -VI respectively).</p><p>In particular, TC -V improves the F1-measure and accuracy by 0.4% and 0.3% with respect to TC -IV.</p><p>-Contrast normalization brings a further little but consistent improvement, with TC -VII improving the F1measure and accuracy by 2.4% and 0.2% with respect to TC -V.</p><p>-Sample weighting instead (adopted in TC -VIII and TC -X), which consists in weighting training examples according to the degree of overlap between the object proposal and ground truth regions, results in lowering the final performance of the method.</p><p>The best configuration (i.e. TC -VII) trained on our extended training set is highlighted in bold in <ref type="table" target="#tab_3">Table 3</ref> and compared with the state of the art in <ref type="table" target="#tab_5">Table 4</ref>. Performances of the other methods are taken from the respective papers and thus for some of them some performance measures are missing. From the results reported it is possible to see that the proposed solution is able to improve the F1-measure with respect to the best method in the state of the art by 3.8%, and the accuracy by 1.7%. It is worth to underline that the best results for the two metrics were obtained by different methods in the state of the art, i.e. by Romberg et. al <ref type="bibr" target="#b7">[8]</ref> and BoW SIFT <ref type="bibr" target="#b7">[8]</ref> respectively.</p><p>As a further comparison, we report the results obtained by our solution using only FlickrLogos-32 for training and keeping all the other training choices unchanged. This results in a drop in F1-measure by 14.7% and by 4.8% in accuracy, giving an idea of the benefit of real data augmentation with respect to a purely synthetic one <ref type="bibr" target="#b13">[14]</ref>. As a final analysis, to understand if the major source of error in our method is the Selective Search module that is unable to have a high recall or if its the CNN itself that mispredicts the logo class, we perform an additional test by adding the actual logo ground truth region to the object proposals. This increases the F1-measure by 0.6% and the accuracy by 0.2% indicating that its the major source of error in our method is the CNN itself. Some examples of wrongly labeled candidate logo regions are reported in <ref type="figure" target="#fig_5">Figure 8</ref>. Candidates are generated by the object proposal and they have a IoU larger than 0.5 with the corresponding ground truth. The first and the third row depict the wrongly recognized regions labeled with their actual class, while the second and fourth one depict the nearest example in the training set using as features the activations of the last network layer before the softmax. Images are reported with the same resolution used to feed the CNN, i.e. 32?32 pixels. <ref type="table" target="#tab_6">Table 5</ref> shows the timings for the whole recognition procedure at test time. Experiments are performed on the same computer (Intel i7 3.40 GHz -16 GB RAM) averaging the timings of 100 runs on different images. Two different solutions are compared: the use of CPU or GPU (GeForce GTX 650) for the classification step. The proposals extraction step runs always on CPU. The prepocessing time include the resize of every patch to match the CNN input size, the contrast normalization (negligible processing time) and eventually the time to copy the data from CPU to GPU memory. In <ref type="table" target="#tab_6">Table 5</ref> it is possible to notice that the overhead caused by the CPU-GPU memory transfer makes the overall time of the GPU solution higher than that of the CPU solution. To this extent, in the future it might be interesting to evaluate a fully GPU-based pipeline, for example generating and pre-processing proposals according to <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Timings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Logo recognition is fundamental in many application domains. The problem is that logos may appear in any position, scale and under any point of view in an image. Moreover, the images may be corrupted by many image artifacts and distortions.</p><p>The traditional approaches to logo recognition involve keypoint-based detectors and descriptors, or the use of CNNs pretrained on different tasks. Our solution employs a CNN specifically trained for the task of logo classification, even if they are not perfectly localized. We designed a complete recognition pipeline including a recall-oriented candidate logo region proposal that feeds our CNN.    Experiments are carried out on the FlickrLogos-32 database and on its enlarged version, Logos-32plus, collected by the authors. We systematically investigated the effect on recognition performance of synthetic versus real data augmentation, image pre-processing, and the benefits of different training choices such as class-balancing, sample-weighting and explicit modeling the background class (i.e. no-logo regions). Our best solution outperforms the methods in the state of the art and makes use of an explicit modeling of the background class, both precise and actual object-proposal logo annotations during training, synthetic data augmentation, epoch-based class balancing, and image contrast normalization as pre-processing, while sample weighting is disabled. Both the newly collected Logos-32plus and the trained CNN are made available for research purposes 1 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Simplified logo classification pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Logo recognition training framework. Logo recognition testing framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Example images for each of the 32 classes of the FlickrLogos-32 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(i.e. Training Configuration I, TC -I), the different training choices considered are able to give a large increase in performance: -The first jump in performance is obtained by including the background (i.e. no-logo examples) as a new class in training. Results are identified as TC -II and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>i n g e r e s s o f e d e x f e r r a r i f o r d f o s t e r s g o o g l e g u i n n e s s h e i n e k e n m i l k a n v i d i a p a u l a n e r p e p s i r i t t e r s p o r t s h e l l s i n g h a s t a r b u c k s s t e l l a a r t o i s t e x a c o t s i n g t a o u p Graphical comparison of the distribution of the 32 logo classes between FlickLogos-32 and our augmented Logos-32plus dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Wrongly labeled logos ordered by confidence. Highest confidence prediction is top-left. Images resolution is 32x32 pixels, i.e. the same used to feed the CNN. The first and third rows are the wrong labeled logos, the second and the fourth rows represent the nearest example in the training set (using the last network layer activations before the softmax as feature vector).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Neural Network Architecture</figDesc><table><row><cell></cell><cell>Layers</cell></row><row><cell>1</cell><cell>Conv 32 filters of 5x5</cell></row><row><cell>2</cell><cell>Pool (max) with stride 2</cell></row><row><cell>3</cell><cell>Relu</cell></row><row><cell>4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison between FlickrLogos-32 and Logos-32plus datasets</figDesc><table><row><cell>FlicrkLogos-32 Logos-32plus</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimental results showing the impact of the different training choices described in Section 2 on the final classification. Results are reported in terms of Precision, Recall, F1-measure and Accuracy.</figDesc><table><row><cell>Train.</cell><cell>BG</cell><cell>BBs</cell><cell cols="2">Data</cell><cell cols="2">Class</cell><cell>Contr.</cell><cell>Sample</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell><cell>Acc.</cell></row><row><cell>Config.</cell><cell>class</cell><cell></cell><cell cols="2">Augm.</cell><cell cols="2">bal.</cell><cell>norm.</cell><cell>weight</cell><cell></cell><cell></cell></row><row><cell>I</cell><cell>No</cell><cell>GT</cell><cell>No</cell><cell></cell><cell>No</cell><cell></cell><cell>No</cell><cell>No</cell><cell>0.370</cell><cell>0.370</cell><cell>0.370</cell><cell>0.096</cell></row><row><cell>II</cell><cell>Yes</cell><cell>GT</cell><cell>No</cell><cell></cell><cell>No</cell><cell></cell><cell>No</cell><cell>No</cell><cell>0.713</cell><cell>0.665</cell><cell>0.688</cell><cell>0.620</cell></row><row><cell>III</cell><cell>Yes</cell><cell cols="2">GT+OP No</cell><cell></cell><cell>No</cell><cell></cell><cell>No</cell><cell>No</cell><cell>0.816</cell><cell>0.787</cell><cell>0.801</cell><cell>0.744</cell></row><row><cell>IV</cell><cell>Yes</cell><cell cols="2">GT+OP Yes</cell><cell></cell><cell>No</cell><cell></cell><cell>No</cell><cell>No</cell><cell>0.987</cell><cell>0.858</cell><cell>0.918</cell><cell>0.953</cell></row><row><cell>V</cell><cell>Yes</cell><cell cols="2">GT+OP Yes</cell><cell></cell><cell cols="2">Epoch</cell><cell>No</cell><cell>No</cell><cell>0.986</cell><cell>0.865</cell><cell>0.922</cell><cell>0.956</cell></row><row><cell>VI</cell><cell>Yes</cell><cell cols="2">GT+OP Yes</cell><cell></cell><cell cols="2">Batch</cell><cell>No</cell><cell>No</cell><cell>0.980</cell><cell>0.833</cell><cell>0.901</cell><cell>0.945</cell></row><row><cell>VII</cell><cell>Yes</cell><cell cols="2">GT+OP Yes</cell><cell></cell><cell cols="2">Epoch</cell><cell>Yes</cell><cell>No</cell><cell>0.989</cell><cell>0.906</cell><cell>0.946</cell><cell>0.958</cell></row><row><cell>VIII</cell><cell>Yes</cell><cell cols="2">GT+OP Yes</cell><cell></cell><cell cols="2">Epoch</cell><cell>Yes</cell><cell>Yes</cell><cell>0.984</cell><cell>0.875</cell><cell>0.926</cell><cell>0.951</cell></row><row><cell>IX</cell><cell>Yes</cell><cell cols="2">GT+OP Yes</cell><cell></cell><cell cols="2">Batch</cell><cell>Yes</cell><cell>No</cell><cell>0.984</cell><cell>0.887</cell><cell>0.933</cell><cell>0.955</cell></row><row><cell>X</cell><cell>Yes</cell><cell cols="2">GT+OP Yes</cell><cell></cell><cell cols="2">Batch</cell><cell>Yes</cell><cell>Yes</cell><cell>0.989</cell><cell>0.866</cell><cell>0.923</cell><cell>0.955</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Legend to Table 3</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Train. Config.</cell><cell cols="5">Identifier of the configuration used for training</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>BG class</cell><cell></cell><cell cols="6">Background class (no-logo examples) included in training</cell><cell></cell></row><row><cell></cell><cell></cell><cell>BBs</cell><cell></cell><cell cols="5">Bounding Boxes used as training examples</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>GT</cell><cell></cell><cell cols="4">Precise ground-truth logo annotations</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GT+OP</cell><cell cols="5">Precise ground-truth and Object-proposal logo annotations</cell></row><row><cell></cell><cell></cell><cell>Data Augm.</cell><cell></cell><cell cols="4">Data Augmentation (translation)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Class bal.</cell><cell></cell><cell cols="6">Class balancing to account for different cardinalities</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell cols="4">Classes are balanced in each epoch</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Batch</cell><cell></cell><cell cols="4">Classes are balanced in each batch as well</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Contr. norm.</cell><cell cols="7">Pre-processing of training examples with contrast normalization</cell></row><row><cell></cell><cell></cell><cell cols="2">Sample weight</cell><cell cols="6">Weighting examples based on overlap between OP and GT</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the best configuration inTable 3with the methods in the state of the art.</figDesc><table><row><cell>Method</cell><cell cols="3">Train data Precision Recall</cell><cell cols="2">F1 Accuracy</cell></row><row><cell>BoW SIFT [8]</cell><cell>FL32</cell><cell>0.991</cell><cell>0.784</cell><cell>0.875</cell><cell>0.941</cell></row><row><cell>BoW SIFT + SP + SynQE [8]</cell><cell>FL32</cell><cell>0.994</cell><cell>0.826</cell><cell>0.902</cell><cell>N/A</cell></row><row><cell>Romberg et al. [9]</cell><cell>FL32</cell><cell>0.981</cell><cell>0.610</cell><cell>0.752</cell><cell>N/A</cell></row><row><cell>Revaud et al. [10]</cell><cell>FL32</cell><cell>?0.980</cell><cell cols="2">0.726 0.834?0.841</cell><cell>N/A</cell></row><row><cell>Romberg et al. [8]</cell><cell>FL32</cell><cell>0.999</cell><cell>0.832</cell><cell>0.908</cell><cell>N/A</cell></row><row><cell>Bianco et al. [13]</cell><cell>FL32</cell><cell>0.909</cell><cell>0.845</cell><cell>0.876</cell><cell>0.884</cell></row><row><cell>Bianco et al. + Q.Exp. [13]</cell><cell>FL32</cell><cell>0.971</cell><cell>0.629</cell><cell>0.763</cell><cell>0.904</cell></row><row><cell>Eggert et al. [14]</cell><cell>FL32</cell><cell>0.996</cell><cell>0.786</cell><cell>0.879</cell><cell>0.846</cell></row><row><cell>Oliveira et al. [16]</cell><cell>FL32</cell><cell>0.955</cell><cell>0.908</cell><cell>0.931</cell><cell>N/A</cell></row><row><cell>DeepLogo [15]</cell><cell>FL32</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.896</cell></row><row><cell>Ours (TC -VII)</cell><cell>FL32</cell><cell>0.976</cell><cell>0.676</cell><cell>0.799</cell><cell>0.910</cell></row><row><cell>Ours (TC -VII)</cell><cell>FL32, L32+</cell><cell>0.989</cell><cell>0.906</cell><cell>0.946</cell><cell>0.958</cell></row><row><cell cols="2">Ours (TC -VII, adding GT to the obj. prop.) FL32</cell><cell>0.968</cell><cell>0.755</cell><cell>0.848</cell><cell>0.917</cell></row><row><cell cols="2">Ours (TC -VII, adding GT to the obj. prop.) FL32, L32+</cell><cell>0.989</cell><cell>0.917</cell><cell>0.952</cell><cell>0.960</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Timings of the whole recognition pipelineDevice Proposal Preproc. Classif. Overall</figDesc><table><row><cell>CPU</cell><cell>1.24 s</cell><cell>0.93 s</cell><cell>0.71 s</cell><cell>2.91 s</cell></row><row><cell>GPU</cell><cell>1.24 s</cell><cell>2.12 s</cell><cell>0.36 s</cell><cell>3.74 s</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vehicle logo recognition using a sift-based enhanced matching scheme, Intelligent Transportation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Psyllos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><forename type="middle">E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kayafas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="322" to="328" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Brand data gathering from live social media streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimedia Retrieval</title>
		<meeting>International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">169</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shape recognition and pose estimation for mobile augmented reality, Visualization and Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hagbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bergig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>El-Sana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1369" to="1379" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Trademark matching and retrieval in sports video databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international workshop on Workshop on multimedia information retrieval</title>
		<meeting>the international workshop on Workshop on multimedia information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatial pyramid mining for logo detection in natural scenes, in: Multimedia and Expo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1077" to="1080" />
			<date type="published" when="2008" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Logo retrieval with a contrario visual query expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM international conference on Multimedia</title>
		<meeting>the 17th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="581" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive visual object search through mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1147" to="1150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bundle min-hashing for logo recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM conference on International conference on multimedia retrieval</title>
		<meeting>the 3rd ACM conference on International conference on multimedia retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable logo recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Pueyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM International Conference on Multimedia Retrieval</title>
		<meeting>the 1st ACM International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Correlation-based burstiness for logo retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Multimedia</title>
		<meeting>the 20th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="965" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Logo localization and recognition in natural images using homographic class graphs, Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dogaru</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="287" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Elliptical asift agglomeration in class prototype for logo detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Florea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>BMVC</publisher>
			<biblScope unit="page" from="115" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Logo recognition using cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buzzelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and ProcessingI-CIAP 2015</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="438" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the benefit of synthetic data for company logo detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eggert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Winschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia Conference</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1283" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deeplogo</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02131</idno>
		<title level="m">Hitting logo recognition with the deep neural network hammer</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic graphic logo detection via fast region-based convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fraz?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks (IJCNN), 2016 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="158" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity, Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
