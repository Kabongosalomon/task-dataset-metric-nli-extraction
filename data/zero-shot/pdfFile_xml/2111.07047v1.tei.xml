<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accepted in Computer Vision and Image Understanding Journal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Pourramezan</forename><surname>Fard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<addrLine>2155 E Wesley Ave</addrLine>
									<postCode>80208</postCode>
									<settlement>Denver</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Accepted in Computer Vision and Image Understanding Journal</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial landmark detection is a vital step for numerous facial image analysis applications. Although some deep learning-based methods have achieved good performances in this task, they are often not suitable for running on mobile devices. Such methods rely on networks with many parameters, which makes the training and inference time-consuming. Training lightweight neural networks such as Mo-bileNets are often challenging, and the models might have low accuracy. Inspired by knowledge distillation (KD), this paper presents a novel loss function to train a lightweight Student network (e.g., MobileNetV2) for facial landmark detection. We use two Teacher networks, a Tolerant-Teacher and a Tough-Teacher in conjunction with the Student network. The Tolerant-Teacher is trained using Softlandmarks created by active shape models, while the Tough-Teacher is trained using the ground truth (aka Hard-landmarks) landmark points. To utilize the facial landmark points predicted by the Teacher networks, we define an Assistive Loss (ALoss) for each Teacher network. Moreover, we define a loss function called KD-Loss that utilizes the facial landmark points predicted by the two pre-trained Teacher networks (EfficientNet-b3) to guide the lightweight Student network towards predicting the Hard-landmarks. Our experimental results on three challenging facial datasets show that the proposed architecture will result in a better-trained Student network that can extract facial landmark points with high accuracy. Recently, knowledge distillation (KD) was utilized in image classification Hinton et al. (2015); Romero et al. (2014), object detection Li et al. (2017), and semantic segmentation Xie et al. (2018). Initially, the idea was to train a lightweight network with acceptable accuracy by transferring features and knowledge generated by an ensemble network into the single smaller network Bucilu? et al. (2006). Later, Hinton et al. Hinton et al.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial image alignment based on landmark points is a crucial step in many facial image analysis applications including face recognition <ref type="bibr" target="#b26">Lu and Tang (2015)</ref>; <ref type="bibr" target="#b41">Soltanpour et al. (2017)</ref>, face verification <ref type="bibr" target="#b45">Sun et al. (2014</ref><ref type="bibr" target="#b44">Sun et al. ( , 2013</ref>, face frontalization <ref type="bibr" target="#b16">Hassner et al. (2015)</ref>, pose estimation <ref type="bibr" target="#b50">Vicente et al. (2015)</ref>, and facial expression recognition <ref type="bibr" target="#b45">Sun et al. (2014)</ref>; <ref type="bibr" target="#b65">Zhao et al. (2003)</ref>. The goal is to detect and localize the coordinates of predefined landmark points on human faces and use them for face alignment. In the past two decades, great progress has been made toward improving facial landmark detection algorithms' accuracy. However, most of the previous research does not focus on designing and/or training lightweight networks that can run on mobile devices with limited computational power.</p><p>While facial landmark points detection is still considered a challenging task for faces with large pose variations and occlusion <ref type="bibr" target="#b9">Dong et al. (2018a)</ref>; <ref type="bibr" target="#b54">Wu et al. (2018)</ref>, recent methods have designed heavy models with a large number of parameters, which making them unsuitable for real-time applications. Moreover, with the growth of Internet-of-Things (IoT), robotics, and mobile devices, it is vital to balance accuracy and model efficiency (i.e., computational time). Recently, deep learning-based methods have caught the attention of people in tackling this problem too. Among many lightweight neural network models, MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> is proven to be a good trade-off between accuracy and speed. However, because of the small number of network parameters, the face alignment task's accuracy using MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> might not be enough, especially when applied to faces with extreme poses or occlusions.</p><p>Tan and Le <ref type="bibr" target="#b46">Tan and Le (2019)</ref> have recently proposed Effi-cientNet, a family of eight different networks designed to put a trade-off between the accuracy and model size. The designer of EfficientNet found a strong connection between the accuracy of a network and its depth, width, and resolution. Consequently, the proposed EfficientNet family is designed to be efficient. In other words, EfficientNet family are designed to achieve good accuracy while they are relatively small means having a fewer number of network parameters and fast means having a smaller number of floating points operation (FLOPs). 2 (2015) introduced the term knowledge distillation as a technique to create a small model, called Student network, learned to generate the results that are created by a more cumbersome model, called Teacher network.</p><p>Inspired by the concept of KD, we propose a novel loss function called KD-Loss to improve face alignment accuracy. Specifically, we propose a KD-based architecture using two different Teacher networks -EfficientNet-B3 <ref type="bibr" target="#b46">Tan and Le (2019)</ref> -to guide the lightweight Student-Network, which is Mo-bileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>, to better cope with the facial landmark detection task. Using the facial landmarks predicted by each of the Teacher networks, we introduce two ALoss functions. Being assisted by the ALoss, the KD-Loss considers the geometrical relation between the facial landmarks predicted by the Student and the two Teacher networks to improve the accuracy of the MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>. In other words, we proposed to use two independent sets of facial landmark points which are predicted by our Teacher network to guide the lightweight Student network towards better localization of the landmark points.</p><p>We train our method in two phases. In the first phase, we create Soft-landmarks inspired by <ref type="bibr">ASM Cootes et al. (1998)</ref>. Soft-landmarks are more similar to the Mean-landmark compared to the Hard-landmarks, which are the original facial landmarks. Hence, as a rule of thumb, it is easier for a lightweight model to predict the distribution of these Soft-landmarks compared to the original ground truth. We use this attribute to create a Teacher-Student architecture to improve the accuracy of the Student network. More clearly, in the first phase, we train one Teacher network using the Hard-landmarks and call it Tough-Teacher, and another Teacher network using the Soft-landmarks as the ground truth landmark points and call it Tolerant-Teacher. Then, in the second phase, we use our proposed KD-Loss to transfuse the information gathered by both Teacher networks into the Student model during the training phase. <ref type="figure" target="#fig_4">Fig. 5</ref> depicts a general architecture of our proposed training architecture. We tested our proposed method on the challenging 300W <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref>, <ref type="bibr">WFLW Wu et al. (2018)</ref>, and COFW Burgos- <ref type="bibr" target="#b4">Artizzu et al. (2013)</ref> datasets. The results of our experiments show that the accuracy of facial landmark points detection using MobileNetV2 trained using our KD-Loss approach is more accurate than the original MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>. The results are also comparable with state-of-the-art methods, while the network size is significantly smaller than most of the previously proposed networks.</p><p>The contributions of our approach are summarized as follows. First, to the best of our knowledge, this is the first time the concept of KD is applied to a coordinate-based regression facial landmark detection model. Second, we proposed two different Teacher networks for guiding the Student network toward the ground truth landmark point. Third, different from the popular loss functions, the magnitude of our proposed ALoss can be either a positive or a negative number. Finally, using ALoss, we propose KD-Loss, which uses the geometrical relation between the Student and Teacher networks to improve accuracy in facial landmark detection.</p><p>The remaining of this paper is organized as follows. Sec. 2 reviews the related work in facial landmark points detection. Sec. 3 explains the details of the proposed method and the training process. Then, the evaluation of the method as well as the experimental results are provided in Sec. 4. Finally, Sec. 5 concludes with discussion on the proposed method and future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The facial landmark detection task dates back to over twenty years ago, when classical methods (aka template-based methods) were introduced. Active Shape Model (ASM) <ref type="bibr" target="#b5">Cootes et al. (2000)</ref> and Active Appearance Model (AAM) <ref type="bibr" target="#b6">Cootes et al. (1998)</ref>; <ref type="bibr" target="#b29">Martins et al. (2013)</ref> are among the first methods for facial landmark detection. Based on these methods, Principal Components Analysis (PCA) is applied to simplify the problem and learn parametric features of faces to model facial landmarks variations. The model is iteratively fit into new instances. To match a 3D deformable face model to 2D images, <ref type="bibr" target="#b29">Martins et al. (2013)</ref> proposed a 2.5D AAM that combines a 3D metric Point Distribution Model (PDM) a 2D appearance model. The Constrained Local Model (CLM) proposed by Cristinacce and Cootes <ref type="bibr" target="#b7">Cristinacce and Cootes (2006)</ref> and its various extensions including <ref type="bibr" target="#b0">Asthana et al. (2013)</ref>; <ref type="bibr" target="#b1">Baltru?aitis et al. (2012)</ref>; <ref type="bibr" target="#b38">Saragih et al. (2011);</ref><ref type="bibr" target="#b52">Wang et al. (2008)</ref>, are among the most promising methods for face alignment. CLM models the face shapes with Procrustes analysis and principal component analysis. However, CLM methods are sensitive to occlusion as well as illumination when detecting landmarks in unseen datasets. To reduce the effect of outliers, Robust Cascade Pose Regression (RCPR) <ref type="bibr" target="#b4">Burgos-Artizzu et al. (2013)</ref> was introduced to detect occlusions explicitly while using robust shape-indexed features. Another computationally lightweight method was Local Binary Features (LBF) <ref type="bibr" target="#b34">Ren et al. (2014)</ref>, which uses the locality principle to learn a set of highly discriminative local binary features from each facial landmarks independently. More over, <ref type="bibr" target="#b20">Kazemi Kazemi and Sullivan (2014)</ref> proposed a gradient boosting framework using ensemble of regression trees for computationally-efficient face alignment. In their proposed framework, they first extract a sparse subset of intensity values form the input image, and then using cascade of regression trees to localize the facial landmark points.</p><p>Coordinate-based regression models predict the facial landmark coordinates vector from the input image directly. Mnemonic Descent Method (MDM) <ref type="bibr" target="#b47">Trigeorgis et al. (2016)</ref> has utilized a recurrent convolutional network to detect facial landmarks. <ref type="bibr" target="#b12">Feng et al. Feng et al. (2018)</ref> introduced Wingloss, a new loss function that is capable of overcoming the widely used L2 loss in conjunction with a strong data augmentation method as well as a pose-based data balancing (PDB). To ease the parts variations and regresses the coordinates of different parts, Two-Stage Re-initialization Deep Regression MODEL (TSR) <ref type="bibr" target="#b27">Lv et al. (2017)</ref> splits face into several parts. <ref type="bibr" target="#b64">Zhang and Hu (2018)</ref> proposed Exemplar-based Cascaded Stacked Auto-Encoder Network (ECSAN) for face alignment, which is utilized to handle partial occlusion in the image. To cope with self-occlusions and large face rotations, <ref type="bibr" target="#b49">Valle et al. (2019)</ref> proposed a face alignment algorithm based on a coarse-to-fine cascade of ensembles of regression trees, which is initialized by robustly fitting a 3D face model to the probability maps produced by a pre-trained convolutional neural network (CNN). <ref type="bibr" target="#b15">Guo Guo et al. (2019)</ref> proposed a framework for practical face alignment, which estimates the rotation information during the train phase and use such information to better cope with the challenging faces with extreme pose, lighting and occlusion. <ref type="bibr" target="#b13">Feng Feng et al. (2020)</ref> proposed RWing loss, a piece-wise loss that amplifies the impact of the samples with small-medium errors, while rectifying the loss function for very small errors. More recently, <ref type="bibr" target="#b11">Fard Fard et al. (2021)</ref> proposed a ASMNet, a lightweight multi-task network for jointly detecting facial landmark points as well as the estimation of face pose.</p><p>In heatmap-based regression models, first, the likelihood heatmaps for each facial landmark are created, and then the network is trained to generate those heatmaps for each input image. A two-part network proposed by <ref type="bibr" target="#b61">Yang Yang et al. (2017)</ref>, including a supervised transformation to normalize faces and a stacked hourglass network <ref type="bibr" target="#b31">Newell et al. (2016)</ref>, is designed to predict heatmaps. In another work JMFA <ref type="bibr" target="#b8">Deng et al. (2019)</ref> leveraged stacked hourglass network for multi-view face alignment, and it achieved state-of-the-art accuracy and demonstrated more accurate than the best three entries of the last Menpo Challenge <ref type="bibr" target="#b63">Zafeiriou et al. (2017)</ref>. <ref type="bibr">LAB Wu et al. (2018)</ref> proposed by Wu first expressed that the facial boundary line contains valuable information. Hence, they utilized boundary lines as the geometric structure of a face to help facial landmark detection. In another work, for a better initialization to Ensemble of Regression Trees (ERT) regressor, <ref type="bibr" target="#b48">Valle Valle et al. (2018)</ref> proposed a simple CNN to generate heatmaps of landmark locations. Additionally, <ref type="bibr" target="#b43">Sun et al. (2019)</ref> introduced HRNet, a high-resolution network that is applicable in many Computer Vision tasks such as facial landmark detection and achieves a reasonable accuracy. <ref type="bibr" target="#b19">Iranmanesh et al. (2020)</ref> proposed an approach that provides a robust facial landmark detection algorithm that handles shape variations in facial landmark detection while the aggregating set of manipulated images to capture robust landmark representation. In another work, <ref type="bibr" target="#b58">Xiong et al. (2020)</ref> proposed the Gaussian heatmap vectors instead of the widely used heatmap for facial landmark points detection. More recently, to deal with the more challenging faces, <ref type="bibr" target="#b28">Mahpod et al. (2021)</ref> proposed a two-paired cascade subnetwork to generate heatmap and accordingly the coordinates of the facial landmark points.</p><p>Although heatmap regression models are more accurate than coordinate regression models, we follow the latter models since such models are significantly smaller in terms of both memory usage and the number of FLOPs, hence more suitable for mobile and embedded applications.</p><p>In addition, most of the previous work has proposed and/or utilized heavy networks with a large number of parameters and arithmetic operations. Consequently, such models are not applicable when utilized by embedded and mobile devices. In contrast, we propose a KD-based architecture and our novel KD-Loss to train lightweight models (e.g., MobileNetV2 having about 2.2 million parameters) that have significantly fewer parameters and arithmetic operations, while its accuracy is comparable with previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Model</head><p>In this section, we first explain the process of creating the Soft-landmarks inspired from ASM. The Soft-landmarks are utilized for training the Tolerant-Teacher network. Then, we illustrate our proposed Student-Teacher architecture. After that,  we explain our proposed KD-Loss function using the the proposed assistive loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Soft and Hard Landmarks</head><p>Inspired by ASM, we model a face shape object f, which is a vector containing the coordinates of landmark points for each face, using Eq. 1:</p><formula xml:id="formula_0">f (k?1) ? f k?1 + V k?m b m?1<label>(1)</label></formula><p>where k is the number of the landmark points, f, the Meanlandmark, is the point-wise mean of all facial landmarks in the training set, and V = {v 1 , v 2 , ..., v m } is a set containing m Eigenvectors of the covariance matrix of all facial landmarks. b is also a m?dimensional vector given by Eq. 2:</p><formula xml:id="formula_1">b m?1 = V m?k [f k?1 ? f k?1 ]<label>(2)</label></formula><p>To ensure that the generated face is similar to the original face, b is defined by placing a restriction over b vector and limiting each of its elements to be between ?3 ? ? i and +3 ? ? i <ref type="bibr" target="#b5">Cootes et al. (2000)</ref>, where ? i is the statistical variance of the i th parameter of b. In Eq. 3, the new face shape f-new is created after applying this constraint:</p><formula xml:id="formula_2">f-new k?1 = f k?1 + V k?mbm?1<label>(3)</label></formula><p>Then we define the parameterm as the proportion of the total number of Eigenvectors we use to generate the Soft-landmarks.</p><p>In other words,m define the similarity between the generated Soft-landmarks and the Hard-landmarks. Consequently, the smaller the parameterm, the fewer Eigenvectors we use to create f-new and thus, the generated f-new becomes more similar to the Mean-landmark. This effect is shown in <ref type="figure">Fig. 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Main</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALoss Tol ALoss Tou</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hard-landmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tough Teacher</head><p>Network Student Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accurate-landmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weak-landmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smooth-landmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KD-Loss</head><p>Input Images</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EfficientNet-B3 MobileNetv2</head><p>Updating the weights of the Student Network landmarks, in order to display the variations more clearly, in <ref type="figure" target="#fig_1">Fig. 3</ref>-(b) we only visualize the landmark points belonging to the face boundary. As <ref type="figure" target="#fig_1">Fig. 3</ref> shows, there are less variations in Soft-landmarks, and therefore it is easier for a deep neural network to learn such distributions. In this paper, we choose the parameterm as 90% of all the Eigenvectors and accordingly generate the Soft-landmarks using Eqs. 1, 2, 3 (examples are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed Architecture</head><p>Our proposed model consists of three main parts, two Teacher networks and a Student network. Being one of the best in the category of lightweight networks, we use Mo-bileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> as the Student network. Furthermore, we choose EfficientNet-B3 Tan and Le (2019) as our Teacher network. In our proposed architecture, we have two Teacher networks: Tough-Teacher, which is trained using Hard-landmarks, and Tolerant-Teacher trained using Softlandmarks. Since the variation of Soft-landmarks is smaller compared to the Hard-landmarks (see Sec.3.1), it is easier for deep neural networks to learn the face alignment task over the Soft-landmarks. However, the accuracy of the Soft-landmarks is lower in comparison to the original Hard-landmarks. We introduce the KD-Loss which uses the advantages of both Teachers to guide the Student network to learn the facial alignment task better. Tolerant-Teacher, trained on the Soft-landmarks has lower accuracy, but is easier to predict, while Tough-Teacher trained on the Hard-landmarks has higher accuracy, but is harder to predict.</p><p>Moreover, our proposed method consists of two phases. In the first phase, we independently train both Teacher networks using a standard L2 loss. In the second phase, we train the Student network using KD-Loss. More specifically, KD-Loss uses the landmark points generated by Tough and Tolerant Teachers to guide the Student towards learning the face alignment task more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Assistive Loss</head><p>We define ALoss to make the advantages of the geometrical knowledge of the Teacher networks. In other words, ALoss uses the facial landmark points predicted by each Teacher network to guide the Student network towards the ground truth. After training both the Tough-Teacher and Tolerant-Teacher on the training set independently, we have two different kinds of soft targets: the Accurate-landmarks predicted by the Tough-Teacher as well as the Smooth-landmarks predicted by the Tolerant-Teacher. We define P Gt as an arbitrary facial landmark point from Hard-landmarks set, P Ac , and P S m the corresponding landmark points from the Accurate-landmarks and Smoothlandmarks sets respectively. Likewise, P Pr is the corresponding predicted points by the Student-Network. The idea behind the ALoss is to use the facial landmark points predicted using the Teacher networks as either a Positive or negative assistant. Positive assistant means that the assistive loss function penalize the network to generate a landmark point which is close to the corresponding point predicted by the Teacher network, while the Negative assistant means the assistive loss function penalize the Student network to predict a landmark point which is far from the corresponding landmark point predicted by the Teacher network.</p><p>As an example, in <ref type="figure">Fig. 6</ref>-A, in order to minimize the distance between P Pr and P Gt , we use the ALoss T ou as a Positive assistant, which means the ALoss T ou penalize the network to reduces the distance between P Ac and P Pr . In contrary, the ALoss T ol is a Negative assistant, which means the it penalize the network to predict P Pr to be far from P S m . More clearly, besides penalizing the Student network to learn the distribution of the Hard-landmarks, we guide it to learn both the distribution of the Accurate-landmarks as well as the Smooth-landmarks which are easier for a lightweight model. Moreover, to simplify and make the assistive loss function symmetric with respect to the coordinate of P Gt , we need both P T e (a facial landmark point predicted using either of the Teacher networks) and P Pr to be in one side of P Gt . Therefore, we adapt the coordinate of P T e using Eq. 4:</p><formula xml:id="formula_3">P T e = P Gt + sign(P Pr ? P Gt ) |P T e ? P Gt |<label>(4)</label></formula><p>As <ref type="figure">Fig. 6</ref>-B shows, P Ac (or P S m ) is not between P Pr and P Gt , we use its symmetric point P Ac calculated using Eq.4 to make the assistive loss a symmetric function with respect to P Gt . Then, we define parameter ? T e with respect to the coordinate of both P Gt and P T e using Eq. 5:</p><formula xml:id="formula_4">? T e = P Gt + ? T e sign(P Pr ? P Gt ) |P T e ? P Gt | (5)</formula><p>? T e defines a threshold for the ALoss. As such, if the distance between P Pr and P Gt is smaller than the distance between ? T e and P Gt , then P Pr is considered as accurate enough, and the ALoss gradually reduces the magnitude of the assistive loss. As shown in Eq. 5, we define ? T e to be a portion (? T e ) of the distance between P Gt and P T e , where smaller values of ? T e penalize the Student network more. However, it forces the network to put too much effort toward improving the accuracy of landmark points which can reduce the accuracy in general. We choose ? T e as 0.4, so the distance between ? T e and P Gt is 40% of the distance between P Gt and P T e . Then the ALoss consider the prediction as accurate enough.</p><p>In order to define the ALoss, we define an assistant weight function ? T e , which enables the ALoss to adjust its magnitude according to the coordinates of the P Gt , P T e and P Pr . We define ? T e using Eq. 6:</p><formula xml:id="formula_5">? T e (p) = ? ? ? ? ? ? ? ? ? ? ? 1 ? p ? R P ?0.5 ? p ? R N ?0.5 ? T e ?P Gt ? p ? R L<label>(6)</label></formula><p>We define R P as a region in which ALoss acts as a Positive assistant. We define this region using the relation between the location of the P Gt , P Pr , and P T e as follow in Eq.7:</p><formula xml:id="formula_6">R P : |P Pr ? P Gt | ? |P T e ? P Gt | ? 0<label>(7)</label></formula><p>which means for any predicted P Pr , if the distance between P Pr and P Gt is greater than the distance between the P T e and P Gt , ALoss acts as a Positive assistant and penalize the Student network to predict P Pr to be close to P T e . Similarly, we define R N as a region in which ALoss acts as a Negative assistant, and penalize the Student network to predict P Pr to be far from P T e .  <ref type="figure">Fig. 7</ref>: ? T e and ALoss for P Gt = 0, and P T e = 0.4.</p><p>We define this region as follow in Eq.8:</p><formula xml:id="formula_7">R N : ? ? ? ? ? ? ? ? ? |P T e ? P Gt | ? |P Pr ? P Gt | ? 0 &amp; |P Pr ? P Gt | ? |B T e ? P Gt | ? 0 (8)</formula><p>Then, we define R L as the Low Influence region, meaning we consider the prediction of the landmark point P Pr as accurate enough if it is located in this region. Although ALoss acts as a Negative assistant in this region, the magnitude of the ALoss decreases as P Pr get closer to P Gt . we define R L as follows in Eq.9:</p><formula xml:id="formula_8">R L : |B T e ? P Gt | ? |P Pr ? P Gt | ? 0<label>(9)</label></formula><p>In other words, we consider a predicted P Pr as accurate enough if it is located between the ground truth point, P Gt , and B T e (see Eq.5).</p><p>Next, we define ALoss T e as the assistive loss with respect to P T e using Eq. 10:</p><formula xml:id="formula_9">ALoss T e = ? T e |P T e ? P Pr |<label>(10)</label></formula><p>We define Both ? T e and consequently ALoss T e according to three different regions(see <ref type="figure">Fig. 7</ref>, and Eqs. 6 and 10): a) the Positive Assistant region, R P where the value of ? T e is 1, a positive number, and hence, ALoss T e is also a positive assistive loss, meaning that the closer P Pr is to P T e , the smaller the value of the ALoss T e . b) the Negative Assistant region, R N , where P Pr ? [? T e , P T e ). In this region, ? T e is define as ?0.5, which is a negative number. Consequently, ALoss T e is defined as a negative assistive loss, meaning that the further the P Pr is from the P T e , the smaller the value of ALoss T e becomes. In other words, in this region we design ALoss T e to train the Student network to use the coordinates of P T e and tries to increase the distance between P Pr and P T e . c) the Low Influence region, R L , where P Pr ? [P Gt , ? T e ). As shown in Eq. 6, in this region P Gt <ref type="figure">Fig. 8</ref>: Loss Main for P Gt = 0.</p><p>? T e is a linear function with negative slope where its minimum value, ?0.5, is at ? T e and its maximum value, 0, is at P Gt . If the predicted point P Pr is closer to P Gt than ? T e , we consider the prediction as good enough, and we design ? T e to gradually reduce the loss magnitude. Furthermore, ? T e is designed such that ALoss T e be continuous and its value is 0 when P Pr is equal to P Gt . In practice, the facial landmark points used for training the models are zero-centered normalized, which means for any arbitrary facial landmark point P x,y , both x and y coordinates are in [?0.5, 0.5] range. As Eq. 10 shows, ALoss T e is a piece-wise continuous function. In addition, it is a linear function in R P , and R N regions, and a quadratic function in R L region. <ref type="figure">Fig. 7</ref> shows ? T e and ALoss T e functions, for P Gt = 0 and P T e = 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Proposed KD-Loss</head><p>Hinton <ref type="bibr" target="#b17">Hinton et al. (2015)</ref> used the term Knowledge Distillation, as they created soft targets as the class probabilities generated by the Teacher model, which replaced the soft max layer of the network with a logit layer. In contrast, our novel KD-Loss and the proposed a Teacher-Student architecture are different than the standard KD concept in 2 ways: first, while KD is mostly used for classification tasks, to the best of our knowledge, this is the first time KD is used in a coordinate regression task. Second, in the original KD concept, the Student network tries to mimic the output results of the Teacher network while in our proposed KD-Loss, we provide two different assistant loss functions, ALoss T ou and ALoss T ol corresponding to the facial landmark points predicted by the Tough and the Tolerant Teachers respectively. Accordingly, opposite to the original KD idea, where the Student network tries to mimic the results predicted by the Teacher network, our Student-Network, guided by KD-Loss, uses the coordinates of facial landmark points predicted by the Teacher networks to better predict the ground truth points. Third, we utilize two different Teacher networks, Tough-Teacher and call its predicted facial landmark points as Accurate-landmarks, as well as the Tolerant-Teacher and call 7 P Gt P Gt P Ac P Ac</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P Sm P Sm ALoss Tou</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALoss Tol</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALoss Tou</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALoss Tol</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KD-Loss KD-Loss</head><p>Loss Main Loss Main <ref type="figure">Fig. 9</ref>: Two examples of Proposed KD-Loss. The larger the distance between P Gt and P Ac (or P S m ), the greater the absolute value of the corresponding ALoss T ou (or ALoss T ol ) in R N . As a consequence, the effect of the negative assistive loss is more in the left figure, compared to the right <ref type="figure">figure.</ref> its corresponding predicted facial landmark points as Smoothlandmarks. Comparing with the ground truth set, called as the Hard-landmarks, Accurate-landmarks are more accurate than the Smooth-landmarks, while having more complex distribution.</p><p>In order to define the KD-Loss, we first need to define the formal equation for the both assistive loss functions. We define P GT [i, n] as the ground truth coordinate of the i th landmark point of the n th image in the training set. Likewise, we define P Pr [i, n], P Ac [i, n], and P S m [i, n] as the corresponding landmark points from the predicted, accurate, and smooth landmark points set respectively. In addition, for each facial landmark point P GT [i, n], we define ? T ou <ref type="bibr">[i, n]</ref> and ? T ol [i, n] as the assistant weight functions corresponding to the Tough, and Tolerant Teachers, respectively (see Eq. 6).</p><p>Then, we define ALoss T ou [i, n] and ALoss T ol [i, n], the assistive loss functions corresponding to the Tough, and Tolerant Teachers, respectively using Eq. 10. Accordingly, we define the assistant loss functions for all facial landmark points in the training set using Eq. 11:</p><formula xml:id="formula_10">ALoss T ou = 1 N k N n=1 k i=1 ALoss T ou [i, n] ALoss T ol = 1 N k N n=1 k i=1 ALoss T ol [i, n]<label>(11)</label></formula><p>where k is the number of the facial landmark points in a face, and N is the number of all samples in the training set. Moreover, we define Loss Main in Equations. 12, 13:</p><formula xml:id="formula_11">?[i, n] = |P Gt [i, n] ? P Pr [i, n]|<label>(12)</label></formula><formula xml:id="formula_12">Loss Main = 1 N k N n=1 k i=1 ?[i, n] If: ?[i, n] ? 0.5 1 N k N n=1 k i=1 ? 2 [i, n] + C otherwise<label>(13)</label></formula><p>where C = 0.25 is defined to smoothly connect the two pieces together. As shown in <ref type="figure">Fig. 8</ref>, for any facial landmark point P Gt [i, n], we define Loss Main as a continuous piece-wise function with respect to the parameter ?. Accordingly, for any facial landmark points if ?[i, n] is greater than 0.5, we define Loss Main as L2 loss. On the contrary, for ?[i, n] smaller than 0.5, we define Loss Main as L1 loss. To make the most advantages of both L2 and L1 loss, we define our proposed Loss Main as a combination of both loss functions. L2 loss (y = x 2 ) penalize the model more for the large. Since its derivative (y = 2x) is a linear function of the errors, the larger the error, the larger the magnitude of the derivative (the influence of the loss function). For small errors (errors that are smaller than 1), its influence becomes very low, leading the network to focus more on large errors, while neglecting small errors. On the contrary, the influence of L1 loss (y = x) is 1 (y = 1), meaning L1 is not sensitive to the large errors. Accordingly, the magnitude and the influence of the errors in L1 loss is larger compared to L2 loss for small errors. Given this, we define Loss Main with the parameter ? = 0.5 (in Sec.3.3, we discussed that coordinates of all facial landmarks are in [-0.5, 0.5]) to both be sensitive to small and large errors.</p><p>Then, we define KD-Loss as a linear combination of ALoss T ou , ALoss T ou and Loss Main in Eq. 14, and also depict it in <ref type="figure">Fig. 9</ref>:</p><formula xml:id="formula_13">KD-Loss = ? ? Loss Main + ALoss T ou + ALoss T ou<label>(14)</label></formula><p>As we have two assistant loss functions, we the parameter ? to be 2 to equalize the effect of Loss Main with the sum of ALoss T ou and ALoss T ou . Moreover, since we have trained both Tough and Tolerant Teachers separately before, the coordinates of the points predicted by them is in [?0.5, +05]. Besides, According to Equations. 6 and 10, the minimum value for ALoss will be at its corresponding ?. By defining the parameter ? = 0.4 (see Eq. 5), we ensure that the proposed KD-Loss will never be negative in its domain of declaration.</p><p>According to Eq. 10 (also is shown in <ref type="figure">Fig 9)</ref>, the further the points predicted by the Teacher from its corresponding ground truth point, the higher the negative value of the corresponding assistant loss. In other words, the distance between the points predicted by Teacher networks from their corresponding ground truth can imply how hard is the distribution of that point. Accordingly, when the distribution of the ground truth points is hard, KD-Loss tries to guide the network by increasing the effect of the assistant loss functions in order to put more attention on corresponding Smooth or Accurate faces rather than Hardlandmarks.</p><p>While for the L2 loss, the curvature of the loss function corresponds to the difference between the ground truth points and the corresponding predicted points, the curvature of KD-Loss is defined according to ? T ou and ? T ol (see Eq. 5). For each point P Gt [i, n], we define the parameters ? T ou [i, n] and ? T ol [i, n] with respect to P Ac [i, n] and P S m [i, n] respectively. The relation between the ? T ou [i, n] and ? T ol <ref type="bibr">[i, n]</ref>, and the corresponding points predicted by Teacher networks, P Ac [i, n] and P S m [i, n], adjusts the curvature of the corresponding KD ? Loss <ref type="bibr">[i, n]</ref>. This means that, the further the distance between the points predicted by Teacher networks from their corresponding ground truth, the further the corresponding ? n T ou i and ? n T ol i , and thus the wider the Low Influence region (see Sec. 3.3), and the smoother the curvature of KD ? Loss <ref type="bibr">[i, n]</ref>. It is also shown in <ref type="figure">Fig. 9</ref>, that the curvature of the KD-Loss relates to the both ? T ou and ? T ol .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we first explain the training phase and the datasets that we used in evaluating our proposed model. We then describe the test phase as well as the implementation details and the evaluation metrics. Lastly, we present the results of facial landmark points detection using our proposed KD-Loss method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conducted the training and evaluation of our models on three popular and challenging datasets: 300W Sagonas et al. 300W: Following the protocol described in <ref type="bibr" target="#b34">Ren et al. (2014)</ref>, we train our networks using all 3148 68-points manually annotated faces. We also perform the testing on the common subset (554 images), the challenging subset (135 images), and the full set, which is the sum of the challenging subset and the common subset with 689 images consisting of 2000 images from the training subset of HELEN <ref type="bibr" target="#b24">Le et al. (2012)</ref> dataset, 337 images from the full set of AFW <ref type="bibr" target="#b69">Zhu and Ramanan (2012)</ref> dataset, and 811 images from the training subset of LFPW <ref type="bibr" target="#b2">Belhumeur et al. (2013)</ref> dataset with a 68-point annotation. Images in HE-LEN, LFPW, and AFW datasets are collected in the wild environment; as a result, expression and large pose variations, as well as partial occlusions, may exist. For the test phase, the images are divided into two subsets, such that the common subset contains 554 images from LFPW <ref type="bibr" target="#b2">Belhumeur et al. (2013)</ref> and <ref type="bibr">HELEN Le et al. (2012)</ref>, and the challenging subset contains </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>NME FR SFPD  6.40 -DAC- <ref type="bibr">CSR Feng et al. (2017)</ref> 6.03 4.73 CNN6 (Wing + PDB) <ref type="bibr" target="#b12">Feng et al. (2018)</ref> 5.44 3.75 ResNet50 (Wing + PDB) <ref type="bibr" target="#b12">Feng et al. (2018</ref><ref type="bibr">) 5.07 3.16 LAB Wu et al. (2018</ref> 3.92 0.39 <ref type="bibr">ODN Zhu et al. (2019)</ref> 5.30 -HRNetV2 <ref type="bibr" target="#b43">Sun et al. (2019)</ref> 3.45 0.19 ResNet50-FFLD <ref type="bibr" target="#b59">Yan et al. (2020)</ref> 5.32 -GV(HRNet) <ref type="bibr" target="#b58">Xiong et al. (2020)</ref> 3.37 0.39 mnv2 5.04 3.74 mnv2 KD 4.11 2.36 efn 3.81 1.97 135 images from IBUG <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref>. The sum of the challenging subset and the common subset is considered as the full subset, containing 689 images. WFLW: This is another widely used facial dataset, which contains 7500 images for training, and 2500 images for testing and recently has been proposed based on WIDER <ref type="bibr">FACE Yang et al. (2016)</ref>. Each image in this dataset contains 98 manually annotated landmarks. This dataset consists of 6 subsets, including 326 large pose images, 314 expression images, 698 illumination images, 206 make-up images, 736 occlusion images, and 773 blurred images. Consequently, it is possible to validate the robustness of the proposed model against each different condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We evaluate our proposed KD-based architecture using normalized mean error (NME), failure rate (FR), and the areaunder-the-curve (AUC) <ref type="bibr" target="#b60">Yang et al. (2015)</ref>. For the NME, we use "inter-ocular" distance (the distance between the outer-eyecorners) as the normalizing factor followed by MDM Trigeorgis et al. <ref type="formula" target="#formula_0">(2016)</ref>   <ref type="formula" target="#formula_0">2018)</ref>, we calculate the FR, which is the proportion of failed detected faces for a maximum error of 0.1. We report the AUC for WFLW <ref type="bibr" target="#b54">Wu et al. (2018)</ref> as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>For each training image, we first crop and extract the faces. For 300W <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref>, no bounding boxes are provided, and for COFW <ref type="bibr" target="#b4">Burgos-Artizzu et al. (2013)</ref> and <ref type="bibr">WFLW Wu et al. (2018)</ref>, we figure out that the provided bounding boxes are not accurate enough. Accordingly, we generate the new bounding boxes based on the ground truth facial landmark points for each image. We then expand the bounding boxes randomly up 10%. The next step is to resize each facial  <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Normalized Mean Error Common Challenging Fullset <ref type="bibr">DAN Kowalski et al. (2017)</ref> 3.19 5.24 3.59 <ref type="bibr">DSRN Miao et al. (2018)</ref> 4.12 9.68 5.21 <ref type="bibr">RCN Honari et al. (2016)</ref> 4.67 8.44 5.41 CPM <ref type="bibr" target="#b10">Dong et al. (2018b)</ref> 3.39 8.14 4.36 PCD-CNN <ref type="bibr" target="#b23">Kumar and Chellappa (2018)</ref> 3.67 7.62 4.44 <ref type="bibr">ODN Zhu et al. (2019)</ref> 3.56 6.67 4.17 <ref type="bibr">SAN Dong et al. (2018a)</ref> 3.34 6.60 3.98 LAB <ref type="bibr" target="#b54">Wu et al. (2018)</ref> 2.98 5.19 3.49 DCFE <ref type="bibr" target="#b48">Valle et al. (2018)</ref> 2.76 5.22 3.24 PFLD 1X <ref type="bibr" target="#b15">Guo et al. (2019)</ref> 3.01 5.08 3.40 LRefNets <ref type="bibr" target="#b42">Su et al. (2019)</ref> 2.71 4.78 3.12 HRNetV <ref type="bibr" target="#b43">Sun et al. (2019)</ref> 2.87 5.15 3.32 AWing <ref type="bibr" target="#b51">Wang et al. (2019)</ref> 2.72 4.52 3.07 3DDE <ref type="bibr" target="#b49">Valle et al. (2019)</ref> 2.69 4.92 3.13 ResNet50-FFLD <ref type="bibr" target="#b59">Yan et al. (2020)</ref> 3.06 5.44 3.50 <ref type="bibr">GEAN Iranmanesh et al. (2020)</ref> 2.68 4.71 3.05 GV(HRNet) <ref type="bibr" target="#b58">Xiong et al. (2020)</ref> 2.62 4.51 2.99 GV(HRNet) <ref type="bibr" target="#b58">Xiong et al. (2020)</ref> 2  image to 224 ? 224 pixels. To improve our models' accuracy and robustness, we augment each facial image multiple times in terms of brightness, contrast, and color modification as well as adding Gaussian noises. Moreover, we randomly rotate each image by ?45 degree and flip horizontally with the probability of 50%. In the first stage, we train the both Tough and Tolerant Teacher networks independently for about 250 epochs with a batch size of 40 using L2 loss. Then, we train the Student network for about 250 epochs with a batch size of 70, using the proposed KD-Loss. We use Adam optimizer <ref type="bibr" target="#b21">Kingma and Ba (2014)</ref> with a learning rate 10 ?3 , ? 1 = 0.9, ? 2 = 0.999, and decay = 10 ?6 . We use TensorFlow library to implement our codes and run them on a NVidia 1080Ti GPU. The code is publicly available online on GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Existing Approaches</head><p>In order to assess the performance of our proposed KD architecture, we conducted three different experiments. We report the performance of our Tough-Teacher, which is EfficientNet-B3 <ref type="bibr" target="#b46">Tan and Le (2019)</ref>, named efn, our Student-Network, which is MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>, named as mnv2 KD , as well as MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>, named mnv2. <ref type="table" target="#tab_4">Table 1</ref> shows the state-of-the-art results as well as our Teacher and Student networks. As shown, Student-Network, called mnv2 KD , achieves 4.11% NME with 2.36% FR, while the these metrics are 5.04% and 3.74% for MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> respectively. The table shows that training our proposed Student-Teacher architecture results in significantly better performance in comparison to the base-network, Mo-bileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>. <ref type="table" target="#tab_5">Table 2</ref> shows a comparison between mnv2 KD , mnv2 and the state-of-the-art methods on 300W <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref> dataset. While the calculated NME for the Teacher network, and Mo-bileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> over the Challenging set are 5.80% and 6.84% respectively, mnv2 KD achieves 6.13% by far outperforms MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>. For the Common set, the NME for Student network is 3.56%, which is better than mnv2 NME, 3.93%. The reduction in NME for the Challenging subset, which is about 0.71 %, is much higher than the Common subset (about 0.37%), showing that the proposed KDLoss function performs much better on the challenging faces. The calculated NME for the Full subset are 4.06% as well 4.50% for Student network and mnv2 respectively, indicating about 0.44% reduction. The results show that the proposed KDLoss performs a vital role in better training of the Student-Model leading to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Evaluation on COFW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Evaluation on 300W</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Evaluation on WFLW</head><p>In <ref type="table" target="#tab_8">Table 4</ref> we compare the performance of the recently proposed methods as well as our Student-Network, mnv2 KD , Teacher network and MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> on <ref type="bibr">WFLW Wu et al. (2018)</ref> and its 6 subsets. Although the performance of mnv2 KD does not outperform the state-of-theart methods, the performance of mnv2 KD is better than mnv2 in terms of NME (from 9.07% reduced to 8.57%), FR (from 27.12% reduced to 24.08%), and AUC (from 0.3758 increased to 0.4134). Similar to evaluation on 300W <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref> dataset, mnv2 KD performs much better over the pose set, faced 1.0%(from 16.06% to 15.06%), 4.91% (from 86.50% to 81.59%) reduction on NME, and FR respectively, as well as 2.09% increase (from 0.0321 to 0.0530) on AUC. In addition, we discovered that MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> is not able to perform very well over this dataset, as the number of its parameters (about 2.4M) might not be enough for predicting 98 different pair of landmark points. However, the qualitative study in Sec. 4.8 shows that mnv2 KD has an acceptable qualitative performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Cumulative Error Distribution Curve Comparison</head><p>Cumulative Error Distribution (CED) curve is the cumulative distribution function of the normalized mean error(NME). Although NME can be considered as a good evaluation metric, it  is very sensitive to the outliers. As such, when the average error over the testing set is low except for some outlier samples, NME can dramatically be aggravated.</p><p>Hence, we visualize the CED curve with the failure rate threshold as 0.1 for Teacher network, Student network and the base-network, MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>. <ref type="figure" target="#fig_0">Fig. 10</ref> shows the CED curves for COFW <ref type="bibr" target="#b4">Burgos-Artizzu et al. (2013)</ref>, the Full subset of 300W <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref>, and the Full subset of WFLW <ref type="bibr" target="#b54">Wu et al. (2018)</ref>. We also show FR (in %) and AUC to better compare performance of our proposed KD-Loss as well as the KD-based architecture. As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, the performance of Student network is much better than the MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> and a bit less accurate than Teacher network on COFW Burgos- <ref type="bibr" target="#b4">Artizzu et al. (2013)</ref> dataset . Similarly, on 300W <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref>, Student network performs better than MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>, but its performance is not as good as the Teacher network. On WFLW <ref type="bibr" target="#b54">Wu et al. (2018)</ref>, although the performance of Student network is better than MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>, Teacher network performs much better than both for-mer networks.</p><p>According to <ref type="figure" target="#fig_0">Fig. 10, our</ref> proposed Student network performs much better comparing to the base-network on all datasets. However, as the the CED curves which are depicted in <ref type="figure" target="#fig_0">Fig. 10</ref> show, the best accuracy improvement accrues on COFW Burgos-Artizzu et al. (2013) (the CED Student network curve is much closer to the Teacher network curves). We conclude that since the number of parameters in our basenetwork, MobileNetV2 <ref type="bibr">Sandler et al. (2018) (about 2.44 million)</ref>, is much smaller than that of Teacher network (about 12 million), its accuracy has a heavy reliance on the number of facial landmark points that exists in a dataset. Consequently, the best improvement in the results that Student network achieved comparing to the Teacher network is on COFW Burgos- <ref type="bibr" target="#b4">Artizzu et al. (2013)</ref>, containing only 29 landmark points, and the least improvement on WFLW <ref type="bibr" target="#b54">Wu et al. (2018)</ref>, with 98 landmark points.</p><p>Ground Truth mnv2 mnv2 KD <ref type="figure" target="#fig_0">Fig. 11</ref>: Face alignment using mnv2 KD and mnv2 on COFW Burgos- <ref type="bibr" target="#b4">Artizzu et al. (2013)</ref> dataset. For each landmark point if the error rate with respect to the normalization factor, is more than 0.1, it is considered as a failure and we printed it red, and otherwise it is green.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Model Size and Computational Cost Study</head><p>To evaluate the model size and computational complexity, we report the number of network parameters and the FLOPs (over the resolution of 224 ? 224 ) in <ref type="table" target="#tab_9">Table 5</ref>. According to <ref type="table" target="#tab_9">Table 5</ref>, our Student network, mnv2 KD (the architecture is same as MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>) has only 2.4M parameters as well as 0.6G FLOPs indicating that the models is very efficient, while the its accuracy is comparable with the state-ofthe-art methods. Only ASMNet <ref type="bibr" target="#b11">Fard et al. (2021)</ref> has smaller number of parameter (1.43M), and FLOPs (0.51G) compared to the mnv2 KD , while its accuracy is much less than our Student network.</p><p>Model size is another important factor in the context of lightweight CNNs. In <ref type="table" target="#tab_10">Table 6</ref> we compare the size of our proposed mnv2 KD model with the recently proposed lightweight models for face alignment. According to <ref type="table" target="#tab_10">Table 6</ref>, the size of mnv2 KD is only 3.2MB, which is the smallest among other proposed models. The model size has a heavy reliance on the time of loading the graph of a model to the memory (either GPU or RAM). Accordingly, our mnv2 KD can be considered to be more efficient (in terms of the model size) than the other methods reported in <ref type="table" target="#tab_10">Table 6</ref>.</p><p>As <ref type="figure" target="#fig_0">Fig. 13</ref> shows, we compare the model response time as well as the fps of our mnv2 KD (the Student network), efn (the Teacher network), and ASMNet. For the first experiment shown in <ref type="figure" target="#fig_0">Fig. 13 (the left figure)</ref>, we used an Intel i7-6850K CPU and repeated each experiments 100 times and report the average response time. As <ref type="figure" target="#fig_0">Fig. 13</ref> shows, on CPU, the response time of mnv2 KD is 3.95ms, while the response time is 9.44ms and 2.38ms for the Teacher network and ASMNet <ref type="bibr" target="#b11">Fard et al. (2021)</ref>, respectively. On CPU, the speed of mnv2 KD is very close to the ASMNet <ref type="bibr" target="#b11">Fard et al. (2021)</ref> (which is the smallest in terms of the number of the parameters and FLOPs among all the state-of-the-art models in the context of face alignment), but its accuracy is much better. Moreover, mnv2 KD can reaches to 253 fps on CPU indicating the efficiency of the model. For the second experiment, we used a 1080ti GPU. As <ref type="figure" target="#fig_0">Fig. 13</ref> shows, our mnv2 KD model can reach to 417 fps indicating how fast the model can perform for detecting the landmark points in a facial image.</p><p>In addition, our proposed models do not have any postprocessing costs, while for the heatmap-based models, converting heatmaps to points should be considered. By utilizing our proposed Tough and Tolerant Teachers and KD-Loss, we improve the accuracy of MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>, known as one of the best lightweight models, and consequently, we create a balance between efficiency and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Ablation Study</head><p>In order to figure out the effectiveness of using Tough-Teacher as well as Tolerant-Teacher, we train two different model on 300W <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref> dataset, mnv2 Tou using only Accurate-landmarks, as well as mnv2 Tol using only Smooth-landmarks. Then we modify the proposed KD-Loss function such that the former only defined using ALoss T ou and the latter using ALoss T ol . As it is shown in <ref type="table">Table.</ref> 7, although using the modified KD-Loss results in better performance compared to the original mnv2 being trained using L2 loss, mnv2 Tou performs worse than mnv2 Tol . It indicates that since the distribution of Accurate-landmarks is much harder for the lightweight MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> to be learned in comparison to the distribution of Smooth-landmarks, the modified KD-Loss generated using only Accurate-landmarks performs not much better when compared to the original L2 loss. The better performance of mnv2 Tol highlights the effect of different regions that are defined using the assistant loss functions. In KD-Loss, since the distribution of the Smoothlandmarks is much easier than distribution of the Accuratelandmarks, the Negative Assistant region defined by ALoss T ol Ground Truth mnv2 mnv2 KD <ref type="figure" target="#fig_0">Fig. 12</ref>: Face alignment using mnv2 KD and mnv2 on the 300W <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref>. For each landmark point if the error rate with respect to the normalization factor, is more than 0.1, it is considered as a failure and we printed it red, and otherwise it is green.   ing the Accurate-landmarks and Tough-Teacher it the network towards learning the original ground truths. We compare our KD-Loss with the other widely-used loss functions for face alignment in <ref type="table" target="#tab_12">Table 8</ref>. In <ref type="table" target="#tab_12">Table 8</ref>, we also present the effect of different loss functions by comparing the NME of the face alignment task using MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> as the network on 300W <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref> dataset. On all the 3 subsets, the L2 loss performs the least accurate face alignment, while KD-Loss achieves the best accuracy. In addition, the NME generated by the L1 Loss and the Smooth L1 loss are very similar. On the Challenging subset of 300W <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref> dataset, the NME reduces from 6.84% for L2 loss to 6.33% for Smooth L1 loss (0.51% reduction), and then to 6.13% for KD-Loss (0.2% reduction compared to Smooth L1). For the Common subset of 300W Sagonas et al. (2013) dataset, the NME of the MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> trained using the L1 loss is 3.93%, which reduces to 3.66% using the Smooth L1 and then reduces to 6.13% for training the model using KD-Loss. Hence, KD-Loss performers more accurately compared to other widely-used loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Qualitative Results</head><p>To better shows the quality of our proposed KD-based architecture and loss function, we provide some examples of facial landmark detection using our Student network, and the 13 Ground Truth mnv2 mnv2 KD <ref type="figure" target="#fig_0">Fig. 14:</ref> Face alignment using mnv2 KD and mnv2 on WFLW <ref type="bibr" target="#b54">Wu et al. (2018)</ref> dataset. For each landmark point if the error rate with respect to the normalization factor, is more than 0.1, it is considered as a failure and we printed it red, and otherwise it is green.  base-network, MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>. The result shows that Student network outperforms the base-network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>This paper proposed a novel architecture inspired by the KD concept for the facial landmark detection task. Using the Active Shape Model <ref type="bibr" target="#b5">Cootes et al. (2000)</ref>, we defined Mean-landmark, Soft-landmarks as well as Hard-landmarks terms, and, used them to train our proposed Tolerant as well as Tough Teacher networks, which are EfficientNet-B3 <ref type="bibr" target="#b46">Tan and Le (2019)</ref>. In addition, we used MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref> as our Student-Network. The main novelty and idea of our paper are to design KD-Loss as well as a Teacher-Student architecture utilizing a Tough and a Tolerant Teacher to help a lightweight Student network to learn the facial landmark points better. Moreover, we proposed KD-Loss, an adaptive point-wise loss function that uses the corresponding landmark points to direct the network toward the ground truth. The results of evaluating our proposed Student-Teacher architecture on widely used 300W <ref type="bibr" target="#b36">Sagonas et al. (2013)</ref>, COFW Burgos-Artizzu et al. <ref type="bibr">(2013)</ref>, and WFLW <ref type="bibr" target="#b54">Wu et al. (2018)</ref> datasets show that the accuracy of the Student network is significantly better than the original MobileNetV2 <ref type="bibr" target="#b37">Sandler et al. (2018)</ref>, specifically when it comes to faces with an extreme pose. Overall, the accuracy of Student network is comparable to state-of-the-art methods in facial landmark points detection. The proposed architecture can potentially be used in other similar computer vision tasks such as body-joint tracking. We will investigate using this method for such applications as a future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We wish to show our appreciation to Dr. Julia Madsen for her great help on editing this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The process of creating Mean-landmark, and Soft-landmarks with different accuracy using Hard-landmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Distribution of Hard-landmarks and the Soft-landmarks created on WFLW Wu et al. (2018) training set. (a) shows the distribution of all facial landmark, while for better visualization, (b) displays the landmark points belonging to the face boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>We train the Tough-Teacher, and the Tolerant-Teacher networks independently using the Hard-landmarks and the Soft-landmarks respectively utilizing the L2 loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>by providing some examples of Soft-landmarks.Moreover,Fig. 3shows the distribution of the Hardlandmarks alongside Soft-landmarks created onWFLW Wu  et al. (2018)  data set. WhileFig. 3-(a)shows all the facial</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Proposed KD-based architecture for training the Student network. KD-Loss uses the knowledge of the previously trained Teacher networks by utilizing the assistive loss functions ALoss T ou and ALoss T ol , to improve the performance the face alignment task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(2013), COFW Burgos-Artizzu et al. (2013), and WFLW Wu et al. (2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>COFW: This dataset has 1345 facial images as training and 507 facial images as the testing set. The dataset provides us with facial images having large pose variations plus heavy occlusions. Each image in the COFW Burgos-Artizzu et al. (2013) dataset has 29 manually annotated landmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>andSagonas et al. (2013). On COFW Burgos-Artizzu et al. (2013), and WFLW Wu et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>CED curve, FR (in %), and AUC being generated using Teacher network, Student-Network, and MobileNetV2 Sandler et al. (2018) on COFW Burgos-Artizzu et al. (2013), 300W Sagonas et al. (2013), and WFLW Wu et al. (2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 :</head><label>13</label><figDesc>Comparison of the model response time and the fps of 3 lightweight models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Fig. 11shows some example of face alignment using onCOFW Burgos-Artizzu et al. (2013) dataset. In addition, Fig. 14 shows examples of facial landmark detection on WFLW Wu et al. (2018) dataset. Finally, in Fig. 12 we display examples of face alignment on 300W Sagonas et al. (2013) dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Examples of Soft-landmarks generated usingm = 90% of all the Eigenvectors, as well as Hard-landmarks.</figDesc><table><row><cell>Hard-landmarks</cell></row><row><cell>Soft-landmarks 90%</cell></row><row><cell>Fig. 2:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Input Images Tough Teacher Network EfficientNet-B3 Tolerant Teacher Network EfficientNet-B3 Hard-landmarks Accurate-landmarks Soft-landmarks Smooth-landmarks L2 L2</head><label></label><figDesc></figDesc><table><row><cell>Updating the weights of</cell><cell>Updating the weights of</cell></row><row><cell>the Tough Teacher network</cell><cell>the Tolerant Teacher network</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Examples of using the assistive loss functions as a Positive and Negative assistant. A: ALoss T ou performs as a Positive assistant, penalizing the Student network to predict P Pr to be close to P Ac , while ALoss T ol is a Negative assistant, penalizing the Student network to predict P Pr such that it is far from P S m . B: First, since P Ac is not between P Pr and P Gt , we implicitly define P AC , and then consider both ALoss T ou and ALoss T ol as a Positive assistant.</figDesc><table><row><cell>A</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>P Gt</cell><cell>P Ac</cell><cell></cell><cell>P Pr</cell><cell>P Sm</cell></row><row><cell></cell><cell></cell><cell>Positive</cell><cell>Negative</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Assistant</cell><cell>Assistant</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ALoss Tou</cell><cell>ALoss Tol</cell><cell></cell></row><row><cell>B</cell><cell></cell><cell></cell><cell>ALoss Tol</cell><cell></cell></row><row><cell>P Ac</cell><cell>P Gt</cell><cell>P' Ac</cell><cell>P Sm Positive Assistant</cell><cell>P Pr</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Positive</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Assistant</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ALoss Tou</cell><cell></cell></row><row><cell>Fig. 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>NME (in %) and failure rate of 29-point landmarks detection on COFW Burgos-Artizzu et al. (2013) dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>NME (in %) of 68-point landmarks detection on 300W</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the NME (in %) of lightweight models in landmarks localization on 300W Sagonas et al. (2013) dataset.</figDesc><table><row><cell>Method</cell><cell cols="3">NME Common Challenging Fullset</cell></row><row><cell></cell><cell cols="3">inter-ocular normalization</cell></row><row><cell>res loss Ning et al. (2020)</cell><cell>-</cell><cell>-</cell><cell>4.93</cell></row><row><cell>ASMNet Fard et al. (2021)</cell><cell>4.82</cell><cell>8.2</cell><cell>5.50</cell></row><row><cell>MobileNet+ASMLoss Fard et al. (2021)</cell><cell>3.88</cell><cell>7.35</cell><cell>4.59</cell></row><row><cell>mnv2 KD</cell><cell>3.56</cell><cell>6.13</cell><cell>4.06</cell></row><row><cell></cell><cell cols="3">inter-pupil normalization</cell></row><row><cell>LBF Ren et al. (2014)</cell><cell>4.95</cell><cell>11.98</cell><cell>6.32</cell></row><row><cell>LBF fast Ren et al. (2014)</cell><cell>5.38</cell><cell>15.50</cell><cell>7.37</cell></row><row><cell>CFSS Zhu et al. (2015)</cell><cell>4.73</cell><cell>9.98</cell><cell>5.76</cell></row><row><cell>3DDFA Zhu et al. (2016)</cell><cell>6.15</cell><cell>10.59</cell><cell>7.01</cell></row><row><cell>DOF Wu et al. (2021)</cell><cell>4.86</cell><cell>9.13</cell><cell>5.55</cell></row><row><cell>MuSiCa68 Shapira et al. (2021)</cell><cell>4.63</cell><cell>8.16</cell><cell>5.32</cell></row><row><cell>G&amp;LSR? Shao et al. (2021)</cell><cell>4.52</cell><cell>7.82</cell><cell>5.17</cell></row><row><cell>mnv2 KD</cell><cell>4.97</cell><cell>8.90</cell><cell>5.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>NME (in %), FR (in %), and AUC of 98-point landmarks detection onWFLW Wu et al. (2018)  dataset.</figDesc><table><row><cell cols="2">Metric Method</cell><cell>Test set</cell><cell>Pose</cell><cell cols="4">Expression Illumination Make-Up Occlusion</cell><cell>Blur</cell></row><row><cell></cell><cell>DVLN Wu and Yang (2017)</cell><cell>6.08</cell><cell>11.54</cell><cell>6.78</cell><cell>5.73</cell><cell>5.98</cell><cell>7.33</cell><cell>6.88</cell></row><row><cell></cell><cell>LAB Wu et al. (2018)</cell><cell>5.27</cell><cell>10.24</cell><cell>5.51</cell><cell>5.23</cell><cell>5.15</cell><cell>6.79</cell><cell>6.32</cell></row><row><cell>NME</cell><cell>ResNet50(Wing+PDB) Feng et al. (2018) 3DDE Valle et al. (2019)</cell><cell>5.11 4.68</cell><cell>8.75 8.62</cell><cell>5.36 5.21</cell><cell>4.93 4.65</cell><cell>5.41 4.60</cell><cell>6.37 5.77</cell><cell>5.81 5.41</cell></row><row><cell></cell><cell>GV(HRNet) Xiong et al. (2020)</cell><cell>4.33</cell><cell>7.41</cell><cell>4.51</cell><cell>4.24</cell><cell>4.18</cell><cell>5.19</cell><cell>4.93</cell></row><row><cell></cell><cell>mnv2</cell><cell>9.07</cell><cell>16.06</cell><cell>9.16</cell><cell>8.72</cell><cell>9.11</cell><cell>10.46</cell><cell>9.88</cell></row><row><cell></cell><cell>mnv2 KD</cell><cell>8.57</cell><cell>15.06</cell><cell>8.81</cell><cell>8.15</cell><cell>8.75</cell><cell>9.92</cell><cell>9.40</cell></row><row><cell></cell><cell>efn</cell><cell>7.86</cell><cell>14.15</cell><cell>7.97</cell><cell>7.60</cell><cell>8.43</cell><cell>9.28</cell><cell>8.69</cell></row><row><cell></cell><cell>DVLN Wu and Yang (2017)</cell><cell>10.84</cell><cell>46.93</cell><cell>11.15</cell><cell>7.31</cell><cell>11.65</cell><cell>16.30</cell><cell>13.71</cell></row><row><cell></cell><cell>LAB Wu et al. (2018)</cell><cell>7.56</cell><cell>28.83</cell><cell>6.37</cell><cell>6.73</cell><cell>7.77</cell><cell>13.72</cell><cell>10.74</cell></row><row><cell>FR</cell><cell>ResNet50(Wing+PDB) Feng et al. (2018) 3DDE Valle et al. (2019)</cell><cell>6.00 5.04</cell><cell>22.70 22.39</cell><cell>4.78 5.41</cell><cell>4.30 3.86</cell><cell>7.77 6.79</cell><cell>12.50 9.37</cell><cell>7.76 6.72</cell></row><row><cell></cell><cell>GV(HRNet) Xiong et al. (2020)</cell><cell>3.52</cell><cell>16.26</cell><cell>2.55</cell><cell>3.30</cell><cell>3.40</cell><cell>6.79</cell><cell>5.05</cell></row><row><cell></cell><cell>mnv2</cell><cell>27.12</cell><cell>86.50</cell><cell>27.70</cell><cell>23.35</cell><cell>25.24</cell><cell>36.27</cell><cell>33.63</cell></row><row><cell></cell><cell>mnv2 KD</cell><cell>24.08</cell><cell>81.59</cell><cell>27.38</cell><cell>19.91</cell><cell>22.33</cell><cell>33.42</cell><cell>29.10</cell></row><row><cell></cell><cell>efn</cell><cell>19.68</cell><cell>70.55</cell><cell>19.42</cell><cell>16.04</cell><cell>23.30</cell><cell>29.21</cell><cell>23.80</cell></row><row><cell></cell><cell>DVLN Wu and Yang (2017)</cell><cell>0.4551</cell><cell>0.1474</cell><cell>0.3889</cell><cell>0.4743</cell><cell>0.4494</cell><cell>0.3794</cell><cell>0.3973</cell></row><row><cell></cell><cell>LAB Wu et al. (2018)</cell><cell>0.5323</cell><cell>0.2345</cell><cell>0.4951</cell><cell>0.5433</cell><cell>0.5394</cell><cell>0.4490</cell><cell>0.4630</cell></row><row><cell>AUC</cell><cell>ResNet50(Wing+PDB) Feng et al. (2018) 3DDE Valle et al. (2019)</cell><cell>0.5504 0.5544</cell><cell>0.3100 0.2640</cell><cell>0.4959 0.5175</cell><cell>0.5408 0.5602</cell><cell>0.5582 0.5536</cell><cell>0.4885 0.4692</cell><cell>0.4918 0.4957</cell></row><row><cell></cell><cell>GV(HRNet) Xiong et al. (2020)</cell><cell>0.5775</cell><cell>0.3166</cell><cell>0.5636</cell><cell>0.5863</cell><cell>0.5881</cell><cell>0.5035</cell><cell>0.5242</cell></row><row><cell></cell><cell>mnv2</cell><cell>0.3758</cell><cell>0.0321</cell><cell>0.3200</cell><cell>0.4015</cell><cell>0.3651</cell><cell>0.3092</cell><cell>0.3166</cell></row><row><cell></cell><cell>mnv2 KD</cell><cell>0.4134</cell><cell>0.0530</cell><cell>0.3570</cell><cell>0.4377</cell><cell>0.3940</cell><cell>0.3390</cell><cell>0.3521</cell></row><row><cell></cell><cell>efn</cell><cell>0.4755</cell><cell>0.0905</cell><cell>0.4366</cell><cell>0.4944</cell><cell>0.4493</cell><cell>0.3842</cell><cell>0.4067</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different methods in terms of number of parameters and Flops. guide the Student network towards the Accuratelandmarks. Accordingly, when we define the KD-Loss using only ALoss T ou the performance of the corresponding mnv2 Tou is not as good as the performance of mnv2 Tol .Besides, although mnv2 Tol performs better than mnv2 Tou , it cannot outperform the performance of mnv2 KD since Smoothlandmarks is not accurate enough to be used as the only Teacher. Using both Teachers simultaneity with the conjunction of the proposed ALoss T ou and ALoss T ol achieves the best accuracy, as Tolerant-Teacher guides the Student network towards learn-</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">#Params (M) FLOPs (B)</cell></row><row><cell>DVLN Wu and Yang (2017)</cell><cell>VGG-16</cell><cell>132.0</cell><cell>14.4</cell></row><row><cell>SAN Dong et al. (2018a)</cell><cell>ResNet-152</cell><cell>57.4</cell><cell>10.7</cell></row><row><cell>LAB Wu et al. (2018)</cell><cell>Hourglass</cell><cell>25.1</cell><cell>19.1</cell></row><row><cell>ResNet50 (Wing + PDB) Feng et al. (2018)</cell><cell>ResNet-50</cell><cell>25</cell><cell>3.8</cell></row><row><cell>HRNetV Sun et al. (2019)</cell><cell>HRNetV2-W18</cell><cell>9.3</cell><cell>4.3</cell></row><row><cell>ASMNet Fard et al. (2021)</cell><cell>reduced MobileNetV2</cell><cell>1.43</cell><cell>0.51</cell></row><row><cell>EfficientNet-B3 Tan and Le (2019)</cell><cell>-</cell><cell>12</cell><cell>1.8</cell></row><row><cell>mnv2KD</cell><cell>MobileNetV2</cell><cell>2.4</cell><cell>0.6</cell></row><row><cell>is able to</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the model size of the lightweight models for face alignment.</figDesc><table><row><cell>Method</cell><cell>efn</cell><cell cols="2">mnv2 KD ASMNet</cell><cell>MuSiCa68</cell><cell>G&amp;LSR?Shao</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Fard et al.</cell><cell>Shapira et al.</cell><cell>et al. (2021)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(2021)</cell><cell>(2021)</cell><cell></cell></row><row><cell>Model Size</cell><cell>7.7</cell><cell>3.2</cell><cell>3.6</cell><cell>9.1</cell><cell>5.9</cell></row><row><cell>(MB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparing the performance of mnv2 KD , mnv2 Tou , mnV2 Tol , and mnV2 with respect to NME (in %), FR (in %), and AUC on 300W<ref type="bibr" target="#b36">Sagonas et al. (2013)</ref>.</figDesc><table><row><cell></cell><cell></cell><cell>mnv2</cell><cell cols="3">mnv2 Tou mnv2 Tol mnv2 KD</cell></row><row><cell></cell><cell>Challenging</cell><cell>6.84</cell><cell>6.59</cell><cell>6.41</cell><cell>6.13</cell></row><row><cell>NME</cell><cell>Common</cell><cell>3.93</cell><cell>3.85</cell><cell>3.73</cell><cell>3.56</cell></row><row><cell></cell><cell>Full</cell><cell>4.50</cell><cell>4.39</cell><cell>4.25</cell><cell>4.06</cell></row><row><cell></cell><cell>Challenging</cell><cell>7.40</cell><cell>5.92</cell><cell>4.44</cell><cell>3.70</cell></row><row><cell>FR</cell><cell>Common</cell><cell>0.18</cell><cell>0.18</cell><cell>0.18</cell><cell>0.18</cell></row><row><cell></cell><cell>Full</cell><cell>1.59</cell><cell>1.30</cell><cell>1.01</cell><cell>0.87</cell></row><row><cell></cell><cell cols="2">Challenging 0.5425</cell><cell>0.5509</cell><cell>0.5736</cell><cell>0.6029</cell></row><row><cell>AUC</cell><cell>Common</cell><cell>0.8102</cell><cell>0.8146</cell><cell>0.8230</cell><cell>0.8356</cell></row><row><cell></cell><cell>Full</cell><cell>0.7578</cell><cell>0.7630</cell><cell>0.7742</cell><cell>0.7900</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>NME (in %) of different loss functions using MobileNetV2<ref type="bibr" target="#b37">Sandler et al. (2018)</ref> for 68-point landmarks detection on 300W<ref type="bibr" target="#b36">Sagonas et al. (2013)</ref>.</figDesc><table><row><cell>Loss Function</cell><cell cols="3">Common Challenging Fullset</cell></row><row><cell>L2 Loss</cell><cell>3.93</cell><cell>6.84</cell><cell>4.50</cell></row><row><cell>L1 Loss</cell><cell>3.67</cell><cell>6.38</cell><cell>4.20</cell></row><row><cell>smooth L1 Rashid et al. (2017)</cell><cell>3.66</cell><cell>6.33</cell><cell>4.18</cell></row><row><cell>KD Loss</cell><cell>3.56</cell><cell>6.13</cell><cell>4.06</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3444" to="3451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d constrained local model for rigid and non-rigid facial tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2610" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2930" to="2940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An introduction to active shape models. Image processing and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baldock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="223" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="484" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">3</biblScope>
			<pubPlace>Bmvc, Citeseer</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint multi-view face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3636" to="3648" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Asmnet: A lightweight deep neural network for face alignment and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1521" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2235" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rectified wing loss for efficient and robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="2126" to="2145" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic attention-controlled cascaded shape regression exploiting training data augmentation and fuzzy-set sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2481" to="2490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pfld: A practical facial landmark detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10859</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4295" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recombinator networks: Learning coarse-to-fine feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5743" to="5752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via aggregation on geometrically manipulated faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Iranmanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dabouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="330" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep alignment network: A convolutional neural network for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="88" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disentangling 3d pose in a dendritic cnn for unconstrained 2d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mimicking very efficient network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6356" to="6364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Surpassing human-level face verification performance on lfw with gaussianface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-ninth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3317" to="3326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Facial landmarks localization using cascaded neural networks. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahpod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Maiorana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Campisi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">103171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative face alignment through 2.5 d active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="250" to="268" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Direct shape regression networks for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5040" to="5049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A cpu real-time face alignment for mobile platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="8834" to="8843" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interspecies knowledge transfer for facial keypoint detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6894" to="6903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">300 faces inthe-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mo-bilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deformable model fitting by regularized landmark mean-shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="200" to="215" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust face alignment via deep progressive reinitialization and adaptive errordriven learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Knowing when to quit: Selective cascaded regression with patch attention for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goldin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Jevnisek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.00377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey of local feature methods for 3d face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soltanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boufama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient and accurate face alignment by global regression and cascaded local refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hybrid deep learning for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1489" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A deeplyinitialized coarse-to-fine ensemble of regression trees for face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Face alignment using a 3d deeply-initialized ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page">102846</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Driver gaze tracking and eyes off the road detection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2014" to="2027" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adaptive wing loss for robust face alignment via heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6971" to="6981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Enforcing convexity for improved alignment with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Design of a facial landmark detection system using a dynamic optical flow approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="68737" to="68745" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2129" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Leveraging intra and inter-dataset variations for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="150" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Simultaneous facial landmark detection, pose and deformation estimation under facial occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3471" to="3480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Improving fast segmentation with teacher-student learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08476</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Gaussian vector: An efficient solution for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Fine-grained facial landmark detection exploiting intermediate feature representations. Computer Vision and Image Understanding 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phutane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berthelier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Naturel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blanc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">103036</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">An empirical study of recent face alignment methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05049</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="79" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The menpo facial landmark localisation challenge: A step towards the solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="170" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exemplar-based cascaded stacked auto-encoder networks for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via occlusion-adaptive deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadiq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3486" to="3496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-tofine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
