<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Jeevan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<postCode>400076</postCode>
									<region>MH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavitha</forename><surname>Viswanathan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<postCode>400076</postCode>
									<region>MH</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
							<email>asethi@iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<postCode>400076</postCode>
									<region>MH</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gains in the ability to generalize on image analysis tasks for neural networks have come at the cost of increased number of parameters and layers, dataset sizes, training and test computations, and GPU RAM. We introduce a new architecture -WaveMix-Lite -that can generalize on par with contemporary transformers and convolutional neural networks (CNNs) while needing fewer resources. WaveMix-Lite uses 2D-discrete wavelet transform to efficiently mix spatial information from pixels. WaveMix-Lite seems to be a versatile and scalable architectural framework that can be used for multiple vision tasks, such as image classification and semantic segmentation, without requiring significant architectural changes, unlike transformers and CNNs. It is able to meet or exceed several accuracy benchmarks while training on a single GPU. For instance, it achieves state-of-theart accuracy on five EMNIST datasets, outperforms CNNs and transformers in ImageNet-1K (64?64 images), and achieves an mIoU of 75.32% on Cityscapes validation set, while using less than one-fifth the number parameters and half the GPU RAM of comparable CNNs or transformers. Our experiments show that while the convolutional elements of neural architectures exploit the shift-invariance property of images, new types of layers (e.g., wavelet transform) can exploit additional properties of images, such as scale-invariance and finite spatial extents of objects.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ever since it has been demonstrated that convolutional neural networks (CNNs) generalize better than the alternatives for image classification ; <ref type="bibr" target="#b30">Krizhevsky et al. (2012)</ref>, further improvements have relied on simple but effective architectural changes that allow training of deeper CNNs Simonyan, Zisserman (2014); <ref type="bibr" target="#b57">Szegedy et al. (2014)</ref>; <ref type="bibr" target="#b17">He et al. (2015a)</ref>; <ref type="bibr" target="#b20">Howard et al. (2017)</ref>; <ref type="bibr" target="#b21">Hu et al. (2017)</ref>; <ref type="bibr" target="#b22">Huang et al. (2016)</ref>. On the other hand, adapting transformers that were developed for natural language processing (NLP) for vision tasks was a departure from using convolutional processing, which showed that further improvements in generalization in image classification are possible by using architectures that scale with larger labeled datasets and computational resources <ref type="bibr" target="#b66">Zhao et al. (2020)</ref>; <ref type="bibr" target="#b9">Dosovitskiy et al. (2021)</ref>. However, forsaking inductive priors suitable for images, such as 2D convolutional weight sharing, and adopting global self-attention with quadratic complexity meant that the training requirements increased to a prohibitive extent for most applications <ref type="bibr" target="#b28">Khan et al. (2022)</ref>. Linear approximations to quadratic attention reduce the computational requirements only to some extent, and they do not resolve the problem of not having an appropriate inductive bias for images .</p><p>More recently, there have been attempts to combine the inductive prior of convolutional design with the scalability of transformers in hybrid architectures for image analysis to reduce the training requirements of neural networks <ref type="bibr">(pre-training</ref>   <ref type="bibr" target="#b60">Kolter (2022)</ref>. These works indicate that it is worth exploring priors other than shift-invariance (via convolutions) for images to reduce the training requirements with little to no sacrifice of generalization. However, most of these architectures cannot be easily adapted for multiple image sizes and other vision tasks, such as segmentation and object detection Trockman, <ref type="bibr" target="#b60">Kolter (2022)</ref>.</p><p>An important prior in images that has been much less explored in deep neural networks is the multi-resolution self-similarity (across spatial scales) and sparseness of edges (finite spatial extents of objects), with some notable exceptions <ref type="bibr" target="#b43">Mei et al. (2020)</ref>. Two-dimensional discrete wavelet transforms (2D-DWT) capture this prior well and have been widely used in the pre-deep neural network era for various imaging applications, especially for compression and denoising <ref type="bibr" target="#b38">Lewis, Knowles (1992)</ref>; <ref type="bibr" target="#b53">Ruikar, Doye (2010)</ref>.</p><p>In this work, we explore the use of a 2D-DWT as a spatial token mixer directly into a largely convolutional architecture, which we call WaveMix-Lite. WaveMix-Lite has the following benefits:</p><p>1. Incorporate multi-resolution inductive bias without increasing training costs: A predefined 2D-DWT allows us to mix features spatially in a multi-resolution manner without introducing extra parameters. Additionally, the 2D-DWT also scales the image dimension by positive integral powers of 1 2 ? 1 2 , which reduces the GPU RAM and FLOPs required per training image per forward or backward pass for the subsequent trainable layers. The resultant efficiency in terms of parameters, FLOPs, and GPU RAM allowed us to conduct experiments using a single GPU on Google Colab Pro+ ? .</p><p>2. Impart versatility in architectural design for image recognition as well as segmentation: Unlike other CNN and multi-head attention (MHA) models which need complete redesigning of the architecture to provide good performance in different vision tasks, the WaveMix-Lite architecture is versatile and can perform multiple vision tasks, such as image classification and semantic segmentation, without the need for special architectural modifications. We find this design simplicity and reusable block structure to be attractive in its own right.</p><p>3. Obtain high accuracy with reduced training requirements: For image classification and segmentation on multiple datasets WaveMix-Lite was able to match the test accuracy of some widely used convolutional and transformer architectures while using 5 to 10 times fewer parameters and 2 to 50 times lesser GPU RAM for a fixed batch size. Consequently, its training and testing throughputs were 1.5 to 6 times higher than the models compared. WaveMix-Lite also achieved state-of-the-art accuracy on 5 EMNIST datasets (Byclass, Bymerge, Letters, Digits and Balanced). Additionally, it gives compelling results on TinyImageNet, CIFAR 10 and 100, STL-10, Places-365, Caltech-256 and ImageNet-1K for classification; and Cityscapes for semantic segmentation.</p><p>After describing how WaveMix-Lite is related to previous works in Section 2, we describe its insights and details in Section 3. We compare WaveMix-Lite with other models on image classification and semantic segmentation and provide empirical evidence of its scalability and examine the importance of its components in Section 4. We conclude and list potential future directions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Statistical properties of natural images, which have been studied for the last several decades, include shift-invariance (stationarity), scale-invariance (especially in 2D projections of a 3D world viewed from various distances), high spatial auto-correlation (monochromatic objects or regions), spatial sparseness of edges (finite spatial extent of objects), and certain chromatic contrasts (preponderance of certain colors) <ref type="bibr" target="#b10">Field (1993)</ref>; Ruderman (1994); Lee (1996); <ref type="bibr" target="#b48">P?rraga et al. (2002)</ref>. Of these, only the shift-invariance has been widely exploited in neural architectures for image analysis in the form of convolutional filters     <ref type="bibr" target="#b38">Lewis, Knowles (1992)</ref>. Features extracted using wavelet transforms have also been used extensively with machine learning models <ref type="bibr" target="#b44">Mowlaei et al. (2002)</ref>, such as support vector machines and neural networks Ranaware, Deshpande <ref type="formula">(2016)</ref>, especially for image classification Nayak et al. <ref type="bibr">(2016)</ref>. Representative instances of integration with neural architectures include the following. ScatNet architecture cascades wavelet transform layers with nonlinear modulus and average pooling to extract translation invariant features that are robust to deformations and preserves high-frequency information for image classification <ref type="bibr" target="#b0">Bruna, Mallat (2013)</ref>. WaveCNets replaces max-pooling, strided-convolution, and average-pooling of CNNs with 2D-DWT for noise-robust image classification <ref type="bibr" target="#b39">Li et al. (2020a)</ref>. Multi-level wavelet CNN (MWCNN) has been used for image restoration as well with U-Net architectures for better trade-off between receptive field size and computational efficiency Liu et al. <ref type="bibr">(2018)</ref>. Wavelet transform has also been combined with a fully convolutional neural network for image super resolution <ref type="bibr" target="#b31">Kumar et al. (2017)</ref>.</p><p>We propose using the two-dimensional discrete wavelet transform (2D-DWT) for token mixing. Among the different types of mother wavelets available, we used the Haar wavelet (a special case of the Daubechies wavelet <ref type="bibr" target="#b8">Daubechies (1990)</ref> , also known as Db1), which is frequently used due to its simplicity and faster computation. Haar wavelet is both orthogonal and symmetric in nature, and has been used to extract basic structural information from images <ref type="bibr" target="#b49">Porwik, Lisowska (2004)</ref>. For even-sized images, it reduces the dimensions exactly by a factor of 2, which simplifies the designing of subsequent layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WaveMix-Lite Architecture</head><p>Image pixels have several interesting co-dependencies. The localized and stationary nature of certain image features (e.g., edges) have been exploited using linear space-invariant filters (convolutional kernels) of limited size. Scale-invariance of natural images has been exploited to some extent by pooling . However, we think that scale-invariance can be better modeled by wavelet decomposition due to its natural multi-resolution analysis properties. Additionally, the finer scale of a multi-level wavelet decomposition also incorporates the idea of linear space-invariant feature extraction using convolutional filters of small support; albeit using predefined weights. The basic idea, therefore, behind our proposed architecture is to alternate between learnable spatially repeated feature extraction using convolutional layers (includes, 3 ? 3 conv, MLP, as well as upconv layers), and fixed token mixing using 2D-DWT for a few layer blocks. Injecting fixed (unlearnable) spatial token-mixing that also reduces the image dimensions by a factor of 1 2 ? 1 2 , similar to a pooling layer (which we do not use), reduces the number of computations in some of the subsequent learnable layers and increases the effective receptive field to capture distant spatial relationships more efficiently with the number of layers. This combination requires far fewer layers and parameters than using only convolutional layers with pooling. On the other hand, while transformers and other token mixers have very large effective receptive fields right from the first few layers, they do not utilize inductive priors that are suitable for images. This is where the wavelet transform plays its role in both increasing the effective receptive field at an exponential rate per layer (unlike the linear rate of convolutional layers), while still retaining the essence of convolutional design to keep the architecture flexible. Additionally, compared to pooling, wavelet is a lossless transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall architecture</head><p>As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, the input image is first passed through a convolutional layer that creates feature maps of the image. The use of trainable convolutions before the wavelet transform is a key aspect of our architecture, as it allows the extraction of only those feature maps that are suitable for the chosen wavelet family. This is followed by a series of WaveMix-Lite blocks 1 . A task specific output layer is then attached to the end. For image classification, we add an MLP head, a global average pooling layer, and a softmax layer for generating the class probabilities. For semantic segmentation, we use deconvolution layers to expand the output from WaveMix-Lite block back to the input resolution. A pixel-wise softmax layer is then added to generate the class probabilities for the required number of semantic classes. For both the tasks, the core architecture remains the same and we only replace the classification head with the segmentation head. WaveMix-Lite processes image as a 2D graph and not as sequence of pixels/patches. That is, at no point in the model do we unroll the image into a sequence of pixels/patches as done in transformer models. This key feature allows transfer learning from a source to a target dataset even when the two have different image sizes or tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">WaveMix-Lite block</head><p>In a WaveMix-Lite block <ref type="figure" target="#fig_3">(Figure 2(c)</ref>), the input is first passed through a convolutional layer which decreases the embedding dimension by a factor of four, so that the concatenated output after 2D-DWT  has the same dimension as input. <ref type="bibr">2</ref> We only use one level 2D-DWT in WaveMix-Lite to reduce the parameters and computations. The 2D-DWT produces four output channels (one approximation and three details <ref type="bibr" target="#b8">Daubechies (1990)</ref>) for each input channel. The four outputs are concatenated together (depth or channel-wise) and this output has the same number of channels as the input to the WaveMix-Lite block (embedding dimension). The output resolution (height ? width) after 2D-DWT will be half that of the input; i.e., if the input is 64 ? 64, the output will be 32 ? 32.</p><p>The concatenated output from 2D-DWT is passed to an MLP layer (two 1 ? 1 convolutional layers separated by a GELU non-linearity) having a multiplication factor more than one where channel mixing is performed by the MLP. Since wavelet transform reduces the image resolution by half, the GPU consumption and computations needed by the MLP significantly reduces in each layer. The image size reconciliation is performed using transposed convolutions (up-convolutions) which resizes the image back to the original input resolution. The kernel size and stride of deconvolutional layers were chosen such that the output has the same size as the input to WaveMix-Lite block. We chose deconvolutional layer rather than an inverse 2D-DWT because the former is much faster and consumes less GPU than the latter. The outputs from the deconvolutional layers are then passed through batch normalization. A residual connection He et al. <ref type="formula">(2015b)</ref> is provided within each WaveMix-Lite block so that the model can be made deeper with a larger number of blocks, if necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To demonstrate the applicability of WaveMix-Lite for image classification, we used multiple types of publicly available (under MIT Licenses) datasets based on the number of images and image size. Small datasets of smaller image sizes included CIFAR-10, CIFAR-100 Krizhevsky <ref type="formula">(2009)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation details</head><p>We trained models using AdamW optimizer (? = 0.001, ? 1 = 0.9, ? 2 = 0.999, = 10 ?8 ) with a weight decay coefficient of 0.01 during initial epochs and then used SGD (stochastic gradient descent) with learning rate of 0.001 and momentum = 0.9 during the final 20 epochs Keskar, Socher (2017). We used automatic mixed precision in PyTorch during training to optimize speed and memory consumption. Almost all experiments were done with a 16 GB Tesla V100-SXM2 GPU available in Google Colab Pro+ ? . No image augmentations were used while training the models. Maximum number of epochs in all experiments was set to 150. GPU usage for a batch size of 64 was reported for image classification along with top-1% accuracy from best of three runs with random initialization based on prevailing protocols Hassani et al. <ref type="bibr">(2021)</ref>. We report the semantic segmentation performance using mean intersection over union (mIoU) metric. Cross-entropy loss was used for image classification and pixel-wise focal loss was used for semantic segmentation. Few segmentation experiments were run on A100-SXM4 GPU in Google Colab Pro+ ? . Due to resource constraints, for segmentation experiments in 16 GB V100 GPU, the original 1024 ? 2048 image was resized to 256 ? 512 and for 40 GB A100 GPU, it was resized to 512 ? 1024. We also adjusted the stride of the initial convolutional layers in all WaveMix-Lite models that handled high-resolution images to ensure that smaller side of input was always 64 before it reached WaveMix-Lite blocks. The classification head of ConvMixer was replaced with a segmentation head similar to WaveMix-Lite for segmentation. No pre-training was performed on any of the WaveMix-Lite models. <ref type="table" target="#tab_2">Table 1</ref> shows the performance of WaveMix-Lite compared to the other architectures on image classification using supervised learning on ImageNet-1K scaled down to 64 ? 64 to manage training on a single GPU within a reasonable time. Similarly, for the transformer models, in order to train on a single GPU, fewer layers gave better results, for which we used smaller patch sizes. We see that WaveMix-Lite models outperform ResNets, transformers, hybrid xformers, and other token-mixing models while requiring lesser GPU RAM and number of parameters. WaveMix-Lite does not need a  large number of parameters to give performance comparable to large ResNets and they require very less GPU RAM compared to transformer models for achieving similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Image classification</head><p>Even though convolution has been widely regarded as a GPU-efficient operation, the need for deeper architectures have necessitated the use of networks having over tens to hundreds of layers for achieving high generalization. Even though a single convolutional operation is comparatively cheaper than a 2D-DWT, we can achieve generalization comparable to deep convolutional networks with far fewer layers of the wavelet transforms. This ability of the wavelet transform to provide competitive performance without needing large number of layers helps in improving the efficiency of the network by consuming much lesser GPU RAM than deep convolutional models like ResNets. We also observe that deeper WaveMix-Lite models perform better and this suggests that even further scale-up of WaveMix-Lite in multi-GPU setting could be possible.</p><p>In <ref type="table" target="#tab_3">Table 2</ref> we see that on CIFAR and TinyImageNet datasets, WaveMix-Lite performs much better than the other models, giving accuracy higher than ResNets and MobileNets with 4 to 10 times fewer parameters and less GPU consumption. GPU consumption of WaveMix-Lite is sometimes 50 times lower for similar performance when compared to transformer models. The results of WaveMix-Lite on several large resolution image datasets are provided in the Appendix.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Parameter efficiency</head><p>Since WaveMix-Lite heavily uses unlearnable token-mixers, it uses much fewer parameters compared to the commonly used architectures. <ref type="table" target="#tab_6">Table 5</ref> shows that WaveMix-Lite can achieve all the tasks mentioned with far fewer parameters compared to previous models Jha et al. (2021); Wu (2018). We can further reduce the parameter count of WaveMix-Lite by replacing the deconvolution layers with Upsampling layers using unlearnable interpolation techniques (e.g., IDWT, bilinear or bicubic).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Ablation studies</head><p>We performed multiple ablations on WaveMix-Lite using the CIFAR-10 dataset to understand the effect of each type of layer on performance. When we removed the 2D-DWT layers from WaveMix-Lite, the GPU RAM requirement of the model increased by 61.8 % and accuracy fell by 5%. This is due to the MLP receiving the full resolution instead of the half-resolution feature map from 2D-DWT.</p><p>Replacing the 2D-DWT with the real part of a 2D-discrete Fourier transform showed 12% decrease in accuracy along with 73% increase in GPU consumption as the Fourier transform also does not downscale an image. Additionally, the Fourier transform has global and spatially smoothly varying kernels, which do not model objects in images in a sparse manner. Objects have finite and abruptlyending spatial extents, which are better modeled by wavelet functions that have the local kernels (finite support set) with sharp transitions.</p><p>GPU RAM consumption increased by 7.8% and accuracy decreased by 5% when we replaced the 2D-DWT with 2D-MaxPooling, indicating that the loss of information by the latter hurts generalization.</p><p>Additional ablation studies on the number of layers, embedding dimension, MLP dimension, and multi-level 2D-DWT are included in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions, Future Directions, and Impact</head><p>We proposed a novel and versatile neural architecture -WaveMix-Lite -that can generalize at par with both self-attention networks (transformers), CNNs, and their hybrids for image classification and segmentation, while needing fewer parameters, GPU RAM, and FLOPs. In addition to convolutions, WaveMix-Lite uses a 2D wavelet transform for token mixing in images, which exploits additional image priors, such as scale-invariance and finite spatial extents of objects. It is also better tailored for computer vision applications than transformers as it handles the data in a 2D format without unrolling it as a sequence. It is easy to adapt WaveMix-Lite for different image sizes and tasks, such as image classification and semantic segmentation, without changing its core architecture, as shown by our experiments on multiple datasets and tasks.</p><p>Limitations: Although our own lack of access to a large GPU cluster inspired the objective of designing a neural architecture that can generalize well with limited resources, scalability of WaveMix-Lite to larger datasets (in terms of image size and the number of images) needs to be tested with larger compute resources. Additionally, WaveMix-Lite should also be tested on other vision tasks, such as object detection and image enhancement (e.g., super-resolution, deblurring, denoising, and inpainting) using task-specific architectural variations.</p><p>Impact: The high accuracy of image classification by transformers and CNNs comes with high costs in terms of training data, computations, GPU RAM, hardware costs, form factors, and energy consumption <ref type="bibr">Li et al. (2020b)</ref>. Our research shows that architectural innovations can still reduce these computational requirements by exploiting priors of natural images, such as scale-invariance and finite spatial extents (in addition to shift-invariance that is already exploited by convolutional design elements), without sacrificing accuracy. Such architectures would also be more environment-friendly due to lower energy consumption and accessible for modification to more researchers who may not have access to large computational resources. We hope that our research has shed light into the less traversed area of resource-efficient models that exploit more priors of natural images.</p><p>This work can be extended in several directions, some of which are mentioned in its limitations above. Exploitation of additional properties of images and videos can also be explored. Instead of using a fixed function (e.g., Haar), we can also make the learning of the wavelet function itself as a part of the training process.     the output back to input size will need a lot more parameters. This will consume more resources in terms of GPU and time. Each increment in the level of 2D-DWT results in doubling of the number of parameters, but provides only a very small reduction in GPU consumption, especially when we go to higher levels of 2D-DWT. In each of the higher level decompositions, when the approximation and detail coefficients are concatenated as a tensor, the noise intensity in the detail coefficients would be stronger than that of useful details (object edge, texture, or contour, etc.) which could be the cause of degradation in the performance.</p><p>Influence of number of layers. The performance of WaveMix-Lite models generally improve as the number of layer increases. The behaviour observed in smaller datasets show that the accuracy increases with increase in number of layers, peaks at a particular value and then do not show any increase for any further addition of layers.</p><p>Influence of the Embedding Dimension. Our experiments showed that increasing the embedding dimension of a model usually improved the model performance, but the resource-utilization also increased significantly. Doubling the embedding dimension of model from 128 to 256 results in an increase of parameter count by more than three times and doubles the GPU RAM consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Semantic Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Detailed Results</head><p>The original input image resolution of 1024 ? 2048 could not be used in our experiments due to resource constraints. Experiments which used strided convolutions to process the 1024 ? 2048 input performed worse than using downsized 512 ? 1024 images as input. Even with 512 ? 1024 input, two strided convolutional layers will reduce it to 128 ? 256 before it reaches the WaveMix-Lite layers. The 512 ? 1024 input size was only used in 40 GB A100 GPU and 256 ? 512 input size was used for16 GB V100 GPU. <ref type="table">Table 9</ref> shows the detailed results of our experiments in Cityscapes dataset. Few examples of qualitative results are provided in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Ablation Studies</head><p>Influence of input image size. From the <ref type="table">Table 9</ref> we can see that for the same model size, larger input image resolution gave better results. The results for 512 ? 1024 input was 6-8% better than the corresponding results obtained while using input size of 256 ? 512.</p><p>Influence of number of layers. The number of layers that could be tested were limited due to the GPU constraints as well as the batch size requirements. We observed an increase in mIoU as the number of layers increases, then it peaks at around 16 layers for both the 512 ? 1024 and 256 ? 512 input and then gradually decreases for each additional layer.</p><p>Influence of embedding dimension. We varied the embedding dimension from 128 to 512 in the V100 GPU. The number of layers were adjusted to keep the model fit in the single GPU. For the A100 GPU, it was varied from 128 to 288. Variation of embedding dimension showed a behaviour similar to that shown by increasing the number of layers where mIoU first increases with increase in embedding dimension, then peaks at around 256, and then starts to decrease for both the input image sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>Model Prediction Input <ref type="figure">Figure 5</ref>: Qualitative results of semantic segmentation on Cityscapes dataset by WaveMix-Lite.</p><p>Influence of the MLP multiplication factor. The <ref type="table">Table 9</ref> shows that increasing the multiplication factor (mul) does not increase the parameter count significantly. It can be used to vary the parameter count slightly for a marginal increase the performance. Increasing the MLP multiplication factor beyond 3 showed slight deterioration in performance with input images of size 256 ? 512.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Accuracy scales more efficiently with parameters and GPU RAM for WaveMix-Lite compared to transformers and ResNets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>dataset size, parameters, floating point operations (FLOPs)) Graham et al. (2021); Hassani et al. (2021); Dosovitskiy et al. (2021); Wu et al. (2021); Jeevan, Sethi (2022); Jeevan, sethi (2022). The expensive training requirements of pure transformers also led to research into alternative token-mixing architectures that can replace self-attention Tolstikhin et al. (2021); Lee-Thorp et al. (2021); Guibas et al. (2021); Trockman,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and other architectural elements. There have been some exceptions that incorporate rotational-invariance for remote sensing images Cheng et al. (2016), and multi-resolution analysis for segmentation of histopathology images Kurian et al. (2022); van Rijthoven et al. (2021), but these methods have not been tested on general computer vision benchmarks. Advances in CNN performance have mainly come from architectural changes with the goals of easing gradient flow to deeper layers He et al. (2015b); Szegedy et al. (2014), or reducing parameters per layer by restricting convolutional kernel size Simonyan, Zisserman (2014) or their scope to only one dimension Chollet (2016). Attention mechanisms for space or channel Chen et al. (2017) also seem to improve performance of CNNs, although it has not been explored why a stack of additional convolutional layers cannot model the same function as that of spatial or channel attention. Vision transformers and hybrid architectures, inspired by their success on NLP tasks, have pushed the image classification accuracy beyond those of the largest CNNs, albeit at the cost of several times more data and network parameters Dosovitskiy et al. (2021). Training such data hungry models with hundreds of millions of parameters requires access to large GPU clusters, which is impractical for resource-constrained applications. Reduction in the computational requirements of vision transformers have been made possible by architectural changes that provide image specific inductive biases creating hybrid models with elements including distillation Touvron et al. (2020), convolutional embeddings Jeevan, Sethi (2022); Hassani et al. (2021), convolutional tokens Wu et al. (2021), and encoding overlapping patches Yuan et al.(2021). The quadratic complexity with respect to the sequence length (number of pixels) for vanilla transformers has also led to the search for other linear approximations of self-attention to efficiently mix tokens Jeevan, Sethi (2022).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>WaveMix-Lite architecture: Overall architecture for (a) classification and (b) semantic segmentation, along with (c) details of the WaveMix-Lite block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>,EMNIST Cohen et al. (2017),Fashion MNIST Xiao et al. (2017), and SVHN Netzer et al. (2011). Small datasets of larger image sizes includedSTL-10 Coates et al. (2011), Caltech-256 Griffin  et al. (2007 and Tiny ImageNet<ref type="bibr" target="#b33">Le, Yang (2015)</ref>. For benchmarking, we could not load the whole ImageNet-1K in 224 ? 224 resolution due to the resource constraints of Google Colab Pro+ ? . Therefore, we used ImageNet-1K of lower image resolution 64 ? 64. We also used larger datasets with larger images sizes such as, Places-365 Zhou et al.(2017)and iNaturalist2021-10k (iNAT mini) Horn et al. (2021). We used Cityscapes Cordts et al. (2016) dataset for semantic segmentation experiments and evaluated performance in the Cityscapes validation dataset. 4.2 Models compared WaveMix-Lite was compared with various other CNNs, transformers, and token-mixing models. These include ResNets He et al. (2015b), MobileNetV2 Sandler et al. (2018), UNets and DeepLabV2 as CNNs; ViT (vision transformer) Dosovitskiy et al. (2021), hybrid ViN (vision Nystromformer) Jeevan, Sethi (2022), CPV (convolutional performer for vision) Jeevan, sethi (2022), CCT (compact convolutional transformer) Hassani et al. (2021), CvT (convolutional vision transformer) Wu et al. (2021), and SegFormer Xie et al. (2021) as transformers; and FNet Lee-Thorp et al. (2021), Con-vMixer Trockman, Kolter (2022) and MLP-Mixer Tolstikhin et al. (2021) as token-mixers. Results of the other models that were directly taken from their original papers are cited in results tables.For model notation we use the format Model_Name-Embedding Dimension/Layers?Heads for transformers and exclude the heads for the other architectures. For example, CCT with embedding dimension of 128 having 4 layers and 4 heads is labelled as CCT-128/4 ? 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Visualisation of outputs after each layer shows that the image representation is different in WaveMix-Lite and ResNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>The results of occlusion analysis to find the significance of each pixel in the output decision shows that WaveMix-Lite identifies all the right pixels in an image for making the classification decision. The darker pixels in the image contribute more to the decision than lighter ones. The numbers show the probability of class output when the pixel is occluded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Image classification on ImageNet-1K dataset (64x64) shows improved accuracy as well as throughput due to decreased parameter count and GPU RAM consumption by WaveMix-Lite (arrows show desired directions)</figDesc><table><row><cell>Architecture</cell><cell>Top-1 Accu. (%) ?</cell><cell># Param. ?</cell><cell>GPU RAM for batch size 64 ?</cell><cell>Max batch size in 16 GB GPU ?</cell><cell cols="2">Throughput (im/s) Train ? Test ?</cell></row><row><cell>ResNet-18</cell><cell>50.67</cell><cell>11.2 M</cell><cell>2.6 GB</cell><cell>384</cell><cell>764</cell><cell>1,786</cell></row><row><cell>ResNet-34</cell><cell>55.04</cell><cell>21.3 M</cell><cell>3.5 GB</cell><cell>288</cell><cell>357</cell><cell>943</cell></row><row><cell>ResNet-50</cell><cell>55.66</cell><cell>23.6 M</cell><cell>11.3 GB</cell><cell>96</cell><cell>266</cell><cell>920</cell></row><row><cell>ResNet-101</cell><cell>56.05</cell><cell>44.6 M</cell><cell>15.1 GB</cell><cell>64</cell><cell>163</cell><cell>568</cell></row><row><cell>CPV-128/5x4</cell><cell>50.92</cell><cell>1.6 M</cell><cell>7.8 GB</cell><cell>128</cell><cell>375</cell><cell>1,190</cell></row><row><cell>ViT-128/4x4</cell><cell>30.91</cell><cell>0.7 M</cell><cell>14.7 GB</cell><cell>64</cell><cell>340</cell><cell>1,020</cell></row><row><cell>CCT-128/4x4</cell><cell>48.82</cell><cell>1.0 M</cell><cell>20.4 GB</cell><cell>48</cell><cell>149</cell><cell>514</cell></row><row><cell>WaveMix-Lite-128/7</cell><cell>47.27</cell><cell>2.5 M</cell><cell>4.7 GB</cell><cell>216</cell><cell>1,056</cell><cell>3,125</cell></row><row><cell>WaveMix-Lite-128/14</cell><cell>53.06</cell><cell>4.8 M</cell><cell>7.8 GB</cell><cell>131</cell><cell>555</cell><cell>1,471</cell></row><row><cell>WaveMix-Lite-256/7</cell><cell>56.66</cell><cell>9.0 M</cell><cell>9.0 GB</cell><cell>112</cell><cell>159</cell><cell>420</cell></row><row><cell>WaveMix-Lite-256/12</cell><cell>60.56</cell><cell>16.5 M</cell><cell>14.1 GB</cell><cell>72</cell><cell>118</cell><cell>344</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results for image classification on small datasets (32x32, 64x64) show improved accuracy as well as decreased parameter count and GPU RAM consumption by WaveMix-Lite</figDesc><table><row><cell>Model</cell><cell>#Param. ?</cell><cell>GPU RAM for batch size 64 ?</cell><cell cols="3">Accuracy (%) ? CIFAR-10 CIFAR-100 TinyImNet</cell></row><row><cell>ResNet-18 Hassani et al. (2021)</cell><cell>11.20 M</cell><cell>1.2 GB</cell><cell>90.27</cell><cell>63.41</cell><cell>48.11</cell></row><row><cell>ResNet-34 Hassani et al. (2021)</cell><cell>21.30 M</cell><cell>1.4 GB</cell><cell>90.51</cell><cell>64.52</cell><cell>45.60</cell></row><row><cell>ResNet-50 Hassani et al. (2021)</cell><cell>25.20 M</cell><cell>3.3 GB</cell><cell>90.60</cell><cell>61.68</cell><cell>48.77</cell></row><row><cell>MobileNetV2 Hassani et al. (2021)</cell><cell>8.72 M</cell><cell>-</cell><cell>91.02</cell><cell>67.44</cell><cell>-</cell></row><row><cell>ViT-128/4?4</cell><cell>0.53 M</cell><cell>13.8 GB</cell><cell>56.81</cell><cell>30.25</cell><cell>26.43</cell></row><row><cell>ViT-384/12x6 Hassani et al. (2021)</cell><cell>85.60 M</cell><cell>-</cell><cell>76.42</cell><cell>46.61</cell><cell>-</cell></row><row><cell>ViT-Lite-256/6x4 Hassani et al. (2021)</cell><cell>3.19 M</cell><cell>-</cell><cell>90.94</cell><cell>69.20</cell><cell>-</cell></row><row><cell>HybridViN-128/4?4</cell><cell>0.62 M</cell><cell>4.8 GB</cell><cell>75.26</cell><cell>51.44</cell><cell>34.05</cell></row><row><cell>CCT-128/4?4</cell><cell>0.91 M</cell><cell>15.8 GB</cell><cell>82.23</cell><cell>57.09</cell><cell>39.05</cell></row><row><cell>CvT-128/4?4</cell><cell>1.12 M</cell><cell>15.4 GB</cell><cell>79.93</cell><cell>48.29</cell><cell>40.69</cell></row><row><cell>MLP-Mixer-512/8</cell><cell>2.41 M</cell><cell>1.0 GB</cell><cell>72.22</cell><cell>44.23</cell><cell>26.83</cell></row><row><cell>WaveMix-Lite-16/7</cell><cell>0.04 M</cell><cell>0.1 GB</cell><cell>64.98</cell><cell>23.03</cell><cell>19.15</cell></row><row><cell>WaveMix-Lite-32/7</cell><cell>0.15 M</cell><cell>0.3 GB</cell><cell>84.67</cell><cell>46.89</cell><cell>34.34</cell></row><row><cell>WaveMix-Lite-64/7</cell><cell>0.60 M</cell><cell>0.6 GB</cell><cell>87.81</cell><cell>62.72</cell><cell>46.31</cell></row><row><cell>WaveMix-Lite-128/7</cell><cell>2.42 M</cell><cell>1.1 GB</cell><cell>91.08</cell><cell>68.40</cell><cell>52.03</cell></row><row><cell>WaveMix-Lite-144/7</cell><cell>3.01 M</cell><cell>1.2 GB</cell><cell>92.97</cell><cell>68.86</cell><cell>52.38</cell></row><row><cell>WaveMix-Lite-256/7</cell><cell>9.62 M</cell><cell>2.3 GB</cell><cell>90.72</cell><cell>70.20</cell><cell>51.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>WaveMix-Lite outperforms ResNets Gavrikov, Keuper (2022)  for image classification on various EMNIST, MNIST, and Fashion MNIST datasets (28?28) and achieves SOTA on 5 datasets</figDesc><table><row><cell>Models</cell><cell cols="6">Byclass Bymerge Letters Digits Balanced MNIST Fashion MNIST</cell></row><row><cell>ResNet-18</cell><cell>87.98</cell><cell>91.09</cell><cell>94.76 99.67</cell><cell>89.00</cell><cell>99.64</cell><cell>93.97</cell></row><row><cell>ResNet-34</cell><cell>88.10</cell><cell>91.13</cell><cell>95.04 99.68</cell><cell>89.17</cell><cell>99.60</cell><cell>93.91</cell></row><row><cell>ResNet-50</cell><cell>88.18</cell><cell>91.29</cell><cell>94.64 99.62</cell><cell>89.76</cell><cell>99.56</cell><cell>93.81</cell></row><row><cell>WaveMix-Lite</cell><cell>88.43</cell><cell>91.80</cell><cell>95.96 99.80</cell><cell>91.06</cell><cell>99.75</cell><cell>94.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Lite can be directly used for semantic segmentation by replacing the classifier head with deconvolution layers and a linear layer to generate the segmentation maps. On the other hand, architectural changes -such as encoder-decoder and UNet structures Ronneberger et al. (2015) -are required for base CNNs and transformers, including SegFormer Xie et al. (2021).The WaveMix-Lite performs on par with the other models on half-resolution Cityscapes validation set. We can see fromTable 4that WaveMix-Lite performs better than deep architectures like DeepLabV2 Chen et al.(2016)  and SegFormer model which uses an encoder pre-trained on ImageNet-1K dataset. The low mIoU obtained by replacing the classification head of ConvMixer Trockman, Kolter (2022) with segmentation head similar to WaveMix-Lite shows that token-mixing architectures which work well for classification cannot translate that performance in segmentation tasks without significant architectural modifications. This shows the versatility of our model which can provide high performance efficiently in multiple tasks. See Appendix for detailed results.</figDesc><table><row><cell cols="8">: Results for semantic segmentation on Cityscapes validation dataset show improved mIoU by</cell></row><row><cell cols="4">WaveMix-Lite without compromising throughput</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Architecture</cell><cell cols="2">mIoU ? # Param. ?</cell><cell>GPU RAM for batch size 64 ?</cell><cell>Max batch size for 16 GB ?</cell><cell cols="2">Throughput (im/s) Train ? Test ?</cell><cell>Notes</cell></row><row><cell>UNet Siam et al. (2018)</cell><cell>57.90</cell><cell>28.9 M</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>ResNet-18 Encoder</cell></row><row><cell>UNet Siam et al. (2018)</cell><cell>61.00</cell><cell>9.0 M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>MobileNetV2 Encoder</cell></row><row><cell>DeepLabV2-CRF Chen et al. (2016)</cell><cell>71.40</cell><cell>20.5 M</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>-</cell><cell>ResNet-101, Augmentations</cell></row><row><cell>SegFormer (MiT-B0) Xie et al. (2021)</cell><cell>71.90</cell><cell>3.4 M</cell><cell>-</cell><cell>-</cell><cell>47</cell><cell>-</cell><cell>Augmentations, ImageNet Pretrained</cell></row><row><cell>ConvMixer-512/16</cell><cell>53.40</cell><cell>7.8 M</cell><cell>42 GB</cell><cell>24</cell><cell>10</cell><cell cols="2">11 256 ? 512, 16 GB</cell></row><row><cell>SegFormer (MiT-B0)</cell><cell>62.56</cell><cell>7.7 M</cell><cell>232 GB</cell><cell>4</cell><cell>16</cell><cell cols="2">16 512 ? 1024, 40 GB</cell></row><row><cell>WaveMix-Lite 128/8</cell><cell>63.33</cell><cell>2.9 M</cell><cell>19 GB</cell><cell>55</cell><cell>18</cell><cell cols="2">18 256 ? 512, 16 GB</cell></row><row><cell>WaveMix-Lite 256/12</cell><cell>67.46</cell><cell>16.9 M</cell><cell>38 GB</cell><cell>25</cell><cell>11</cell><cell cols="2">12 256 ? 512, 16 GB</cell></row><row><cell>WaveMix-Lite 256/16</cell><cell>71.75</cell><cell>44.1 M</cell><cell>49 GB</cell><cell>21</cell><cell>9</cell><cell cols="2">11 256 ? 512, 16 GB</cell></row><row><cell>WaveMix-Lite 256/16</cell><cell>75.32</cell><cell>22.2 M</cell><cell>189 GB</cell><cell>6</cell><cell>14</cell><cell cols="2">16 512 ? 1024, 40 GB</cell></row><row><cell cols="8">We can see from Table 3 that WaveMix-Lite models outperform ResNets on all datasets (28 ? 28)</cell></row><row><cell cols="8">tested. It also establishes a new state-of-the-art by outperforming the previous best results Kabir</cell></row><row><cell cols="8">et al. (2020); Pad et al. (2020) by 0.01, 0.08, 0.01, 0.31 and 0.01 percentage points, respectively for</cell></row><row><cell cols="8">Balanced, Letters, Digits, Byclass and Bymerge subsets within EMNIST Cohen et al. (2017).</cell></row><row><cell cols="2">4.5 Semantic segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WaveMix-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>WaveMix-Lite needs very few parameters to achieve the benchmark results on the tasks mentioned below compared to other architectures</figDesc><table><row><cell>Task</cell><cell>Model</cell><cell>Parameters Expansion</cell></row><row><cell>99% accuracy on MNIST</cell><cell>WaveMix-Lite-8/10</cell><cell>3,566 Upsampling</cell></row><row><cell cols="2">90% accuracy on Fashion MNIST WaveMix-Lite-8/5</cell><cell>7,156 Deconvolution</cell></row><row><cell>80% accuracy on CIFAR-10</cell><cell>WaveMix-Lite-32/7</cell><cell>37,058 Upsampling</cell></row><row><cell>90% accuracy on CIFAR-10</cell><cell>WaveMix-Lite-64/6</cell><cell>520,106 Deconvolution</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results for Image classification on larger resolution datasets show improved accuracy compared to ResNets when trained on a single 16 GB V100 GPU.</figDesc><table><row><cell>Models</cell><cell>STL-10 96 ? 96</cell><cell>SVHN 32 ? 32</cell><cell>Caltech-256 256 ? 256</cell><cell>Places-365 256 ? 256</cell><cell>iNAT-2021 256 ? 256</cell></row><row><cell>ResNet-18</cell><cell>70.41</cell><cell>97.4</cell><cell>52.97</cell><cell>48.74</cell><cell>26.35</cell></row><row><cell>ResNet-34</cell><cell>68.07</cell><cell>97.47</cell><cell>50.92</cell><cell>49.02</cell><cell>31.02</cell></row><row><cell>ResNet-50</cell><cell>66.04</cell><cell>97.32</cell><cell>49.97</cell><cell>49.8</cell><cell>33.14</cell></row><row><cell>WaveMix-Lite-256/7</cell><cell>70.88</cell><cell>98.14</cell><cell>54.62</cell><cell>49.83</cell><cell>33.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>State-of-the-art (SOTA) models for the 5 EMNIST Datasets.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell></cell><cell cols="2">#Params SOTA Accu. (%)</cell></row><row><cell>By_Class</cell><cell>WaveMix-Lite-128/7</cell><cell></cell><cell>2.4 M</cell><cell>88.43</cell></row><row><cell>Balanced</cell><cell>WaveMix-Lite-128/7</cell><cell></cell><cell>2.4 M</cell><cell>91.06</cell></row><row><cell>Letters</cell><cell cols="2">WaveMix-Lite-112/16</cell><cell>4.1 M</cell><cell>95.96</cell></row><row><cell>Digits</cell><cell cols="2">WaveMix-Lite-112/16</cell><cell>4.1 M</cell><cell>99.80</cell></row><row><cell cols="3">By_Merge WaveMix-Lite-128/16 (ff =256)</cell><cell>10.1 M</cell><cell>91.80</cell></row><row><cell>Layer 1</cell><cell>Layer 4</cell><cell>Layer 7</cell><cell>Layer 10</cell><cell>Final Layer</cell></row><row><cell>WaveMix-Lite</cell><cell></cell><cell></cell><cell></cell><cell>Input</cell></row><row><cell>ResNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Variation of performance and resource consumption of WaveMix-Lite-144/7 on classification of CIFAR-10 dataset using different levels of 2D-DWT.</figDesc><table><row><cell>Level of 2D-DWT</cell><cell cols="2">Accu. (%) # Params</cell><cell>GPU for batch size of 1024</cell></row><row><cell>1</cell><cell>91.61</cell><cell>3 M</cell><cell>19.6 GB</cell></row><row><cell>2</cell><cell>87.39</cell><cell>5.9 M</cell><cell>14.2 GB</cell></row><row><cell>3</cell><cell>78.07</cell><cell>15.2 M</cell><cell>13.7 GB</cell></row><row><cell>4</cell><cell>65.40</cell><cell>47.7 M</cell><cell>13.2 GB</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/pranavphoenix/WaveMix</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Base code: https://pytorch-wavelets.readthedocs.io/en/latest/readme.html</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Feedforward Dimension and MLP Multiplication Factor</head><p>The feedforward dimension (ff) is the dimension of the embeddings of ouput from the MLP layer before it is passed to the deconvolution layer. The deconvolution layer then changes the embedding dimension back to the value set in the model name. Unless otherwise mentioned, the value of feedforward dimension is set by default as the embedding dimension specified in the model name. Using a value higher than embedding dimension as ff dimension increases the number of parameters of the model and GPU consumption. Feedforward dimension is different from the MLP mulitplication factor (mul) which describes the increase in embedding dimension within the MLP layer (the first 1 ? 1 convolution increases it by a factor and the second 1 ? 1 convolution decreases it after is passes through the GELU activation). For example, MLP multiplication factor of 2 in a WaveMix-Lite-128 will use the first 1 ? 1 convolutional layer inside MLP to increase the embedding dimension from 128 to 256. After the GELU activation, the second 1 ? 1 convolutional layer inside MLP will decreases the embedding dimension back to 128. If we specify the ff dimension to be different from one provided in model name, then the second 1 ? 1 convolutional layer inside MLP will change it to the ff dimension as specified. Unless otherwise mentioned, an MLP multiplication factor of 2 was used in all the models. Influence of levels of 2D-DWT. <ref type="table">Table 8</ref> shows the variation of performance and resource consumption as we use higher levels of 2D-DWT. Level-1 2D-DWT reduced image resolution by half, reducing the computational cost and GPU consumption compared to convolutional layers. Using further levels of 2D-DWT could further reduce the image resolution to one-fourth, one-eight and so on which can further reduce the computational costs. But the deconvolution layer used to resize</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Invariant Scattering Convolution Networks // IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruna</forename><surname>Joan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mallat</forename><surname>Stephane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang-Chieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Papandreou</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kokkinos</forename><surname>Iasonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murphy</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Deeplab</surname></persName>
		</author>
		<title level="m">Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Hanwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nie</forename><surname>Liqiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">Chua Tat-Seng. SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images // IEEE Transactions on Geoscience and Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Peicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Junwei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chollet</forename><surname>Fran?ois</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Analysis of Single-Layer Networks in Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coates</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Honglak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. 15. Fort</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics. 15. Fort<address><addrLine>Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2011-04" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Schaik Andr? van. EMNIST: Extending MNIST to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cohen</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshar</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapson</forename><surname>Jonathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Schiele Bernt. The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordts</forename><surname>Marius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omran</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramos</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rehfeld</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enzweiler</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benenson</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franke</forename><surname>Uwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename><surname>Stefan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The wavelet transform, time-frequency localization and signal analysis // IEEE Transactions on Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="961" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Uszkoreit Jakob, Houlsby Neil. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dosovitskiy</forename><surname>Alexey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beyer</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kolesnikov</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weissenborn</forename><surname>Dirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhai</forename><surname>Xiaohua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unterthiner</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehghani</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minderer</forename><surname>Matthias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heigold</forename><surname>Georg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gelly</forename><surname>Sylvain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Scale-invariance and self-similar &apos;wavelet&apos;transforms: an analysis of natural scenes and mammalian visual systems // Wavelets, fractals, and Fourier transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="151" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavrikov</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keuper</forename><surname>Janis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">LeViT: a Vision Transformer in ConvNet&apos;s Clothing for Faster Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Touvron</forename><surname>El-Nouby Alaaeldin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stock</forename><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?gou</forename><surname>Armand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douze</forename><surname>Herv?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthijs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Griffin</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holub</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perona</forename><surname>Pietro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guibas</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mardani</forename><surname>Morteza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zongyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anandkumar</forename><surname>Anima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catanzaro</forename><surname>Bryan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Wavelet Prediction for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Tiantong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojjat</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monga</forename><surname>Vu Tiep Huu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Escaping the Big Data Paradigm with Compact Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassani</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walton</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shah</forename><surname>Nikhil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abuduweili</forename><surname>Abulikemu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiachen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Humphrey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Jian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Jian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Aodha Oisin Mac. Benchmarking Representation Learning for Natural World Image Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cole</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beery</forename><surname>Elijah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilber</forename><surname>Sara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belongie</forename><surname>Kimberly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Menglong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalenichenko</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Weijun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weyand</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreetto</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Hartwig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Hu Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albanie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Enhua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Maaten Laurens van der, Weinberger Kilian Q. Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Resource-Efficient Hybrid X-Formers for Vision // Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Pranav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethi</forename><surname>Amit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-01" />
			<biblScope unit="page" from="2982" to="2990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Convolutional Xformers for Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Pranav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">LightLayers: Parameter Efficient Dense and Convolutional Layers for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jha</forename><surname>Debesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazidi</forename><surname>Anis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riegler</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johansen</forename><surname>Dag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johansen</forename><surname>H?vard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halvorsen</forename><surname>P?l</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdar</forename><surname>Moloud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jalali Seyed Mohammad</forename><surname>Jafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khosravi</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atiya</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nahavandi</forename><surname>Saeid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Dipti</surname></persName>
		</author>
		<idno>2020. abs/2007.03347</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving Generalization Performance by Switching from Adam to SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Socher</forename><surname>Keskar Nitish Shirish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naseer</forename><surname>Khan Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayat</forename><surname>Muzammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zamir</forename><surname>Munawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Syed Waqas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan Fahad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shah</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mubarak</surname></persName>
		</author>
		<title level="m">Vision: A Survey // ACM Computing Surveys</title>
		<imprint>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Hinton Geoffrey E. Imagenet classification with deep convolutional neural networks // Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krizhevsky</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutskever</forename><surname>Ilya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for wavelet domain super resolution // Pattern Recognition Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Neeraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethi</forename><surname>Verma Ruchika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="65" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep Multi-Scale U-Net Architecture and Noise-Robust Training Strategies for Histopathological Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lohan</forename><surname>Kurian Nikhil Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verghese</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharamshi</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meena</forename><surname>Nimish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Swati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Mengyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gillet</forename><surname>Fangfang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rane</forename><surname>Cheryl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigoriadis</forename><surname>Swapnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethi</forename><surname>Anita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Tiny ImageNet Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Gradient-Based Learning Applied to Document Recognition // Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lecun</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename><surname>Bottou L?on</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haffner</forename><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gradient-based learning applied to document recognition // Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Image representation using 2D Gabor wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Tai Sing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="959" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">FNet: Mixing Tokens with Fourier Transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee-Thorp</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainslie</forename><surname>Joshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eckstein</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ontanon</forename><surname>Santiago</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Image compression using the 2-D wavelet transform // IEEE Transactions on Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Knowles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Wavelet Integrated CNNs for Noise-Robust Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Qiufu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Shen Linlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhihui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Train big, then compress: Rethinking model size for efficient training and inference of transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhuohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wallace</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keutzer</forename><surname>Kurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klein</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. 2020b</title>
		<imprint>
			<biblScope unit="page" from="5958" to="5968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Multi-level Wavelet-CNN for Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Pengju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Hongzhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuo</forename><surname>Wangmeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename></persName>
		</author>
		<title level="m">Evans Hawke A. Facial Expression Recognition in Image Sequences Using 1D Transform and Gabor Wavelet Transform // 2018 International Conference on Applied and Engineering Mathematics (ICAEM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pyramid Attention Networks for Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Yiqun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Yulun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiahui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yuqian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature extraction with wavelet transform for recognition of isolated handwritten Farsi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mowlaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Haghighat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Digital Signal Processing Proceedings. DSP 2002 (Cat. No.02TH8628</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="923" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Brain MR image classification using two-dimensional discrete wavelet transform and AdaBoost with random forests // Neurocomputing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dash</forename><surname>Nayak Deepak Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majhi</forename><surname>Ratnakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banshidhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Ng Andrew Y. Reading Digits in Natural Images with Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netzer</forename><surname>Yuval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coates</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bissacco</forename><surname>Alessandro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Bo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pad</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narduzzi</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundig</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turetken</forename><surname>Engin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bigdeli</forename><surname>Siavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrea</surname></persName>
		</author>
		<title level="m">Efficient Neural Vision Systems Based on Convolutional Image Acquisition // Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Spatiochromatic properties of natural images and human vision // Current biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>P?rraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Troscianko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Tolhurst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="483" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The Haar-Wavelet Transform in Digital Image Processing: Its Status and Achievements // Machine graphics &amp; vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Porwik</forename><surname>Piotr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisowska</forename><surname>Agnieszka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="79" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detection of arrhythmia based on discrete wavelet transform using artificial neural network and support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranaware</forename><surname>Preeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshpande</forename><surname>Rohini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Communication and Signal Processing (ICCSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1767" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronneberger</forename><surname>Olaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fischer</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brox</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The statistics of natural images // Network: computation in neural systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruderman</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="0517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruikar</forename><surname>Sachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doye</surname></persName>
		</author>
		<title level="m">Image denoising using wavelet transform // 2010 International Conference on Mechanical and Electrical Technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="509" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandler</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Menglong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhmoginov</forename><surname>Andrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang-Chieh</surname></persName>
		</author>
		<title level="m">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">RTSeg: Real-time Semantic Segmentation Comparative Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siam</forename><surname>Mennatullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamal</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Razek</forename><surname>Moemen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogamani</forename><surname>Senthil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagersand</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisserman</forename><surname>Simonyan Karen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szegedy</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sermanet</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reed</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelov</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhan</forename><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanhoucke</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabinovich</forename><surname>Andrew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">MLP-Mixer: An all-MLP Architecture for Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolstikhin</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houlsby</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kolesnikov</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beyer</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhai</forename><surname>Xiaohua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unterthiner</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung</forename><surname>Jessica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keysers</forename><surname>Steiner Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uszkoreit</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucic</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dosovitskiy</forename><surname>Mario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers: distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Touvron</forename><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cord</forename><surname>Matthieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douze</forename><surname>Matthijs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massa</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?gou</forename><surname>Sablayrolles Alexandre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herv?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Patches Are All You Need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trockman</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zico</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">ProdSumNet: reducing model parameters in deep neural networks via product-of-sums matrix decompositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wah</forename><surname>Wu Chai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">CvT: Introducing Convolutions to Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Haiping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Codella</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Mengchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Xiyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Lei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasul</forename><surname>Kashif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vollgraf</forename><surname>Roland</surname></persName>
		</author>
		<idno>2017. abs/1708.07747</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xie</forename><surname>Enze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Wenhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhiding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anandkumar</forename><surname>Anima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvarez</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Ping</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yunpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Weihao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Shi Yujun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tay</forename><surname>Zihang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jiashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Shuicheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Exploring Self-attention for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Hengshuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jiaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koltun</forename><surname>Vladlen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Places: A 10 million Image Database for Scene Recognition // IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Bolei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapedriza</forename><surname>Agata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khosla</forename><surname>Aditya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliva</forename><surname>Aude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torralba</forename><surname>Antonio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">HookNet: Multi-resolution convolutional neural networks for semantic segmentation in histopathology whole-slide images // Medical Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balkenhol</forename><surname>Van Rijthoven Mart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maschenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciompi</forename><surname>Van Der Laak Jeroen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francesco</surname></persName>
		</author>
		<idno>Analysis. 2021. 68. 101890</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Results for semantic segmentation using WaveMix-Lite models on Cityscapes validation dataset at different image resolutions in V100 and A100 GPUs</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
