<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SSD: A UNIFIED FRAMEWORK FOR SELF- SUPERVISED OUTLIER DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikash</forename><surname>Sehwag</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mung</forename><surname>Chiang</surname></persName>
							<email>chiang@purdue.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Mittal</surname></persName>
							<email>pmittal@princeton.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SSD: A UNIFIED FRAMEWORK FOR SELF- SUPERVISED OUTLIER DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We ask the following question: what training information is required to design an effective outlier/out-of-distribution (OOD) detector, i.e., detecting samples that lie far away from the training distribution? Since unlabeled data is easily accessible for many applications, the most compelling approach is to develop detectors based on only unlabeled in-distribution data. However, we observe that most existing detectors based on unlabeled data perform poorly, often equivalent to a random prediction. In contrast, existing state-of-the-art OOD detectors achieve impressive performance but require access to fine-grained data labels for supervised training. We propose SSD, an outlier detector based on only unlabeled in-distribution data. We use self-supervised representation learning followed by a Mahalanobis distance based detection in the feature space. We demonstrate that SSD outperforms most existing detectors based on unlabeled data by a large margin. Additionally, SSD even achieves performance on par, and sometimes even better, with supervised training based detectors. Finally, we expand our detection framework with two key extensions. First, we formulate few-shot OOD detection, in which the detector has access to only one to five samples from each class of the targeted OOD dataset. Second, we extend our framework to incorporate training data labels, if available. We find that our novel detection framework based on SSD displays enhanced performance with these extensions, and achieves state-of-the-art performance 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks are at the cornerstone of multiple safety-critical applications, ranging from autonomous driving <ref type="bibr" target="#b46">(Ramanagopal et al., 2018)</ref> to biometric authentication <ref type="bibr" target="#b37">(Masi et al., 2018;</ref><ref type="bibr" target="#b20">G?nther et al., 2017)</ref>. When trained on a particular data distribution, referred to as in-distribution data, deep neural networks are known to fail against test inputs that lie far away from the training distribution, commonly referred to as outliers or out-of-distribution (OOD) samples <ref type="bibr" target="#b19">(Grubbs, 1969;</ref><ref type="bibr" target="#b23">Hendrycks &amp; Gimpel, 2017)</ref>. This vulnerability motivates the use of an outlier detector before feeding the input samples to the downstream neural network modules. However, a key question is to understand what training information is crucial for effective outlier detection? Will the detector require fine-grained annotation of training data labels or even access to a set of outliers in the training process?</p><p>Since neither data labels nor outliers are ubiquitous, the most compelling option is to design outlier detectors based on only unlabeled in-distribution data. However, we observe that most of the existing outlier detectors based on unlabeled data fail to scale up to complex data modalities, such as images. For example, autoencoder (AE) <ref type="bibr" target="#b21">(Hawkins et al., 2002)</ref> based outlier detectors have achieved success in applications such as intrusion detection <ref type="bibr" target="#b38">(Mirsky et al., 2018)</ref>, and fraud detection <ref type="bibr" target="#b54">(Schreyer et al., 2017)</ref>. However, this approach achieves close to chance performance on image datasets. Similarly, density modeling based methods, such as PixelCNN++ <ref type="bibr" target="#b50">(Salimans et al., 2017)</ref> and <ref type="bibr">Glow (Kingma &amp; Dhariwal, 2018)</ref> are known to assign even a higher likelihood to outliers in comparison to indistribution data <ref type="bibr" target="#b41">(Nalisnick et al., 2019)</ref>.</p><p>In contrast, existing state-of-the-art OOD detectors achieve high success on image datasets but assume the availability of fine-grained labels for in-distribution samples <ref type="bibr" target="#b23">(Hendrycks &amp; Gimpel, 2017;</ref><ref type="bibr"></ref> Published as a conference paper at ICLR 2021 <ref type="bibr" target="#b2">Bendale &amp; Boult, 2016;</ref><ref type="bibr" target="#b35">Liang et al., 2018;</ref><ref type="bibr" target="#b11">Dhamija et al., 2018;</ref><ref type="bibr" target="#b61">Winkens et al., 2020)</ref>. This is a strong assumption since labels, in-particular fine-grained labels, can be very costly to collect in some applications <ref type="bibr" target="#b16">(Google AI Pricing, 2020)</ref>, which further motivates the use of unlabeled data. The inability of supervised detectors to use unlabeled data and poor performance of existing unsupervised approaches naturally give rise to the following question.</p><p>Can we design an effective out-of-distribution (OOD) data detector with access to only unlabeled data from training distribution?</p><p>A framework for outlier detection with unlabeled data 2 involves two key steps: 1) Learning a good feature representation with unsupervised training methods 2) Modeling features of in-distribution data without requiring class labels. For example, autoencoders attempt to learn the representation with a bottleneck layer, under the expectation that successful reconstruction requires learning a good set of representations. Though useful for tasks such as dimensionality reduction, we find that these representations are not good enough to sufficiently distinguish in-distribution data and outliers. We argue that if unsupervised training can develop a rich understanding of key semantics in in-distribution data then absence of such semantics in outliers can cause them to lie far away in the feature space, thus making it easy to detect them. Recently, self-supervised representation learning methods have made large progress, commonly measured by accuracy achieved on a downstream classification task <ref type="bibr" target="#b22">He et al., 2020;</ref><ref type="bibr" target="#b43">Oord et al., 2018;</ref><ref type="bibr" target="#b39">Misra &amp; Maaten, 2020;</ref>. We leverage these representations in our proposed cluster-conditioned framework based on the Mahalanobis distance <ref type="bibr" target="#b36">(Mahalanobis, 1936)</ref>. Our key result is that self-supervised representations are highly effective for the task of outlier detection in our self-supervised outlier detection (SSD) framework where they not only perform far better than most of the previous unsupervised representation learning methods but also perform on par, and sometimes even better, than supervised representations.</p><p>What if access to a fraction of OOD data or training data labels is available? How do we move past a detector based on unlabeled data and design a framework which can take advantage of such information? Though access to outliers during training is a strong assumption, it may be feasible to obtain a few prior instances of such outliers <ref type="bibr" target="#b17">(G?rnitz et al., 2013)</ref>. We characterize this setting as few-shot OOD detection, where we assume access to very few, often one to five, samples from the targeted set of outliers. While earlier approaches <ref type="bibr" target="#b35">(Liang et al., 2018;</ref><ref type="bibr" target="#b34">Lee et al., 2018b)</ref> mostly use such data to calibrate the detector, we find that access to just a few outliers can bring an additional boost in the performance of our detector. Crucial to this success is the reliable estimation of first and second order statistics of OOD data in the high dimensional feature space with just a few samples.</p><p>Finally, if class labels are available in the training phase, how can we incorporate them in the SSD framework for outlier detection? Recent works have proposed the addition of the supervised crossentropy and self-supervised learning loss with a tunable parameter, which may require tuning for optimal parameter setting for each dataset <ref type="bibr" target="#b25">(Hendrycks et al., 2019b;</ref><ref type="bibr" target="#b61">Winkens et al., 2020)</ref>. We demonstrate that incorporating labels directly in the contrastive loss achieves 1) a tuning parameterfree detector, and 2) state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">KEY CONTRIBUTIONS</head><p>SSD for unlabeled data. We propose SSD, an unsupervised framework for outlier detection based on unlabeled in-distribution data. We demonstrate that SSD outperforms most existing unsupervised outlier detectors by a large margin while also performing on par, and sometimes even better than supervised training based detection methods. We validate our observation across four different datasets: CIFAR-10, CIFAR-100, STL-10, and ImageNet.</p><p>Extensions of SSD. We provide two extensions of SSD to further improve its performance. First, we formulate few-shot OOD detection and propose detection methods which can achieve a significantly large gain in performance with access to only a few targeted OOD samples. Next, we extend SSD, without using any tuning parameter, to also incorporate in-distribution data labels and achieve state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>OOD detection with unsupervised detectors. Interest in unsupervised outlier detection goes back to <ref type="bibr" target="#b19">Grubbs (1969)</ref>. We categorize these approaches in three groups 1) Reconstruction-error based detection using Auto-encoders <ref type="bibr" target="#b21">(Hawkins et al., 2002;</ref><ref type="bibr" target="#b38">Mirsky et al., 2018;</ref><ref type="bibr" target="#b54">Schreyer et al., 2017)</ref> or Variational auto-encoders <ref type="bibr" target="#b0">(Abati et al., 2019;</ref><ref type="bibr" target="#b1">An &amp; Cho, 2015)</ref> 2) Classification based, such as Deep-SVDD <ref type="bibr" target="#b48">(Ruff et al., 2018;</ref><ref type="bibr" target="#b12">El-Yaniv &amp; Wiener, 2010;</ref><ref type="bibr" target="#b13">Geifman &amp; El-Yaniv, 2017)</ref> and 3) Probabilistic detectors, such as density models like Glow and PixelCNN++ <ref type="bibr" target="#b47">(Ren et al., 2019;</ref><ref type="bibr" target="#b41">Nalisnick et al., 2019;</ref><ref type="bibr" target="#b50">Salimans et al., 2017;</ref><ref type="bibr" target="#b30">Kingma &amp; Dhariwal, 2018)</ref>. We compare with detectors from each category and find that SSD outperforms them by a wide margin.</p><p>OOD detection with supervised learning. Supervised detectors have been most successful with complex input modalities, such as images and language <ref type="bibr" target="#b4">(Chalapathy et al., 2018a;</ref><ref type="bibr" target="#b10">DeVries &amp; Taylor, 2018;</ref><ref type="bibr" target="#b11">Dhamija et al., 2018;</ref><ref type="bibr" target="#b27">Jiang et al., 2018;</ref><ref type="bibr" target="#b62">Yoshihashi et al., 2018;</ref><ref type="bibr" target="#b33">Lee et al., 2018a)</ref>. Most of these approaches model features of in-distribution data at output <ref type="bibr" target="#b35">(Liang et al., 2018;</ref><ref type="bibr" target="#b23">Hendrycks &amp; Gimpel, 2017;</ref><ref type="bibr" target="#b11">Dhamija et al., 2018)</ref> or in the feature space <ref type="bibr" target="#b34">(Lee et al., 2018b;</ref><ref type="bibr" target="#b61">Winkens et al., 2020)</ref> for detection. We show that SSD can achieve performance on par with these supervised detectors, without using data labels. A subset of these detectors also leverages generic OOD data to boost performance <ref type="bibr" target="#b40">Mohseni et al., 2020)</ref>.</p><p>Access to OOD data at training time. Some recent detectors also require OOD samples for hyperparameter tuning <ref type="bibr" target="#b35">(Liang et al., 2018;</ref><ref type="bibr" target="#b34">Lee et al., 2018b;</ref><ref type="bibr" target="#b63">Zisselman &amp; Tamar, 2020)</ref>. We extend SSD to this setting but assume access to only a few OOD samples, referred to as few-shot OOD detection, which our framework can efficiently utilize to bring further gains in performance.</p><p>In conjunction with supervised training. <ref type="bibr" target="#b59">Vyas et al. (2018)</ref> uses ensemble of leave-one-out classifier, <ref type="bibr" target="#b61">Winkens et al. (2020)</ref> uses contrastive self-supervised training, and <ref type="bibr" target="#b25">Hendrycks et al. (2019b)</ref> uses rotation based self-supervised loss, in conjunction with supervised cross-entropy loss to achieve state-of-the-art performance in OOD detection. Here we extend SSD, to incorporate data labels, when available, and achieve better performance than existing state-of-the-art.</p><p>Anomaly detection. In parallel to OOD detection, this research direction focuses on the detection of semantically related anomalies in applications such as intrusion detection, spam detection, disease detection, image classification, and video surveillance. We refer the interested reader to <ref type="bibr" target="#b44">Pang et al. (2020)</ref> for a detailed review. While a large number of works focuses on developing methods particularly for single-class modeling in anomaly detection <ref type="bibr" target="#b45">(Perera et al., 2019;</ref><ref type="bibr" target="#b52">Schlegl et al., 2017;</ref><ref type="bibr" target="#b48">Ruff et al., 2018;</ref><ref type="bibr" target="#b5">Chalapathy et al., 2018b;</ref><ref type="bibr" target="#b15">Golan &amp; El-Yaniv, 2018;</ref><ref type="bibr" target="#b60">Wang et al., 2019)</ref>, some recent work achieve success in both OOD detection and anomaly detection <ref type="bibr" target="#b56">Tack et al. (2020)</ref>; <ref type="bibr" target="#b3">Bergman &amp; Hoshen (2020)</ref>. We provide a detailed comparison of our approach with previous work in both categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SSD: SELF-SUPERVISED OUTLIER/OUT-OF-DISTRIBUTION DETECTION</head><p>In this section, we first provide the necessary background on outlier/out-of-distribution (OOD) detection and then present the underlying formulation of our self-supervised detector (SSD) that relies on only unlabeled in-distribution data. Finally, we describe two extensions of SSD to (optionally) incorporate targeted OOD samples and in-distribution data labels (if available).</p><p>Notation. We represent the input space by X and corresponding label space as Y. We assume in-distribution data is sampled from P in X ?Y . In the absence of data labels, it is sampled from marginal distribution P in X . We sample out-of-distribution data from P ood X . We denote the feature extractor by f : X ? Z, where Z ? R d , a function which maps a sample from the input space to the d-dimensional feature space (Z). The feature extractor is often parameterized by a deep neural network. In supervised learning, we obtain classification confidence for each class by g ? f : X ? R c . In most cases, g is parameterized by a shallow neural network, generally a linear classifier.</p><p>Problem Formulation: Outlier/Out-of-distribution (OOD) detection. Given a collection of samples from P in X ? P ood X , the objective is to correctly identify the source distribution, i.e., P in X or P ood X , for each sample. We use the term supervised OOD detectors for detectors which use in-distribution data labels, i.e., train the neural network (g ? f ) on P in X ?Y using supervised training techniques.</p><p>Unsupervised OOD detectors aim to solve the aforementioned OOD detection tasks, with access to only P in X . In this work, we focus on developing effective unsupervised OOD detectors. Background: Contrastive self-supervised representation learning. Given unlabeled training data, it aims to train a feature extractor, by discriminating between individual instances from data, to learn a good set of representations. Using image transformations, it first creates two views of each image, commonly referred to as positives. Next, it optimizes to pull each instance close to its positive instances while pushing away from other images, commonly referred to as negatives. Assuming that (x i , x j ) are positive pairs for the ith image from a batch of N images and h(.) is a projection header, ? is the temperature, contrastive training minimizes the following loss, referred to as Normalized temperature-scaled cross-entropy (NT-Xent), over each batch.</p><formula xml:id="formula_0">L batch = 1 2N 2N i=1 ?log e u T i uj /? 2N k=1 1(k = i)e u T i u k /? ; u i = h (f (x i )) h(f (x i )) 2<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">UNSUPERVISED OUTLIER DETECTION WITH SSD</head><p>Leveraging contrastive self-supervised training. In the absence of data labels, SSD consists of two steps: 1) Training a feature extractor using unsupervised representation learning, 2) Developing an effective OOD detector based on hidden features which isn't conditioned on data labels. Higher eigenvalues dominates euclidean distance, but are least helpful for outlier detection. Mahalnobis distance avoid this bias with appropriate scaling and performs much better.</p><p>We leverage contrastive self-supervised training for representation learning in our outlier detection framework, particularly due to its state-of-the-art performance . We will discuss the effect of different representation learning methods later in Section 4.2.</p><p>Cluster-conditioned detection. In absence of data labels, we develop a cluster-conditioned detection method in the feature space. We first partition the features for in-distribution training data in m clusters. We represent features for each cluster as Z m . We use k-means clustering method, due to its effectiveness and low computation cost. Next, we model features in each cluster independently, and calculate the following outlier score (s</p><formula xml:id="formula_1">x ) = min m D(x, Z m )</formula><p>for each test input x, where D(., .) is a distance metric in the feature space. We discuss the choice of the number of clusters in Section 4.2.</p><p>Choice of distance metric: Mahalanobis distance. We use Mahalanobis distance to calculate the outlier score as follows:</p><formula xml:id="formula_2">s x = min m (z x ? ? m ) T ? ?1 m (z x ? ? m )<label>(2)</label></formula><p>where ? m and ? m are the sample mean and sample covariance of features (Z) of the in-distribution training data. We justify this choice with quantitative results in <ref type="figure" target="#fig_0">Figure 1</ref></p><formula xml:id="formula_3">. With eigendecomposition of sample covariance (? m = Q m ? m Q ?1 m ), s x = min m Q T m (z x ? ? m ) T ? ?1 m Q T m (z x ? ? m )</formula><p>which is equivalent to euclidean distance scaled with eigenvalues in the eigenspace. We discriminate between in-distribution (CIFAR-10) and OOD (CIFAR-100) data along each principal eigenvector (using AUROC, higher the better). With euclidean distance, i.e., in absence of scaling, components with higher eigenvalues have more weight but provide least discrimination. Scaling with eigenvalues removes the bias, making Mahalnobis distance effective for outlier detection in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FEW-SHOT OOD DETECTION (SSD k )</head><p>In this extension of the SSD framework, we consider the scenario where a few samples from the OOD dataset used at inference time are also available at the time of training. We focus on one-shot and five-shot detection, which refers to access to only one and fives samples, from each class of the targeted OOD dataset, respectively. Our hypothesis is that in-distribution samples and OOD samples will be closer to other inputs from their respective distribution in the feature space, while lying further away from each other. We incorporate this hypothesis by using following formulation of outlier score.</p><formula xml:id="formula_4">s x = (z x ? ? in ) T ? ?1 in (z x ? ? in ) ? (z x ? ? ood ) T ? ?1 ood (z x ? ? ood )<label>(3)</label></formula><p>where ? in , ? in and ? ood , ? ood are the sample mean and sample covariance in the feature space for in-distribution and OOD data, respectively.</p><p>Challenge. The key challenge is to reliably estimate the statistics for OOD data, with access to only a few samples. Sample covariance is not an accurate estimator of covariance when the number of samples is less than the dimension of feature space <ref type="bibr" target="#b55">(Stein, 1975)</ref>, which is often in the order of thousands for deep neural networks.</p><p>Shrunk covariance estimators and data augmentation. We overcome this challenge by using following two techniques: 1) we use shrunk covariance estimators <ref type="bibr" target="#b32">(Ledoit &amp; Wolf, 2004)</ref>, and 2) we amplify number of OOD samples using data augmentation. We use shrunk covariance estimators due to their ability to estimate covariance better than sample covariance, especially when the number of samples is even less than the feature dimension. To further improve the estimation we amplify the number of samples using data augmentation at the input stage. We use common image transformations, such as geometric and photometric changes to create multiple different images from a single source image from the OOD dataset. Thus given a set of k OOD samples {u 1 , u 2 , . . . , u k }, we first create a set of k ? n samples using data augmentation, U = {u 1 1 , . . . , u n 1 , . . . u 1 k , . . . , u n k }. Using this set, we calculate the outlier score for a test sample in the following manner.</p><formula xml:id="formula_5">s x = (z x ? ? in ) T ? ?1 in (z x ? ? in ) ? (z x ? ? U ) T S ?1 U (z x ? ? U )<label>(4)</label></formula><p>where ? U and S U are the sample mean and estimated covariance using shrunk covariance estimators for the set U, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HOW TO BEST USE DATA LABELS (SSD+)</head><p>If fine-grained labels for in-distribution data are available, an immediate question is how to incorporate them in training to improve the success in detecting OOD samples.</p><p>Conventional approach: Additive self-supervised and supervised training loss. A common theme in recent works <ref type="bibr" target="#b25">(Hendrycks et al. 2019b;</ref><ref type="bibr" target="#b61">Winkens et al. 2020</ref>) is to add self-supervised (L ssl ) and supervised (L sup ) training loss functions, i.e., L training = L sup + ?L ssl , where the hyperparameter ? is chosen for best performance on OOD detection. A common loss function for supervised training is cross-entropy.</p><p>Our approach: Incorporating labels in contrastive self-supervised training. As we show in Section 4.2, even without labels, contrasting between instances using self-supervised learning is highly successful for outlier detection. We argue for a similar instance-based contrastive training, where labels can also be incorporated to further improve the learned representations. To this end, we use the recently proposed supervised contrastive training loss function <ref type="bibr" target="#b28">(Khosla et al., 2020)</ref>, which uses labels for a more effective selection of positive and negative instances for each image. In particular, we minimize the following loss function. <ref type="bibr">5)</ref> where N yi refers to number of images with label y i in the batch. In comparison to contrastive NT-Xent loss (Equation 1), now we use images with identical labels in each batch as positives. We will show the superior performance of this approach compared to earlier approaches, and note that it is also a parameter-free approach which doesn't require additional OOD data to tune parameters. We further use the proposed cluster-conditioned framework with Mahalnobis distance, as we find it results in better performance than using data labels. We further summarize our framework in Algorithm 1.</p><formula xml:id="formula_6">L batch = 1 2N 2N i=1 ?log 1 2Ny i ?1 2N k=1 1(k = i)1(y k = y i )e u T i u k /? 2N k=1 1(k = i)e u T i u k /?<label>(</label></formula><p>Algorithm 1: Self-supervised outlier detection framework (SSD) Input :X in , X test , feature extractor (f ), projection head (h), Required True-positive rate (T ),</p><formula xml:id="formula_7">Optional: X ood , Y in # X in ? P in X , X ood ? P ood X Output :Is outlier or not? ?x ? X test Function getFeatures(X ): return {f (x i )/ f (x i ) 2 , ? x i ? X }; Function SSDScore(Z, ?, ?): return {(z ? ?) T ? ?1 (z ? ?), ? z ? Z}; Function SSDkScore(Z, ? in , ? in , ? ood , ? ood ): return {(z ? ? in ) T ? ?1 in (z ? ? in ) ? (z ? ? ood ) T ? ?1 ood (z ? ? ood ), ? z ? Z }; end Parition X in in training set (X train ) and calibration set (X cal ); if Y in is not available then L batch = 1 2N 2N i=1 ?log e u T i u j /? 2N k=1 1(k =i)e u T i u k /? ; ui = h(f (x i )) h(f (x i )) 2 ; # Train feature extractor else L batch = 1 2N 2N i=1 ?log 1 2Ny i ?1 2N k=1 1(k =i)1(y k =y i )e u T i u k /? 2N k=1 1(k =i)e u T i u k /? ; end</formula><p>Train feature extractor (f ) by minimizing L batch over X train ; </p><formula xml:id="formula_8">Z train = getFeatures(X train ), Z cal = getFeatures(X cal ) Z test = getFeatures(X test ), if X ood is available: Z ood = getFeatures(X ood ); if X ood is not available then s cal = SSDScore(Z cal , ? train ,<label>?</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">COMMON SETUP ACROSS ALL EXPERIMENTS</head><p>We use recently proposed NT-Xent loss function from SimCLR (Chen et al., 2020) method for self-supervised training. We use the ResNet-50 network in all key experiments but also provide ablation with ResNet-18, ResNet-34, and ResNet-101 network architecture. We train each network, for both supervised and self-supervised training, with stochastic gradient descent for 500 epochs, 0.5 starting learning rate with cosine decay, and weight decay and batch size set to 1e-4 and 512, respectively. We set the temperature parameter to 0.5 in the NT-Xent loss. We evaluate our detector with three performance metrics, namely FPR (at TPR=95%), AUROC, and AUPR. For the supervised training baseline, we use identical training budget as SSD while also using the Mahalanobis distance based detection in the feature space. Due to space constraints, we present results with AUROC, which refers to area under the receiver operating characteristic curve, in the main paper and provide detailed results with other performance metrics in Appendix B.5. Our setup incorporates six image datasets along with additional synthetic datasets based on random noise. We report the average results over three independent runs in most experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of clusters.</head><p>We find the choice of the number of clusters dependent on which layer we extract the features from in the Residual neural networks. While for the first three blocks, we find an increase in AUROC with the number of clusters, the trend is reversed for the last block (Appendix B.2). Since the last block features achieve the highest detection performance, we model the in-distribution features as a single cluster in subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PERFORMANCE OF SSD</head><p>Comparison with unsupervised learning based detectors. We present this comparison in <ref type="table" target="#tab_1">Table 1</ref>. We find that SSD improves average AUROC by up to 55, compared to standard outlier detectors  <ref type="formula" target="#formula_0">(2018)</ref>). A common limitation of each of these three detectors is to find images from the SVHN dataset as more in-distribution when trained on CIFAR-10 or CIFAR-100 dataset. In contrast, SSD is able to successfully detect a large fraction of outliers from SVHN dataset. We also experiment with Rotation-loss <ref type="bibr" target="#b14">Gidaris et al. (2018)</ref>, a non-contrastive self-supervised training objective. We find that SSD with contrastive NT-Xent loss achieves 9.6% higher average AUROC compared to using Rotation-loss.</p><p>Ablation studies. We ablate along individual parameters in self-supervised training with CIFAR-10 as in-distribution data <ref type="figure" target="#fig_2">(Figure 2</ref>). While architecture doesn't have a very large effect on AUROC for most OOD dataset, we find that the number of training epochs and batch size plays a key role in detecting outliers from the CIFAR-100 dataset, which is hardest to detect among the four OOD datasets. We also find an increase in the size of training dataset helpful in the detection of all four OOD datasets.   Comparison with supervised representations. We earlier asked the question whether data labels are even necessary to learn representations crucial for OOD detection? To answer it, we compare SSD with a supervised network, trained with an identical budget as SSD while also using Mahalanobis distance in the feature space, across sixteen different pairs of in-distribution and out-of-distribution datasets ( <ref type="table" target="#tab_3">Table 2)</ref>. We observe that self-supervised representations even achieve better performance than supervised representations for 56% of the tasks in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Success in anomaly detection. We now measure the performance of SSD in anomaly detection where we consider one of the CIFAR-10 classes as in-distribution and the rest of the classes as a source of anomalies. Similar to the earlier setup, we train a ResNet-50 network using self-supervised training with NT-Xent loss function. While the contrastive loss attempt to separate individual instances in the feature space, we find that adding an 2 regularization in the feature space helps in improving performance. In particular, we add this regularization (with a scaling coefficient of 0.01) to bring individual instance features close to the mean of all feature vectors in the batch. Additionally, we reduce the temperature from 0.5 to 0.1 to reduce the separability of individual instances due to the contrastive loss. Overall, we find that our approach outperforms all previous works and achieves competitive performance with the concurrent work of <ref type="bibr" target="#b56">Tack et al. (2020)</ref>  <ref type="table" target="#tab_4">(Table 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">FEW-SHOT OOD DETECTION (SSD k )</head><p>Setup. We focus on one-shot and five-shot OOD detection, i.e., set k to 1 or 5 in Equation 4 and use <ref type="bibr" target="#b32">Ledoit-Wolf (Ledoit &amp; Wolf, 2004)</ref> estimator for covariance estimation. To avoid a bias on selected samples, we report average results over 25 random trials.  Results. Compared to the baseline SSD detector, one-shot and five-shot settings improve the average AUROC, across all OOD datasets, by 1.6 and 2.1, respectively <ref type="table" target="#tab_1">(Table 1</ref>, 2). In particular, we observe large gains with CIFAR-100 as in-distribution and CIFAR-10 as OOD where five-shot detection improves the AUROC from 69.6 to 78.3. We find the use of shrunk covariance estimator most critical in the success of our approach. Use of shrunk covariance estimation itself improves the AUROC from 69.6 to 77.1. Then data augmentation further improves it 78.3 for the five-shot detection. With an increasing number of transformed copies of each sample, we also observe improvement in AUROC, though it later plateaus close to ten copies (Appendix B.3).</p><p>What if additional OOD images are available Note that some earlier works, such as <ref type="bibr" target="#b35">Liang et al. (2018)</ref>, assume that 1000 OOD inputs are available for tuning the detector. We find that with access to this large amount of OOD samples, SSD k can improve the state-of-the-art by an even larger margin. For example, with CIFAR-100 as in-distribution and CIFAR-10 as out-of-distribution, it achieves 89.4 AUROC, which is 14.2% higher than the current state-of-the-art <ref type="bibr" target="#b61">(Winkens et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SUCCESS WHEN USING DATA LABELS (SSD+)</head><p>Now we integrate labels of training data in our framework and compare it with the existing state-ofthe-art detectors. We report our results in <ref type="table" target="#tab_5">Table 4</ref>. Our approach improves the average AUROC by 0.8 over the previous state-of-the-art detector. Our approach also achieves equal or better performance than previous state-of-the-art across individual pairs of in and out-distribution dataset. For example, using labels in our framework improves the AUROC of Mahalanobis detector from 55.5 to 72.1 for CIFAR-100 as in-distribution and CIFAR-10 as the OOD dataset. Using the simple softmax probabilities, training a two-layer MLP on learned representations further improves the AUROC to 78.3. Combining SSD+ with a five-shot OOD detection method further brings a gain of 1.4 in the average AUROC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND CONCLUSION</head><p>On tuning hyperparameters in SSD. In our framework, we either explicitly avoid the use of additional tuning-parameters (such as when combining self-supervised and supervised loss functions   93.3 98.4 75.7 86.9 88.6 Rotation-loss + Supervised <ref type="bibr" target="#b25">(Hendrycks et al., 2019b)</ref> 90.9 98.9 ---Contrastive + Supervised <ref type="bibr" target="#b61">(Winkens et al., 2020)</ref>  in SSD+) or refrain from tuning the existing set of parameters for each OOD dataset. For example, we use a standard set of parameters for self-supervised training and model the learned features with a single-cluster. Why contrastive self-supervised learning is effective in the SSD framework? We focus on the NT-Xent loss function, which is parameterized by a temperature variable (? ). Its objective is to pull positive instances, i.e., different transformations of an image, together while pushing away from other instances. Earlier works have shown that such contrastive training forces the network to learn a good set of feature representations. However, a smaller value of temperature quickly saturates the loss, discouraging it to further improve the feature representations. We find that the performance of SSD also degrades with lower temperature, suggesting the necessity of learning a good set of feature representation for effective outlier detection <ref type="table" target="#tab_7">(Table 5</ref>).</p><p>How discriminative ability of feature representations evolves over the course of training. We analyze this effect in <ref type="figure" target="#fig_4">Figure 3</ref> where we compare both SSD and supervised training based detector over the course of training. While discriminative ability of self-supervised training in SSD is lower at the start, it quickly catches up with supervised representations after half of the training epochs.  Performance of SSD improves with the amount of available unlabeled data. A compelling advantage of unsupervised learning is to learn from unlabeled data, which can be easily collected. As presented in <ref type="figure" target="#fig_2">Figure 2</ref>, we find that performance of SSD increases with the size of training dataset. We conduct another experiment with the STL-10 dataset, where in addition to the 5k training images, we also use additional 10k images from the unlabeled set. This improves the AUROC from 94.7 to 99.4 for CIFAR-100 as the OOD dataset, further demonstrating the success of SSD in leveraging unlabeled data (Appendix B.4). In conclusion, our framework provides an effective &amp; flexible approach for outlier detection using unlabeled data.</p><p>hyperparameter selection, we set the image augmentation pipeline to be the same as the one used in training. Finally, we use the Ledoit-Wolf method <ref type="bibr" target="#b32">(Ledoit &amp; Wolf, 2004)</ref> to estimate the covariance of the OOD samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PERFORMANCE METRICS FOR OUTLIER DETECTORS</head><p>We use the following three performance metrics to evaluate the performance of outlier detectors.</p><p>? FPR at TPR=95%. It refers to the false positive rate (= FP / (FP+TN)), when true positive rate (= TP / (TP + FN)) is equal to 95%. Effectively, its goal is to measure what fraction of outliers go undetected when it is desirable to have a true positive rate of 95%.</p><p>? AUROC. It refers to the area under the receiver operating characteristic curve. We measure it by calculating the area under the curve when we plot TPR against FPR.</p><p>? AUPR. It refers to the area under the precision-recall curve, where precision = TP / (TP+FP) and recall = TP / (TP+FN). Similar to AUROC, AUPR is also a threshold independent metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 DATASETS USED IN THIS WORK</head><p>We use the following datasets in this work. Whenever there is a mismatch between the resolution of images in in-distribution and out-of-distribution (OOD) data, we appropriately scale the OOD images with bilinear scaling. When there is an overlap between the classes of the in-distribution and OOD dataset, we remove the common classes from the OOD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 LIMITATIONS OF OUTLIER DETECTORS BASED ON SUPERVISED TRAINING</head><p>Existing supervised training based detector assumes that fine-grained data labels are available for the training data. What happens to the performance of current detectors if we relax this assumption by assuming that only coarse labels are present. We simulate this setup by combining consecutive classes from the CIFAR-10 dataset into two groups, referred to as CIFAR-2, or five groups referred to as CIFAR-5. We use CIFAR-100 as the out-of-distribution dataset. We find that the performance of existing detectors degrades significantly when only coarse labels are present <ref type="figure">(Figure 4)</ref>. In contrast, SSD operates on unlabeled data thus doesn't suffer from similar performance degradation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ON CHOICE OF NUMBER OF CLUSTERS</head><p>We find that the choice of optimal number of clusters is dependent on which layer we use as the feature extractor in a Residual Neural network. We demonstrate this trend in <ref type="figure">Figure 5</ref>, with CIFAR-10 as in-distribution dataset and CIFAR-100 as out-of-distribution dataset. We extract features from the last layer of each block in the residual network and measure the SSD performance with them. While for the first three blocks, we find an increase in AUROC with number of clusters, the trend is reversed for the last block ( <ref type="figure">Figure 5</ref>). Since last block features achieve highest detection performance, we model in-distribution features using a single cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 ABLATION STUDY FOR FEW-SHOT OOD DETECTION</head><p>For few shot OOD detection, we ablate along the number of transformations used for each sample. We choose CIFAR-100 as in-distribution and CIFAR-10 as OOD dataset with SSD k , set k to five, and choose ResNet-18 network architecture. When increasing number of transformations from 1, 5, 10, 20, 50 the AUROC of detector is 74.3, 75.7, 76.1, 76.3, 76.7. To achieve a balance between the performance and computational cost, we use ten transformations for each sample in our final experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 PERFORMANCE OF SSD IMPROVES WITH AMOUNT OF UNLABELED DATA</head><p>With easy access to unlabeled data, it is compelling to develop detectors that can benefit from the increasing amount of such data. We earlier demonstrated this ability of SSD for the CIFAR-10 dataset in <ref type="figure" target="#fig_2">Figure 2</ref>. Now we present similar results with the STL-10 dataset. We first train a self-supervised network, and an equivalent supervised network with 5,000 training images from the STL-10 dataset. We refer to these networks by SSD-5k and Sup-5k, respectively. Next, we include additional 10,000 images from the available 100k unlabeled images in the dataset. As we show in <ref type="figure">Figure 6</ref>, SSD is able to achieve large gains in performance with access to the additional unlabeled training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 RESULTS WITH DIFFERENT PERFORMANCE METRICS</head><p>We provide our detailed experimental results for each component in the SSD framework with three different performance metrics in <ref type="table" target="#tab_8">Table 6</ref>, 7,.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>AUROC along individual principle eigenvector with CIFAR-10 as indistribution and CIFAR-100 as OOD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Ablating across different training parameters in SSD under following setup: In-distribution dataset = CIFAR-10, OOD dataset = CIFAR-100, Training epochs = 500, Batch size = 512.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>AUROC over the course of training with CIFAR-10 as indistribution and CIFAR-100 as OOD set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Relationship of AU-ROC with clusters depends on which block we use as the feature extractor. Using extra unlabeled training data can help to further improve the performance of SSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>results of SSD detector with multiple metrics over CIFAR-10, CIFAR-100, and STL-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of SSD with different outlier detectors using only unlabeled training data.</figDesc><table><row><cell>In-distribution (Out-of-distribution)</cell><cell>CIFAR-10 (SVHN)</cell><cell>CIFAR-10 (CIFAR-100)</cell><cell>CIFAR-100 (SVHN)</cell><cell>CIFAR-100 (CIFAR-10)</cell><cell>Average</cell></row><row><cell>Autoencoder (Hawkins et al., 2002)</cell><cell>2.5</cell><cell>51.3</cell><cell>3.0</cell><cell>51.4</cell><cell>27.0</cell></row><row><cell>VAE (Kingma &amp; Welling, 2014)</cell><cell>2.4</cell><cell>52.8</cell><cell>2.6</cell><cell>47.1</cell><cell>26.2</cell></row><row><cell>PixelCNN++ (Salimans et al., 2017)</cell><cell>15.8</cell><cell>52.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Deep-SVDD (Ruff et al., 2018)</cell><cell>14.5</cell><cell>52.1</cell><cell>16.3</cell><cell>51.4</cell><cell>33.5</cell></row><row><cell>Rotation-loss (Gidaris et al., 2018)</cell><cell>97.9</cell><cell>81.2</cell><cell>94.4</cell><cell>50.1</cell><cell>80.9</cell></row><row><cell>CSI (Tack et al., 2020)</cell><cell>99.8</cell><cell>89.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSD</cell><cell>99.6</cell><cell>90.6</cell><cell>94.9</cell><cell>69.6</cell><cell>88.7</cell></row><row><cell>SSD k (k = 5)</cell><cell>99.7</cell><cell>93.1</cell><cell>99.1</cell><cell>78.2</cell><cell>92.5</cell></row><row><cell cols="6">based on Density modeling (PixelCNN++ (Salimans et al., 2017)), input reconstruction (Auto-</cell></row><row><cell cols="6">encoder (Hawkins et al., 2002), Variational Auto-encoder (Kingma &amp; Welling, 2014)), and One-class</cell></row><row><cell cols="2">classification (Deep-SVDD Ruff et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparing performance of self-supervised (SSD) and supervised representations. We also provide results for few-shot OOD detection (SSD k ) for comparison with our baseline SSD detector.</figDesc><table><row><cell>In-distribution</cell><cell>OOD</cell><cell cols="2">SSD Supervised</cell><cell>SSDk k=1 k=5</cell><cell>In-distribution</cell><cell>OOD</cell><cell cols="2">SSD Superivsed</cell><cell cols="2">SSDk k=1 k=5</cell></row><row><cell></cell><cell cols="2">CIFAR-100 90.6</cell><cell>90.6</cell><cell>91.7 93.0</cell><cell></cell><cell>CIFAR-100</cell><cell>94.8</cell><cell>84.0</cell><cell>90.1</cell><cell>90.0</cell></row><row><cell>CIFAR-10</cell><cell>SVHN Texture</cell><cell>99.6 97.6</cell><cell>99.6 97.8</cell><cell>99.9 99.7 98.9 99.4</cell><cell>STL-10</cell><cell>SVHN Texture</cell><cell>98.7 85.8</cell><cell>95.7 75.5</cell><cell>98.7 85.7</cell><cell>99.4 84.5</cell></row><row><cell></cell><cell>Blobs</cell><cell>98.8</cell><cell>99.9</cell><cell>99.7 100.0</cell><cell></cell><cell>Blobs</cell><cell>96.4</cell><cell>88.6</cell><cell>96.5</cell><cell>99.9</cell></row><row><cell></cell><cell>LSUN</cell><cell>96.5</cell><cell>93.8</cell><cell>97.6 97.8</cell><cell></cell><cell>LSUN</cell><cell>88.8</cell><cell>66.8</cell><cell>94.1</cell><cell>94.5</cell></row><row><cell></cell><cell>Places365</cell><cell>95.2</cell><cell>92.7</cell><cell>96.7 97.3</cell><cell></cell><cell>Places365</cell><cell>88.3</cell><cell>64.9</cell><cell>95.4</cell><cell>95.6</cell></row><row><cell></cell><cell cols="2">CIFAR-10 69.6</cell><cell>55.3</cell><cell>74.8 78.3</cell><cell></cell><cell>SVHN</cell><cell>99.1</cell><cell>99.4</cell><cell cols="2">99.7 100.0</cell></row><row><cell>CIFAR-100</cell><cell>SVHN Texture</cell><cell>94.9 82.9</cell><cell>94.5 98.8</cell><cell>99.5 99.1 96.8 94.2</cell><cell>ImageNet</cell><cell>Texture Blobs</cell><cell>95.4 99.5</cell><cell>85.1 98.4</cell><cell cols="2">94.7 100.0 100.0 97.3</cell></row><row><cell></cell><cell>Blobs</cell><cell>98.1</cell><cell>57.3</cell><cell>98.1 100.0</cell><cell></cell><cell cols="2">Gaussian Noise 100.0</cell><cell>100.0</cell><cell cols="2">100.0 100.0</cell></row><row><cell></cell><cell>LSUN</cell><cell>79.5</cell><cell>69.4</cell><cell>92.3 93.4</cell><cell></cell><cell>ImageNet-O</cell><cell>45.2</cell><cell>75.5</cell><cell>89.4</cell><cell>93.3</cell></row><row><cell></cell><cell>Places365</cell><cell>79.6</cell><cell>62.6</cell><cell>90.7 92.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of SSD with other detectors for anomaly detection task on CIFAR-10 dataset.</figDesc><table><row><cell></cell><cell cols="3">Airplane Automobile Bird</cell><cell>Cat</cell><cell cols="7">Deer Dog Frog Horse Ship Truck Average</cell></row><row><cell>Randomly Initialized network</cell><cell>77.4</cell><cell>44.1</cell><cell cols="5">62.4 44.1 62.1 49.6 59.8</cell><cell>48.0</cell><cell>73.8</cell><cell>53.7</cell><cell>57.5</cell></row><row><cell>VAE (Kingma &amp; Welling, 2014)</cell><cell>70.0</cell><cell>38.6</cell><cell cols="5">67.9 53.5 74.8 52.3 68.7</cell><cell>49.3</cell><cell>69.6</cell><cell>38.6</cell><cell>58.3</cell></row><row><cell>OCSVM (Sch?lkopf et al., 2001)</cell><cell>63.0</cell><cell>44.0</cell><cell cols="5">64.9 48.7 73.5 50.0 72.5</cell><cell>53.3</cell><cell>64.9</cell><cell>50.8</cell><cell>58.5</cell></row><row><cell>AnoGAN (Schlegl et al., 2017)</cell><cell>67.1</cell><cell>54.7</cell><cell cols="5">52.9 54.5 65.1 60.3 58.5</cell><cell>62.5</cell><cell>75.8</cell><cell>66.5</cell><cell>61.8</cell></row><row><cell>PixelCNN (Van den Oord et al., 2016)</cell><cell>53.1</cell><cell>99.5</cell><cell cols="5">47.6 51.7 73.9 54.2 59.2</cell><cell>78.9</cell><cell>34.0</cell><cell>66.2</cell><cell>61.8</cell></row><row><cell>DSVDD (Ruff et al., 2018)</cell><cell>61.7</cell><cell>65.9</cell><cell cols="5">50.8 59.1 60.9 65.7 67.7</cell><cell>67.3</cell><cell>75.9</cell><cell>73.1</cell><cell>64.8</cell></row><row><cell>OCGAN (Perera et al., 2019)</cell><cell>75.7</cell><cell>53.1</cell><cell cols="5">64.0 62.0 72.3 62.0 72.3</cell><cell>57.5</cell><cell>82.0</cell><cell>55.4</cell><cell>65.6</cell></row><row><cell>RCAE (Chalapathy et al., 2018b)</cell><cell>72.0</cell><cell>63.1</cell><cell cols="5">71.7 60.6 72.8 64.0 64.9</cell><cell>63.6</cell><cell>74.7</cell><cell>74.5</cell><cell>68.2</cell></row><row><cell>DROCC (Goyal et al., 2020)</cell><cell>81.7</cell><cell>76.7</cell><cell cols="5">66.7 67.1 73.6 74.4 74.4</cell><cell>71.4</cell><cell>80.0</cell><cell>76.2</cell><cell>74.2</cell></row><row><cell>Deep-SAD (Ruff et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>77.9</cell></row><row><cell>E3Outlier (Wang et al., 2019)</cell><cell>79.4</cell><cell>95.3</cell><cell cols="5">75.4 73.9 84.1 87.9 85.0</cell><cell>93.4</cell><cell>92.3</cell><cell>89.7</cell><cell>85.6</cell></row><row><cell>GT (Golan &amp; El-Yaniv, 2018)</cell><cell>74.7</cell><cell>95.7</cell><cell cols="5">78.1 72.4 87.8 87.8 83.4</cell><cell>95.5</cell><cell>93.3</cell><cell>91.3</cell><cell>86.0</cell></row><row><cell>InvAE (Huang et al., 2019)</cell><cell>78.5</cell><cell>89.8</cell><cell cols="5">86.1 77.4 90.5 84.5 89.2</cell><cell>92.9</cell><cell>92.0</cell><cell>85.5</cell><cell>86.6</cell></row><row><cell>GOAD (Bergman &amp; Hoshen, 2020)</cell><cell>77.2</cell><cell>96.7</cell><cell cols="5">83.3 77.7 87.8 87.8 90.0</cell><cell>96.1</cell><cell>93.8</cell><cell>92.0</cell><cell>88.2</cell></row><row><cell>CSI (Tack et al., 2020)</cell><cell>89.9</cell><cell>99.9</cell><cell cols="5">93.1 86.4 93.9 93.2 95.1</cell><cell>98.7</cell><cell>97.9</cell><cell>95.5</cell><cell>94.3</cell></row><row><cell>SSD</cell><cell>82.7</cell><cell>98.5</cell><cell cols="5">84.2 84.5 84.8 90.9 91.7</cell><cell>95.2</cell><cell>92.9</cell><cell>94.4</cell><cell>90.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of SSD+, i.e., incorporating labels in the SSD detector, with state-of-the-art detectors based on supervised training.</figDesc><table><row><cell>In-distribution (Out-of-distribution)</cell><cell>CIFAR-10 (CIFAR-100)</cell><cell>CIFAR-10 (SVHN)</cell><cell>CIFAR-100 (CIFAR-10)</cell><cell>CIFAR-100 (SVHN)</cell><cell>Average</cell></row><row><cell>Softmax-probs (Hendrycks &amp; Gimpel, 2017)</cell><cell>89.8</cell><cell>95.9</cell><cell>78.0</cell><cell>78.9</cell><cell>85.6</cell></row><row><cell>ODIN(Liang et al., 2018) ?</cell><cell>89.6</cell><cell>96.4</cell><cell>77.9</cell><cell>60.9</cell><cell>81.2</cell></row><row><cell>Mahalnobis (Lee et al., 2018b)  ?</cell><cell>90.5</cell><cell>99.4</cell><cell>55.3</cell><cell>94.5</cell><cell>84.8</cell></row><row><cell>Residual Flows (Zisselman &amp; Tamar, 2020)  ?</cell><cell>89.4</cell><cell>99.1</cell><cell>77.1</cell><cell>97.5</cell><cell>90.7</cell></row><row><cell>Gram Matrix (Sastry &amp; Oore, 2019)</cell><cell>79.0</cell><cell>99.5</cell><cell>67.9</cell><cell>96.0</cell><cell>85.6</cell></row><row><cell>Outlier exposure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">: Test Accuracy and AUROC</cell></row><row><cell cols="2">with different temperature values in NT-</cell></row><row><cell cols="2">Xent (Equation 1) loss. Using CIFAR-</cell></row><row><cell cols="2">10 as in-distribution and CIFAR-100 as</cell></row><row><cell cols="2">OOD dataset with ResNet18 network.</cell></row><row><cell>Temperature</cell><cell>0.001 0.01 0.1 0.5</cell></row><row><cell cols="2">Test -Accuracy 70.8 76.7 86.9 90.5</cell></row><row><cell>AUROC</cell><cell>66.7 71.6 85.5 89.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Experimental results of SSD detector with multiple metrics for ImageNet dataset.</figDesc><table><row><cell>In-distribution</cell><cell>OOD</cell><cell cols="4">FPR (TPR = 95%) ? SSD Supervised SSDk</cell><cell cols="3">AUROC ? SSD Superivsed</cell><cell cols="2">SSDk</cell><cell cols="2">AUPR ? SSD Supervised</cell><cell>SSDk</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">k=1 k=5</cell><cell></cell><cell></cell><cell cols="2">k=1</cell><cell>k=5</cell><cell></cell><cell>k=1</cell><cell>k=5</cell></row><row><cell>ImageNet</cell><cell>SVHN</cell><cell>1.3</cell><cell>0.6</cell><cell>0.0</cell><cell>0.0</cell><cell>99.4</cell><cell>99.1</cell><cell cols="3">100.0 100.0</cell><cell>98.4</cell><cell>96.6</cell><cell>100.0 100.0</cell></row><row><cell></cell><cell>Texture</cell><cell>57.2</cell><cell>23.2</cell><cell cols="2">20.1 11.4</cell><cell>85.4</cell><cell>95.4</cell><cell cols="2">95.4</cell><cell>97.4</cell><cell>41.7</cell><cell>75.8</cell><cell>78.6</cell><cell>84.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is publicly available at https://github.com/inspire-group/SSD</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We refer to OOD detection without using class labels of in-distribution data as unsupervised OOD detection.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We refer to the commonly used ILSVRC 2012 release of ImageNet dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Chong Xiang, Liwei Song, and Arjun Nitin Bhagoji for their helpful feedback on the paper. This work was supported in part by the National Science Foundation under grants CNS-1553437 and CNS-1704105, by a Qualcomm Innovation Fellowship, by the Army Research Office Young Investigator Prize, by Army Research Laboratory (ARL) Army Artificial Intelligence Institute (A2I2), by Office of Naval Research (ONR) Young Investigator Award, by Facebook Systems for ML award, by Schmidt DataX Fund, and by Princeton E-ffiliates Partnership.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL DETAILS ON EXPERIMENTAL SETUP</head><p>A.1 TRAINING AND EVALUATION SETUP FOR DEEP NEURAL NETWORKS.</p><p>We use ResNet-50 architecture for all our major experiments and ResNet-18 for ablation studies. We also provide results with ResNet-34 and ResNet-101 architecture. We use a two-layer fully connected network as the projection header (h(.)). To contrast with a large number of negatives, NT-Xent loss requires a much larger batch size compared to the supervised cross-entropy loss function. We train it using a batch size of 512. When evaluating self-supervised models, even when we incorporate labels in SSD, we achieve the best performance when modeling in-distribution features with only a single cluster. However, for supervised training, which refers to the supervised baseline in the paper, we find that increasing the number of clusters helps. For it, we report the best of the results obtained from cluster indexes or using true labels of the data. For each dataset, we use the test set partition, if it exists, as the OOD dataset. For consistent comparison, we re-implement Softmax-probabilities <ref type="bibr" target="#b23">(Hendrycks &amp; Gimpel, 2017)</ref>, ODIN <ref type="bibr" target="#b35">(Liang et al., 2018)</ref>, and Mahalanobis detector <ref type="bibr" target="#b34">(Lee et al., 2018b)</ref> and evaluate their performance on the identical network, trained with supervised training for 500 epochs. We set the perturbation budget to 0.0014 and temperature to 1000 for ODIN, since these are the most successful set of parameters reported in the original paper <ref type="bibr" target="#b35">(Liang et al., 2018)</ref>. We primarily focus on one-shot and five-shot OOD detection, i.e., set k to one or five. It implies access to one and five images, respectively, from each class of the targeted OOD dataset. We create ten randomly transformed samples from each available OOD image in the SSD k detector. With very small k, such as one, we find that increasing number of transformations may degrade performance in some cases. In this case we simply resort to using only one transformation per sample. To avoid any</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Lecture on IE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1lK_lBtvS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Anomaly detection using one-class neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06360</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Anomaly detection using one-class neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06360</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning confidence for out-of-distribution detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04865</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reducing network agnostophobia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Akshay Raj Dhamija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>G?nther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9175" to="9186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the foundations of noise-free selective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1605" to="1641" />
			<date type="published" when="2010-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Selective classification for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4878" to="4887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1v4N2l0-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9758" to="9769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Data Labelling Pricing -Google AI Platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Api Google</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pricing</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/ai-platform/data-labeling/pricing" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Brefeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="235" to="262" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Drocc: Deep robust one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moksh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Harsha Vardhan Simhadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Procedures for detecting outlying observations in samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grubbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward open-set face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>G?nther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">M</forename><surname>Rudd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Outlier detection using replicator neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Warehousing and Knowledge Discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="170" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyxCxhRcY7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15663" to="15674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Inverse-transform autoencoder for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10676</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">To trust or not to trust a classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5546" to="5557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6114" />
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Honey, i shrunk the sample covariance matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ledoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Portfolio Management</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="110" to="119" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training confidence-calibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1VGkIxRZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On the generalized distance in statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanta</forename><surname>Chandra Mahalanobis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1936" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Science of India</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep face recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 31st SIBGRAPI conference on graphics, patterns and images (SIBGRAPI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Kitsune: an ensemble of autoencoders for online network intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisroel</forename><surname>Mirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Doitshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Elovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Shabtai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09089</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-supervised learning for generalizable out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Pitale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Yadawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5216" to="5223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilan</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1xwNhCcYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Longbing Cao, and Anton van den Hengel. Deep learning for anomaly detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02500</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Failing to learn: autonomously identifying perception failures for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Manikandasriram Srinivasan Ramanagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson-Roberson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3860" to="3867" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14707" to="14718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Detecting out-of-distribution examples with indistribution examples and gram matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shama</forename><surname>Chandramouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sageev</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12510</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Detection of anomalies in large scale accounting data using deep autoencoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Schreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Sattarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Reimer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05254</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Estimation of a covariance matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="https://ci.nii.ac.jp/naid/10020185297/en/" />
	</analytic>
	<monogr>
		<title level="m">39th Annual Meeting IMS</title>
		<meeting><address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection using an ensemble of self supervised leave-out classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataraj</forename><surname>Jammalamadaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><forename type="middle">L</forename><surname>Willke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="550" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Effective end-to-end unsupervised outlier detection via inlier priority of discriminative network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanfu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5962" to="5975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Contrastive training for improved out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudy</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit Guha</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Joseph R Ledsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Macwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Classification-reconstruction learning for open-set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Yoshihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rei</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Naemura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04246</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Deep residual flow for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ev</forename><surname>Zisselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05419</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">It consists of 50,000 training images and 10,000 test images from 10 different classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Cifar- ; Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Each image size is 32?32 pixels</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">CIFAR-100 also has 50,000 training images and 10,000 test images. However, it has 100 classes which are further organized in 20 sub-classes. Note that its classes aren&apos;t identical to the CIFAR-10 dataset, with a slight exception with class truck in CIFAR-10 and pickup truck in CIFAR-100. However, their classes share multiple similar semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krizhevsky</surname></persName>
		</author>
		<idno>? CIFAR-100</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>making it hard to catch outliers from the other dataset</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">SVHN is a real-world street-view housing number dataset. It has 73,257 digits available for training, and 26,032 digits for testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Svhn (netzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Similar to the CIFAR-10/100 dataset, the size of its images is also 32?32 pixels</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">STL-10 has identical classes as the CIFAR-10 dataset but focuses on the unsupervised learning. It has 5,000 training images, 8,000 test images, and a set of 100,000 unlabeled images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">? Stl- ;</forename><surname>Coates</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Unlike the previous three datasets, the size of its images is 96?96 pixels</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Describable Textures Dataset (DTD) is a collection of textural images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Dtd (cimpoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>It includes a total of 5,640 images, split equally between 47 categories where the size of images range between 300?300 and 640?640 pixels</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">ImageNet is a large scale dataset of 1,000 categories with 1.2 Million training images and 50,000 validation images. It has high diversity in both inter-and intra-class images and is known to have strong generalization properties to other datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? ImageNet</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Blobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Similar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendrycks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>we algorithmically generate these amorphous shapes with definite edges</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">We generate images with Gaussian noise using a mean of 0.5 and a standard deviation of 0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? Gaussian Noise</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>25. We clip the pixel value to the valid pixel range of [0, 1</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? Uniform Noise</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>It refers to images where each pixel value is uniformly sampled from the [0, 1] range</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
