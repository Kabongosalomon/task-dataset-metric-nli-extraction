<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Mesh Reconstruction from Single RGB Images via Topology Modification Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
								<orgName type="institution">Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">USC Institute for Creative Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Mesh Reconstruction from Single RGB Images via Topology Modification Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reconstructing the 3D mesh of a general object from a single image is now possible thanks to the latest advances of deep learning technologies. However, due to the nontrivial difficulty of generating a feasible mesh structure, the stateof-the-art approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref> often simplify the problem by learning the displacements of a template mesh that deforms it to the target surface. Though reconstructing a 3D shape with complex topology can be achieved by deforming multiple mesh patches, it remains difficult to stitch the results to ensure a high meshing quality. In this paper, we present an end-to-end single-view mesh reconstruction framework that is able to generate high-quality meshes with complex topologies from a single genus-0 template mesh. The key to our approach is a novel progressive shaping framework that alternates between mesh deformation and topology modification. While a deformation network predicts the per-vertex translations that reduce the gap between the reconstructed mesh and the ground truth, a novel topology modification network is employed to prune the error-prone faces, enabling the evolution of topology. By iterating over the two procedures, one can progressively modify the mesh topology while achieving higher reconstruction accuracy. Moreover, a boundary refinement network is designed to refine the boundary conditions to further improve the visual quality of the reconstructed mesh. Extensive experiments demonstrate that our approach outperforms the current state-of-the-art methods both qualitatively and quantitatively, especially for the shapes with complex topologies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image-based 3D reconstruction plays a fundamental role in a variety of tasks in computer vision and computer <ref type="bibr">Figure 1</ref>. Given a single image of an object (a) as input, the existing mesh-deformation based learning approaches <ref type="bibr" target="#b8">[9]</ref> can not well capture the complex topology, regardless of a single (b) or multiple template meshes (c). In contrast, our proposed method is capable of updating the topologies dynamically by removing faces in the initial sphere mesh and achieves better reconstruction results (d).</p><p>graphics, such as robot perception, autonomous driving, virtual/augmented reality, etc. Conventional approaches mainly leverage the stereo correspondence based on multiview geometry but are restricted to the coverage provided by the input views. Such requirement renders single-view reconstruction particularly difficult due to the lack of correspondence and large occlusions. With the availability of large-scale 3D shape dataset <ref type="bibr" target="#b2">[3]</ref>, shape priors can be efficiently encoded in a deep neural network, enabling faithful 3D reconstruction even from a single image. While a variety of 3D representations, e.g. voxels <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref> and point cloud <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35]</ref>, have been explored for single-view reconstruction, triangular mesh receives the most attentions as it is more desirable for a wide range of real applications and capable of modeling geometric details.</p><p>Recent progresses in single-view mesh reconstruction <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b8">9]</ref> propose to reconstruct a 3D mesh by deforming a template model based on the perceptual features extracted from the input image. Though promising results have been achieved, the reconstructed results are limited to the identical topological structure with the template model, leading to large reconstruction errors when the target object has a different topology (cf. <ref type="figure">Figure 1 (b)</ref>). Although it is possible to approximate a complex shape with non-disk topology by deforming multiple patches to cover the target surface, there remain several drawbacks that limit its practical usability. Firstly, the reconstructed result is composed of multiple disconnected surface patches, leading to severe self-intersections and overlaps that require tedious efforts to remove the artifacts. Secondly, as obtaining a high-quality global surface parameterization remains a challenging problem, it is nontrivial to generate a proper atlas that can cover the surface with low distortion, only based on a single image. Lastly, it is difficult to determine an appropriate number of surface patches that adapts to varying shapes.</p><p>In this work, we strive to generate the 3D mesh with complex topology from a single genus-0 template mesh.</p><p>Our key idea is a mechanism that dynamically modifies the topology of the template mesh by face pruning, targeting at a trade-off between the deformation flexibility and the output meshing quality. The basic model for deformation learning is a cascaded version of AtlasNet <ref type="bibr" target="#b8">[9]</ref> that predicts per-vertex offsets instead of positional coordinates. Starting from an initial mesh M 0 , we first apply such deformation network and obtain a coarse output M 1 . Then, the key problem is to determine which faces on M 1 to remove. To this end, we propose to train an error-prediction network that estimates the reconstruction error (i.e. distance to the ground truth) of the reconstructed faces on M 1 . The faces with large error would be removed to achieve better reconstruction accuracy. However, it remains nontrivial to determine a proper pruning threshold and to guarantee the smoothness of the open boundaries introduced by the face pruning. Towards this end, we propose two strategies to address these issues: 1) a progressive learning framework that alternates between a mesh deformation network, which reduces the reconstruction error, and a topology modification network that prunes the faces with large approximation error; 2) a boundary refinement network that imposes smoothness constraints on the boundary curves, to refine the boundary conditions. Both qualitative and quantitative evaluations demonstrate the superiority of our approach over the existing methods, in terms of both the reconstruction accuracy and the meshing quality. As seen in <ref type="figure">Figure 1</ref>, the proposed method is able to better capture the complex topology with a single sphere template mesh while achieving better meshing quality compared to the state-of-the-art AtlasNet <ref type="bibr" target="#b8">[9]</ref>.</p><p>In summary, our main contributions are:</p><p>? The first end-to-end learning framework for singleview object reconstruction that is capable of modeling complex mesh topology from a single genus-0 template mesh.</p><p>? A novel topology modification network, which can be integrated into other mesh learning frameworks.</p><p>? We demonstrate the advantage of our approach over the state-of-the-arts in terms of both reconstruction accuracy and the meshing quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Reconstructing 3D surfaces from color images has been investigated since the very beginning of the field <ref type="bibr" target="#b26">[27]</ref>. To infer 3D structures from 2D images, conventional approaches mainly leverage the stereo correspondences from multiview geometry <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8]</ref>. Though high-quality reconstruction can be achieved, stereo based approaches are restricted to the coverage provided by the multiple views and specific appearance models that cannot be generalized to nonlambertian object reconstruction. Hence, learning-based approaches have stood out as the major trend in recent years thanks to its scalability to single or few images.</p><p>With the success of deep neural network and the availability of large-scale 3D shape collections, e.g. ShapeNet <ref type="bibr" target="#b2">[3]</ref>, deep learning-based 3D shape generation has made great progress. In order to replicate the success of 2D convolutional neural network to 3D domain, various forms of 3D representations have been explored. As a natural extension of 2D pixels, volumetric representation has been widely used in recent works on 3D reconstruction <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33]</ref> due to its simplicity of implementation and compatibility with convolutional neural network. However, deep voxel generators are constrained by its resolution due to the data sparsity and computation cost of 3D convolution. As a flexible form of representing a 3D structure, point cloud has become another major alternative for 3D learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4]</ref> and shape generation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b19">20]</ref> due to its high memory efficiency and simple and unified structure. Though enjoying the flexibility to match 3D shape with arbitrary topology, point cloud is not a well suited for imposing geometry constraints, which are critical for ensuring smoothness and appealing visual appearance of the reconstructed surface. Implicit field based 3D reconstruction approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b12">13]</ref> share the similar advantages with point cloud representation in providing good trade-offs across fidelity, flexibility and compression capabilities. Yet it also remains difficult to regularize the generation of a volumetric implicit field to achieve specific geometry properties.</p><p>In contrast, mesh representation is more desirable for real applications since it can model fine shape details and is compatible with various geometry regularizers. Due to the complexity of modifying the mesh topology, most mesh learning approaches strive to obtain a target shape by deforming a template mesh <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref>   <ref type="figure">Figure 2</ref>. The overview of our pipeline. Given an input image, we first employ multiple mesh deformation and topology modification modules to progressively deform the mesh vertices and update the topologies to approximate the target object surface. A module of boundary refinement is then adopted to refine the boundary conditions. renderer <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref> have proposed to train a mesh generator based on rendering loss, eliminating the need of 3D supervision. However, no prior approaches can dynamically modify the topology of the template mesh, while we propose the first topology modification network that is able to generate meshes with complex topologies from a genus-0 3D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Topology-adaptive Mesh Reconstruction</head><p>Overview. Given a single image I of an object, we attempt to reconstruct the surface S of it. We adopt the triangular mesh as a natural and flexible discretization of the target surface. A mesh is typically defined by M = (V, E, T ), where V ? R 3 is the set of mesh vertices, E is the set of edges connecting the neighboring vertices, and T is the set of triangles enclosed by the connected edges. To reconstruct the triangular mesh representation of an object, one could choose to deform a template mesh to approximate the target surface. Nevertheless, the existing deformation-based mesh reconstruction approaches, such as <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref>, are not allowed to update the faces-to-vertices relationships and thus are restricted by the predefined topology. In order to overcome this limitation, we propose an end-to-end learning pipeline, consisting of three modules, to progressively modify the coordinates and connectivity of the vertices on a predefined mesh M 0 . To be specific, the mesh deformation module is adopted to map the vertices on M 0 to the target surface while maintain the connectivity over them; the topology modification module is developed to update the connection relationship between the vertices by pruning the faces which deviate from the ground truth; the boundary refinement module is designed to refine the open boundaries introduced by face pruning. Note that the mesh deformation and topology modification are performed in an alternative manner to gradually recover the overall shape and topology of the target object.</p><p>Network structure. We propose a progressive structure to deform a template mesh M 0 to fit the target surface S. In our implementation, M 0 is instantiated as a sphere mesh with 2562 vertices. <ref type="figure">Figure 2</ref> illustrates the overall pipeline. We leverage an encoder-decoder network for shape generation. On the encoder side, the input image is fed into ResNet-18 <ref type="bibr" target="#b11">[12]</ref> to extract a 1024-dimensional feature vector x. The decoder contains three successive subnets. Each of the first two subnets consists of a mesh deformation module and a topology modification module, and the last subnet comprises a single boundary refinement module. Note that each mesh deformation module predicts the per-vertex offset, which can be added to the input mesh to obtain the reconstructed result. The topology modification module then estimates the reconstruction error of the outcome of the preceding deformation module and removes the faces with large error in order to update the mesh topology. Finally, the boundary refinement module enhances the smoothness of the open boundaries to further improve the visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mesh DeformNet</head><p>Our mesh deformation module consists of a single multilayer perceptron (MLP). Specifically, the MLP is composed of four fully-connected layers of size 1024, 512, 256, 128 with non-linear activation ReLU on the first three layers and tanh on the final output layer. Given an initial mesh M and the shape feature vector x that contains the prior knowledge of the object, we replicate the vector x and concatenate it with the matrix containing all the vertices of M before feeding them into the MLP. The MLP performs the affine transformation on each vertex of M and generates the vertex displacements. Note that we choose to predict the offsets instead of directly regressing the coordinates. Such a design paradigm enables more accurate learning of fine geometric details with even less training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Topology Modification</head><p>To generate objects with various topologies, it is necessary to modify the faces-to-vertices relationship dynamically. Towards this goal, we propose a topology modification network that updates the topological structure of the reconstructed mesh by pruning the faces which deviate significantly from the ground truth. The topology modification network is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, which includes two components: error estimation and face pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Error Estimation</head><p>To perform face pruning, it is key to locate the triangle faces that have large reconstruction errors at test time. We propose an error estimation network that predicts the per-face errors of the reconstructed mesh from the preceding mesh deformation network. It leverages a similar architecture with that of the mesh deformation network. In particular, we sample points randomly on the faces of the predicted mesh M and concatenate the replicated shape feature vector x with the matrix containing all the sampling points. The MLP takes as input the feature matrix and predicts the perpoint errors (distances to the ground truth). The final error for each triangle face is obtained by averaging the predicted errors for all the sampling points of the triangle face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Face Pruning</head><p>Given the estimated error for each triangle face, we then apply a thresholding strategy that removes the faces whose estimated errors are beyond the predefined threshold to update the mesh topology. However, to obtain a properly pruned mesh structure, the threshold ? needs to be carefully configured: a higher value of ? tends to generate reconstructions with higher errors while a low decision threshold may remove too many triangles and destroy the surface geometry of the generated mesh. To this end, we propose a progressive face pruning strategy that removes error-prone faces in a coarse-to-fine fashion. In particular, we set a higher value for ? at the first subnet and decrease it by a constant factor at the subsequent subnet. Such a strategy enables the face pruning to be performed in a much more accurate manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Boundary Refinement</head><p>As shown in <ref type="figure">Figure 2</ref>, a naive pruning of triangles will introduce jagged boundaries that adversely impact the visual appearance. To prevent such artifacts and further improve the visual quality of the reconstructed mesh, we design a boundary refinement module to enhance the smoothness of the open boundaries. It is similar to the mesh deformation module but only predicts the displacement with respect to each input boundary vertex. Note that each boundary vertex is only allowed to move on the 2D plane estab-  lished by the two boundary edges that intersect at the vertex. We further propose a novel regularization term which penalizes the zigzags by enforcing the boundary curves to stay smooth and consistent. The boundary regularizer is defined as follows:</p><formula xml:id="formula_0">L bound = x?E p?N (x) (x ? p) x ? p ,<label>(1)</label></formula><p>where {x ? E} is the set of vertices which lie on the open boundary and {p ? N (x)} is the set of neighboring vertices of x on the boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Objectives</head><p>Our network is supervised by a hybrid of losses. For mesh deformation and boundary refinement, we employ the commonly-used Chamfer distance (CD) for measuring the discrepancy between the reconstructed result and the ground truth. The error estimation network is trained by the quadratic loss for regressing the reconstruction errors. The boundary regularizer is proposed to guarantee the smoothness of the boundary curves. Besides, we also apply a combination of geometry constraints to regularize the smoothness of the mesh surface during mesh deformation.</p><p>CD loss. The CD measures the nearest neighbor distance between two point sets. In our setting, we minimize the two directional distances between the point set randomly sampled from the generated mesh M and the ground truth point set. The CD loss is defined as:</p><formula xml:id="formula_1">L cd = x?M min y?S x ? y 2 2 + y?S min x?M x ? y 2 2 ,<label>(2)</label></formula><p>where {x ? M } and {y ? S} are respectively the point sets sampled from the generated mesh M and the ground truth surface S. For each point, CD finds the nearest point in another point set, and sums the squared distances up.</p><p>Error estimation loss. We adopt the quadratic loss to train our error estimation network, which is defined as:</p><formula xml:id="formula_2">L error = x?M |f e (x) ? e x | 2 ,<label>(3)</label></formula><p>where {x ? M } is the point set sampled from the generated mesh M , f e is the error estimation network, and e x is the corresponding ground truth error. Geometry regularizers. For the mesh deformation module, since the CD loss does not take into account the connectivity of mesh vertices, the predicted mesh could suffer from severe flying vertices and self-intersections. To improve the smoothness of the mesh surface, we add several geometry regularizers. We employ three regularization techniques defined in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b15">16]</ref>: the normal loss L normal which measures the normal consistency between the generated mesh and ground truth, the smoothness loss L smooth which flattens the intersection angles of the triangle faces and supports the surface smoothness, and the edge loss L edge which penalizes the flying vertices and overlong edges to guarantee the high quality of recovered 3D geometry. The final training objective of our system is defined as:</p><formula xml:id="formula_3">L = L cd + ? 1 L error + ? 2 L bound + ? 3 L normal +? 4 L smooth + ? 5 L edge ,<label>(4)</label></formula><p>where ? i are hyper-parameters weighting the importance of each loss term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset. Our experiments are performed on the 3D models collected from five categories in the ShapeNet <ref type="bibr" target="#b2">[3]</ref> dataset. To ensure fair comparisons with the existing methods, we adopt the experiment setup in <ref type="bibr" target="#b8">[9]</ref>. We use the rendered images provided by <ref type="bibr" target="#b5">[6]</ref> as the inputs, where each 3D model corresponds to 24 RGB images. For each 3D shape, 10, 000 points are uniformly sampled on the surface as the ground truth.  <ref type="table">Table 1</ref>. Quantitative comparison with the state-of-the-art methods. The CD and EMD are computed on 10, 000 points sampled from the generated mesh after performing ICP alignment with the ground truth. The CD is in units of 10 ?3 and the EMD is in units of 10 ?2 .</p><p>200 epochs) for 300 epochs. The entire network is then fine-tuned in an end-to-end manner. The values of hyperparameters used in Equation <ref type="formula" target="#formula_3">(4)</ref> are ? 1 = 1.0, ? 2 = 0.5, ? 3 = 1e ? 2, ? 4 = 2e ? 7, ? 5 = 0.1. The threshold ? for face pruning is set to be 0.1 at the first subnet and decreased by a factor of 2 at the subsequent subnet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparisons with the State-of-the-arts</head><p>We first compare the performance of our approach with three state-of-the-art methods for single view 3D mesh reconstruction, including Neural 3D Mesh Renderer <ref type="bibr" target="#b15">[16]</ref> (N3MR), Pixel2Mesh <ref type="bibr" target="#b31">[32]</ref> and AtlasNet <ref type="bibr" target="#b8">[9]</ref> with 25 patches (AtlasNet-25). We also compare with the baseline approach which refers to our framework without the topology modification and boundary refinement module.</p><p>Qualitative comparisons. The visual comparison results are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. While N3MR can reconstruct the rough shapes, it fails to capture the fine details of the geometry and is not able to model surface with non-disk topology. Pixel2Mesh performs generally better than N3MR in terms of the capability of modeling the fine structures. However, as Pixel2Mesh employs a similar mesh deformation strategy, it struggles to reconstruct shapes with complex topologies, especially for the chairs and tables. Our baseline approach also has the same problem as the topology modification module is not applied. Thanks to the use of multiple squares as the template model, AtlasNet-25 can generate meshes with various topologies. However, it suffers from severe self-intersections and overlapping and still fails to reconstruct some instances with more complex topologies, e,g, the desk in the third row and the chair in the fifth row. In comparison, our approach has outperformed the other approaches in terms of visual quality. We are able to generate meshes with complex topologies while maintaining high reconstruction accuracy thanks to the topology-modification modules. In addition, our method scales well to the shapes with simple topologies. For the objects that can be well reconstructed from a template sphere (e.g. the plane), the  spherical topology is faithfully preserved.</p><p>Quantitative comparisons. We adopt the widely used Chamfer Distance (CD) and Earth Mover's Distance (EMD) to quantitatively evaluate the results. Both metrics are computed between the ground truth point cloud and 10, 000 points uniformly sampled from the generated mesh. Since the outputs of Pixel2Mesh <ref type="bibr" target="#b31">[32]</ref> are non-canonical, we align their predictions to the canonical ground truth by using the pose metadata available in the dataset. Additionally, we apply the iterative closest point algorithm (ICP) <ref type="bibr" target="#b1">[2]</ref> on all the results for finer alignment with the ground truth. The quantitative comparison results are shown in <ref type="table">Table 1</ref>. Our approach consistently outperforms the state-of-the-art methods in both metrics over all five categories, especially on the models with complex topologies (e.g. chair and table).</p><p>Poisson surface reconstruction. Although our method can generate visually appealing meshes with smooth surfaces and complex topologies, it still has the inherent draw- back of producing open surfaces due to the face pruning operations. To avoid the above mentioned drawbacks, one could densely sample the surface and reconstruct the mesh from the obtained point cloud. In particular, we first sample 100, 000 points, together with their oriented normals, from the reconstructed surface and then apply Poisson surface reconstruction (PSR) <ref type="bibr" target="#b16">[17]</ref> to produce a closed triangle mesh.</p><p>To evaluate the performance of applying PSR on our results, we quantitatively compare with AtlasNet-25. Specifically, for both methods, we randomly selected 20 shapes from the chair category, and run the PSR algorithm to get the corresponding closed meshes. In <ref type="table">Table 2</ref>, we show the quantitative comparisons measured in CD and EMD. As seen from the results, our approach generates more accurate results under both measurements. We show the visual comparisons in <ref type="figure" target="#fig_3">Figure 5</ref>. Note that we generate meshes with significantly fewer artifacts and correct topologies compared to the At-lasNet, proving the better meshing quality of our method.</p><p>Reconstructing real-world objects. To qualitatively evaluate the generalization performance of our method on the real images, we test our network on the Pix3D <ref type="bibr" target="#b27">[28]</ref> dataset by using the model trained on the ShapeNet <ref type="bibr" target="#b2">[3]</ref>. <ref type="figure" target="#fig_4">Figure 6</ref> shows the results reconstructed by our method and AtlasNet, where the objects in the images are manually segmented. Our approach is still able to faithfully reconstruct a variety of objects with complex topologies and achieves better accuracy compared against AtlasNet, indicating that our method scales reasonably well on the real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>Robustness to initial meshes. We first test our approach with different initial meshes (e.g. sphere and unit square). As seen from the visualization results in <ref type="figure" target="#fig_5">Figure 7</ref>, our  method achieves similar performance with the two different initial meshes, demonstrating the robustness of our method.</p><p>Progressive shaping. Our proposed architecture consists of multiple mesh deformation and topology modification modules that progressively recover the 3D shape. To validate the effectiveness of such progressive shaping strategy, we retrain our network with removing the first subnet in the decoder. <ref type="figure" target="#fig_6">Figure 8</ref> shows the visualization results. Without progressive shaping, the face pruning cannot be performed in an accurate manner, which could destroy the surface geometry of the generated mesh.</p><p>Face pruning threshold. We investigate the effect of the threshold ? by using 20% of training samples as the validation set, where Chamfer distance (CD) is used as the measure that sums two directional reconstruction errors of Prediction ? GT and GT ? Prediction (cf. Equation <ref type="formula" target="#formula_1">(2)</ref>). <ref type="figure" target="#fig_7">Figure 9</ref> plots the results, suggesting that ? ? [0.05, 0.2] strikes a good balance between the two directional distances. We thus set ? = 0.1 in all our experiments.  Boundary refinement. To demonstrate the effectiveness of the proposed boundary refinement module, we show the visualized reconstruction results with and without the boundary refinement in <ref type="figure" target="#fig_8">Figure 10</ref>. By using the proposed boundary refinement module, one can achieve significantly cleaner mesh with higher visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Shape Autoencoding</head><p>Besides the single-view 3D reconstruction, our framework can also be applied for 3D shape autoencoding. In this section, we demonstrate the capability of our approach to reconstruct meshes from the input point clouds. Toward this goal, we randomly select 2, 500 points from the groundtruth point cloud and employ the PointNet <ref type="bibr" target="#b25">[26]</ref> to extract the corresponding latent features. Again, we compare both the quantitative and qualitative results against the state-ofthe-art AtlasNet <ref type="bibr" target="#b8">[9]</ref>. To ensure fair comparisons, we use the same experiment settings in <ref type="bibr" target="#b8">[9]</ref>. The results are shown in <ref type="table">Table 3</ref> and <ref type="figure" target="#fig_9">Figure 11</ref>. As shown in the results, our approach achieves superior performance both qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CD EMD AtlasNet-25 (AE) 0.765 8.467 Ours (AE) 0.655 6.754 <ref type="table">Table 3</ref>. Quantitative results of 3D shape autoencoding. The results take the means on the five shape categories used in the singleview reconstruction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed an end-to-end learning framework that is capable of reconstructing meshes of various topologies from single-view images. The overall framework includes multiple mesh deformation and topology modification modules that progressively recover the 3D shape, and a boundary refinement module that refines the boundary conditions. Extensive experiments show that our method significantly outperforms the existing methods, both quantitatively and qualitatively. One limitation of our method is the inherent drawback of producing non-closed meshes. But it can be resolved by a post-processing procedure that reconstructs closed surfaces from densely sampled point clouds. Future research directions include designing a differentiable mesh stitching operation to stitch the open boundaries introduced by the face pruning operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledge</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Topology Modification Network. The red color indicates the sampled points with higher estimated errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results. (a) Input image; (b) N3MR; (c) Pixel2Mesh; (d) AtlasNet-25; (e) Baseline; (f) Ours; (g) Ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparison with AtlasNet-25 after PSR. (a): Input images; (b): AtlasNet-25; (c): Ours; (d): Ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparison with AtlasNet-25 on real images. (a): Input images; (b): Input masks; (c): AtlasNet-25; (d): Ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results with different initial meshes. (a): Input images; (b): Unit square; (c): Sphere; (d): Ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Ablation study on progressive shaping. (a): Input images; (b): Reconstructions w/o progressive shaping; (c): Reconstructions with progressive shaping; (d): Ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Effect of ? values on Chamfer distance, distance from GT to prediction, and distance from prediction to GT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Ablation study on boundary refinement. (a): Input images; (b): Reconstructions w/o boundary refinement; (c): Reconstructions with boundary refinement; (d): Ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Qualitative results of 3D shape autoencoding. (a): Ground truth Meshes; (b): AtlasNet-25; (c): Ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>via the learned shape prior. More recently, the advances in differentiable</figDesc><table><row><cell>Encoder</cell><cell>Feature Vector</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Subnet-1</cell><cell>Subnet-2</cell><cell></cell><cell>Subnet-3</cell></row><row><cell>DeformNet</cell><cell>Topology Modification</cell><cell>DeformNet</cell><cell>Topology Modification</cell><cell>Boundary Refinement</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Forward path</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Intermediate results</cell></row><row><cell></cell><cell>Feature Vector</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Error</cell><cell></cell><cell>Face</cell><cell></cell></row><row><cell></cell><cell>Estimation</cell><cell></cell><cell>Pruning</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
	<note>Ioannis Mitliagkas, and Leonidas Guibas</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04302</idno>
		<title level="m">Deep rbfnet: Point cloud feature learning using radial basis functions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2463" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1362" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AtlasNet: A Papier-M?ch? Approach to Learning 3D Surface Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-resolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep volumetric video from very sparse multi-view performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Legendre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="336" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised learning of shape and pose with differentiable point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2802" to="2812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning category-specific mesh reconstruction from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="371" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Screened poisson surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning efficient point cloud generation for dense 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Hsuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Soft rasterizer: A differentiable renderer for image-based 3d reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D-LMNet: Latent embedding matching for accurate and diverse 3d point cloud reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Mandikal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Navaneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep level sets: Implicit surface representations for 3d shape inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Michalkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jhony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eriksson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06802</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Siclope: Silhouette-based clothed people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4480" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Residual meshnet: Learning to deform meshes for single-view 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="719" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image2mesh: A learning framework for single image 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jhony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Machine perception of threedimensional solids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pix3d: Dataset and methods for single-image 3d shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2974" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2088" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2626" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-fidelity facial reflectance and geometry inference from an unconstrained image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuco</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">162</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning singleview 3d object reconstruction without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1696" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hairnet: Single-view hair reconstruction using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Wei</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="235" to="251" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
