<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Residual Networks with LSTMs for Lipreading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
							<email>themos.stafylakis@nottingham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision</orgName>
								<orgName type="institution">Laboratory University of Nottingham</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<email>yorgos.tzimiropoulos@nottingham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision</orgName>
								<orgName type="institution">Laboratory University of Nottingham</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Residual Networks with LSTMs for Lipreading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: visual speech recognition</term>
					<term>lipreading</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an end-to-end deep learning architecture for wordlevel visual speech recognition. The system is a combination of spatiotemporal convolutional, residual and bidirectional Long Short-Term Memory networks. We train and evaluate it on the Lipreading In-The-Wild benchmark, a challenging database of 500-size target-words consisting of 1.28sec video excerpts from BBC TV broadcasts. The proposed network attains word accuracy equal to 83.0%, yielding 6.8% absolute improvement over the current state-of-the-art, without using information about word boundaries during training or testing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual speech recognition (also known as lipreading) is a field of growing attention. It is a natural complement to audio-based speech recognition that can facilitate dictation in noisy environments and enable silent dictation in offices and public spaces. It is also useful in applications related to improved hearing aids and biometric authentication, <ref type="bibr" target="#b0">[1]</ref>. Lipreading is the field where the speech recognition and computer vision communities meet each other and combine the advances of each field. The tremendous success of deep learning in both fields has already affected visual speech recognition, by shifting the research direction from handcrafted features and HMM-based models to deep feature extractors and end-to-end deep architectures. Recently introduced deep learning systems beat human lipreading experts by a large margin, at least for the constrained vocabulary defined by each database, <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>.</p><p>One way to categorize visual and audio-visual speech recognition approaches is (i) to those that model words (e.g. <ref type="bibr" target="#b2">[3]</ref>  <ref type="bibr" target="#b3">[4]</ref>) and (ii) to those that model visemes (e.g. <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>), i.e. visual units that correspond to sets of visually indistinguishable phonemes, <ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b5">[6]</ref>. The former approach is considered more pertinent to tasks like isolated word recognition, classification and detection, while the latter to sentence-level classification and large vocabulary continuous speech recognition (LVCSR). Nevertheless, recent advances in speech recognition and natural language processing show that direct modeling of words is feasible even for LVCSR, <ref type="bibr" target="#b6">[7]</ref> [8] <ref type="bibr" target="#b8">[9]</ref>.</p><p>The proposed system belongs to the former category, although it can support viseme-level recognition by using viseme instead of word labels at the SoftMax layer. It combined three sub-networks: (i) The front-end, which applies spatiotemporal convolution to the frame sequence, (ii) a Residual Network (ResNet) that is applied to each time step, and (iii) the backend, which is a two-layer Bidirectional Long Short-Term Memory (Bi-LSTM) network. The SoftMax layer is applied to all time steps and the overall loss is the aggregation of the per time step losses, and the system is trained in an end-to-end fashion. Finally, the system performs not merely word recognition but also implicit key-word spotting, since the target words are not isolated, but they are part of whole utterances of fixed duration (1.28sec). Information regarding word boundaries is not utilized neither during training nor during evaluation. <ref type="bibr" target="#b0">1</ref> The rest of the paper is organized as follows. In Section 2, we refer to recent works on visual speech recognition, with emphasis on those that apply deep learning methods. The Lipreading In-The-Wild (LRW) database is discussed in Section 3, while in Section 4 we present analytically the proposed model, together with some useful detail about preprocessing and implementation. Finally, in Section 5 we present our experimental results, together with baseline and state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Prior to the advent of deep learning ( <ref type="bibr" target="#b9">[10]</ref>) most of the work in lipreading was based on hand-engineered features, that were usually modeled by HMM-based pipeline, <ref type="bibr" target="#b10">[11]</ref> [12] <ref type="bibr" target="#b12">[13]</ref> [14] <ref type="bibr" target="#b14">[15]</ref>. Spatiotemporal descriptors such as active appearance models and optical flow, and SVM classifiers have also been proposed, <ref type="bibr" target="#b15">[16]</ref>. For an analytic review on traditional lipreading methods we refer to <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref>. More recent works deploy deep learning methods either for extracting "deep" features <ref type="bibr">([19]</ref> [20] <ref type="bibr" target="#b20">[21]</ref>) or for building end-to-end architectures. In <ref type="bibr" target="#b21">[22]</ref>, Deep Belief Networks were deployed for audio-visual recognition and 21% relative improvement was reported over a baseline multi-stream audio-visual GMM/HMM system. In <ref type="bibr" target="#b22">[23]</ref>, bottleneck features are extracted using Deep Autoencoder. The bottleneck features are concatenated with DCT features and the overall system is trained jointly using an LSTM backend. In <ref type="bibr" target="#b2">[3]</ref>, a fully LSTM architecture is proposed, which attains superior results compared to traditional methods on the GRID audiovisual corpus, <ref type="bibr" target="#b24">[24]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, an end-to-end sentencelevel lipreading network (LipNet) is introduced, that combines spatiotemporal convolutional layers, LSTMs and Connectionist Temporal Classification (CTC, <ref type="bibr" target="#b25">[25]</ref>). It attains 95.2% sentencelevel accuracy on a subset of speakers from GRID database, while trained on the remaining GRID speakers. Finally, in <ref type="bibr" target="#b1">[2]</ref>, the encoder-decoder with attention mechanism is explored, in both audio-visual and visual settings. Using solely visual information, 97.0% word accuracy is reported on GRID and 76.2% word accuracy on LRW. To the best of our knowledge, the latter results define the current state-of-the-art for both databases, insofar as additional training resources may be leveraged, <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Database</head><p>We train and evaluate the algorithm on the challenging LRW database, <ref type="bibr" target="#b3">[4]</ref>. The database consists of audiovisual speech seg- <ref type="figure">Figure 1</ref>: Random frames from the LRW database ments extracted from BBC TV broadcasts (News, Talk Shows, a.o.) and it is characterized by its high variability with respect to speakers and pose. Moreover, the number of target words is 500, which is an order of magnitude higher than other publicly available databases (GRID <ref type="bibr" target="#b24">[24]</ref>, CUAVE <ref type="bibr" target="#b26">[26]</ref>, a.o.). Another feature that renders the database so challenging is the existence of pairs of words that share most of their visemes. Such examples are nouns in both singular and plural forms (e.g. benefitbenefits, 23 pairs) as well as verbs in both present and past tenses (e.g. allow-allowed, 4 pairs).</p><p>However, perhaps the most difficult aspect of the database -and of the setting we chose to proceed with-is the fact that the target-words appear within utterances rather than being isolated. Hence, the network should learn not merely how to discriminate between 500 target-words, but also how to ignore the irrelevant parts of the utterance and spot one of the target-words. And it should learn how to do so without knowing the word boundaries. Some random examples of utterances are "...the election victory...", "...the day's other news...", "...and so senior labour..." and "...point, I think the...", where italics denote the target-word of each utterance.</p><p>The collection of the database was fully automatic, involving OCR on the subtitles, synchronization with the audio (forced alignment), as well as verification that the speaker is visible (see <ref type="bibr" target="#b3">[4]</ref> for a detailed description). The training set consists of up to 1000 occurrences per target word, while the validation and evaluation sets both consist of 50 occurrences per word. Each clip is of fixed duration (1.28sec, 31 frames with 25fps frame rate). Random frames from the database are depicted in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep Learning modeling and preprocessing 4.1. Facial landmarks and data augmentation</head><p>In the first preprocessing step, we discard redundant information in order to focus on the mouth region. To do so, we use the 2D version of the algorithm proposed in <ref type="bibr" target="#b27">[27]</ref> and <ref type="bibr" target="#b28">[28]</ref>. The algorithm tackles regression in two steps. It first applies detection to extract a set of heatmaps (one per landmark) which are used as side information for the subsequent regression network. Based on the 66 facial landmarks, we crop the images and resize them to a fixed 112?112 size. A common cropping is applied to all frames of a given clip, using the median coordinates of each landmark. The frames are transformed to grayscale and are normalized with respect to the overall mean and variance. Finally, data augmentation is performed during training, by applying random cropping (?5 pixels) and horizontal flips, that are common across all frames of a given clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Spatiotemporal front-end</head><p>The first set of layers applies spatiotemporal convolution to the preprocessed frame stream. Spatiotemporal convolutional layers are capable of capturing the short-term dynamics of the mouth region and are proven to be advantageous, even when recurrent networks are deployed for back-end, <ref type="bibr" target="#b0">[1]</ref>. They consist of a convolutional layer with 64 3-dimensional (3D) kernels of 5?7?7 size (time/width/height), followed by Batch Normalization (BN, <ref type="bibr" target="#b29">[29]</ref>) and Rectified Linear Units (ReLU). The extracted feature maps are passed through a spatiotemporal maxpooling layer, which drops the spatial size of the 3D feature maps. The number of parameters of the spatiotemporal frontend is ?16K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Residual Network</head><p>The 3D features maps are passed through a residual network (ResNet, <ref type="bibr" target="#b30">[30]</ref>), one per time-step. We use the 34-layer identitymapping version, which was proposed for ImageNet, <ref type="bibr" target="#b31">[31]</ref>. Its building blocks are composed of two convolutional layers, and with BN and ReLU, while the skip connections facilitate information propagation, <ref type="bibr" target="#b31">[31]</ref>. The ResNet drops progressively the spatial dimensionality with max pooling layers, until its output becomes a single dimensional tensor per time step. We should emphasize that we did not make use of pretrained models, as they are optimized for completely different tasks (e.g. static colored images from ImageNet or CIFAR). The number of parameters of the ResNet is ?21M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Bidirectional LSTM back-end and optimization criterion</head><p>The back-end of the model is a Bidirectional LSTM network. For each of the two directions, we stack two LSTMs, and the outputs of the final LSTMs are concatenated. The number of parameters of the LSTM back-end is ?2.4M. When using word-level recognition without explicit modelling of visemes, several approaches exist in terms of the optimization criterion. One approach is to place the SoftMax layer at the last time step of the LSTM output, i.e. when the overall sequence is encoded by the LSTM. Backpropagation through time is capable of propagating the errors all the way back to the first time step of the sequence, given the resilience of LSTM to the problem of vanishing gradients, <ref type="bibr" target="#b2">[3]</ref>. A second approach is to apply the criterion for each time step. This approach is closer to the typical use of LSTMs in speech recognition, where instead of phoneme/viseme labels, the word label is repeated at every time step. This approach fits well to bidirectional LSTMs, since hidden states have in all time steps access to the overall video, <ref type="bibr" target="#b32">[32]</ref>. After experimentation with both approaches, we concluded that the latter leads to much higher word accuracy (about 3% absolute improvement). Hence, the overall loss is defined as the aggregated loss over all time steps, which coincides to the summation of negative logarithm of word posteriors. Notice again that the word label is applied to all time steps of the clip, since word boundaries are unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Implementation details</head><p>Our implementation is based on Torch7 ( <ref type="bibr" target="#b33">[33]</ref>) and the networks are trained on a NVIDIA Titan X (Pascal Architecture) GPU with 12GB memory. We use the standard SGD training algorithm, with momentum 0.9. BN follows all convolutional and linear layers, apart from the one preceding the SoftMax layer. We do not apply dropout, since it is not part of the ResNet train- ing recipe (BN seems to suffice, <ref type="bibr" target="#b29">[29]</ref>). The initial learning rate is 5 ? 10 ?3 for the experiments with the convolutional backend and 5 ? 10 ?4 for those with Bi-LSTM, while the final is 5 ? 10 ?5 , decreasing on log scale. Training is considered complete when the results on the validation set do no longer improve, with a delay of 3 epochs. All our models converge after 15 to 20 epoches.</p><p>A block-diagram of the network is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. BN layers have been omitted for clarity. The size of the tensors each layer outputs is also presented. For the 3D-convolutional front-end, tensor dimensions denote channels, time, width and height.</p><p>We should emphasize that although the overall system can be directly trained end-to-end, we use the following three steps approach. Initially, a temporal convolutional back-end is used instead of the Bi-LSTM. After convergence, the temporal convolutional back-end is removed and the Bi-LSTM back-end is attached. The Bi-LSTM is trained for 5 epochs, keeping the weights of the 3D convolution front-end and the ResNet fixed. Finally, the overall system is trained end-to-end. A comparison between the two back-ends is presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baseline results</head><p>The best baseline result published in <ref type="bibr" target="#b3">[4]</ref> is the multi-tower VGG-M. It consists of a set of parallel VGG models (towers) with shared weights, which are concatenated channel-wise using pooling, while the rest of the network is the same as the regular VGG-M. The results are presented in <ref type="table">Table 1</ref> in terms of word accuracy. Top-1 corresponds to the percentage of times the word was correctly identified, while more generally Top-N corresponds to the percentage of times the correct word was among the N best scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Top-1 Top-5 Top-10 Baseline 61.1% -90.4% <ref type="table">Table 1</ref>: Word accuracies for the baseline network (VGG-M).</p><p>In <ref type="bibr" target="#b1">[2]</ref>, an attentional encoder-decoder architecture is proposed, <ref type="bibr" target="#b34">[34]</ref>. It is trained on a different set of BBC TV Broadcasts, which contains whole sentences rather than words. A visual-only version of the system (termed "Watch, Attend and Spell", WAS) is evaluated on GRID and on LRW. The network is pretrained on the BBC TV Broadcasts, while the training sets of GRID and LRW are used for fine-tuning. Word accuracies (Top-1) equal to 97.0% and 76.2% are reported on GRID and LRW respectively, which according to our knowledge represent the current state-of-the-art on both databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results using our network</head><p>We begin by using a simpler model than the proposed one, in order to examine the contribution of each individual component of the network. The first network applied 2D convolution instead of 3D. The 2D convolution is followed by the ResNet, while the back-end is based on temporal convolution rather than LSTMs. More specifically, we use two temporal convolutional layers, each of which is followed by BN, ReLU and Max Pooling which reduce the temporal dimensions by a factor of 2. Finally, a Mean Pooling layer is added, followed by a linear and a SoftMax layer. The results are presented in <ref type="table">Table 2</ref> (denoted by N1). The results of the same model, but with 3D convolution are also presented in <ref type="table">Table 2</ref> (denoted by N2).</p><p>In order to verify the effectiveness of the ResNet we replace it with a Deep Neural Network (DNN) of approximately the same number of parameters (?20M). The DNN is composed of 3 fully connected hidden layers, with BN and ReLU. Its inputs are 3D convolutional maps, treated as vectors (one per time step). The DNN progressively reduces the size of the vectors as 50176 ? 384 ? 384 ? 256. The results are presented in <ref type="table">Table  2</ref> (denoted by N3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Top-  <ref type="table">Table 2</ref>: Word accuracies using temporal convolution back-end.</p><p>We now focus on the back-end of the network and use LSTMs instead of temporal convolutions. The first network in <ref type="table">Table 3</ref> (denoted by N4) uses a single-layer Bi-LSTM, while the second one (denoted by N5) uses a double-layer Bi-LSTM. These two networks are not trained end-to-end. While training the back-end, the 3D convolutional layer and the ResNet (that are copied from N2) remain fixed. Moreover, the outputs of the two directional LSTMs are added together instead of concatenated together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Top-1 Top-5 Top-10 N4</p><p>78.4% 94.9% 97.4% N5 79.6% 95.3% 96.3% <ref type="table">Table 3</ref>: Word accuracies using different LSTM in the back-end. For the final set of results we use end-to-end training of the overall network. The first network in <ref type="table">Table 4</ref> (denoted by N6) is the same as N5, but trained end-to-end, using the weights of N5 as starting point. Finally, N7 is also trained end-to-end and the sole difference with N6 is that the outputs of the two directional LSTMs are concatenated together instead of added together (as depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Top-  <ref type="table">Table 4</ref>: Word accuracies using LSTM back-end and end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Discussion and error analysis</head><p>Several conclusions can be drawn from the results presented above (see also <ref type="figure" target="#fig_1">Fig. 3 for clarity)</ref>. First of all, by comparing the baseline to N1, we observe our simplest system yielding 8.5% absolute improvement over the VGG-M baseline. Moreover, the use of 3D (N2) instead of 2D (N1) leads to a further 5.0% absolute improvement, emphasizing the need of modeling the short-term dynamics of the mouth region in the frontend. By comparing N2 and N3 we notice that the ResNet yields 4.9% better work accuracy compared to a 3-layer DNN with the same number of parameters. In addition, by using a single-layer Bi-LSTM (N4) instead of a temporal convolutional back-end, a further 3.8% absolute improvement is attained, highlighting the expressive power of LSTMs in modeling temporal sequences. Furthermore, the use of a two-layer Bi-LSTMs (N5) offers a further 1.2% absolute improvement. The final set of results demonstrates the importance of end-to-end training towards achieving higher word accuracy. By training N5 in an end-to-end fashion (N6) we obtain a 1.9% absolute improvement, while by concatenating (N7) rather than adding together (N6) the Bi-LSTM outputs we obtain our best result, a 83.0% work accuracy. <ref type="table" target="#tab_2">Table 5</ref> contains the most frequent errors made by our best system (N7). We observe that most of the word pairs are mutually close with respect to their phonetic and "visemic" content. We should emphasize again that the clips contain coarticulation with preceding and succeeding words, as they are excerpted from continuous speech. Hence, correct identification of the first and last visemes of a word is occasionally hard.</p><p>The list of words for which the system yields the best and worst performance is presented in <ref type="table">Table 6</ref>. As expected, the system does very well on words with rich phonetic/visemic content</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Word</head><p>Decision Error Rate <ref type="table">(%)  SPEND  SPENT  26  WANTS  WANTED  18  RUSSIAN  RUSSIA  18  BENEFIT  BENEFITS  18  BENEFITS  BENEFIT  16  RUSSIA  RUSSIAN  16  CANCER  AGAINST  16  GIVING  LIVING  16  DIFFERENCE DIFFERENT  14  MAKES</ref> MEANS 14  <ref type="table">Table 6</ref>: Words with the highest accuracy (left) vs. words with the lowest accuracy (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We proposed a spatiotemporal deep learning network for wordlevel visual speech recognition. The network is a stack of a 3D convolutional front-end, a ResNet and an LSTM-based back-end, and trained using an aggregated per time step loss. We chose to experiment with the LRW database, since it combines many attractive characteristics, such as large size (?500K clips), high variability in speakers, pose and illumination, nonlaboratory in-the-wild conditions, and target-words as part of whole utterances rather than isolated. We explored several network configurations, and we demonstrated the importance of each building block of the network as well as the gain in performance attained by training the network end-to-end. The proposed network yielded 83.0% work accuracy, which corresponds to less that half the error rate of the baseline VGG-M network and 6.8% absolute improvement over the state-of-theart 76.2% accuracy, attained by an attentional encoder-decoder network, [2] <ref type="bibr" target="#b3">[4]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The block-diagram of the proposed network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Word accuracy of the networks examined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>Most frequent errors made by the proposed system. and vice versa. There are 8 words for which the system made no errors, and only 3 words for which the word accuracy dropped below 50%. Recall that the number of evaluation clips is 50 per target word (i.e. 25000 clips overall).</figDesc><table><row><cell>Target Word</cell><cell>Acc (%)</cell><cell cols="2">Target Word Acc (%)</cell></row><row><cell>SUNSHINE</cell><cell>100</cell><cell>SPEND</cell><cell>58</cell></row><row><cell>ECONOMIC</cell><cell>100</cell><cell>AROUND</cell><cell>58</cell></row><row><cell>TEMPERATURES</cell><cell>100</cell><cell>THING</cell><cell>56</cell></row><row><cell>WESTMINSTER</cell><cell>100</cell><cell>THEIR</cell><cell>56</cell></row><row><cell>POLITICIANS</cell><cell>100</cell><cell>UNTIL</cell><cell>54</cell></row><row><cell>SITUATION</cell><cell>100</cell><cell>GETTING</cell><cell>52</cell></row><row><cell>OBAMA</cell><cell>100</cell><cell>SAYING</cell><cell>50</cell></row><row><cell>INQUIRY</cell><cell>100</cell><cell>THERE</cell><cell>48</cell></row><row><cell>MINISTER</cell><cell>98</cell><cell>GOING</cell><cell>48</cell></row><row><cell>FAMILIES</cell><cell>98</cell><cell>UNDER</cell><cell>42</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and pre-trained models in Torch7 are available at https://github.com/tstafylakis/Lipreading-ResNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This work has been funded by the European Commission program Horizon 2020, under grant agreement no. 706668 (Talking Heads). The views expressed in this paper are those of the authors and do not engage any official position of the funding agencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<title level="m">Lipnet: Sentence-level lipreading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lipreading with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6115" to="6119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Confusions among visually perceived consonants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Speech and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="796" to="804" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decoding visemes: improving machine lip-reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2009" to="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word embeddings for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1053" to="1057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural speech recognizer: Acoustic-to-word LSTM model for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09975</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Direct acoustics-to-word models for english conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07754</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Continuous automatic speech recognition by lipreading,&quot; in Motion-Based recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Goldschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Petajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="321" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lipreading from color video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1192" to="1195" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recent advances in the automatic recognition of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1306" to="1326" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The natural statistics of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stillittano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ghazanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pitsikalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="435" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lip reading using optical flow and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Azemin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gubbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Congress on Image and Signal Processing (CISP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="327" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A review of recent advances in visual speech decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="590" to="605" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Audio-visual automatic speech recognition: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note>Issues in visual and audio-visual speech processing</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="722" to="737" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving lip-reading performance for robust audiovisual speech recognition using DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thangthai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-J</forename><surname>Theobald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVSP</title>
		<imprint>
			<biblScope unit="page" from="127" to="131" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved speaker independent lip reading using speaker adaptive training and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Almajai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2722" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Audio-visual deep learning for noise robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7596" to="7599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep complementary bottleneck features for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2304" to="2308" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An audiovisual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cuave: A new audio-visual database for multimodal human-computer interface research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gurbuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tufekci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Gowdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional aggregation of local evidence for large pose face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-stage convolutional part heatmap regression for the 1st 3D face alignment in the wild (3DFAW) challenge</title>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bidirectional LSTM networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
