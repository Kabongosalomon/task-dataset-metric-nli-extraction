<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
							<email>yongjiang.jy@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
							<email>nguyen.bach@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
							<email>z.huang@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>f.huang@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ? DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recent advances in Named Entity</head> Recognition (NER)  <p>show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained contextual embeddings such as ELMo <ref type="bibr" target="#b34">(Peters et al., 2018)</ref>, Flair <ref type="bibr" target="#b1">(Akbik et al., 2018)</ref> and BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b52">Yu et al., 2020;</ref><ref type="bibr" target="#b51">Yamada et al., 2020)</ref> found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios * Yong Jiang and Kewei Tu are the corresponding authors. ? : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. <ref type="bibr">1</ref> Our code is publicly available at https://github. com/Alibaba-NLP/CLNER. senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections .</p><p>President Obama called for eliminating the legislative filibuster last month , which could occur if Democrats retake the Senate . Some Republicans say it ' s time to undo a wrong committed by Reid . Senate Republicans are considering using the " nuclear option " to end a potential Democratic filibuster and confirm Neil Gorsuch to the Supreme Court . Senate Republicans deployed the " nuclear option " on Wednesday to drastically reduce the time it takes to confirm hundreds of President Trump ' s nominees .</p><p>Label: Group Label: Non Entity Input Sentence:</p><p>Retrieved Texts: ? ? ? ? <ref type="figure">Figure 1</ref>: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of "democrats" and "republican".</p><p>in which document-level contexts are unavailable in practice. For example, there are sometimes no available contexts in users' search queries, tweets and short comments in various domains such as social media and E-commerce domains. When professional annotators annotate ambiguous named entities in such cases, they usually rely on domain knowledge for disambiguation. This kind of knowledge can often be found through a search engine. Moreover, when the annotators are not sure about a certain entity, they are usually encouraged to find related knowledge through a search engine . Therefore, we believe that NER models can benefit from such a process as well.</p><p>In this paper, we propose to improve NER models by retrieving texts related to the input sentence by an off-the-shelf search engine. We re-rank the retrieved texts according to their semantic relevance to the input sentence and select several top-ranking texts as the external contexts. Consequently, we concatenate the input sentence and external contexts together as a new retrieval-based input view and feed it to the pretrained contextual embedding module, so that the resulting semantic representations of the input tokens can be improved. The token representations are then fed into a CRF layer for named entity prediction. A motivating example is shown in <ref type="figure">Figure 1</ref>.</p><p>Moreover, we consider utilizing the new input view to improve model performance with the original input view that does not have external contexts. This can be useful in application scenarios when external contexts are unavailable or undesirable (e.g., in time-critical scenarios). To this end, we propose Cooperative Learning (CL) that encourages the two input views to produce similar predictions. We propose two approaches to CL which minimize either the L 2 distances between the token representations of the two input views or the Kullback-Leibler (KL) divergence between the prediction distributions of the two input views during training.</p><p>Our experiments show that including the retrieved external contexts can significantly improve the accuracy of NER models on 8 NER datasets from 5 domains. With CL, the accuracy of the NER models with both input views can be further improved. Our approaches outperform previous state-of-the-art approaches in each domain.</p><p>The contributions of this paper are:</p><p>1. We propose a simple and straight-forward way to improve the contextual representation of an input sentence through retrieving related texts using a search engine. We take the retrieved texts together with the input sentence as a new retrieval-based view.</p><p>2. We propose Cooperative Learning to jointly improve the accuracy of both input views in a unified model. We propose two approaches in CL based on the L 2 norm and KL divergence respectively. CL can utilize unlabeled data for further improvement.</p><p>3. We show the effectiveness of our approaches in several NER datasets across 5 domains and our approaches achieve state-of-the-art accuracy. By leveraging a large amount of unlabeled data, the performance can be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework</head><p>Given a sentence of n tokens x = {x 1 , ? ? ? , x n }, the input sentence is fed into a search engine as a query. The search engine returns the top k relevant texts {x 1 , ? ? ? ,x k }. Our framework feeds these texts into a re-ranking model. We concatenate l top-ranking texts output from the re-ranking model as the external contexts. The NER model is fed with either an input view with the input sentence (original input view) or a concatenation of the input sentence and external contexts (retrieval-based input view) as input. The model outputs the predictions of labels y = {y 1 , ? ? ? , y n } at each position based on the CRF layer. To further improve the model, we use Cooperative Learning to train a unified model that is strong in both input views. With CL, the model is additionally constrained to be consistent in the internal representations or the output distributions of both input views. The architecture of our framework is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Re-ranking</head><p>Given an input sentence as a search query, the search engine returns ranked relevant texts. However, the off-the-shelf search engine is highly optimized for a fast speed over a large set of documents, so it may sometimes produce semantically irrelevant results or rank the results using inaccurate relevance scores. Since the NER task targets at semantically recognizing named entities, it is more helpful if the relevant texts are semantically similar to the input sentence. Therefore, we need to re-rank the retrieved texts so that the most semantically relevant texts are chosen. We propose to apply BERTScore <ref type="bibr" target="#b54">(Zhang et al., 2020)</ref> to score the relatedness of each retrieved text to the input sentence. BERTScore is a language generation metric that calculates a sum of cosine similarity between token representations of two sentences. Therefore, it is more likely that the search query and the retrieved texts have strong semantic relations when BERTScore is large. The token representations are generated from pretrained contextual embeddings such as BERT. Given the corresponding prenormalized token representations {r 1 , ? ? ? , r n } of the input sentence x and the pre-normalized token representations {r 1 , ? ? ? ,r m } of a certain retrieved textx with m words, the Precision (P), Recall (R) of BERTScore measure the semantic similarities from one to another:</p><formula xml:id="formula_0">R = 1 n x i ?x max x j ?x r ir j ; P = 1 m x j ?x max x i ?x r ir j</formula><p>We re-rank the retrieved texts by the F1 scores F1=2 P?R P+R and concatenate l top-ranking texts {x 1 , ? ? ? ,x l } with F1 scores together as the external contexts:  <ref type="figure">Figure 2</ref>: The architecture of our framework. An input sentence x is fed into a search engine to get k related texts. The related texts are then fed into the re-ranking module. The framework selects l highest ranking related texts output from the re-ranking module and feeds the texts to a transformer-based model together with the input sentence. Finally, we calculate the negative likelihood loss L NLL and L NLL-EXT together with the CL loss (either L CL-L2 or L CL-KL ).</p><p>where sep_token is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, "[SEP]" in BERT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NER Model</head><p>We solve the NER task as a sequence labeling problem. We apply a neural model with a CRF layer, which is one of the most popular state-of-the-art approaches to the task <ref type="bibr" target="#b22">(Lample et al., 2016;</ref><ref type="bibr" target="#b30">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b0">Akbik et al., 2019)</ref>. In the sequence labeling model, the input sentence x is fed into a transformer-based pretrained contextual embeddings model to get the token representations</p><formula xml:id="formula_1">{v 1 , ? ? ? , v n } by v i =embed i (x).</formula><p>The token representations are fed into a CRF layer to get the conditional probability p ? (y|x):</p><formula xml:id="formula_2">?(y , y, v i ) = exp(W T y v i + b y ,y ) (1) p ? (y|x) = n i=1 ?(y i?1 , y i , v i ) y ?Y(x) n i=1 ?(y i?1 , y i , v i )</formula><p>where ? is the potential function and ? represents the model parameters. Y(x) denotes the set of all possible label sequences given x. y 0 is defined to be a special start symbol. W T ? R t?d and b ? R t?t are parameters computing emission and transition scores respectively. d is the hidden size of v and t is the size of the label set. During training, the negative log-likelihood loss for the input sequence with gold labels y * is defined by:</p><formula xml:id="formula_3">L NLL (?) = ? log p ? (y * |x)<label>(2)</label></formula><p>In our approach, we concatenate the external contextsx at the end of the input sentence x to form the retrieval-based input view. The token representations are now given by:</p><formula xml:id="formula_4">{v 1 , ? ? ? , v n , ? ? ? } = embed([x;x])</formula><p>The architecture of our NER model is shown in <ref type="figure" target="#fig_0">Figure 3</ref>. Now the conditional probability p ? (y|x) becomes p ? (y|x,x). The loss function in Eq. 2 becomes:</p><formula xml:id="formula_5">L NLL-EXT (?) = ? log p ? (y * |x,x)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cooperative Learning</head><p>In practice, there are two application scenarios for the NER model: 1) offline prediction, which requires high accuracy of the prediction but the prediction speed is less emphasized; 2) online serving, which requires a faster prediction speed. The retrieval-based input view meets the requirement of the first scenario for its strong token representations. However, it does not meet the requirement of the second scenario. The external contexts are usually significantly longer than the input sentence and a search engine may not meet the latency requirements. These two issues significantly slow down the prediction speed of the model. Therefore, it is essential to improve the accuracy of the original input views in a unified model to meet these two scenarios.</p><formula xml:id="formula_6">[CLS] 1 ? [SEP] 1 ? [SEP] Input Sentence External Contexts ? ? ? Transformer-Based Embedding ? ? ? 1 ? ? 1 ? +1 ? +m ? 2 ? 1 2 ? CRF Layer</formula><p>Cooperative Learning targets at using the retrieval-based input view to help improve the accuracy of the model when there are no external contexts available. CL adds constraints between the internal representations or the output distributions between two input views to enforce that the predictions of both views should be near. The objective function of CL is calculated by:</p><formula xml:id="formula_7">L CL (?) = D(h([x;x]), h([x]))<label>(4)</label></formula><p>where D is a distance function between a function h with different inputs. Because the representations or the distributions with retrieval-based input view are usually informative, we do not backpropagate the gradient through h([x;x]). We propose two approaches for CL.</p><p>Token Representations: Stronger token representations usually lead to better accuracy on the task. Therefore, CL constrains the token representations of two input views to be similar. This helps the model learn to predict the token representations with external contexts even if the contexts are not available. In this approach, D is the L 2 norm to represent the distances of the token representations:</p><formula xml:id="formula_8">L CL-L 2 (?) = n i=1 ||v i ? v i || 2 2 (5)</formula><p>Label Distributions: Since CL enforces the label predictions of both input views to be similar, a straight-forward approach is constraining the label distributions predicted by the model to be similar with the two input views. In this approach, we use the KL divergence as the function D. Then objective function in Eq. 4 becomes the KL divergence between p ? (y|x,x) and p ? (y|x):</p><formula xml:id="formula_9">L CL-KL (?)= y?Y(x) KL(p ? (y|x,x)||p ? (y|x)) (6)</formula><p>With the CRF layer, the loss function is difficult to calculate because the output space of p ? (y|?) is exponential in size. To alleviate this issue, we calculate the KL divergence between the marginal distributions q ? (y i |x,x) and q ? (y i |x) at each position of the sentence to approximate Eq. 6. The marginal distributions can be obtained using the forward-backward algorithm:</p><formula xml:id="formula_10">?(y k ) = {y 0 ,...,y k?1 } k i=1 ?(y i?1 , y i , v i ) ?(y k ) = {y k+1 ,...,yn} n i=k+1 ?(y i?1 , y i , v i ) q ? (y k |x) ? ?(y k ) ? ?(y k )<label>(7)</label></formula><p>As mentioned earlier, we do not back-propagate the gradient through p ? (y|x,x). Therefore calculating the KL divergence is equivalent to calculating the cross-entropy loss between q(y|x,x) and q(y|x):</p><formula xml:id="formula_11">L CL-KL (?)=? n i=1 t y i =1 q ? (y i |x,x)logq ? (y i |x) (8)</formula><p>Together with the negative log-likelihood losses in Eq. 2, 3, the total loss in training is a summation of label losses and a CL loss:</p><formula xml:id="formula_12">L(?) = L NLL (?) + L NLL-EXT (?) + L CL (?) (9)</formula><p>where L CL (?) can be one of the CL loss in Eq. 5, 8 or a summation of both of them. 3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Settings</head><p>Datasets To show the effectiveness of our approach, we experiment on 8 NER datasets across 5 domains:</p><p>? Social Media: We use WNUT-16 <ref type="bibr" target="#b38">(Strauss et al., 2016)</ref> and <ref type="bibr">WNUT-17 (Derczynski et al., 2017)</ref> datasets collected from social media. We use the standard split for these datasets.</p><p>? News: We use CoNLL-03 English <ref type="bibr" target="#b42">(Tjong Kim Sang and De Meulder, 2003)</ref> dataset and CoNLL++  dataset. The CoNLL-03 dataset is the most popular dataset for NER. CoNLL++ is a revision of the CoNLL-03 datasets.  fixed annotation errors on the test set by professional annotators and improved the quality of the training data through their CrossWeigh approach. We use the standard dataset split for these datasets.</p><p>? Biomedical: We use BC5CDR <ref type="bibr" target="#b25">(Li et al., 2016)</ref> and NCBI-disease (Dogan et al., 2014) datasets, which are two popular biomedical NER datasets. We merge the training and development data as training set following <ref type="bibr" target="#b33">Nooralahzadeh et al. (2019)</ref>.</p><p>? Science and Technology: We use CBS SciTech News dataset collected by <ref type="bibr" target="#b19">Jia et al. (2019)</ref>. The dataset only contains the test set with the same label set as the CoNLL-03 dataset. We use the dataset to evaluate the effectiveness of crossdomain transferability from the news domain.</p><p>? E-commerce: We collect and annotate an internal dataset from one anonymous E-commerce website. The dataset contains 25 named entity labels for goods in short texts. We also collect 300,000 unlabeled sentences for semi-supervised training.</p><p>We show the statistics of the datasets in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Annotations of the E-commerce dataset We manually labeled the user queries through crowdsourcing from www.aliexpress.com, which is a real-world E-commerce website. For each query, we asked one annotator to label the entities and ask another annotator to check the quality. After that, we randomly select 10% of the dataset and ask the third annotator to check the accuracy. As a result, the overall averaged query-level accuracy 2 is 95%. The dataset will not be released due to user privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieving and Ranking</head><p>We use an internal E-commerce search engine for the E-commerce dataset. For the other datasets, we use Google Search as the search engine. Google Search is an off-the-shelf search engine and can simulate the offline search over various domains. We use summarized descriptions from the search results as the retrieved texts 3 . As Google Search limits the maximal length of searching queries to 32 words, we chunk a sentence into multiple sub-sentences based on punctuation if the sentence is longer than 30, feed each sub-sentence to the search engine, and retrieve up to 20 results. We filter the retrieved texts that contain any part of the datasets. Our reranking module selects top 6 relevant texts 4 as the external contexts of the input sentence and chunk the external contexts if the total sub-token lengths of the input sentence and external contexts exceeds 510.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Configurations</head><p>For the re-ranking module, we use Roberta-Large  for token representations which is the default configuration in the code 5 of BERTScore <ref type="bibr" target="#b54">(Zhang et al., 2020)</ref>. For token representations in the NER model, we use pretrained Bio-BERT <ref type="bibr" target="#b23">(Lee et al., 2020)</ref> for datasets from the biomedical domain and use XLM-RoBERTa <ref type="bibr" target="#b8">(Conneau et al., 2020)</ref> for datasets from other domains.</p><p>Training During training, we fine-tune the pretrained contextual embeddings by AdamW <ref type="bibr" target="#b28">(Loshchilov and Hutter, 2018)</ref> optimizer with a batch size of 4. We use a learning rate of 5 ? 10 ?6 to update the parameters in the pretrained contextual embeddings. For the CRF layer parameters, we use a learning rate of 0.05. We train the NER models for 10 epochs for the datasets in Social Media and Biomedical domains while we train the NER models for 5 epochs for other datasets for efficiency as these datasets have more training sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We experiment on the following approaches:</p><p>? LUKE is a very recent state-of-the art model on CoNLL-03 NER dataset proposed by <ref type="bibr" target="#b51">Yamada et al. (2020)</ref>. We use the same parameter setting as <ref type="bibr" target="#b51">Yamada et al. (2020)</ref> and use a single sentence as the input instead of taking document-level contexts in the dataset as in <ref type="bibr" target="#b51">Yamada et al. (2020)</ref> for fair comparison.</p><p>? W/O CONTEXT represents training the NER model without external contexts (Eq. 2), which is the baseline of our approaches.</p><p>? W/ CONTEXT represents training the NER model with external contexts (Eq. 3).</p><p>? CL-L 2 represents minimizing the L 2 distance between token representations (Eq. 5).</p><p>? CL-KL represents minimizing the KL divergence (Eq. 8) between CRF output distributions.</p><p>Besides, we also compare our approaches with previous state-of-the-art approaches over entity-level F1 scores 6 . During the evaluation, our approaches are evaluated using inputs without external contexts (W/O CONTEXT) and inputs with them (W/ CONTEXT). We report the results averaged over 5 runs in our experiments. The results are listed in <ref type="bibr">6</ref> We do not compare the results from previous work such as <ref type="bibr" target="#b52">Yu et al. (2020)</ref>; Luoma and Pyysalo (2020); <ref type="bibr" target="#b51">Yamada et al. (2020)</ref> that utilizes the document-level contexts in CoNLL-03 NER here. We conduct a comparison with these approaches in Appendix A. 2 7 . With the external contexts, our models with CL outperform previous state-of-the-art approaches on most of the datasets. Our approaches significantly outperform the baseline that is trained without external contexts with only one exception. Comparing with LUKE, our approaches and our baseline outperform LUKE in all the cases. The possible reason is that LUKE is pretrained only using long word sequences, which makes the model prone to fail to capture the information of entities based on short sentences 8 . For our approaches, with CL, the accuracy can be improved on both input views comparing with W/O CONTEXT and W/ CONTEXT, which shows adding constraints between the two views during training helps the model better utilize the original text information. For the two constraints in CL, we find that CL-KL is relatively stronger than CL-L 2 in a majority of the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-Domain Transfer</head><p>For cross-domain transfer, we train the models on the CoNLL-03 datasets, evaluate the accuracy on the CBS SciTech News dataset, and compare the results with those in <ref type="bibr" target="#b19">Jia et al. (2019)</ref>. We evaluate our approaches with each input view and the results are shown in <ref type="table" target="#tab_5">Table 3</ref>. Our approaches can improve the accuracy in cross-domain evaluation. The external contexts during evaluation can help to improve the accuracy of W/ CONTEXT. However, the gap between the two input views for the CL approaches is diminished. The observation shows that CL is able to improve the accuracy in crossdomain transfer for both views and eliminate the gap between the two views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Semi-supervised Cooperative Learning</head><p>Cooperative learning can take advantage of large amounts of unlabeled text for further improvement. We jointly train on the labeled data and unlabeled data in training to form a semi-supervised training manner. During training, we alternate between minimizing the loss (Eq. 9) for labeled data and the CL loss for unlabeled data (Eq. 4). We conduct the experiment on the E-commerce dataset as an exam-    <ref type="table">Table 4</ref>: A comparison between of CL approaches with and without semi-supervised learning. SEMI represents the approaches with semi-supervised learning. ? represents the approach is significantly (p &lt; 0.05) stronger than the approach without semi-supervised learning with the same input view. <ref type="table">Table 4</ref> show that the accuracy of both input views can be improved especially for the input without external contexts, which shows the effectiveness of CL in semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ple. Results in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>We use the WNUT-17 dataset in the analysis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison of Re-ranking Approaches</head><p>Various re-ranking approaches may affect the token representations of the model. We compare our approach with three other re-ranking approaches. The first is the ranking from the search engine without any re-ranking approaches. The second is reranking through a fuzzy match score. The approach has been widely applied in a lot of previous work <ref type="bibr" target="#b12">(Gu et al., 2018;</ref><ref type="bibr" target="#b53">Zhang et al., 2018;</ref><ref type="bibr" target="#b15">Hayati et al., 2018;</ref><ref type="bibr" target="#b50">Xu et al., 2020)</ref>. The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring. We train our models (W/ CONTEXT) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT-17 in <ref type="table" target="#tab_7">Table 5</ref>. Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with p &lt; 0.05). The possible reason might be that the tf-idf weighting gives high weights to irrelevant texts with rare words during re-ranking.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">How the Context Quality Affects Accuracy</head><p>We analyze how the NER model will perform when the quality of external contexts varies. We train and evaluate the NER model in four conditions with various contexts. The first one takes each dataset split as a document and encodes each sentence with document-level contexts. In this case, we encode the document-level contexts following the approach of <ref type="bibr" target="#b51">Yamada et al. (2020)</ref>. The second one uses GPT-2 <ref type="bibr" target="#b35">(Radford et al., 2019)</ref> to generate 6 relevant sentences as external contexts. The other two conditions randomly select from the retrieved texts or the dataset as external contexts. Results in <ref type="table" target="#tab_9">Table  6</ref> show that all these conditions result in inferior accuracy comparing with the model without any external context. However, our external contexts are more semantically relevant to the input sentence and helpful for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To show the effectiveness of CL, we conduct three ablation studies for our approach. The first one is training the NER model based on one view and predict on the other. The second is jointly training both views without the CL loss term (removing L CL (?) in Eq. 9). The final one is using both CL losses to train the model (L CL (?) = L CL-L 2 (?) + L CL-KL (?) in Eq. 9). Results in <ref type="table" target="#tab_10">Table 7</ref> show that the external context can help to improve the accuracy even when the NER model is trained without the contexts. However, when the model is trained with the external contexts, the accuracy of the model drops when predicting the inputs without external contexts. In joint training without CL, the accuracy of the model over inputs without contexts can be slightly improved but the accuracy over inputs with contexts drops, which shows the benefit of adding CL. For the model trained with both CL losses, we find no improvement over the models trained with a single CL loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Named Entity Recognition Named Entity Recognition <ref type="bibr" target="#b40">(Sundheim, 1995)</ref> has been studied for decades. Most of the work takes NER as a sequence labeling problem and applies the linear-chain CRF <ref type="bibr" target="#b21">(Lafferty et al., 2001)</ref> to achieve state-of-the-art accuracy <ref type="bibr" target="#b30">(Ma and Hovy, 2016;</ref><ref type="bibr" target="#b22">Lample et al., 2016;</ref><ref type="bibr" target="#b1">Akbik et al., 2018</ref><ref type="bibr" target="#b0">Akbik et al., , 2019</ref><ref type="bibr" target="#b45">Wang et al., 2020b)</ref>. Recently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings such as BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, Flair <ref type="bibr" target="#b1">(Akbik et al., 2018)</ref> and LUKE <ref type="bibr" target="#b51">(Yamada et al., 2020)</ref>. Very recent work <ref type="bibr" target="#b52">(Yu et al., 2020;</ref><ref type="bibr" target="#b51">Yamada et al., 2020)</ref> utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets <ref type="bibr" target="#b41">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b42">Tjong Kim Sang and De Meulder, 2003)</ref>.</p><p>Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation <ref type="bibr" target="#b12">(Gu et al., 2018;</ref><ref type="bibr" target="#b53">Zhang et al., 2018;</ref><ref type="bibr" target="#b50">Xu et al., 2020)</ref>, text generation <ref type="bibr" target="#b48">(Weston et al., 2018;</ref>, semantic parsing <ref type="bibr" target="#b14">(Hashimoto et al., 2018;</ref><ref type="bibr" target="#b13">Guo et al., 2019)</ref>. Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens.</p><p>For the re-ranking models, fuzzy match score <ref type="bibr" target="#b12">(Gu et al., 2018;</ref><ref type="bibr" target="#b53">Zhang et al., 2018;</ref><ref type="bibr" target="#b15">Hayati et al., 2018;</ref><ref type="bibr" target="#b50">Xu et al., 2020)</ref>, attention mechanisms <ref type="bibr" target="#b6">(Cao et al., 2018;</ref><ref type="bibr" target="#b5">Cai et al., 2019)</ref>, and dot products between sentence representations <ref type="bibr">(Lewis et al., 2020;</ref><ref type="bibr" target="#b50">Xu et al., 2020)</ref> are usual scoring functions to re-rank the retrieved texts. Instead, we use BERTScore to re-rank the retrieved texts instead as BERTScore evaluates semantic correlations between the texts based on pretrained contextual embeddings.</p><p>Multi-View Learning Multi-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training <ref type="bibr" target="#b3">(Blum and Mitchell, 1998</ref>) and co-regularization <ref type="bibr" target="#b37">(Sindhwani and Niyogi, 2005</ref>) train a separate model for each view. These approaches are semi-supervised learning techniques that require two independent views of the data. The model with higher confidence is applied to construct additional labeled data by predicting on unlabeled data. <ref type="bibr" target="#b39">Sun (2013)</ref> and <ref type="bibr" target="#b49">Xu et al. (2013)</ref> have extensively studied various multiview learning approaches. <ref type="bibr" target="#b17">Hu et al. (2021)</ref> shows the effectiveness of multi-view learning on crosslingual structured prediction tasks. Recently,  proposed Cross-View Training (CVT), which trains a unified model instead of multiple models and targets at minimizing the KL divergence between the probability distributions of the model and auxiliary prediction modules. Comparing with CVT, CL targets at improving the accuracy of two kinds of inputs rather than only one of them. We also propose to minimize the distance of token representations between different views in addition to KL-divergence. Besides, CL utilizes the external contexts and therefore we do not need to construct auxiliary prediction modules in the model. Moreover, CVT cannot be directly applied to our transformer-based embeddings. Finally, our decoding layer in the model uses the CRF layer instead of the simple Softmax layer as in CVT. The CRF layer is stronger but more difficult for KLdivergence computation.</p><p>Knowledge Distillation Knowledge distillation <ref type="bibr" target="#b4">(Bucilu? et al., 2006;</ref><ref type="bibr" target="#b16">Hinton et al., 2015)</ref> transfers the knowledge of "teacher" models to smaller "student" models through minimizing the KL divergence of prediction probability distribution between the models. In speech recognition <ref type="bibr" target="#b18">(Huang et al., 2018)</ref> and natural language processing <ref type="bibr" target="#b43">(Wang et al., 2020a</ref><ref type="bibr" target="#b46">(Wang et al., , 2021b</ref>, the marginal probability distribution of the linear-chain CRF layer has been applied to distill the knowledge between teacher models and student models. Comparing with these approaches, our approaches train a single unified model instead of transferring the knowledge between two models. We also show that the accuracy of both views can be improved with our approaches, unlike in knowledge distillation only the student model is updated and improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose to improve the NER model's accuracy by retrieving related contexts from a search engine as external contexts of the inputs. To improve the robustness of the models when no external contexts are available, we propose Cooperative Learning. Cooperative Learning adds constraints between two input views over either the token representations or label distributions of both input views to be consistent. Empirical results show that our approach significantly outperforms the baseline models and previous state-of-the-art approaches on the datasets over 5 domains. We also show the effectiveness of Cooperative Learning in a semi-supervised training manner.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Retrieved Contexts Versus Document-level contexts on CoNLL-03</head><p>We conduct a comparison between our retrieved contexts and the document-level contexts on CoNLL-03 datasets. In <ref type="table" target="#tab_12">Table 8</ref>, we report the best model on development set following <ref type="bibr" target="#b51">Yamada et al. (2020)</ref>. Comparing with previous state-of-the-art approaches with encoding document-level contexts, our approaches are competitive and even stronger than some of the previous approaches utilizing maximal document-level contexts. Comparing with our model trained on document-level contexts (W/ DOC CONTEXT), we find that there is still a gap between the document-level contexts and retrieved contexts but our CL approaches can reduce the gap between these two contexts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>An illustration of our NER model architecture. "[CLS]" and "[SEP]" are an example of cls token and sep token in the embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the dateset split, number of entity types and the average lengths with and without external contexts.</figDesc><table><row><cell></cell><cell cols="6"># Train # Dev # Test # Entity Labels Avg. Length Avg. Length w/ Context</cell></row><row><cell>WNUT-16</cell><cell>2,394</cell><cell>1,000</cell><cell>3,849</cell><cell>10</cell><cell>19.41</cell><cell>138.58</cell></row><row><cell>WNUT-17</cell><cell>3,394</cell><cell>1,009</cell><cell>1,287</cell><cell>6</cell><cell>18.48</cell><cell>139.49</cell></row><row><cell>CONLL-03</cell><cell>14,987</cell><cell>3,466</cell><cell>3,684</cell><cell>4</cell><cell>13.64</cell><cell>116.23</cell></row><row><cell>CONLL++</cell><cell>14,987</cell><cell>3,466</cell><cell>3,466</cell><cell>4</cell><cell>13.64</cell><cell>116.23</cell></row><row><cell>BC5CDR</cell><cell>4,560</cell><cell>4,581</cell><cell>4,797</cell><cell>2</cell><cell>25.91</cell><cell>144.13</cell></row><row><cell>NCBI</cell><cell>5,424</cell><cell>923</cell><cell>940</cell><cell>1</cell><cell>25.01</cell><cell>135.76</cell></row><row><cell>E-COMMERCE</cell><cell>38,959</cell><cell>5,000</cell><cell>5,000</cell><cell>26</cell><cell>2.54</cell><cell>124.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>A comparison among recent state-of-the-art models, the baseline and our approaches. ? represents the model is significantly stronger than the baseline model (W/O CONTEXT) with p &lt; 0.05 on Student's T test.</figDesc><table><row><cell></cell><cell>Evaluation</cell><cell></cell></row><row><cell></cell><cell cols="2">Science and Technology</cell></row><row><cell>Approach</cell><cell cols="2">W/O CONTEXT W/ CONTEXT</cell></row><row><cell>Jia et al. (2019)</cell><cell>73.59</cell><cell>-</cell></row><row><cell>W/O CONTEXT</cell><cell>75.87</cell><cell>75.74</cell></row><row><cell>W/ CONTEXT</cell><cell>75.72</cell><cell>75.94</cell></row><row><cell>CL-L2</cell><cell>76.16</cell><cell>76.10</cell></row><row><cell>CL-KL</cell><cell>76.37</cell><cell>76.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">: A comparison of different approaches in trans-</cell></row><row><cell cols="3">fer learning. The models are trained on the CoNLL-03</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Evaluation</cell><cell></cell></row><row><cell>Approach</cell><cell cols="2">W/O CONTEXT W/ CONTEXT</cell></row><row><cell>CL-L2</cell><cell>82.43</cell><cell>83.87</cell></row><row><cell>CL-KL</cell><cell>82.31</cell><cell>83.99</cell></row><row><cell>CL-L2+SEMI</cell><cell>82.88  ?</cell><cell>83.92</cell></row><row><cell>CL-KL+SEMI</cell><cell>82.58  ?</cell><cell>84.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>A comparison of different re-ranking ap- proaches by the F1 scores on WNUT-17. SE: Search engine. FM: Fuzzy match score. BS: BERTScore.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>A comparison among different contexts types.</figDesc><table><row><cell></cell><cell>Evaluation</cell><cell></cell></row><row><cell>Approach</cell><cell cols="2">W/O CONTEXT W/ CONTEXT</cell></row><row><cell>W/O CONTEXT</cell><cell>57.86</cell><cell>59.40</cell></row><row><cell>W/ CONTEXT</cell><cell>57.46</cell><cell>60.20</cell></row><row><cell>W/O CL</cell><cell>58.14</cell><cell>59.64</cell></row><row><cell>CL-L2 + CL-KL</cell><cell>58.69</cell><cell>60.16</cell></row><row><cell>CL-L2</cell><cell>58.68</cell><cell>60.26</cell></row><row><cell>CL-KL</cell><cell>59.33</cell><cell>60.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>An ablation study of the training and prediction of models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>A comparison of retrieved contexts and document-level contexts. ?: These approaches are trained on training and development sets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">the accuracy of a query counts 1.0 if all the entities in the query are correctly recognized and 0.0 otherwise.3 If the descriptions are not available, we use the titles of the results instead.4  We determined that 6 is a reasonable number based on preliminary experiments. 5 https://github.com/Tiiiger/bert_score</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">For the result of Bio-BERT<ref type="bibr" target="#b23">(Lee et al., 2020)</ref> on NCBIdisease dataset, we report the results reported in official code (https://github.com/dmis-lab/biobert). The results (89.71 in NCBI-disease) reported in the paper used token-level F1 score instead of entity-level F1 score.8  We have confirmed with the authors of LUKE<ref type="bibr" target="#b51">(Yamada et al., 2020)</ref> that the accuracy on the CoNLL-03 dataset is consistent with their experimental results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Natural Science Foundation of China (61976139) and by Alibaba Group through Alibaba Innovative Research Program. We thank Kaibo Zhang for his help in crawling related texts from Google Search and thank Jiong Cai and Zhuo Chen for their comments and suggestions on writing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pooled contextualized embeddings for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1078</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota. As</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="724" to="728" />
		</imprint>
		<respStmt>
			<orgName>Long and Short Papers</orgName>
		</respStmt>
	</monogr>
	<note>sociation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cloze-driven pretraining of self-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1539</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5360" to="5369" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150464</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retrievalguided dialogue response generation via a matchingto-generation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1195</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1866" to="1875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Retrieve, rerank and rewrite: Soft template based neural summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="152" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence modeling with cross-view training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1217</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1914" to="1925" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Results of the WNUT2017 shared task on novel and emerging entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marieke</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4418</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy User-generated Text</title>
		<meeting>the 3rd Workshop on Noisy User-generated Text<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="140" to="147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ncbi disease corpus: a resource for disease name recognition and concept normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Rezarta Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Search engine guided neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coupling retrieval and metalearning for context-dependent semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="855" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A retrieve-and-edit framework for predicting structured outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10073" to="10083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Retrieval-based neural code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Shirley Anugrah Hayati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravalika</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Avvaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Tomasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1111</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="925" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-View Cross-Lingual Structured Prediction with Minimum Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge distillation for sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhehuai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-1589</idno>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3703" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Crossdomain NER using cross-domain language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1236</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2464" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Retrieval-augmented controllable review generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyeok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungtaek</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Kim Amplayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2284" to="2295" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA. Morgan</addrLine></address></meeting>
		<imprint>
			<publisher>Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11401</idno>
		<title level="m">Tim Rockt?schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database: The Journal of Biological Databases and Curation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dice loss for dataimbalanced NLP tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.45</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring cross-sentence contexts for named entity recognition with BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jouni</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="904" to="914" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">BERTweet: A pre-trained language model for English tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Named entity recognition for social media texts with semantic augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhe</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.107</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1383" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reinforcement-based denoising of distantly supervised NER with partial annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhad</forename><surname>Nooralahzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Tore</forename><surname>L?nning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilja</forename><surname>?vrelid</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</title>
		<meeting>the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Bioflair: Pretrained pooled contextualized embeddings for biomedical sequence labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Daniel</surname><genName>Jr</genName></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A coregularized approach to semi-supervised learning with multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML Workshop on Learning with Multiple Views</title>
		<meeting>the ICML Workshop on Learning with Multiple Views</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Citeseer</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Results of the WNUT16 named entity recognition shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bethany</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)</title>
		<meeting>the 2nd Workshop on Noisy User-generated Text (WNUT)<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="138" to="144" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A survey of multi-view machine learning. Neural computing and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2031" to="2038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Named entity task definition, version 2.1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><forename type="middle">M</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Message Understanding Conference</title>
		<meeting>the Sixth Message Understanding Conference</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="319" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Structure-level knowledge distillation for multilingual sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.304</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3317" to="3330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automated Concatenation of Embeddings for Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021). Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">More embeddings, better sequence labelers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Zhongqiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Structural Knowledge Distillation: Tractably Distilling Information for Structured Predictor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021). Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">CrossWeigh: Training named entity tagger from imperfect annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5154" to="5163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Retrieve and refine: Improved sequence generation models for dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5713</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</title>
		<meeting>the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.5634</idno>
		<title level="m">A survey on multi-view learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Boosting neural machine translation with similar translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.144</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1580" to="1590" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">LUKE: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Named entity recognition as dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.577</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6470" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Guiding neural machine translation with retrieved translation pieces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1120</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1325" to="1335" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dual adversarial neural transfer for low-resource named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick Siow Mong</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Kwok</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1336</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3461" to="3471" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
