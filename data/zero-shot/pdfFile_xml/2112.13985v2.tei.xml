<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LatteGAN: Visually Guided Language Attention for Multi-Turn Text-Conditioned Image Manipulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoya</forename><surname>Matsumori</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Keio University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Abe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Keio University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosuke</forename><surname>Shingyouchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Keio University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Komei</forename><surname>Sugiura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Keio University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michita</forename><surname>Imai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Keio University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LatteGAN: Visually Guided Language Attention for Multi-Turn Text-Conditioned Image Manipulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-guided image manipulation tasks have recently gained attention in the vision-and-language community.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advancements in deep generative models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> have pushed the boundary of limitations in vision and language studies to a great extent <ref type="bibr" target="#b2">[3]</ref>. Text-guided image generation tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> are among the prominent topics of study in this discipline. This topic has a wide range of applications spanning education, entertainment, computersupported design, and human-robot collaboration. In particular, in a human-robot interaction scenario <ref type="bibr" target="#b5">[6]</ref>, this technol- <ref type="figure">Figure 1</ref>. Overview of the Generative Neural Visual Artist (GeNeVA) task. In this task, a Drawer iteratively modifies an image according to a sequence of instructions given by a Teller. The objective of this task is to build a Drawer model that can perform pixel-wise generation of an image. ogy enables the visualization of the consequence of future actions and the internal beliefs of a robot <ref type="bibr" target="#b6">[7]</ref>.</p><p>One of the most prominent studies on interactive textbased image manipulation was by El-Nouby et al. <ref type="bibr" target="#b7">[8]</ref>. This research introduced a multi-turn text-conditioned image manipulation (MTIM) task, namely, the GeNeVA task ( <ref type="figure">Fig. 1)</ref>, whose objective is to iteratively manipulate an image according to a given sequence of instructions and a previously generated image. The GeNeVA-GAN architecture they proposed involves recurrent structures in which objects are successfully generated on a background and then simple transformations are applied to existing objects. However, that approach suffers from the under-generation of instructed objects, resulting in a low recall rate. This is especially problematic for iterative manipulation, since instructions often refer to previously generated images.</p><p>To overcome that problem, we propose a Visually Guided Language Attention GAN (LatteGAN), a novel Generative Adversarial Network (GAN) architecture for multi-turn text-conditioned image manipulation, which is accompanied by a Visually Guided Language Attention (Latte) module, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. The Latte module exploits a source-target attention structure that takes word embeddings as keys and values and takes relational visual representations as queries. With this structure, spatially variant fine-grained instruction representations, which store a specific instruction feature at a specific location, can be extracted. Additionally, a Text-Conditioned U-Net discriminator is introduced. This approach enables both global-and local-level discrimination of images by using a global textconditioned loss to verify whether an image was modified as instructed and a local unconditioned loss to evaluate the image at the local level.</p><p>In summary, the contributions of this paper are twofold:</p><p>? We propose Visually Guided Language Attention GAN (LatteGAN), a multi-turn text-conditioned image generation GAN accompanied by two key components: (1) a Latte module that can extract the finegrained instruction representations that are crucial for image modification; and (2) a Text-Conditioned U-Net discriminator that can discriminate images on the basis of both their modification and their quality.</p><p>? We conducted experiments on two different MTIM tasks: CoDraw <ref type="bibr" target="#b8">[9]</ref> and i-CLEVR <ref type="bibr" target="#b7">[8]</ref>. Our experimental results demonstrate that the proposed approach can outperform the baseline method, showing significant effectiveness in the multi-turn text-conditioned image generation task. Additionally, the effectiveness of our approach was validated by ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Conditional image generation. There is an extensive amount of research in the field of image synthesis using GANs <ref type="bibr" target="#b1">[2]</ref>. Conditional image generation, an approach that takes extra inputs to guide the output, is one of the most studied topics in this field. Various kinds of conditioning have been examined, such as discrete labels <ref type="bibr" target="#b9">[10]</ref>, images <ref type="bibr" target="#b10">[11]</ref>, and masks <ref type="bibr" target="#b11">[12]</ref>.</p><p>Text-conditioned image generation. This setting, which takes a natural language text as an input, has also gained attention recently <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. One of the first attempts to use GAN models in this field was presented in the work by Reed et al. <ref type="bibr" target="#b3">[4]</ref>. Following that, Zhang et al. <ref type="bibr" target="#b15">[16]</ref> proposed Stack-GAN, which uses two-stage generation consisting of lowresolution generation and high-resolution refinement. Hong et al. <ref type="bibr" target="#b16">[17]</ref> presented a three-stage generation approach consisting of bounding-box generation, mask generation, and image generation. Xu et al. <ref type="bibr" target="#b4">[5]</ref> proposed AttnGAN, which incorporates an attention module using fine-grained representations obtained from word embeddings in addition to a text embedding.</p><p>Text-conditioned image manipulation. This approach aims to semantically manipulate an image rather than create it from scratch. Similarly to Xu et al. <ref type="bibr" target="#b4">[5]</ref>, Nam et al. <ref type="bibr" target="#b17">[18]</ref> introduced TAGAN, which includes word-level local discriminators. ManiGAN <ref type="bibr" target="#b18">[19]</ref> used two modules to manipulate only related regions that match text descriptions. Another branch of research aims to perform manipulations based on complex instructions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> that include specifications for the target location (where), the target object (what), and the target operation type (how).</p><p>Multi-turn text-conditioned image manipulation. While most of the above studies are related to text-conditioned image manipulation concentrated on a single turn of generation, our research target is multi-turn text-guided image generation (MTIM). El-Nouby et al. <ref type="bibr" target="#b7">[8]</ref> first introduced the MTIM task known as Generative Neural Visual Artist (GeNeVA; <ref type="figure">Fig. 1</ref>), accompanied by two distinct datasets, CoDraw <ref type="bibr" target="#b8">[9]</ref> and i-CLEVR <ref type="bibr" target="#b7">[8]</ref>. Fu et al. <ref type="bibr" target="#b21">[22]</ref> introduced the self-supervised counterfactual reasoning (SSCR) framework to overcome data scarcity in the MTIM task.</p><p>Limitations. Although the previous works have successfully pioneered this field of study, some limitations remain to be addressed. The most prominent is the fact that current methods often overlook manipulation instructions and fail to generate objects. This is particularly problematic for the MTIM task, as generating an image at a certain step often involves reference to the previous images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task Specifications</head><p>The GeNeVA task is a multi-turn text-conditioned image generation (MTIM) task. It involves two participants: a Teller that instructs how to modify the image, and a Drawer that draws the image according to the Teller's instructions. The final goal of this task is to train the Drawer model. v is given to the Drawer. In the following turns, the Drawer takes as input its own previously generated imagex</p><formula xml:id="formula_0">(t?1) v .</formula><p>To achieve this goal, the Drawer model needs to fulfill the following three requirements. (1) It must comprehend the natural language instructions and map them to the actual canvas. This process includes visually grounded language understanding, such as referring expression comprehension (e.g., "Put a pine tree to the left of the girl"). Additionally, the model has to (2) perform pixel-based image generation according to instructions. This may involve training a generative model such as a GAN. Finally, the model needs to (3) generate images several times in accordance with a dialogue. Unlike in previous text-conditioned image generation tasks, the model is supposed to generate several intermediate results in an auto-regressive manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LatteGAN</head><p>Architecture. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates the overview of LatteGAN. Its main components are (1) the Latte module in the generator and (2) the Text Conditioned U-Net discriminator. (1) The Latte module consists of a Relational Visual Feature Extractor, which extracts relational visual features from the source image, and a source-target-attention structure, which attends to the necessary instruction text tokens queried via the relational visual features. This structure efficiently extracts fine-grained features. (2) The Text-Conditioned U-Net discriminator consists of a global text-conditioned discriminator and a local-level unconditioned discriminator. By globally discriminating the manipulation based on the instructions and locally discriminating the image, this struc-ture verifies that the generated image is based on the instructions and has high quality.</p><p>Inputs and output. At timestep t, there are two inputs to the model: an image input x src =x</p><formula xml:id="formula_1">(t?1) v and a raw-text input x l = x (t) l . The output is a generated imagex tgt = x (t) v .</formula><p>The transition between timesteps t ? 1 and t will be explained in the following part; for the sake of clarity, the notation of timestep t is omitted unless it is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generator</head><p>Overview. The generator produces a target imagex tgt = G(x src , e, e, z), where z ? N (0, I) ? R Nz is a noise vector whose size is N z . A sentence embedding e ? R E , where E is the size of an embedding vector, and a set of word embeddings e = [e i |i = 1, ..., L] ? R L?E , where L is the length of the embeddings, are obtained from an embedding function f text given a history of instructions as (e, e) = f text (x</p><formula xml:id="formula_2">(t) l , x (1:t?1) l ).</formula><p>More specifically, a computation flow of the generator G is described as follows. First, a visual feature map h = G image (x src ) ? R C h ?H?W and a text feature map u = G text (e, z) ? R Cu?H?W are computed, where H and W are respectively the height and width of the feature maps, and C h and C u are respectively the channel sizes of the feature maps. Then, both visual and textual feature maps are fused to produce a target feature map h out = G fusion (h, u, e) ? R C h ?H?W . Finally, a target image is generated asx tgt = G dec (h out , e). The G fusion , which is the most important block of our model, is made up of the modules described in the rest of this section. </p><formula xml:id="formula_3">h (W ) i = 1 W W j=1 h ij , u (W ) i = 1 W W j=1 u ij .<label>(1)</label></formula><p>Then, a relational visual representation vector r</p><formula xml:id="formula_4">(W ) i is ob- tained by r (W ) i = H k=1 F rel ([h (W ) i + p (W ) i , h (W ) k + p (W ) k , u (W ) i ]),<label>(2)</label></formula><p>where p (W ) k is a trainable positional embedding for the k-th position and F rel consists of linear layers with an activation function. We also obtain a height-wise pooled relational visual representation vector r (H) j in the same way. Finally, we get a relational visual representation map r ? R Cr?H?W as follows:</p><formula xml:id="formula_5">r = r (HW ) * W r ,<label>(3)</label></formula><formula xml:id="formula_6">r (HW ) ij = [r (H) j , r (W ) i ],<label>(4)</label></formula><p>where * represents the convolution operator with kernel W r . Source-target-attention structure. This structure extracts spatially variant text features that properly reflect the instructions ( <ref type="figure" target="#fig_2">Fig. 4 middle)</ref>. We use a source-target attention structure in which the source is a tuple of word embeddings and a sentence embedding? = (e, e), and the target is the relational visual representation map r. Given? ? R L+1?E , a key-value pair (K, V ) is generated as follows: </p><formula xml:id="formula_7">K = W K? , V = W V? ,<label>(5)</label></formula><formula xml:id="formula_8">Q = W Qr ,<label>(6)</label></formula><p>where W Q ? R d model ?Cr is a weight for the query. The output feature is obtained by the scaled-dot-product attention <ref type="bibr" target="#b22">[23]</ref> as follows:</p><formula xml:id="formula_9">g = V softmax( K Q ? d model ),<label>(7)</label></formula><p>where d model is a scaling factor. We extend this attention mechanism to a multi-head version (eight heads) as in Vaswani et al. <ref type="bibr" target="#b22">[23]</ref>. Finally, the output vector g ? R d model ?H?W is obtained by reshapingg. Semantic synthesis module. Given the image vector h, the global text feature u, and the fine-grained text feature vector g, the semantic synthesis module produces a modified representation h out for image generation <ref type="figure" target="#fig_2">(Fig. 4 right)</ref>. Following Vo et al. <ref type="bibr" target="#b23">[24]</ref> and Kenan et al. <ref type="bibr" target="#b19">[20]</ref>, we introduce a text-image residual gating structure. The key design idea of this structure is to modify only the area designated in the instruction and leave unrelated areas as they are. The modified representation h out is obtained by applying the gate structure to the residual representation h res and the original representation h:</p><formula xml:id="formula_10">h out = w gate (h m) + w res (h res (1 ? m)),<label>(8)</label></formula><p>where denotes the Hadamard product, and w gate and w res are trainable scaling parameters. In the residual path, the images are normalized in a spatially adaptive manner as follows:</p><formula xml:id="formula_11">h res = F sres (h, m p ),<label>(9)</label></formula><formula xml:id="formula_12">m p = [h, u, g] * W 1 , m = ?(m p * W 2 ),</formula><p>where F sres consists of SPADE residual blocks <ref type="bibr" target="#b24">[25]</ref>. The gating matrix m is generated by applying two convolution operations and the sigmoid activation ?(?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discriminator</head><p>Inspired by Schonfeld et al. <ref type="bibr" target="#b25">[26]</ref>, we introduce the Text-Conditioned U-Net discriminator D, which has an image encoder D enc , a local-level unconditional discriminator D local , and a global-level text-conditional discriminator D global . Given a real image input x tgt , the output of the local-level discriminator is</p><formula xml:id="formula_13">d local = D local (D enc (x tgt )),<label>(10)</label></formula><p>where D enc and D local respectively denote the U-Net encoder and decoder. The shape of d local is the same as the shape of x tgt except for the channel size, which is one. The global-level text-conditional discriminator takes a source image x src and the real target image x tgt as inputs and yields a scalar,</p><formula xml:id="formula_14">d global = D global ((D enc (x tgt ) ? D enc (x src )), e),<label>(11)</label></formula><p>where D global is the encoder that takes a visual feature difference and a sentence vector e for projection <ref type="bibr" target="#b26">[27]</ref>. For discriminating a generated image, x tgt should be replaced byx tgt in Eqs. <ref type="formula" target="#formula_3">(10)</ref> and <ref type="formula" target="#formula_3">(11)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Objectives</head><p>The training steps are divided into two phases: a pretraining phase and an adversarial training phase. In the pretraining phase, the goal is to tune the text-embedding module f text by minimizing the 1 loss, which is defined as</p><formula xml:id="formula_15">L pretrain = (xsrc,x l ,xtgt)?D |f image (x tgt ) ?? tgt |, (12) h tgt = f fusion (f image (x src ), f text (x l )),</formula><p>where D is a set of triplets formed by a source image, an instruction, and a target image, and f fusion consists of a source-target-attention structure and a text-image residual gating structure. Once f text is pretrained, we freeze its weights in the following training.</p><p>For the adversarial training, the loss for the discriminator is defined as the sum of two terms:</p><formula xml:id="formula_16">L (global) D = L D real + 1 2 (L D fake + L Dwrong ) + ?L aux ,<label>(13)</label></formula><formula xml:id="formula_17">L (local) D = L D real + L D fake ,</formula><p>where L D real and L D fake are respective losses for real and generated detection, L Dwrong is a loss for wrongly matched image-text input pairs of real samples, and L aux is an auxiliary loss with a loss-scaling factor ?. We predict the added object in the auxiliary task, which is formalized as multi-label classification. The adversarial loss terms in the discriminator loss are defined as the adversarial hinge loss <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. The loss for the generator is defined in the same way as the discriminator loss. More details about the training are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets.</head><p>For a fair comparison, we mostly followed the GeNeVA task experiment settings <ref type="bibr" target="#b7">[8]</ref> and adopted the same two datasets and evaluation metrics * .</p><p>We used two datasets for the GeNeVA task: CoDraw [9] and i-CLEVR <ref type="bibr" target="#b7">[8]</ref>. Both datasets consist of scenes containing a series of images with corresponding instructions. The statistics of the CoDraw and i-CLEVR datasets are presented in <ref type="table" target="#tab_1">Table 1</ref>. CoDraw. CoDraw is a clip-art-like dataset collected using Amazon Mechanical Turk. The images consist of combinations of 58 unique objects (such as a boy, girl, and tree) on a static background image consisting of a grassy yard with a blue sky. The object instances in an image can take various poses and sizes that are different from the clip-art template. The instructions consist of conversations between the Teller and Drawer in natural language. In the conversation, the Drawer sequentially updates the image according to the Teller's instructions. The Drawer can ask the Teller a question to clarify an instruction.</p><p>We preprocessed this dataset in the same way as the prior work <ref type="bibr" target="#b7">[8]</ref>. We concatenated consecutive turns into a single turn until a new object was added or removed by the Drawer. This means that, on every turn, at least one part of an image has been modified from the previous turn. * All materials, including the datasets and pretrained weights of GeNeVA-GAN, are available online at https: //www.microsoft.com/en-us/research/project/ generative-neural-visual-artist-geneva/.  <ref type="table">Table 2</ref>. Quantitative comparison and ablation studies. The methods are compared on the CoDraw and i-CLEVR datasets in accordance with four metrics: the average precision (AP), average recall (AR), F1-score, and relational similarity (RSIM). For each metric and dataset, the best scores are in bold, and the second best ones are underlined. The mean and standard deviation have been calculated on five different seed runs.</p><p>Some instructions are also composed of multiple Teller instructions or Drawer reply messages as a result of the preprocessing. We inserted the special tokens "&lt;teller&gt;" and "&lt;drawer&gt;" at the beginning of each one's sentences. To fix any spelling mistakes , we applied the Bing Spell Check API ? to all sentences in the dataset. Data splitting was carried out in the same way as in prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>, with 8K for training, 1K for validation, and 1K for testing. i-CLEVR. i-CLEVR is a synthetic dataset generated using the CLEVR <ref type="bibr" target="#b29">[30]</ref> engine. Each scene contains five imageinstruction pairs. Starting from a background image, new objects are added sequentially in a scene. Objects can take several attributes, including three shapes and eight colors. The instructions are generated via a scene graph. Except on the first turn, which asks to add an object at the center, all instructions in the i-CLEVR dataset include positional referring expressions (e.g., "in front of the red cube") and contextual referring expressions such as "it". Data splitting was carried out in the same way as in prior work <ref type="bibr" target="#b7">[8]</ref>, with 6K for training, 2K for validation, and 2K for testing. Implementation details. The network architecture for the generator and discriminator followed the ResBlocks architecture as used by Miyato et al. <ref type="bibr" target="#b28">[29]</ref>. We used instance normalization <ref type="bibr" target="#b30">[31]</ref> and AdaIN <ref type="bibr" target="#b31">[32]</ref>. For training, we applied teacher forcing by using the ground-truth image x</p><formula xml:id="formula_18">(t?1) v in- stead of the generated imagex (t?1) v</formula><p>; however, we used x (t?1) v during testing. We trained all models until convergence with 400 epochs (or until 72 hours had elapsed) without early-stopping. We chose the weights of the last step to provide the final models. More details are provided in the Appendix. ? https://azure.microsoft.com/en-us/services/ congitive-services/spell-check/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Evaluation</head><p>Evaluation metrics. We adopted the same evaluation metrics used in the previous study <ref type="bibr" target="#b7">[8]</ref> to ensure a fair comparison. The metrics were (i) the object presence matches and (ii) the object position relation matches between the groundtruth and generated images.</p><p>The metrics for (i) were the average precision (AP), average recall (AR), and F1 score between the detection results for both the ground-truth and generated images. The AP, AR, and F1 were calculated for each scene. Note that, in the CoDraw and i-CLEVR datasets, more than two instances of the same category do not appear in the same scene. Thus, the object detection could be formulated as a multi-label classification.</p><p>As for (ii), we used the relational similarity (RSIM) <ref type="bibr" target="#b7">[8]</ref>, which evaluates the arrangement of objects. The RSIM is defined as</p><formula xml:id="formula_19">RSIM(E Ggt , E Ggen ) = recall ? |E Ggt ? E Ggen | |E Ggt | ,<label>(14)</label></formula><p>where "recall" indicates the recall over the objects detected in the generated image w.r.t. the objects detected in the ground-truth image. E Ggt is the set of relational edges for the ground-truth image that correspond to vertices that are common to both images. Similarly, E Ggen is the set of relational edges for the generated image that correspond to the vertices in common to both images. We used the last image of each scene to calculate the RSIM.</p><p>To obtain the detection and localization results for both the ground-truth and generated images, we used a pretrained Inception object detector and localizer available on- <ref type="figure">Figure 5</ref>. Qualitative comparison for CoDraw (left) and i-CLEVR (right) datasets. Each side shows the instructions, the ground truth samples from the datasets (GT), the generated images by our LatteGAN (Ours), and the generated images by GeNeVA-GAN <ref type="bibr" target="#b7">[8]</ref>. The generated images were produced in an auto-regressive manner from a background image. We have omitted the images from the first turn for i-CLEVR ("Add a red sphere at the center") due to space limitations. Note that the generated images can be considered acceptable even if there is a difference from the GT image, due to the ambiguities induced by the instructions. line for CoDraw ? and for i-CLEVR ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline comparison.</head><p>We compared our model with four baselines: GeNeVA-GAN <ref type="bibr" target="#b7">[8]</ref>, GeNeVA-GAN ? , which represents re-implemented results ? reported by <ref type="bibr" target="#b21">[22]</ref>, SSCR <ref type="bibr" target="#b21">[22]</ref>, and TIRG <ref type="bibr" target="#b19">[20]</ref>. Since TIRG is a non-iterative image manipulation model, we made some changes to fit into the iterative scheme; for a fair comparison, we implemented it based on the (w/o Latte&amp; TxtCond) settings explained in the ablation studies. <ref type="table">Table 2</ref> quantitatively compares our model with the baselines on the basis of the AP, AR, F1, and RSIM scores on two different datasets. We can see that the F1 and RSIM scores for the CoDraw dataset obtained by LatteGAN were respectively 77.51 and 54.16, which outperformed the baselines at great margins. Our model also outperformed the baselines on the other two metrics. The LatteGAN results ? https://figureqadataset.blob.core.windows. net/live-dataset/GeNeVA-models/codraw_inception_ best_checkpoint.pth?st=2019-08-16T20%3A33% 3A41Z&amp;se=3019-08-17T20%3A33%3A00Z&amp;sp=rl&amp; sv=2018-03-28&amp;sr=b&amp;sig=0aheTm73x5pHzwde% 2FRcRBBfrBgVRhE1uxVB4kwk8k7g%3D ? https://figureqadataset.blob.core.windows. net/live-dataset/GeNeVA-models/iclevr_ inception_best_checkpoint.pth?st=2019-08-16T20% 3A34%3A22Z&amp;se=3019-08-17T20%3A34%3A00Z&amp;sp= rl&amp;sv=2018-03-28&amp;sr=b&amp;sig=U9eRRPZHoZDOLO% 2FWYnNAZ9attfFJKlGo28ZX7D%2BTIDk%3D ? GeNeVA-GAN ? is reported since we also found that the GeNeVA-GAN results could not be reproduced following the official implementation provided on github (https://github.com/Maluuba/GeNeVA).</p><p>for the i-CLEVER dataset were 97.26 for F1 and 83.21 for RSIM scores, which is also the best performance among all compared models.</p><p>In summary, our model (LatteGAN) significantly outperformed the baselines on both datasets. In particular, the ARs were greatly improved, which boosted the F1 and RSIM scores (by their definitions). This indicates that our approach has effectively reduced the under-generation of objects. Ablation studies. As shown in <ref type="table">Table 2</ref>, five ablation conditions were tested. Three of the following ablations were conducted by removing two major components, the Latte module and the Text-Conditioned U-Net Discriminator: (1) a condition without the Latte module (w/o Latte), in which the concatenation of the source image feature h and the global text representation u was simply fed to the semantic synthesis function F SS ; (2) a condition in which the Text-Conditioned U-Net discriminator was replaced with a single global text-conditioned discriminator (w/o TxtCond); and (3) a condition combining (1) and (2) (w/o Latte &amp; TxtCond). Additionally, to provide more insights on the Latte module, we tested two conditions by removing its two sub-components based on (2): (4) a condition without the Relational Feature Module (RFE) of the Latte Module; and (5) a condition without the Source Target Attention (STA) of the Latte Module. <ref type="table">Table 2</ref> shows that the Latte module improved on all of the metrics for CoDraw, especially the AR (by 4.46 points) and the RSIM (by 6.00 points). The improvement of the AR indicates that the Latte module can successfully extract information that cannot be obtained via a global sentence vector. The Text-Conditioned U-Net discriminator improves the precision by 2.46 when it is used along with the Latte module. This indicates that by discriminating images at the local level, the Text-Conditioned U-Net discriminator ensures both the appropriateness of manipulation and the quality of synthesized objects.</p><p>As for the i-CLEVR dataset, for which the results were almost saturated, the effect of ablation was less significant. However, the Text-Conditioned U-Net discriminator contributed to the local image quality; as a result, it improved all of the metrics.</p><p>The results also show that both the RFE and the STA improved performances. The RFE had a greater improvement on the i-CLEVR, which frequently includes referring expressions, increasing the RSIM by <ref type="bibr" target="#b0">1</ref>  <ref type="figure">Fig. 5</ref> shows the qualitative results obtained by our model and the baseline on the CoDraw and i-CLEVR datasets, displayed in tiles. The left, middle, and right columns show the ground-truth images, the images generated by our model, and those generated by the baseline, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Evaluation</head><p>In the fourth row (t = 4) of the CoDraw results, the Drawer was instructed to draw "a sad girl facing left"; however, the baseline generated an image of a girl facing right, while our model successfully generated a correct image. Moreover, the baseline did not clearly draw the boy's head and legs at t = 3, and the image collapsed. In comparison, our model generated better images in general, except for t = 1, where the apple tree trunk was not drawn.</p><p>In the first row (t = 2) of the i-CLEVR results, the Drawer was instructed to draw "a brown cube"; however, the baseline generated a red sphere, while our model successfully generated an image of a brown cube at the desired position. Similarly, the baseline added wrong objects (red spheres) for t = 3 and t = 4, while our model successfully added the instructed objects at every step. <ref type="figure">Fig. 6</ref> presents examples of failed samples produced by LatteGAN. In CoDraw, LatteGAN tended to fail to generate details, such as the posture and facial expressions of people. In i-CLEVR, LatteGAN failed in a very limited situation <ref type="figure">Figure 6</ref>. Example failures of LatteGAN. CoDraw: Failed to generate a facial expression as instructed; a surprised face was generated instead. i-CLEVR: At t = 3, the red sphere was placed in a slightly wrong position because the instruction at t = 2 was ambiguous.</p><p>where it followed the past instructions correctly but could not follow the next because there was no space left to add an object. This is an inevitable error that stems from the data including ambiguous instructions. However, we believe we can mitigate this problem in the future by retaining multiple generated results and selecting an appropriate image on each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented LatteGAN, a novel GAN architecture for MTIM. The Latte module is implemented to feed the fine-grained text representations to the generator. We also proposed a Text-Conditional U-Net discriminator for handling not only text-conditioned global representations but also text-unconditioned local representations to improve the distinguishability of generated objects. Experimental results demonstrated that our model outperformed several baselines on both the CoDraw and i-CLEVR datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. Training Details</head><p>Loss of the discriminator. The loss for the discriminator is defined as follows:</p><formula xml:id="formula_20">L D = L (global) D + L (local) D ,<label>(15)</label></formula><formula xml:id="formula_21">L (global) D = L D real + 1 2 (L D fake + L Dwrong ) + ?L aux , L (local) D = L D real + L D fake ,</formula><p>where the auxiliary loss function for L aux is defined as a binary cross entropy over the added object at the current timestep. The adversarial loss terms in Eq. (15) are defined as the adversarial hinge loss <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> as follows:</p><formula xml:id="formula_22">L D real = ?E (xv,x l )?p data [min(0, ?1 + D(x v , x l ))] ,<label>(16)</label></formula><formula xml:id="formula_23">L D fake = ?E z?N ,x l ?p data [min(0, ?1 ? D(G(z, x l ), x l ))] ,<label>(17)</label></formula><formula xml:id="formula_24">L Dwrong = ?E (xv,x l )?p data [min(0, ?1 ? D(x v ,x l ))] ,<label>(18)</label></formula><p>where D and G are respectively a discriminator and generator function, p data is a dataset, N is the standard Gaussian distribution, andx l is a wrong instruction. In computing L (global) D , a discriminator D will return its decision at the global level, while in L (local) D , it will return its decision at the local level <ref type="bibr" target="#b25">[26]</ref>. Loss of the generator. The loss for the generator is defined as</p><formula xml:id="formula_25">L G = L (global) G + L (local) G ,<label>(19)</label></formula><formula xml:id="formula_26">L (global) G = L G fake + ?L aux , L (local) G = L G fake ,</formula><p>where L G fake is the adversarial hinge loss</p><formula xml:id="formula_27">L G fake = ?E [d fake ] ,<label>(20)</label></formula><p>where the scalar d fake corresponds to the output of the discriminator given a generated sample, and belongs to d local or d global depending which loss (L (global) G or L (local) G ) is computed. Regularization terms. To increase the stability of the adversarial training, we apply two regularization terms to the additional loss terms in Eqs. <ref type="bibr" target="#b14">(15,</ref><ref type="bibr" target="#b18">19)</ref>. One is the zerocentered gradient penalty <ref type="bibr" target="#b32">[33]</ref>, which is added to the discriminator loss of Eq. <ref type="bibr" target="#b14">(15)</ref>. It is applied to the discriminator D parameters ?, considering only the real target data:</p><formula xml:id="formula_28">L reg = ? 2 E ?D ? (x tgt ) 2 ,<label>(21)</label></formula><p>where ? is a weighting factor. The other is for a conditioning augmentor <ref type="bibr" target="#b15">[16]</ref>. Its loss is added to the generator loss of Eq. <ref type="bibr" target="#b18">(19)</ref>. The conditioning augmentor is a module that samples text-conditioned latent variable c from independent Gaussian distribution N (?(e), ?(e)), where the mean ?(e) and diagonal covariance matrix ?(e) are functions of the sentence embedding e. The text-conditioned latent variable c is used as the input of G text instead of the sentence embedding e. The Kullback-Leibler (KL) divergence is introduced as a regularization term in order to increase smoothing on the conditioning manifold, as L ca = ?D KL (N (?(e), ?(e)) N (0, I)),</p><p>where N (0, I) is the standard Gaussian distribution and ? is a weighting factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Transforms.</p><p>All images were resized to (128, 128) and normalized between (?1, 1). Instructions were tokenized using the uncased BERT tokenizer available online ? . When training the GAN model, all instructions were transformed in advance into text embeddings and word embeddings by using the fine-tuned BERT sentence encoder. A small amount of noise sampled from a uniform distribution Uniform(0, 1/64) was added to the image at every step to stabilize the GAN training dynamics. Training. We used spectral normalization <ref type="bibr" target="#b28">[29]</ref> for all layers in both the generator and discriminator. We used the Adam <ref type="bibr" target="#b33">[34]</ref> optimizer with learning rate of 0.0004 for the discriminator and 0.0001 for the generator and betas of (0.0, 0.9) for both. We prepared the evaluation-specific generator and updated its weights by using an exponential moving average of the generator weights during training <ref type="bibr" target="#b34">[35]</ref>. We used the truncation trick for the latent noise with a threshold of 2.0 during testing <ref type="bibr" target="#b34">[35]</ref>. Hardware. The training process took 72 hours with four NVIDIA Tesla V100 (SXM2) GPUs, two Intel Xeon Gold 6148 (27.5M cache, 2.40 GHz, 20 cores) processors, and 384 GiB of memory. The average inference time per batch of ten samples was 0.76 s for CoDraw and 0.91 s for i-CLEVR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Supplemental Results</head><p>The convergence curve of LatteGAN for the F1 and the RSIM is presented in <ref type="figure" target="#fig_3">Fig. 7</ref>.</p><p>Additional images generated from our LatteGAN are presented in <ref type="figure">Fig. 8</ref> for the CoDraw dataset and in <ref type="figure">Fig. 9</ref> for the i-CLEVR dataset. ? https://huggingface.co/transformers/model_ doc/bert.html#transformers.BertTokenizerFast </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the Latte module. LatteGAN iteratively generates images according to a sequence of instructions. The Visually Guided Language Attention (Latte) module uses the relational features as queries to extract fine-grained representations from each instruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Fig. 1shows typical scenes in the GeNeVA task. Given an image BERT Architecture of LatteGAN. For each timestep t, the generator takes as inputs a text instruction x l and a previous generated image xsrc, and generates a target imagextgt. For readability, notations for timestep t are omitted in the figure. FCA denotes the conditional augmentor and SS denotes the semantic synthesis module. The discriminator judges whether the generated image is real or fake. Here, two types of discriminators are introduced: a local level unconditional discriminator and a global level text-conditioned discriminator.inputx (t?1) v and an instruction x (t) l , the Drawer generates an imagex (t) v . In the first turn of generation, an empty canvas with a background image x (0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>VFigure 4 .</head><label>4</label><figDesc>Relational Visual Feature Extractor. A Relational Visual Feature Extractor extracts visual representations, each feature of which includes the relationships with others (Fig. 4 left). Given a visual feature map h and a text feature map Q K Architecture of the fusion function in the generator. The fusion function Gfusion consists of the Latte module and semantic synthesis module. The Latte module takes an image feature h and a global text feature u as inputs, and extracts a fine-grained text-feature vector g. Following the Latte module is the semantic synthesis module, which takes a concatenation of h, u, and g as input and yields the modified features hout as output. u, width-wise pooled feature vectors h (W ) i and u (W ) i at the position i = {1, ..., H} over positions j = {1, ..., W } are computed as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Learning curve of LatteGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ntoken (23.05, 11.79, 222, 1) (14.99, 5.69, 23, 6) Total vocabulary size 4090 25 Statistics of CoDraw and i-CLEVR datasets. Dataset size shows the number of images in each training split (train, valid, and test, respectively).Nturn represents the number of turns in a scene in a quadruple representing the average, the standard deviation, the maximum value, and the minimum value, respectively. Ntoken represents the number of tokens in a sentence in a quadruple as well. Finally, the total vocabulary size shows the size of the vocabulary in each dataset.</figDesc><table><row><cell>Statistics</cell><cell>CoDraw</cell><cell>i-CLEVER</cell></row><row><cell>Dataset size</cell><cell>(7989, 1002, 1002)</cell><cell>(6003, 2000, 2000)</cell></row><row><cell>Nturn</cell><cell>(4.25, 1.46, 14, 1)</cell><cell>(5, 0, 5, 5)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by JST CREST Grant Number JPMJCR19A1, Japan, and JSPS KAKENHI Grant Number JP21J13789, Japan. The experiments were partially conducted on the AI Bridging Cloud Infrastructure (ABCI) provided by the National Institute of Advanced Industrial Science and Technology (AIST). The training progress was tracked and the reports were created with Weight &amp; Biases. Finally, we thank our colleagues, Nathan Boyer and David Felices, for helpful comments and discussion.  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unpaired image-tospeech synthesis with multimodal information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7598" to="7607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1316" to="1324" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Development and evaluation of interactive humanoid robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1839" to="1850" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multimodal classifier generative adversarial network for carry and place tasks from ambiguous language instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Magassouba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sugiura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE RAL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3113" to="3120" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tell, draw, and repeat: Generating and modifying images based on continual linguistic instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>312. 1, 2, 5</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Codraw: Collaborative drawing as a testbed for grounded goal-driven communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8798" to="8807" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantics-enhanced adversarial nets for text-toimage synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">510</biblScope>
			<biblScope unit="page" from="10" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring global and local linguistic representations for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3075" to="3087" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ckd: Cross-task knowledge distillation for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="7986" to="7994" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text-adaptive generative adversarial networks: manipulating images with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Manigan: Text-guided image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7880" to="7889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning crossmodal representations for language-based image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Kenan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Text as neural operator: Image manipulation by text instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04556</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Iterative language-based image editing via self-supervised counterfactual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grafton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Composing text and image for image retrieval-an empirical odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6439" to="6448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A unet based discriminator for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">cgans with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Geometric gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6924" to="6932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3481" to="3490" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
