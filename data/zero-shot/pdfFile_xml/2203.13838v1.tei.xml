<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Schumann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Computational Linguistics Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
							<email>rschuman|riezler@cl.uni-heidelberg.de</email>
							<affiliation key="aff1">
								<orgName type="department">Computational Linguistics &amp;</orgName>
								<orgName type="institution">IWR Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision and language navigation (VLN) is a challenging visually-grounded language understanding task. Given a natural language navigation instruction, a visual agent interacts with a graph-based environment equipped with panorama images and tries to follow the described route. Most prior work has been conducted in indoor scenarios where best results were obtained for navigation on routes that are similar to the training routes, with sharp drops in performance when testing on unseen environments. We focus on VLN in outdoor scenarios and find that in contrast to indoor VLN, most of the gain in outdoor VLN on unseen data is due to features like junction type embedding or heading delta that are specific to the respective environment graph, while image information plays a very minor role in generalizing VLN to unseen outdoor areas. These findings show a bias to specifics of graph representations of urban environments, demanding that VLN tasks grow in scale and diversity of geographical environments. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vision and language navigation (VLN) is a challenging task that requires the agent to process natural language instructions and ground them in a visual environment. The agent is embodied in the environment and receives navigation instructions. Based on the instructions, the observed surroundings, and the current trajectory the agent decides its next action. Executing this action changes the position and/or heading of the agent within the environment, and eventually the agent follows the described route and stops at the desired goal location. The most common evaluation metric in VLN is the proportion of successful agent navigations, called task completion (TC). 1 Code:</p><p>https://github.com/raphael-sch/ map2seq_vln Data &amp; Demo: https://map2seq.schumann.pub/ vln/ While early work on grounded navigation was confined to grid-world scenarios <ref type="bibr" target="#b14">(MacMahon et al., 2006;</ref><ref type="bibr" target="#b2">Chen and Mooney, 2011)</ref>, recent work has studied VLN in outdoor environment consisting of real-world urban street layouts and corresponding panorama pictures <ref type="bibr" target="#b3">(Chen et al., 2019)</ref>. Recent agent models for outdoor VLN treat the task as a sequence-to-sequence problem where the instructions text is the input and the output is a sequence of actions <ref type="bibr" target="#b3">(Chen et al., 2019;</ref><ref type="bibr" target="#b26">Xiang et al., 2020;</ref><ref type="bibr" target="#b31">Zhu et al., 2021b)</ref>. In contrast to indoor VLN <ref type="bibr" target="#b13">Ku et al., 2020)</ref>, these works only consider a seen scenario, i.e., the agent is tested on routes that are located in the same area as the training routes. However, studies of indoor VLN <ref type="bibr" target="#b28">(Zhang et al., 2020)</ref> show a significant performance drop when testing in previously unseen areas.</p><p>The main goal of our work is to study outdoor VLN in unseen areas, pursuing the research question of which representations of an environment and of instructions an agent needs to succeed at this task. We compare existing approaches to a new approach that utilizes features based on the observed environment graph to improve generalization to unseen areas. The first feature, called junction type embedding, encodes the number of outgoing edges at the current agent position; the second feature, called heading delta, encodes the agent's heading change relative to the previous timestep. As our experimental studies show, representations of full images do not contribute very much to successful VLN in outdoor scenarios beyond these two features. One reason why restricted features encoding junction type and heading delta are successful in this task is that they seem to be sufficient to encode peculiarities of the graph representation of the environments. Another reason is the current restriction of outdoor environments to small urban areas. In our case, one dataset is the widely used Touchdown dataset introduced by <ref type="bibr" target="#b3">Chen et al. (2019)</ref>, the other dataset is called map2seq and has recently been introduced by <ref type="bibr" target="#b21">Schumann and Riezler (2021)</ref>. The map2seq dataset was created for the task of navigation instructions generation but can directly be adopted to VLN. We conduct a detailed analysis of the influence of general neural architectures, specific features such as junction type or heading delta, the role of image information and instruction token types, to outdoor VLN in seen and unseen environments on these two datasets.</p><p>Our specific findings unravel the contributions of these features on several VLN subtasks such as orientation, directions, stopping. Our general finding is that current outdoor VLN suffers a bias towards urban environments and to artifacts of their graph representation, showing the necessity of more diverse datasets and tasks for outdoor VLN.</p><p>Our main contributions are the following:</p><p>? We describe a straightforward agent model that achieves state-of-the-art task completion and is used as a basis for our experiments.</p><p>? We introduce the unseen scenario for outdoor VLN and propose two environment-dependent features to improve generalization in that setting.</p><p>? We compare different visual representations and conduct language masking experiments to study the effect in the unseen scenario.</p><p>? We adopt the map2seq dataset to VLN and show that merging it with Touchdown improves performance on the respective test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">VLN Problem Definition</head><p>The goal of the agent is to follow a route and stop at the desired target location based on natural language navigation instructions. The environment is a directed graph with nodes v ? V and labeled edges (u, v) ? E. Each node is associated with a 360 ? panorama image p and each edge is labeled with an angle ? <ref type="bibr">(u,v)</ref> . The agent state s ? S consists of a node and the angle at which the agent is heading:</p><formula xml:id="formula_0">(v, ? (v,u) | u ? N out v ), where N out v</formula><p>are all outgoing neighbors of node v. The agent can navigate the environment by performing an action a ? {FORWARD, LEFT, RIGHT, STOP} at each timestep t. The FORWARD action moves the agent from state <ref type="bibr">(v, ? (v,u)</ref> ) to <ref type="bibr">(u, ? (u,u )</ref> ), where (u, u ) is the edge with an angle closest to ? <ref type="bibr">(v,u)</ref>  Heading Delta <ref type="figure">Figure 1</ref>: The ORAR model for outdoor vision and language navigation follows a sequence-to-sequence architecture. The instructions text is encoded and used along the visual features to predict the next agent action. The recurrent decoder has two layers, the first encodes observations about the current environment state, the second allows attention over the input text and panorama view. The predicted action changes the state of the agent in the environment and with it the panorama view of the next timestep.</p><p>the closest edge angle in clockwise or counterclockwise direction, respectively: <ref type="bibr">(v, ? (v,u )</ref> ). Given a starting state s 1 and instructions text x, the agent performs a series of actions a 1 , ..., a T until the STOP action is predicted. If the agent stops within one neighboring node of the desired target node (goal location), the navigation was successful. The described environment and location finding task was first introduced by <ref type="bibr" target="#b3">(Chen et al., 2019)</ref> and we will also refer to it as "outdoor VLN task" throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Architecture</head><p>In this section we introduce the model that we use to analyze navigation performance in the unseen and seen scenario for outdoor VLN. The architecture is inspired by the cross-modal attention model for indoor VLN <ref type="bibr" target="#b12">(Krantz et al., 2020)</ref>. First we give a high level overview of the model architecture and rough intuition. Afterwards we provide a more formal description. As depicted in <ref type="figure">Figure 1</ref>, the model follows a sequence-to-sequence architecture where the input sequence is the navigation instructions text and the output is a sequence of agent actions. At each decoding timestep, a new visual representation of the current agent state within the environment is computed, where the agent state is dependent on the previously predicted actions. The decoder RNN has two layers where the first encodes metadata and a visual representation. The second RNN layer encodes a contextualized text and visual representation and eventually predicts the next action.</p><p>The intuition behind the model architecture is to firstly accumulate plain observations available at the current timestep and entangle them with previous observations in the first recurrent layer. Based on these observations, the model focuses attention to certain parts of the instructions text and visual features which are again entangled in the second recurrent layer. Thus, we use the acronym ORAR (observation-recurrence attention-recurrence) for the model.</p><p>In detail, the instructions encoder embeds and encodes the tokens in the navigation instructions sequence x = x 1 , ..., x L using a bidirectional LSTM <ref type="bibr" target="#b5">(Graves et al., 2005)</ref>:</p><formula xml:id="formula_1">x i = embedding(x i ) ((w 1 , ..., w L ), z w L ) = Bi-LSTM(x 1 , ...,x L )</formula><p>, where w 1 , ..., w L are the hidden representations for each token and z w L is the last LSTM cell state. The visual encoder, described in detail below, emits a fixed size representationp t of the current panorama view and a sequence of sliced view representations p 1 t , ...,p S t . The state z f irst 0 of the cell in the first decoder LSTM layer is initialized using z w L . The input to the first decoder layer is the concatenation (?) of visual representationp t , previous action embedding? t?1 , junction type embeddingn t , and heading delta d t . The output of the first decoder layer,</p><formula xml:id="formula_2">h f irst t = LSTM f irst ([? t?1 ?n t ? d t ?p t ]),</formula><p>is then used as the query of multi-head attention <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref> over the text encoder. The resulting contextualized text representation c w t is then used to attend over the sliced visual representations:</p><formula xml:id="formula_3">c w t = MultiHeadAttention(h f irst t , (w 1 , ..., w L )) c p t = MultiHeadAttention(c w t , (p 1 t , ...,p S t )).</formula><p>The input and output of the second decoder layer are</p><formula xml:id="formula_4">h second t = LSTM second ([t ? h f irst t ? c w t ? c p t ]),</formula><p>wheret is the embedded timestep t. The hidden representation h second t of the second decoder LSTM layer is then passed through a feed forward network to predict the next agent action a t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual Encoder</head><p>At each timestep t the panorama at the current agent position is represented by extracted visual features. We slice the panorama into eight projected rectangles with 60 ? field of view, such that one of the slices aligns with the agent's heading. This centering slice and the two left and right of it are fed into a ResNet pretrained 2 on Ima-geNet <ref type="bibr" target="#b20">(Russakovsky et al., 2015)</ref>. We consider two variants of ResNet derived panorama features. One variant extracts low level features from the fourth to last layer (4th-to-last) of a pretrained ResNet-18 and concatenates each slice's feature map along the width dimension, averages the 128 CNN filters and cuts out 100 dimensions around the agents heading. This results in a feature matrix of 100 ? 100 (p 1 t , ...,p 100 t ). The full procedure is described in detail in <ref type="bibr" target="#b3">Chen et al. (2019)</ref> and <ref type="bibr" target="#b31">Zhu et al. (2021b)</ref>. The other variant extracts high level features from a pretrained ResNet-50's pre-final layer for each of the 5 slices:p 1 t , ...,p 5 t . Each slice vectorp s t is of size 2, 048 resulting in roughly the same number of extracted ResNet features for both variants, making a fair comparison. Further, we use the semantic segmentation representation of the panorama images. We employ omnidirectional semantic segmentation  to classify each pixel by one of the 25 classes of the Mapillary Vistas dataset <ref type="bibr" target="#b18">(Neuhold et al., 2017)</ref>. The classes include e.g. car, truck, traffic light, vegetation, road, sidewalk. See <ref type="figure">Figure 1</ref> bottom right for a visualization. Each panorama slice (p 1 t , ...,p 5 t ) is then represented by a 25 dimensional vector where each value is the normalized area covered by the corresponding class <ref type="bibr" target="#b28">(Zhang et al., 2020)</ref>. For either feature extraction method, the fixed sized panorama representationp t is computed by concatenating the slice featuresp 1 t , ...,p S t and passing them to a feed forward network. 3) The environment automatically rotates the agent towards the closest outgoing edge. 4) The agent has no explicit information about the automatic rotation and predicts a right turn as instructed, leading to a failed navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Junction Type Embedding</head><p>The junction type embedding is a feature that we introduce to better analyze generalization to unseen areas. It embeds the number of outgoing edges of the current environment node and is categorized into {2, 3, 4, &gt;4}. It provides the agent information about the type of junction it is positioned on: a regular street segment, a three-way intersection, a four way intersection or an intersection with more than four outgoing streets. We want to point out that the number of outgoing edges isn't oracle information in the environment described in Section 2. The agent can rotate left until the same panorama view is observed and thus counting the number of outgoing edges by purely interacting with the environment. But it is clear that the feature leverages the fact that the environment is based on a graph and it would not be available in a continuous setting <ref type="bibr" target="#b12">(Krantz et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Heading Delta</head><p>As described in Section 2, the environment defined and implemented by <ref type="bibr" target="#b3">Chen et al. (2019)</ref> only allows states where the agent is heading towards an outgoing edge. As a consequence the environment automatically rotates the agent towards the closest outgoing edge after transitioning to a new node. The environment behavior is depicted in <ref type="figure">Fig-ure</ref> 2a) for a transition between two regular street segments. However, as depicted in <ref type="figure" target="#fig_0">Figure 2b</ref>), a problem arises when the agent is walking towards a three-way intersection. The automatic rotation introduces unpredictable behavior for the agent and we hypothesis that it hinders generalization to unseen areas. To correct for this environment artifact, we introduce the heading delta feature d t which encodes the change in heading direction relative to the previous timestep. The feature is normalized to (?1, 1] where a negative value indicates a left rotation and a positive value indicates a right rotation. The magnitude signals the degree of the rotation up to 180 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>We use the Touchdown <ref type="bibr" target="#b3">(Chen et al., 2019)</ref> and the map2seq <ref type="bibr" target="#b21">(Schumann and Riezler, 2021)</ref> datasets in our experiments. Both datasets contain human written navigation instructions for routes located in the same environment. The environment consists of 29,641 panorama images from Manhattan and the corresponding connectivity graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Touchdown</head><p>The Touchdown dataset <ref type="bibr" target="#b3">(Chen et al., 2019)</ref> for vision and language navigation consists of 9,326 routes paired with human written navigation instructions. The annotators navigated the panorama environment based on a predefined route and wrote down navigation instructions along the way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Map2seq</head><p>The map2seq <ref type="bibr" target="#b21">(Schumann and Riezler, 2021)</ref> dataset was created for the task of navigation instructions generation. The 7,672 navigation instructions were written by human annotators who saw a route on a rendered map, without the corresponding panorama images. The annotators were told to include visual landmarks like stores, parks, churches, and other amenities into their instructions. A different annotator later validated the written navigation instructions by using them to follow the described route in the panorama environment (without the map). This annotation procedure allows us to use the navigation instructions in the map2seq dataset for the vision and language navigation task. We are the first to report VLN results on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison</head><p>Despite being located in the same environment, the routes and instructions from each dataset differ in multiple aspects. The map2seq instructions typically include named entities like store names, while Touchdown instructions focus more on visual features like the color of a store. Both do not include street names or cardinal directions and are written in egocentric perspective. Further, in map2seq the agent starts by facing in the correct direction, while in Touchdown the initial heading is random and the first part of the instruction is about orientating the agent ("Turn around such that the scaffolding is on your right"). A route in map2seq includes a minimum of three intersections and is the shortest path from the start to the end location. 3 In Touchdown there are no such constraints and a route can almost be circular. The routes in both datasets are around 35-45 nodes long with some shorter outliers in Touchdown. On average instructions are around 55 tokens long in map2seq and around 89 tokens long in Touchdown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We are interested in the generalization ability to unseen areas and how it is influenced by the two proposed features, types of visual representation, navigation instructions and training set size. Alongside of the results in the unseen scenario, we report results in the seen scenario to interpret performance improvements in relation to each other. All experiments 4 are repeated ten times with different random seeds. The reported numbers are the average over the ten repetitions. Results printed in bold are significantly better than non-bold results in the same column. Significance was established by a paired t-test 5 on the ten repetition results and a p-value ? 0.05 without multiple hypothesis corrections factor. Individual results can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Splits</head><p>To be able to compare our model with previous work, we use the original training, development and test split <ref type="bibr" target="#b3">(Chen et al., 2019)</ref> for the seen scenario on Touchdown. Because we are the first to use the map2seq data for VLN we create a new split for it. The resulting number of instances can be   seen in the left column of <ref type="table" target="#tab_2">Table 1</ref>. For the unseen scenario, we create new splits for both datasets. We separate the unseen area geographically by drawing a boundary across lower Manhattan (see <ref type="figure" target="#fig_1">Figure 3</ref>). Development and test instances are randomly chosen from within the unseen area. Routes that are crossing the boundary are discarded. The right column of <ref type="table" target="#tab_2">Table 1</ref> shows the number of instances for both splits. Additionally, we merge the two datasets for both scenarios. This is possible because both datasets are located in the same environment and the unseen boundary is equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training Details</head><p>We train the models with Adam (Kingma and Ba, 2015) by minimizing cross entropy loss in the teacher forcing paradigm. We set the learning rate to 5e-4, weight decay to 1e-3 and batch size to 64. After 150 epochs we select the model with the best shortest path distance (SPD) performance on the development set. We apply dropout of 0.3 after each dense layer and recurrent connection. The multi-head attention mechanism is regularized   by attention dropout of 0.3 and layer normalization. The navigation instructions are lower-cased and split into byte pair encodings <ref type="bibr" target="#b22">(Sennrich et al., 2016)</ref> with a vocabulary of 2,000 tokens and we use BPE dropout <ref type="bibr" target="#b19">(Provilkov et al., 2020)</ref> during training. The BPE embeddings are of size 32 and the bidirectional encoder LSTM has two layers of size 256. The feed forward network in the visual encoder consists of two dense layers with 512 and 256 neurons, respectively, and 64 neurons in case of using semantic segmentation features. The embeddings that encode previous action, junction type, and step count are of size 16. The two decoder LSTM layers are of size 256 and we use two attention heads. Training the full model takes around 3 hours on a GTX 1080 Ti.</p><formula xml:id="formula_5">- - - - - - - - - - - - ARC+l2s -19.5 -16.7 - - - - - - - - - - - - VLN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Comparison</head><p>We compare the ORAR model to previous works. Because these works only report results for the seen scenario on Touchdown, we evaluate those for which we could acquire the code, on the map2seq dataset and the unseen scenario.  <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> to encode the instructions and VLN-BERT  to fuse the modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Metrics</head><p>We use task completion (TC) as the main performance metric. It represents the percentage of successful agent navigations <ref type="bibr" target="#b3">(Chen et al., 2019)</ref>. We further report normalized Dynamic Time Warping (nDTW) which quantifies agent and gold trajectory overlap for all routes <ref type="bibr" target="#b9">(Ilharco et al., 2019)</ref>. The shortest path distance (SPD) is measured within the environment graph from the node the agent stopped to the goal node <ref type="bibr" target="#b3">(Chen et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results &amp; Analysis</head><p>The two upper sections of <ref type="table" target="#tab_5">Table 2</ref>   nificantly outperforms all previous work on both datasets, our main focus is analyzing generalization to the unseen scenario. It is apparent that the type of image features influences agent performance and will be discussed in the next section. The bottom section of <ref type="table" target="#tab_5">Table 2</ref> ablates the proposed heading delta and junction type features for the best models. Removing the heading delta feature has little impact in the seen scenario, but significantly reduces task completion in the unseen scenario of the map2seq dataset. Surprisingly, the feature has no impact in the unseen scenario of Touchdown. We believe this is a consequence of the different data collection processes. Touchdown was specifically collected for VLN and annotators navigated the environment graph, while map2seq annotators wrote instructions only seeing the map. Removing the junction type embedding leads to a collapse of task completion in the unseen scenario on both datasets. This shows that without this explicit feature, the agent lacks the ability to reliably identify intersections in new areas.  Providing oracle actions for two of the three sub-tasks allows an isolated look at the remaining one. Underlined results are best for the sub-task, e.g. 85.5 is the best TC for the directions task on the test set in the seen scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Visual Features</head><p>intersections from any type of visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sub-task Oracle</head><p>The agent has to predict a sequence of actions in order to successfully reach the goal location. In Touchdown this task can be divided into three subtasks (see Section 4). First the agent needs to orientate itself towards the correct starting heading.</p><p>Next the agent has to predict the correct directions at the intersections along the path. The third subtask is stopping at the specified location. Providing oracle actions (during testing) for two of the three sub-tasks lets us look at the completion rate of the remaining sub-task. <ref type="table" target="#tab_10">Table 4</ref> shows the completion rates for each of the three sub-tasks when using ResNet pre-final, 4th-to-last and no image features. In the seen scenario we can observe that the pre-final features lead to the best performance for the directions task. The 4th-to-last features on the other hand lead to the best orientation task performance and the stopping task is not influenced by the choice of visual features. In the unseen scenario 4th-to-last features again provide best orientation task performance but no image features lead to the best performance for the directions task. This shows that the ResNet 4th-to-last features are primarily useful for the orientation sub-task and explains the discrepancy of the no image baseline on Touchdown and map2seq identified in the previous subsection. In the Appendix we use this knowledge to train a mixed-model that uses 4th-to-last features for the orientation sub-task and pre-final/no image features for directions and stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Token Masking</head><p>To analyze the importance of direction and object tokens in the navigation instructions, we run masking experiments similar to <ref type="bibr" target="#b30">Zhu et al. (2021a)</ref>, except that we mask the tokens during training and testing instead of during testing only. <ref type="figure" target="#fig_2">Figure 4</ref> shows the resulting task completion rates for an increasing number of masked direction or object tokens. From the widening gap between masking object and direction tokens, we can see that the direction tokens are more important to successfully reach the goal location. Task completion nearly doesn't change when masking object tokens, indicating that they are mostly ignored by the model. While task completion significantly drops when direction tokens are masked, the agent still performs on a high level. This finding is surprising and in dissent with <ref type="bibr" target="#b30">Zhu et al. (2021a)</ref> who report that task completion nearly drops to zero when masking direction tokens during testing only. We believe that in our setting (masking during testing and training), the model learns to infer the correct directions from redundancies in the instructions or context around the direction tokens. Besides the general trend of lower performance on the unseen scenario, we can not identify different utilization of object or direction tokens in the seen and unseen scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Merged Datasets</head><p>We train the ORAR full model on the merged dataset (see Section 5.1). Model selection is performed on the merged development set but results are also reported for the individual test sets of Touchdown and map2seq. For comparison with models trained on the non-merged datasets, the first row of <ref type="table" target="#tab_12">Table 5</ref> shows the best results of Table 2. Training on the merged dataset significantly improves nDTW and task completion across both datasets and scenarios. This shows that both datasets are compatible and the merged dataset can further be used by the VLN community to evaluate their models on more diverse navigation instructions. Despite being trained on twice as many instances, the no image baseline still performs on par on map2seq unseen. From this we conclude that the current bottleneck for better generalization to unseen areas is the number of panorama images seen during training instead of number of instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Natural language instructed navigation of embodied agents has been studied in generated grid environments that allow a structured representation of the observed environment <ref type="bibr" target="#b14">(MacMahon et al., 2006;</ref><ref type="bibr" target="#b2">Chen and Mooney, 2011)</ref>. Fueled by the advances in image representation learning <ref type="bibr" target="#b6">(He et al., 2016)</ref>, the environments became more realistic by using real-world panorama images of indoor locations <ref type="bibr" target="#b13">Ku et al., 2020)</ref>. Complementary outdoor environments contain street level panoramas connected by a real-world street layout <ref type="bibr" target="#b17">(Mirowski et al., 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2019;</ref><ref type="bibr" target="#b16">Mehta et al., 2020)</ref>. Agents in this outdoor environment are trained to follow human written navigation instructions <ref type="bibr" target="#b3">(Chen et al., 2019;</ref><ref type="bibr" target="#b26">Xiang et al., 2020)</ref>, instructions generated by Google Maps <ref type="figure" target="#fig_0">(Hermann et al., 2020)</ref>, or a combination of both <ref type="bibr" target="#b31">(Zhu et al., 2021b)</ref>. Recent work focuses on analyzing the navigation agents by introducing better trajectory overlap metrics <ref type="bibr" target="#b9">Ilharco et al., 2019)</ref> or diagnosing the performance under certain constraints such as uni-modal inputs <ref type="bibr" target="#b24">(Thomason et al., 2019)</ref> and masking direction or object tokens <ref type="bibr" target="#b30">(Zhu et al., 2021a)</ref>. Other work used a trained VLN agent to evaluate auto-  matically generated navigation instructions <ref type="bibr" target="#b29">(Zhao et al., 2021)</ref>. An open problem in indoor VLN is the generalization of navigation performance to previously unseen areas. Proposed solutions include back translation with environment dropout <ref type="bibr" target="#b23">(Tan et al., 2019)</ref>, multi-modal environment representation <ref type="bibr" target="#b8">(Hu et al., 2019)</ref> or semantic segmented images <ref type="bibr" target="#b28">(Zhang et al., 2020)</ref>. Notably the latter work identifies the same problem in the Touchdown task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented an investigation of outdoor vision and language navigation in seen and unseen environments. We introduced the heading delta feature and junction type embedding to correct an artifact of the environment and explicitly model the number of outgoing edges, respectively. Both are helpful to boost and analyze performance in the unseen scenario. We conducted experiments on two datasets and showed that the considered visual features poorly generalize to unseen areas. We conjecture that VLN tasks need to grow in scale and diversity of geographical environments and navigation tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Architecture Ablation</head><p>We perform ablation studies on the ORAR full model in the seen scenario to measure the impact of individual architecture components. As seen in <ref type="table" target="#tab_14">Table 6</ref>, removing the second decoder RNN layer or BPE dropout results in a decrease of six and three task completion points, respectively. The largest drop in performance is observed when removing the text attention mechanism. This again shows the importance of attention over the encoder in sequence-to-sequence models. Removing the image attention mechanism on the other hand does not affect task completion on the map2seq dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Mixed-Model</head><p>The findings in Section 6.2 inspire us to modify the ORAR model to use distinct visual features for the orientation and directions/stopping task. The orientation task is equivalent to the very first action prediction by the agent. Thus we modify the model architecture to use the ResNet 4th-to-last features (+text representation) to predict the first action and then start the recurrent prediction of the remaining actions with a different set of visual features (pre-final for the seen scenario and no image features for the unseen scenario). The results for this ORAR mixed model trained on the merged dataset are shown in <ref type="table" target="#tab_16">Table 7</ref>. We only test it on Touchdown because map2seq does not have the orientation task. The mixed model significantly outperforms the single visual feature model on the Touchdown seen test set but unfortunately shows no improvement in the unseen scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Metrics and Individual Runs</head><p>We present the results of the individual repetitions and additional metrics for the main results in <ref type="table" target="#tab_5">Table 2</ref> and the results on the merged dataset in         </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of automatic agent rotation initiated by the environment. Grey circles and interconnecting edges are part of the environment graph. Black solid arrows are actions initiated by the agent. Black dotted arrows depict agent heading and automatic rotation by the environment. a): 1) The agent moves forward. 2) Agent's heading does not point to an outgoing edge. 3) Agent is automatically rotated to the closest edge without causing problems. b): The agent receives instructions like "Turn right at the next intersection". 1) The agent moves forward. 2) Agent's heading does not point to an outgoing edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the environment area located in Manhattan. The seen scenario is depicted on the left and the unseen scenario on the right. Each white dot is a training route and each black dot is a test route in the Touchdown and map2seq dataset. The unseen scenario is characterized by geographic separation of the training and testing area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Masking experiments on the seen and unseen test set of Touchdown. Object or direction tokens are masked during training and testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Number of instances in the data splits for the seen and unseen scenario of Touchdown and map2seq.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results on Touchdown and map2seq for the seen and unseen scenario. Metrics are normalized Dynamic Time Warping (nDTW) and task completion (TC). In the first section we list results for the comparison models:</figDesc><table /><note>RConcat, GA, VLN Transformer (Zhu et al., 2021b) and ARC, ARC+learn2stop (Xiang et al., 2020). In the second section we present results for the ORAR model with two different types of image features: ResNet pre-final features are extracted from the last layer before the classification and ResNet 4th-to-last are low level features extracted from the fourth to last layer of a pretrained ResNet. The last section ablates the two proposed features: heading delta and junction type embedding.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Study of visual features for the unseen sce-</cell></row><row><cell>nario of Touchdown and map2seq. Metric is task com-</cell></row><row><cell>pletion.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows results for different types of visual</cell></row><row><cell>features in the unseen scenario. We compare high</cell></row><row><cell>level ResNet features (pre-final), low level ResNet</cell></row><row><cell>features (4th-to-last), semantic segmentation fea-</cell></row><row><cell>tures and using no image features. For the ResNet</cell></row><row><cell>based features, the low level 4th-to-last features</cell></row><row><cell>perform better than pre-final on both datasets. On</cell></row><row><cell>map2seq the no image baseline performs on par</cell></row><row><cell>with models that have access to visual features.</cell></row><row><cell>When we remove the junction type embedding,</cell></row><row><cell>the task completion rate drops significantly, which</cell></row><row><cell>shows that the agent is not able to reliably locate</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Oracle analysis on Touchdown. Division into three sub-tasks: orientation, directions and stopping.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Results for models trained on the merged dataset. Test results are presented for the merged test set and individual Touchdown and map2seq test sets. Metrics are normalized Dynamic Time Warping (nDTW) and task completion (TC). In the first row the best results of Table 2 (non-merged training sets) are listed for comparison. The bottom section presents results on the ORAR full model with different types of image features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>ORAR full model ablation study on the seen scenario of Touchdown and map2seq. Metric is task completion and ablations are not cumulative.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 5 .</head><label>5</label><figDesc>The additional metrics are success weighted normalized Dynamic Time Warping<ref type="bibr" target="#b9">(Ilharco et al., 2019)</ref> and shortest-path distance<ref type="bibr" target="#b3">(Chen et al., 2019)</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Unseen</cell></row><row><cell></cell><cell></cell><cell cols="2">Merged</cell><cell></cell><cell cols="3">Touchdown map2seq</cell><cell></cell><cell cols="2">Merged</cell><cell></cell><cell cols="2">Touchdown map2seq</cell></row><row><cell></cell><cell>dev</cell><cell></cell><cell>test</cell><cell></cell><cell>test</cell><cell>test</cell><cell></cell><cell>dev</cell><cell></cell><cell>test</cell><cell></cell><cell>test</cell><cell>test</cell></row><row><cell>Model</cell><cell cols="4">nDTW TC nDTW TC</cell><cell>nDTW TC</cell><cell cols="2">nDTW TC</cell><cell cols="4">nDTW TC nDTW TC</cell><cell>nDTW TC</cell><cell>nDTW TC</cell></row><row><cell>best non-merged</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>44.9 29.1</cell><cell cols="2">62.3 46.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>21.6 14.9</cell><cell>42.2 30.3</cell></row><row><cell>best merged</cell><cell cols="2">53.4 37.8</cell><cell cols="2">51.8 35.7</cell><cell>46.0 30.1</cell><cell cols="2">67.3 52.8</cell><cell cols="2">35.7 25.4</cell><cell cols="2">33.6 24.2</cell><cell>27.0 19.3</cell><cell>46.1 33.5</cell></row><row><cell></cell><cell></cell><cell cols="4">? 4th-to-last + pre-final</cell><cell></cell><cell></cell><cell></cell><cell cols="4">? 4th-to-last + no image</cell></row><row><cell>ORAR mixed model</cell><cell cols="2">58.6 44.4</cell><cell cols="2">57.4 42.9</cell><cell>51.3 36.9</cell><cell>-</cell><cell>-</cell><cell cols="2">36.3 26.1</cell><cell cols="2">33.6 23.9</cell><cell>26.3 18.3</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 7 :</head><label>7</label><figDesc>Results for the mixed model in comparison to previous best results. Metrics are normalized Dynamic Time Warping (nDTW) and task completion (TC). In the first two rows the best results ofTable 2 and Table 5are listed for comparison. The last section presents results for the ORAR mixed model which uses different image features for different sub-tasks.</figDesc><table><row><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Unseen</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Touchdown</cell><cell></cell><cell cols="2">map2seq</cell><cell></cell><cell cols="3">Touchdown</cell><cell></cell><cell cols="3">map2seq</cell><cell></cell></row><row><cell></cell><cell>dev</cell><cell>test</cell><cell>dev</cell><cell></cell><cell>test</cell><cell></cell><cell>dev</cell><cell></cell><cell>test</cell><cell></cell><cell>dev</cell><cell></cell><cell>test</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">SDTW SPD SDTW SPD</cell><cell cols="4">SDTW SPD SDTW SPD</cell><cell cols="4">SDTW SPD SDTW SPD</cell><cell cols="4">SDTW SPD SDTW SPD</cell></row><row><cell>RConcat</cell><cell>9.8 20.4</cell><cell>11.1 20.4</cell><cell cols="2">16.0 19.0</cell><cell cols="2">13.7 20.1</cell><cell cols="2">1.8 29.6</cell><cell cols="2">1.4 29.3</cell><cell cols="2">1.2 33.1</cell><cell cols="2">1.7 34.1</cell></row><row><cell>GA</cell><cell>11.1 18.7</cell><cell>10.9 19.0</cell><cell cols="2">17.2 16.5</cell><cell cols="2">16.0 18.0</cell><cell cols="2">1.3 31.0</cell><cell cols="2">1.7 30.5</cell><cell cols="2">1.4 34.3</cell><cell cols="2">1.3 34.3</cell></row><row><cell>ARC</cell><cell>14.1 18.6</cell><cell>13.5 19.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ARC+l2s</cell><cell>19.0 17.1</cell><cell>16.3 18.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VLN Transformer</cell><cell>12.9 21.5</cell><cell>14.0 21.2</cell><cell cols="2">17.5 18.6</cell><cell cols="2">15.9 19.0</cell><cell cols="2">1.9 29.5</cell><cell cols="2">2.3 29.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 8 :</head><label>8</label><figDesc>Results on Touchdown and map2seq for the seen and unseen scenario. Metrics are success weighted normalized Dynamic Time Warping (SDTW) and shortest-path distance (SPD). For SDTW higher values are better and for SPD lower values are better.</figDesc><table><row><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Unseen</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>task completion of the ten repetitions</cell><cell>mean std</cell><cell></cell><cell></cell><cell cols="6">task completion of the ten repetitions</cell><cell></cell><cell></cell><cell>mean std</cell></row><row><cell>ORAR full model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ResNet pre-final</cell><cell>26.1 18.5 25.8 25.1 26.8 28.7 24.4 25.5 25.6 26.0</cell><cell>25.3 2.5</cell><cell>8.8</cell><cell>9.2</cell><cell>7.3</cell><cell>9.8</cell><cell>8.5</cell><cell cols="2">8.4 10.0</cell><cell>8.2</cell><cell>9.4</cell><cell>8.1</cell><cell>8.8 0.8</cell></row><row><cell cols="2">? ResNet 4th-to-last 28.2 30.0 26.9 29.6 27.4 29.2 30.4 30.0 28.3 30.7</cell><cell>29.1 1.2</cell><cell cols="10">12.0 15.1 14.5 15.5 14.3 16.0 16.5 14.9 14.5 15.3</cell><cell>14.9 1.2</cell></row><row><cell>ORAR full model</cell><cell>? ResNet 4th-to-last</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">? ResNet 4th-to-last</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-no heading delta</cell><cell>29.2 30.0 27.4 29.9 29.0 29.5 31.2 29.3 28.4 29.0</cell><cell>29.3 1.0</cell><cell cols="10">14.7 13.7 15.5 14.9 14.1 13.5 16.0 15.1 16.0 14.5</cell><cell>14.8 0.8</cell></row><row><cell>-no junction type</cell><cell>24.1 24.5 22.6 21.9 24.4 25.7 26.1 24.5 24.5 24.1</cell><cell>24.2 1.2</cell><cell>4.4</cell><cell>5.0</cell><cell>4.2</cell><cell>4.2</cell><cell>4.2</cell><cell>3.8</cell><cell>4.5</cell><cell>4.2</cell><cell>5.1</cell><cell>4.3</cell><cell>4.4 0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 9 :</head><label>9</label><figDesc>Task completion for the ten individual runs with mean and standard deviation on the Touchdown seen and unseen test set.</figDesc><table><row><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Unseen</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>task completion of the ten repetitions</cell><cell>mean std</cell><cell></cell><cell></cell><cell cols="6">task completion of the ten repetitions</cell><cell></cell><cell></cell><cell>mean std</cell></row><row><cell>ORAR full model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ResNet pre-final</cell><cell>41.0 48.8 47.8 47.9 45.8 49.5 45.8 48.2 44.6 47.4</cell><cell>46.7 2.4</cell><cell cols="10">22.4 18.8 26.0 24.5 26.1 28.1 22.1 26.8 24.4 26.6</cell><cell>24.6 2.6</cell></row><row><cell cols="2">? ResNet 4th-to-last 40.5 42.2 42.1 42.1 38.6 42.9 41.2 42.1 45.2 40.5</cell><cell>41.7 1.6</cell><cell cols="10">32.9 29.6 28.9 28.5 27.6 32.2 26.8 33.6 34.0 28.4</cell><cell>30.3 2.5</cell></row><row><cell>ORAR full model</cell><cell>? ResNet pre-final</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">? ResNet 4th-to-last</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-no heading delta</cell><cell>46.0 43.1 47.1 47.5 45.0 48.4 36.2 44.6 47.1 43.8</cell><cell>44.9 3.3</cell><cell cols="10">23.2 24.0 21.6 25.8 24.5 23.6 23.8 23.2 22.0 24.5</cell><cell>23.6 1.2</cell></row><row><cell>-no junction type</cell><cell>44.9 46.1 46.2 44.0 43.2 46.5 44.9 47.1 45.5 42.1</cell><cell>45.1 1.5</cell><cell>5.1</cell><cell>4.5</cell><cell>5.0</cell><cell>5.1</cell><cell>4.6</cell><cell>3.9</cell><cell>5.6</cell><cell>3.8</cell><cell>4.4</cell><cell>4.6</cell><cell>4.7 0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>Task completion for the ten individual runs with mean and standard deviation on the map2seq seen and unseen test set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Unseen</cell></row><row><cell></cell><cell></cell><cell cols="2">Merged</cell><cell></cell><cell cols="2">Touchdown</cell><cell cols="2">map2seq</cell><cell></cell><cell cols="2">Merged</cell><cell></cell><cell cols="2">Touchdown map2seq</cell></row><row><cell></cell><cell>dev</cell><cell></cell><cell>test</cell><cell></cell><cell>test</cell><cell></cell><cell>test</cell><cell></cell><cell>dev</cell><cell></cell><cell>test</cell><cell></cell><cell>test</cell><cell>test</cell></row><row><cell>Model</cell><cell cols="4">SDTW SPD SDTW SPD</cell><cell cols="4">SDTW SPD SDTW SPD</cell><cell cols="4">SDTW SPD SDTW SPD</cell><cell cols="2">SDTW SPD SDTW SPD</cell></row><row><cell>ORAR full model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? no image</cell><cell cols="2">25.0 18.8</cell><cell cols="2">23.2 19.4</cell><cell cols="2">13.9 26.1</cell><cell cols="2">39.8 7.8</cell><cell cols="2">20.6 17.9</cell><cell cols="2">17.7 21.3</cell><cell cols="2">10.5 26.7</cell><cell>31.1 11.4</cell></row><row><cell>? ResNet pre-final</cell><cell cols="2">36.8 12.5</cell><cell cols="2">34.8 14.1</cell><cell cols="2">26.1 18.8</cell><cell cols="2">50.2 5.7</cell><cell cols="2">20.3 20.1</cell><cell cols="2">18.4 22.0</cell><cell cols="2">12.2 25.8</cell><cell>30.2 14.8</cell></row><row><cell>? ResNet 4th-to-last</cell><cell cols="2">35.9 9.3</cell><cell cols="2">33.8 9.8</cell><cell cols="2">28.4 11.7</cell><cell cols="2">43.2 6.5</cell><cell cols="2">23.6 14.9</cell><cell cols="2">22.5 16.6</cell><cell cols="2">17.7 19.2</cell><cell>31.4 11.7</cell></row><row><cell>ORAR mixed model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? 4th-to-last + pre-final</cell><cell cols="2">42.1 8.6</cell><cell cols="2">40.8 9.3</cell><cell cols="2">34.8 11.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>? 4th-to-last + no image</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">24.1 15.1</cell><cell cols="2">22.2 17.2</cell><cell cols="2">16.9 20.4</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 11 :</head><label>11</label><figDesc>Results for models trained on the merged dataset. Test results are presented for the merged test set and individual Touchdown and map2seq test sets. Metrics are success weighted normalized Dynamic Time Warping (SDTW) and shortest-path distance (SPD). For SDTW higher values are better and for SPD lower values are better.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unseen</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">task completion of the ten repetitions</cell><cell></cell><cell></cell><cell cols="2">mean std</cell><cell></cell><cell></cell><cell>task completion of the ten repetitions</cell><cell>mean std</cell></row><row><cell>ORAR full model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? no image</cell><cell cols="10">24.0 24.1 25.3 25.8 24.6 26.1 24.4 24.1 24.1 24.5</cell><cell cols="2">24.7 0.7</cell><cell cols="3">20.1 18.2 19.5 19.7 18.6 18.6 18.7 19.8 18.6 19.9</cell><cell>19.2 0.7</cell></row><row><cell>? ResNet pre-final</cell><cell cols="10">36.7 34.7 35.3 37.1 36.4 36.1 38.8 36.4 36.8 39.5</cell><cell cols="2">36.8 1.4</cell><cell cols="3">18.9 19.8 19.8 20.8 20.1 20.0 20.5 19.9 20.2 19.9</cell><cell>20.0 0.5</cell></row><row><cell>? ResNet 4th-to-last</cell><cell cols="10">34.9 36.2 35.4 35.4 36.2 36.5 34.1 36.3 36.3 35.5</cell><cell cols="2">35.7 0.7</cell><cell cols="3">23.8 24.9 25.8 23.9 24.1 23.4 24.7 23.9 23.5 24.2</cell><cell>24.2 0.7</cell></row><row><cell>ORAR mixed model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">? 4th-to-last + pre-final 43.8 42.6 43.4 43.2 43.6 42.0 44.1 42.1 41.9 42.7</cell><cell cols="2">42.9 0.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>? 4th-to-last + no image</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">23.8 23.4 23.7 23.4 24.1 24.7 24.6 24.1 23.8 22.9</cell><cell>23.8 0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 12 :</head><label>12</label><figDesc>Task completion for the ten individual runs with mean and standard deviation on the merged seen and unseen test set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unseen</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">task completion of the ten repetitions</cell><cell></cell><cell></cell><cell cols="2">mean std</cell><cell></cell><cell></cell><cell>task completion of the ten repetitions</cell><cell>mean std</cell></row><row><cell>ORAR full model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? no image</cell><cell cols="10">14.1 14.4 15.8 16.5 14.1 16.3 14.5 14.9 13.3 14.5</cell><cell cols="2">14.8 1.0</cell><cell cols="3">12.1 10.7 12.1 12.2 11.0 11.5 11.5 12.9 11.0 12.2</cell><cell>11.7 0.7</cell></row><row><cell>? ResNet pre-final</cell><cell cols="10">27.5 25.1 26.4 28.4 26.6 27.4 30.3 27.7 27.0 30.2</cell><cell cols="2">27.7 1.5</cell><cell cols="3">13.1 13.0 13.1 14.1 13.5 13.7 14.1 13.5 13.9 13.7</cell><cell>13.6 0.4</cell></row><row><cell>? ResNet 4th-to-last</cell><cell cols="10">30.7 30.4 30.0 30.1 30.0 30.2 29.2 30.2 30.4 30.0</cell><cell cols="2">30.1 0.4</cell><cell cols="3">18.0 20.3 20.8 18.8 18.9 18.0 20.2 19.8 18.6 19.6</cell><cell>19.3 0.9</cell></row><row><cell>ORAR mixed model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">? 4th-to-last + pre-final 37.6 36.3 36.4 37.8 37.9 35.1 38.0 36.6 35.6 37.4</cell><cell cols="2">36.9 1.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>? 4th-to-last + no image</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">17.8 17.9 18.1 18.8 18.8 19.2 19.2 18.4 17.9 17.1</cell><cell>18.3 0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 13 :</head><label>13</label><figDesc>Task completion for the ten individual runs with mean and standard deviation on the Touchdown seen and unseen test set, trained on the merged training set.</figDesc><table><row><cell></cell><cell>Seen</cell><cell></cell><cell>Unseen</cell><cell></cell></row><row><cell></cell><cell>task completion of the ten repetitions</cell><cell>mean std</cell><cell>task completion of the ten repetitions</cell><cell>mean std</cell></row><row><cell>ORAR full model</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? no image</cell><cell>41.5 41.2 42.1 42.1 43.2 43.4 41.9 40.4 43.1 42.1</cell><cell>42.1 0.9</cell><cell>35.1 32.5 33.6 33.8 32.9 32.0 32.4 32.8 32.8 34.2</cell><cell>33.2 0.9</cell></row><row><cell>? ResNet pre-final</cell><cell>53.0 51.5 51.0 52.4 53.6 51.5 53.6 51.6 53.9 55.8</cell><cell>52.8 1.4</cell><cell>29.9 32.6 32.2 33.4 32.6 32.0 32.6 32.0 32.0 31.5</cell><cell>32.1 0.9</cell></row><row><cell cols="2">? ResNet 4th-to-last 42.5 46.4 44.9 44.6 47.1 47.8 42.8 47.1 46.6 45.2</cell><cell>45.5 1.7</cell><cell>34.8 33.5 35.2 33.5 33.9 33.6 33.1 31.6 32.6 33.0</cell><cell>33.5 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 14 :</head><label>14</label><figDesc>Task completion for the ten individual runs with mean and standard deviation on the map2seq seen and unseen test set, trained on the merged training set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://pytorch.org/vision/0.8/models. html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The shortest path bias reduces the number of reasonable directions at each intersection and thus makes the task easier.4  Except comparison models on the Touchdown seen test set for which we copy the results from the respective work. 5 https://docs.scipy.org/doc/scipy/ reference/generated/scipy.stats.ttest_ rel.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research reported in this paper was supported by a Google Focused Research Award on "Learning to Negotiate Answers in Multi-Pass Semantic Parsing".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Salt Lake City, Utah</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gatedattention architectures for task-oriented language grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanthashree</forename><forename type="middle">Mysore</forename><surname>Sathyendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Kumar Pasumarthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence (AAAI)<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Touchdown: Natural language navigation and spatial reasoning in visual street environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Long Beach, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bidirectional lstm networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Neural Networks: Formal Models and Their Applications (ICANN)</title>
		<meeting>International Conference on Artificial Neural Networks: Formal Models and Their Applications (ICANN)<address><addrLine>Warsaw, Poland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to follow directions in street view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)<address><addrLine>New York, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are you looking? grounding to multiple modalities in vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1655</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6551" to="6557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective and general evaluation for instruction conditioned navigation using dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Visually Grounded Interaction and Language Workshop</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stay on the path: Instruction fidelity in vision-andlanguage navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond the nav-graph: Vision-and-language navigation in continuous environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Room-Across-Room: Multilingual vision-and-language navigation with dense spatiotemporal grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roma</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Walk the talk: Connecting language, knowledge, and action in route instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Macmahon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Stankiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st National Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 21st National Conference on Artificial Intelligence (AAAI)<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving vision-and-language navigation with imagetext pairs from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV), Glasgow</title>
		<meeting>the European Conference on Computer Vision (ECCV), Glasgow<address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Retouchdown: Releasing touchdown on StreetLearn as a public resource for language grounding tasks in street view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Spatial Language Understanding (SpLU)</title>
		<meeting>the Third International Workshop on Spatial Language Understanding (SpLU)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to navigate in cities without a map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">Koichi</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems (NeurIPS)<address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Andrew Zisserman, and Raia Hadsell</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BPE-dropout: Simple and effective subword regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Provilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Emelianenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating landmark navigation instructions from maps as a graph-to-text problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to navigate unseen environments: Back translation with environmental dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Hao Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shifting the baseline: Single modality performance on visual navigation &amp; QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS), Long Beach, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to stop: A simple yet effective approach to urban vision-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannan</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics (ACL Findings)</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Omnisupervised omnidirectional semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Intelligent Transportation Systems (T-ITS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Diagnosing the environment bias in vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the evaluation of vision-and-language navigation instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Diagnosing vision-and-language navigation: What really matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradyumna</forename><surname>Narayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazoo</forename><surname>Sone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugato</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><forename type="middle">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">P</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2103.16561</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multimodal text style transfer for outdoor vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradyumna</forename><surname>Narayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazoo</forename><surname>Sone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugato</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
