<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Unlabeled Data with Vision and Language Models for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Labs America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
								<address>
									<postCode>4 UC</postCode>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Labs America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasis</forename><surname>Stathopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Labs America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Unlabeled Data with Vision and Language Models for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Building robust and generic object detection frameworks requires scaling to larger label spaces and bigger training datasets. However, it is prohibitively costly to acquire annotations for thousands of categories at a large scale. We propose a novel method that leverages the rich semantics available in recent vision and language models to localize and classify objects in unlabeled images, effectively generating pseudo labels for object detection. Starting with a generic and class-agnostic region proposal mechanism, we use vision and language models to categorize each region of an image into any object category that is required for downstream tasks. We demonstrate the value of the generated pseudo labels in two specific tasks, open-vocabulary detection, where a model needs to generalize to unseen object categories, and semi-supervised object detection, where additional unlabeled images can be used to improve the model. Our empirical evaluation shows the effectiveness of the pseudo labels in both tasks, where we outperform competitive baselines and achieve a novel state-of-the-art for open-vocabulary object detection. Our code is available at https://github.com/xiaofeng94/VL-PLM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in object detection build on large-scale datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43]</ref>, which provide rich and accurate human-annotated bounding boxes for many object categories. However, the annotation cost of such datasets is significant. Moreover, the long-tailed distribution of natural object categories makes it even harder to collect sufficient annotations for all categories. Semi-supervised object detection (SSOD) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b61">62]</ref> and open-vocabulary object detection (OVD) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b55">56]</ref> are two tasks to lower annotations costs by leveraging different forms of unlabeled data. In SSOD, a small fraction of fully-annotated training images is given along with a large corpus of unlabeled images. In OVD, a fraction of the desired object categories is annotated (the base categories) in all training images and the task is to also detect a set of novel (or unknown) categories at test time. These object categories can be present in the training images, but are not annotated with ground truth bounding boxes. A common and successful approach for leveraging ? Equal contribution.  unlabeled data is by generating pseudo labels. However, all prior works on SSOD only leveraged the small set of labeled data for generating pseudo labels, while most prior work on OVD does not leverage pseudo labels at all. In this work, we propose a simple but effective way to mine unlabeled images using recently proposed vision and language (V&amp;L) models to generate pseudo labels for both known and unknown categories, which suits both tasks, SSOD and OVD. V&amp;L models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref> can be trained from (noisy) image caption pairs, which can be obtained at a large scale without human annotation efforts by crawling websites for images and their alt-texts. Despite the noisy annotations, these models demonstrate excellent performance on various semantic tasks like zero-shot classification or image-text retrieval. The large amount of diverse images, combined with the free-form text, provides a powerful source of information to train robust and generic models. These properties make vision and language models an ideal candidate to improve existing object detection pipelines that leverage unlabeled data, like OVD or SSOD, see <ref type="figure" target="#fig_0">Fig. 1(a)</ref>.</p><p>Specifically, our approach leverages the recently proposed vision and language model CLIP <ref type="bibr" target="#b37">[38]</ref> to generate pseudo labels for object detection. We first predict region proposals with a two-stage class-agnostic proposal generator which was trained with limited ground truth (using only known base categories in OVD and only labeled images in SSOD), but generalizes to unseen categories. For each region proposal, we then obtain a probability distribution over the desired object categories (depending on the task) with the pre-trained V&amp;L model CLIP <ref type="bibr" target="#b37">[38]</ref>. However, as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, a major challenge of V&amp;L models is the rather low object localization quality, also observed in <ref type="bibr" target="#b58">[59]</ref>. To improve localization, we propose two strategies where the two-stage proposal generator helps the V&amp;L model: <ref type="bibr" target="#b0">(1)</ref> Fusing CLIP scores and objectness scores of the two-stage proposal generator, and (2) removing redundant proposals by repeated application of the localization head (2nd stage) in the proposal generator. Finally, the generated pseudo labels are combined with the original ground truth to train the final detector. We name our method as V&amp;L-guided Pseudo-Label Mining (VL-PLM).</p><p>Extensive experiments demonstrate that VL-PLM successfully exploits the unlabeled data for open-vocabulary detection and outperforms the state-of-the-art ViLD <ref type="bibr" target="#b15">[16]</ref> on novel categories by +6.8 AP on the COCO dataset <ref type="bibr" target="#b31">[32]</ref>. Moreover, VL-PLM improves the performance on known categories in SSOD and beats the popular baseline STAC <ref type="bibr" target="#b45">[46]</ref> by a clear margin, by only replacing its pseudo labels with ours. Besides, we also conduct various ablation studies on the properties of the generated pseudo labels and analyze the design choices of our proposed method. We also believe that VL-PLM can be further improved with better V&amp;L models like ALIGN <ref type="bibr" target="#b22">[23]</ref> or ALBEF <ref type="bibr" target="#b28">[29]</ref>.</p><p>The contributions of our work are as follows: (1) We leverage V&amp;L models for improving object detection frameworks by generating pseudo labels on unlabeled data. (2) A simple but effective strategy to improve the localization quality of pseudo labels scored with the V&amp;L model CLIP <ref type="bibr" target="#b37">[38]</ref>. (3) State-of-the-art results for novel categories on the COCO open-vocabulary detection setting. (4) We showcase the benefits of VL-PLM in a semi-supervised object detection setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The goal of our work is to improve object detection systems by leveraging unlabeled data via vision and language models that carry rich semantic information. Vision &amp; language (VL) models: Combining natural language and images has enabled many valuable applications in recent years, like image captioning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref>, visual question answering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b56">57]</ref>, referring expression comprehension <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>, image-text retrieval <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49]</ref> or languagedriven embodied AI <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>. While early works proposed task-specific models, generic representation learning from vision and language inputs has gained more attention <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47]</ref>. Most recent works like CLIP <ref type="bibr" target="#b37">[38]</ref> or ALIGN <ref type="bibr" target="#b22">[23]</ref> also propose generic vision and language representation learning approaches, but have significantly increased the scale of training data, which led to impressive results in tasks like zero-shot image classification or image-text retrieval. The training data consist of image and text pairs, typically crawled from the web at a very large scale (400M for <ref type="bibr" target="#b37">[38]</ref> and 1.2B for <ref type="bibr" target="#b22">[23]</ref>), but without human annotation effort. In our work, we leverage such pre-trained models to mine unlabeled data and to generate pseudo labels in the form of bounding boxes, suitable for object detection. One challenge with using such V&amp;L models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref> is their limited capability in localizing objects (recall <ref type="figure" target="#fig_0">Fig. 1(b)</ref>), likely due to the lack of regionword alignment in the image-text pairs of their training data. In Sec. 3.2, we show how to improve localization quality with our proposal generator. Vision &amp; language models for dense prediction tasks: The success of CLIP <ref type="bibr" target="#b37">[38]</ref> (and others <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>) has motivated the extension of zero-shot classifica-tion capabilities to dense image prediction tasks like object detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56]</ref> or semantic segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b60">61]</ref>. These works try to map features of individual objects (detection) or pixels (segmentation) into the joint vision-language embedding space provided by models like CLIP. For example, ViLD <ref type="bibr" target="#b15">[16]</ref> trains an object detector in the open-vocabulary regime by predicting the text embedding (from the CLIP text-encoder) of the category name for each image region. LSeg <ref type="bibr" target="#b27">[28]</ref> follows a similar approach, but is applied to zero-shot semantic segmentation. Both works leverage task-specific insights and do not generate explicit pseudo labels. In contrast, our proposed VL-PLM is more generic by generating pseudo labels, thus enabling also other tasks like semi-supervised object detection <ref type="bibr" target="#b45">[46]</ref>. Similar to our work, both Gao et al. <ref type="bibr" target="#b13">[14]</ref> and Zhong et al. <ref type="bibr" target="#b58">[59]</ref> generate explicit pseudo labels in the form of bounding boxes. In <ref type="bibr" target="#b13">[14]</ref>, the attention maps of a pretrained V&amp;L model <ref type="bibr" target="#b28">[29]</ref> between words of a given caption and image regions are used together with object proposals to generate pseudo labels. In contrast, our approach does not require image captions as input and we use only unlabeled images, while still outperforming <ref type="bibr" target="#b13">[14]</ref> in an open-vocabulary setting on COCO. RegionCLIP <ref type="bibr" target="#b58">[59]</ref> assigns semantics to region proposals via a pre-trained V&amp;L model, effectively creating pseudo labels in the form of bounding boxes. While our approach uses such pseudo labels directly for training object detectors, <ref type="bibr" target="#b58">[59]</ref> uses them for fine-tuning the original V&amp;L model, which then builds the basis for downstream tasks like open-vocabulary detection. We believe this contribution is orthogonal to ours as it effectively builds a better starting point of the V&amp;L model, and can be incorporated into our framework as well. Interestingly, even without the refined V&amp;L model, we show improved accuracy with pseudo labels specifically for novel categories as shown in Sec. 4.1.</p><p>The main focus of all the aforementioned works is to enable the dynamic expansion of the label space and to recognize novel categories. While our work also demonstrates state-of-the-art results in this open-vocabulary setting, where we mine unlabeled data for novel categories, we want to stress that our pseudo labels are applicable more generally. In particular, we also use a V&amp;L model to mine unlabeled images for known categories in a semi-supervised object detection setting. Furthermore, by building on the general concept of pseudo labels, our approach may be extended to other dense prediction tasks like semantic segmentation in future works as well.</p><p>Object detection from incomplete annotations: Pseudo labels are proven useful in many recent object detection methods trained with various forms of weak annotations: semi-supervised detection <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b61">62]</ref>, unsupervised object discovery <ref type="bibr" target="#b44">[45]</ref>, open-vocabulary detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b58">59]</ref>, weakly-supervised detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b59">60]</ref>, unsupervised domain adaptation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref> or multi-dataset detection <ref type="bibr" target="#b57">[58]</ref>. In all cases, an initial model trained from base information is applied on the training data to obtain the missing information. Our main proposal is to leverage V&amp;L models to improve these pseudo labels and have one unified way of improving the accuracy in multiple settings, see Sec. 3.3. In this work, we focus on two important forms of weak supervision: zero-shot/open-vocabulary detection (OVD) and semi-supervised object detection (SSOD). In zero-shot detection <ref type="bibr" target="#b3">[4]</ref> a model is trained from a set of base categories. Without ever seeing any instance of a novel category during training, the model is asked to predict novel categories, typically via association in a different embedding space, like attribute or text embeddings. Recent works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56]</ref> relax the setting to include novel categories in the training data, but without bounding box annotations, which also enables V&amp;L models to be used (via additional images that come with caption data). ViLD <ref type="bibr" target="#b15">[16]</ref>, as described above, uses CLIP <ref type="bibr" target="#b37">[38]</ref> with model distillation losses to make predictions in the joint vision-text embedding space. In contrast, we demonstrate that explicitly creating pseudo labels for novel categories via mining the training data can significantly improve the accuracy, see Sec. 4.1. The second task we focus on is semi-supervised object detection (SSOD), where a small set of images with bounding box annotations and a large set of unlabeled images are given. In contrast to OVD, the label space does not change from train to test time. A popular and recent baseline that builds on pseudo labels is STAC <ref type="bibr" target="#b45">[46]</ref>. This approach employs a consistency loss between predictions on a strongly augmented image and pseudo labels computed on the original image. We demonstrate the benefit of leveraging V&amp;L models to improve the pseudo label quality in such a framework. Other works on SSOD, like <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b61">62]</ref> propose several orthogonal improvements which can be incorporated into our framework as well. In this work, however, we focus purely on the impact of the pseudo labels. Finally, note that our concepts may also be applicable to other tasks beyond open-vocabulary and semi-supervised object detection, but we leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The goal of our work is to mine unlabeled images with vision &amp; language (V&amp;L) models to generate semantically rich pseudo labels (PLs) in the form of bounding boxes so that object detectors can better leverage unlabeled data. We start with a generic training strategy for object detectors with the unlabeled data in Sec. 3.1. Then, Sec. 3.2 describes the proposed VL-PLM for pseudo label generation. Finally, Sec. 3.3 presents specific object detection tasks with our PLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training object detectors with unlabeled data</head><p>Unlabeled data comes in many different forms for object detectors. In semisupervised object detection, we have a set of fully-labeled images I L with annotations for the full label space S, as well as unlabeled images</p><formula xml:id="formula_0">I U , with I L ? I U = ?.</formula><p>In open-vocabulary detection, we have partly-labeled images with annotations for the set of base categories S B , but without annotations for the unknown/novel categories S N . Note that partly-labeled images are therefore contained in both I L and I U , i.e., I L = I U .</p><p>A popular and successful approach to learn from unlabeled data is via pseudo labels. Recent semi-supervised object detection methods follow this approach by first training a teacher model on the limited ground truth data, then generating pseudo labels for the unlabeled data, and finally training a student model. In the following, we describe a general training strategy for object detection to handle different forms of unlabeled data.</p><p>We define a generic loss function for an object detector with parameters ? over both labeled and unlabeled images as</p><formula xml:id="formula_1">L(?, I) = 1 N I N I i=1 [I i ? I L ] l s (?, I i ) + ?[I i ? I U ] l u (?, I i ) ,<label>(1)</label></formula><p>where ? is a hyperparameter to balance supervised l s and unsupervised l u losses and [?] is the indicator function returning either 0 or 1 depending on the condition. Note again that I i can be contained in both I L and I U .</p><p>Object detection ultimately is a set prediction problem and to define a loss function, the set of predictions (class probabilities and bounding box estimates) need to be matched with the set of ground truth boxes. Different options exist to find a matching <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> but it is mainly defined by the similarity (IoU) between predicted and ground truth boxes. We define the matching for prediction i as ?(i), which returns a ground truth index j if successfully matched or nil otherwise. The supervised loss l s contains a standard cross-entropy loss for the classification l cls and an ? 1 loss for the box regression l reg . Given I ? I, we define l s as,</p><formula xml:id="formula_2">l s (?, I) = 1 N * i l cls C ? i (I), c * ?(i) + [?(i) ? = nil] l reg T ? i (I), t * ?(i) ,<label>(2)</label></formula><p>where N * is the number of predicted bounding boxes. C ? i (?) and T ? i (?) are the predicted class distributions and bounding boxes of the object detector. The corresponding (matched) ground truth is defined as c * ?(i) and t * ?(i) , respectively. The unsupervised loss l u is similarly defined, but uses pseudo labels with high confidence as supervision signals:</p><formula xml:id="formula_3">l u (?, I) = 1 N u i [max(p u ?(i) ) ? ? ] ? l cls C ? i (I),? u ?(i) + [?(i) ? = nil] l reg T ? i (I), t u ?(i) .<label>(3)</label></formula><p>Here, p u ?(i) defines the probability distribution over the label space of the pseudo label matched with prediction i and N u is the number of adopted pseudo labels, i.e.,</p><formula xml:id="formula_4">N u = i [max(p u ?(i) ) ? ? ].</formula><p>Pseudo labels for the classification and the box regression losses are? u ?(i) = arg max(p u ?(i) ) and t u ?(i) , respectively. The key to successful training of object detectors from unlabeled data are accurate pseudo labels. In the next section, we will present our approach, VL-PLM, to leverage V&amp;L models as external models to exploit unlabeled data for generating pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VL-PLM: Pseudo labels from vision &amp; language models</head><p>V&amp;L models are trained on large scale datasets with image-text pairs that cover a diverse set of image domains and rich semantics in natural text. Moreover, the  Overview of the proposed VL-PLM to mine unlabeled images with vision &amp; language models to generate pseudo labels for object detection. The top part illustrates our class-agnostic proposal generator, which improves the pseudo label localization by using the class-agnostic proposal score and the repeated application of the RoI head. The bottom part illustrates the scoring of cropped regions with the V&amp;L model based on the target category names. The chosen category names can be adjusted for the desired downstream task. After thresholding and NMS, we get the final pseudo labels. For some tasks like SSOD, we will merge external pseudo labels for a teacher model with ours before thresholding and NMS.</p><formula xml:id="formula_5">R 1 R2 ? Bn N1 N2 ? Nk B1 R1B1 ? R1Bn R2N1 R1N2 ? R1Nk R2B1 ? R2B1 R2N1 R2N2 ? R2Nk ? Thresholding ? NMS</formula><p>image-text pairs can be obtained without costly human annotation by using webcrawled data (images and corresponding alt-texts) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23]</ref>. Thus, V&amp;L models are ideal sources of external knowledge to generate pseudo labels for arbitrary categories, which can be used for downstream tasks like open-vocabulary or semi-supervised object detection.</p><p>Overview: <ref type="figure">Fig. 2</ref> illustrates the overall pipeline of our pseudo label generation with the recent V&amp;L model CLIP <ref type="bibr" target="#b37">[38]</ref>. We first feed an unlabeled image into our two-stage class-agnostic detector (described in the next section below) to obtain region proposals. We then crop image patches based on those regions and feed them into the CLIP image-encoder to obtain an embedding in the CLIP vision-and-language space. Using the corresponding CLIP text-encoder and template text prompts, we generate embeddings for category names that are desired for the specific task. For each region, we compute the similarities between the region embedding and the text embeddings via a dot product and use softmax to obtain a distribution over the categories. We then generate the final pseudo labels using scores from both class-agnostic detector and V&amp;L model, which we describe in detail below.</p><p>There are two key challenges in our framework: <ref type="bibr">(</ref> the poor localization quality of the raw CLIP model, see <ref type="figure" target="#fig_0">Fig. 1</ref>(b). We introduce simple but effective solutions to address the two challenges in the following.</p><p>Generating robust and class-agnostic region proposals: To benefit tasks like open vocabulary detection with the unlabeled data, the proposal generator should be able to locate not only objects of categories seen during training but also of objects of novel categories. While unsupervised candidates like selective search <ref type="bibr" target="#b47">[48]</ref> exist, these are often time-consuming and generate many noisy boxes.</p><p>As suggested in prior studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b55">56]</ref>, the region proposal network (RPN) of a two-stage detector generalizes well for novel categories. Moreover, we find that the RoI head is able to improve the localization of region proposals, which is elaborated in the next section. Thus, we train a standard two-stage detector, e.g., Faster-RCNN <ref type="bibr" target="#b41">[42]</ref>, as our proposal generator using available ground truth, which are annotations of base categories for open vocabulary detection and annotations from the small fraction of annotated images in semi-supervised detection. To further improve the generalization ability, we ignore the category information of the training set and train a class-agnostic proposal generator. Please refer to Sec. 4.3 and the supplement for a detailed analysis of the proposal generator.</p><p>Generating pseudo labels with a V&amp;L model: Directly applying CLIP <ref type="bibr" target="#b37">[38]</ref> on cropped region proposals yields low localization quality, as was observed in <ref type="figure" target="#fig_0">Fig. 1</ref>(b) and also in <ref type="bibr" target="#b58">[59]</ref>. Here, we demonstrate how to improve the localization ability with our two-stage class-agnostic proposal generator in two ways. Firstly, we find that the RPN score is a good indicator for localization quality of region proposals. <ref type="figure">Fig. 3</ref>(a) illustrates a positive correlation between RPN and IoU scores. We leverage this observation and average the RPN score with those of the CLIP predictions. Secondly, we remove thresholding and NMS of the proposal generator and feed proposal boxes into the RoI head multiple times, similar to <ref type="bibr" target="#b4">[5]</ref>. We observe that it pushes redundant boxes closer to each other by repeating the RoI head, which can be seen in <ref type="figure">Fig. 3</ref>(b). In this way, we encounter better located bounding boxes and provide better pseudo labels. Please refer to Sec. 4.3 for a corresponding empirical analysis.</p><p>To further improve the quality of our pseudo labels, we adopt the multiscale region embedding from CLIP as described in <ref type="bibr" target="#b15">[16]</ref>. Moreover, as suggested in <ref type="bibr" target="#b45">[46]</ref>, we employ a high threshold to pick pseudo labels with high confidence.</p><p>The confidence score of the pseudo label for the region R i is formulated as</p><formula xml:id="formula_6">c u i = [s u i ? ? ] ? s u i , with s u i = S RP N (R i ) + max(p u i ) 2 ,<label>(4)</label></formula><p>where S RP N (?) denotes the RPN score. The prediction probability distribution p u i is defined as</p><formula xml:id="formula_7">p u i = softmax{?(E im (R i ) + E im (R 1.5? i )) ? E txt (Categories) T }.<label>(5)</label></formula><p>Here, R 1.5?</p><p>i is a region cropped by 1.5? the size of R i . E im and E txt are the image and text encoders of CLIP, respectively, and ?(x) = x/||x||. If c u i = 0, we exclude R i from our pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Using our pseudo labels for downstream tasks</head><p>Finally, we briefly describe how we use the pseudo labels that are generated from unlabeled data for two specific downstream tasks that we focus on in this work. Open-vocabulary detection: In this task, the detector has access to images with annotations for base categories and needs to generalize to novel categories. We leverage the data of the base categories to train a class-agnostic Mask R-CNN as our proposal generator and take the names of novel categories as the input texts of the CLIP text-encoder in aforementioned pseudo label generation process. Then, we train a standard Mask R-CNN with RestNet50-FPN <ref type="bibr" target="#b30">[31]</ref> with both base ground truth and novel pseudo labels as described in Sec. 3.1. Semi-supervised object detection: In this task, relevant methods usually train a teacher model using ground truth from the limited set of labeled images, and then generate pseudo labels with the teacher on the unlabeled images. We also generate those pseudo labels and merge them with pseudo labels from our VL-PLM. Please refer to the supplementary document for details. Thus, the student model is trained on available ground truth and pseudo labels from both our V&amp;L-based approach and the teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We experimentally evaluate the proposed VL-PLM first on open-vocabulary detection in Sec. 4.1 and then on semi-supervised object detection in Sec. 4.2. In Sec. 4.3 we ablate various design choices of VL-PLM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Open-vocabulary object detection</head><p>In this task, we have a training set with annotations for known base categories S B . Our goal is to train a detector for novel categories S N . Usually, the labeled images I L and the unlabeled images I U are the same, i.e., I L = I U .</p><p>Experimental setup: Following prior studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b55">56]</ref>, we base our evaluation on COCO 2017 <ref type="bibr" target="#b31">[32]</ref> in the zero-shot setting (COCO-ZS) where there are 48 known base categories and 17 unknown novel categories. Images from the training set are regarded as labeled for base classes and also as unlabeled for novel classes. We take the widely adopted mean Average Precision at an IoU of 0.5 (AP 50 ) as the metric and mainly compare our method with ViLD <ref type="bibr" target="#b15">[16]</ref>, the state-of-the-art method for open vocabulary detection. Thus, we follow ViLD and report AP 50 over novel categories, base categories and all categories as Novel AP, Base AP, and Overall AP, respectively. Our supplemental material contains results for the LVIS <ref type="bibr" target="#b16">[17]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details:</head><p>We set a NMS threshold of 0.3 for the RPN of the proposal generator. The confidence threshold for pseudo labels (PLs) is ? = 0.8. Finally, we obtain an average of 4.09 PLs per image, which achieve a Novel AP of 20.9. We use the above hyperparameters for pseudo label generation in all experiments, unless otherwise specified. The proposal generator and the final detector were implemented in Detectron2 <ref type="bibr" target="#b49">[50]</ref> and trained on a server with NVIDIA A100 GPUs. The proposal generator was trained for 90,000 iterations with a batch size of 16. Similar to ViLD, the final detector is trained from scratch for 180,000 iterations with input size of 1024 ? 1024, large-scale jitter augmentation <ref type="bibr" target="#b14">[15]</ref>, synchronized batch normalization of batch size 128, weight decay of 4e-5, and an initial learning rate of 0.32.</p><p>Comparison to SOTA: As shown in <ref type="table" target="#tab_3">Table 1</ref>, the detector trained with VL-PLM significantly outperforms the prior state-of-the-art ViLD by nearly +7% in Novel AP. Compared with <ref type="bibr" target="#b55">[56]</ref> and <ref type="bibr" target="#b13">[14]</ref>, our method achieves much better performance not only on novel but also on base categories. This indicates training with our PLs has less impact on the predictions of base categories, where previous approaches suffered a huge performance drop. Overall, we can see that using V&amp;L models to explicitly generate PLs for novel categories to train the model can give a clear performance boost. Although this introduces an overhead compared to ViLD (and others), which can include novel categories dynamically into the label space, many practical applications easily tolerate this overhead in favor of significantly improved accuracy. Such a setup is also similar to prior works that generate synthetic features of novel categories <ref type="bibr" target="#b62">[63]</ref>. Moreover, our method has large potential for further improvement with better V&amp;L model. <ref type="bibr" target="#b15">[16]</ref> demonstrates a 60% performance boost of ViLD when using ALIGN <ref type="bibr" target="#b22">[23]</ref> as the V&amp;L model. We expect similar improvements on VL-PLM if ALIGN is available.</p><p>Generalizing to unseen datasets: Following Gao et al.'s evaluation protocol <ref type="bibr" target="#b13">[14]</ref>, we evaluate COCO-trained models on three unseen datasets: VOC 2007 <ref type="bibr" target="#b10">[11]</ref>, Object365 <ref type="bibr" target="#b42">[43]</ref> and LVIS <ref type="bibr" target="#b16">[17]</ref>. To do so, we generate PLs for the novel label spaces of these datasets on the COCO dataset and train a standard Faster R-CNN model. The results of our approach on the three unseen datasets is compared to <ref type="bibr" target="#b13">[14]</ref> in <ref type="table" target="#tab_4">Table 2</ref>. VL-PLM significantly outperforms <ref type="bibr" target="#b13">[14]</ref> with similar iterations and smaller batch sizes. Note that <ref type="bibr" target="#b13">[14]</ref> requires additional image captions to generate PLs, while VL-PLM can generate PLs for any given category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semi-supervised object detection</head><p>In this task, we have annotations for all categories on a small portion of a large image set. This portion is regarded as the labeled set I L and the remaining images are regarded as the unlabeled set I U i.e. I L ? I U = ?.  Here we report the standard metric for COCO, mAP, which is an average over IoU thresholds from 0.5 to 0.95 with a step size of 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details:</head><p>We follow the same PL generation pipeline and hyperparameters as the OVD experiment, except that we take a class-agnostic Faster R-CNN <ref type="bibr" target="#b41">[42]</ref> as our proposal generator and train it on the different COCO splits. Supervised and Supervised +PLs are implemented in Detectron2 <ref type="bibr" target="#b49">[50]</ref> and trained for 90,000 iterations with a batch size of 16. For models related to STAC <ref type="bibr" target="#b45">[46]</ref>, we use the official code of STAC with default settings.</p><p>Results: As shown in <ref type="table" target="#tab_6">Table 3</ref>, models with VL-PLM outperform Supervised + PLs and STAC by a clear margin, respectively. Since the only change to the baselines is the addition of VL-PLM's PLs, we can conclude that V&amp;L adds clear value to the PLs and can benefit SSOD. Another interesting finding is that models with VL-PLM provide bigger gains for smaller labeled data, which is the most important regime for SSOD as it brings down annotation costs. In that regime, PLs from V&amp;L models are likely stronger than PLs from the small amount of annotated data. We also want to mention two recent SSOD methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b61">62]</ref> that achieve higher absolute performance, however, only with additional and orthogonal contributions. VL-PLM may also improve these methods, but here we focus on a fair comparison to other PL-based methods. Moreover, we believe that with better V&amp;L models, VL-PLM can further improve SSOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of pseudo label generation</head><p>We base our ablation studies on the COCO-ZS setting for OVD unless otherwise specified. All models are trained for 90,000 iterations with a batch size of 16.</p><p>Understanding the quality of PLs: Average precision (AP) is a dominant metric to evaluate object detection methods. However, AP alone does not fully indicate the quality of PLs, and the number of PLs also needs to be considered. To support this claim, we generate 5 sets of PLs as follows. (1) PL v1 : We take the raw region proposals from RPN without RoI refinement in our pseudo label  <ref type="figure">Fig. 4</ref>. The quality of PLs with different combinations of RPN and RoI head. We change the threshold ? to ensure each combination with a similar #@PL. "?N" means we apply RoI head N times to refine the proposal boxes.   <ref type="table" target="#tab_8">Table 4</ref>, we report AP 50 (AP@PL) and the average per-image number (#@PL) of pseudo labels on novel categories. We also report the performance of detection models trained with the corresponding PLs as Novel AP, Base AP and Overall AP. Comparing PL v1 with PL v4 and PL v2 with PL v4, we can see that a good balance between AP@PL and #@PL is desired. Many PLs may achieve high AP@PL, but drop the performance of the final detector. A high threshold reduces the number of PLs but degrades AP@PL as well as the final performance. We found ? = 0.8 to provide a good trade-off. The table also demonstrates the benefit of VL-PLM over no RoI refinement. The supplement contains more analysis and visualizations of our pseudo labels.</p><p>Two-stage proposal generator matters: As mentioned in Sec. 3.2, we improve the localization ability of CLIP with the two-stage proposal generator in two ways: 1) we merge CLIP scores with RPN scores, and 2) we repeatedly refine the region proposals from RPN with the RoI Head. To showcase how RPN and the RoI head help PLs, we evaluate the quality of PLs from different settings in <ref type="figure">Fig. 4</ref>. As shown, RPN score fusion always improves the quality of PLs. As we increase the number of refinement steps with RoI head, the quality increases and converges after about 10 steps. Besides proposals from our RPN with RoI refinement (RPN+RoI), we investigate region proposals from different sources, i.e. 1) Selective search <ref type="bibr" target="#b47">[48]</ref>, 2) RPN only, and 3) RoI head with default thresholding and NMS. <ref type="table" target="#tab_9">Table 5</ref> shows that selective search with a high ? still leads to a large #@PL with a low AP@PL for at least two reasons. First, unlike RPN, selective search does not provide objectiveness scores to improve the localization of CLIP. Second, it returns ten times more proposals than RPN, which contain too many noisy boxes. Finally, the RoI head alone also leads to a poor quality of PLs because it classifies many novel objects as background, due to its training protocol. In the supplement, we show that the proposal generator, which is trained on base categories, generalizes to novel categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper demonstrates how to leverage pre-trained V&amp;L models to mine unlabeled data for different object detection tasks, e.g., OVD and SSOD. We propose a V&amp;L model guided pseudo label mining framework (VL-PLM) that is simple but effective, and is able to generate pseudo labels (PLs) for a task-specific labelspace. Our experiments showcase that training a standard detector with our PLs sets a new state-of-the-art for OVD on COCO. Moreover, our PLs can benefit SSOD models, especially when the amount of ground truth labels is limited. We believe that VL-PLM can be further improved with better V&amp;L models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Materials</head><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Open-vocabulary detection results on LVIS</head><p>In addition to our open-vocabulary detection (OVD) experiments on COCO <ref type="bibr" target="#b31">[32]</ref> in Sec. 4.1, we also evaluate our model on the LVIS <ref type="bibr" target="#b16">[17]</ref> dataset. LVIS is a large-vocabulary dataset with 1203 categories and shares images with COCO <ref type="bibr" target="#b31">[32]</ref>. We follow the experimental setup of ViLD <ref type="bibr" target="#b15">[16]</ref>, the state-of-the-art method on LVIS for OVD (LVIS-OVD): All categories are divided into three sets, namely, frequent, common, and rare, based on the numbers of their objects. Following <ref type="bibr" target="#b15">[16]</ref>, we take frequent and common categories as the base categories and regard rare categories as the novel categories. We leverage base categories to train our two-stage class-agnostic proposal generator and adopt VL-PLM to generate PLs for novel categories. Then, a standard OVD detector was trained with both the ground truth of base categories and our PLs. Comparison with ViLD: <ref type="table" target="#tab_11">Table 6</ref> compares our detector via VL-PLM with Supervised and the state-of-the-art method ViLD. Supervised is the supervised baseline model trained on the whole LVIS with repeat factor sampling <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref>. We report the box mAP to better indicate the performance on detection. For ViLD <ref type="bibr" target="#b15">[16]</ref>, we took the model provided by the authors and re-ran the evaluation to ensure a fair comparison. For the Supervised baseline, we adopted the numbers from <ref type="bibr" target="#b15">[16]</ref>. As shown, our method outperforms ViLD on all splits. We gain +0.6 AP r for rare categories (novel) and +2.6 AP c /+3.5 AP f for common/frequent categories (base). This indicates that training with our PLs has less influence on base categories than the distillation in ViLD does. We observed a similar trend on COCO <ref type="bibr" target="#b31">[32]</ref> in Sec. 4.1. Compared with the improvement on base categories, the improvement on novel categories is relatively small, likely due to the long-tailed distribution of novel categories. Still, VL-PLM outperforms Supervised by a large margin in terms of AP r . A possible explanation is that our PLs provide more annotations for rare categories. In general, our PLs provide more (but noisy) annotation than the grounded truth "federated" annotations of LVIS <ref type="bibr" target="#b16">[17]</ref>, where only subsets of categories are annotated per image. This may explain why VL-PLM even outperforms Supervised in mAP. Although those annotations are not fully accurate, they still provide useful information for rare categories in the training, e.g., the texture of objects of rare categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Fast VL-PLM and Multi-scale Fast VL-PLM</head><p>This section provides more details about Fast VL-PLM and Multi-scale Fast VL-PLM that are mentioned in the discussion of Time efficiency in Sec. 4.3. <ref type="table" target="#tab_12">Table 7</ref> compares original VL-PLM with the two variants in terms of time cost and pseudo label quality. As shown, Fast VL-PLM reduces runtime by 5? with a slight drop in PL quality. Multi-scale Fast VL-PLM almost entirely removes the accuracy drop and still reduces runtime by 3?. Our Fast VL-PLM and Multi-scale Fast VL-PLM are only designed for ResNet-based CLIP not for ViT-based CLIP, which are described as follows. For Fast VL-PLM, we feed the whole input image into CLIP's ResNet50 to get shared feature maps. Then, we bound the coordinates of each region proposal to the close integer. For example, a proposal of {10.9, 50.2, 110.1, 100.9} in xyxy format is converted into a box of {11, 50, 110, 101}. Third, based on the bounded box, we crop features on the shared feature maps for the corresponding proposal. Finally, we ignore the positional embedding and feed the cropped features into the last attention layer of CLIP to output the region embedding for each proposal. Please refer to <ref type="bibr" target="#b37">[38]</ref> for details on the structure of ResNet-based CLIP. Please note the difference to ROI-pooling, where each cropped region would be pooled into the same spatial dimensions. Here, the cropped feature size is proportional to the bounding box and we let the attention layer in CLIP "pool" the input into a fixed-size output. We tried ROI-pooling but observed worse performance.</p><p>Multi-scale Fast VL-PLM is a mutli-scale version of Fast VL-PLM. We first construct an image pyramid and feed those images into CLIP's ResNet50 to get shared multi-scale feature maps. Then, for region proposals of small size, we crop features on shared feature maps of a large scale so that more details are attained in the cropped feature maps. Shared feature maps of a small scale are for region proposals of large size. Specially, we resize the smallest dimension of input images into three scales, i.e., 224, 224 ? 3 = 672, and 224 ? 5 = 1120. Thus, the shared feature maps are in one scale among 7, 7 ? 3 = 21, and 7 ? 5 = 35. Region proposals are assigned to different scales by their areas. The area &gt; 64 is for the first scale, the area between 16 and 64 for the second, and others for the third. Our design is inspired by FPN <ref type="bibr" target="#b30">[31]</ref> and enjoys its advantages, as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Scaling up unlabeled images for SSOD</head><p>To better understand the impact of the ratio between labeled and unlabeled images, we continue our experiments on semi-supervised object detection (SSOD). In Sec. 4.2, we varied the fraction of labeled images. Here, we use a fixed amount of labeled images and vary the number of unlabeled images. We train Faster R-CNN models using 5% of labeled COCO images with different amounts of unlabeled images. We randomly select our unlabeled images from the unlabeled images provided by the COCO dataset <ref type="bibr" target="#b31">[32]</ref>. All models are trained for 90k iterations with a batch size of 16. As shown in <ref type="table" target="#tab_13">Table 8</ref>, as the amount of unlabeled data increases, the performance increases as well, but with diminishing returns. In future work, we want to explore this aspect more and evaluate PLs from VL-PLM in an omni-supervised setting <ref type="bibr" target="#b38">[39]</ref>.  <ref type="figure">Fig. 5</ref>, we use the proposed VL-PLM to generate PLs and merge the PLs from the semi-supervised teacher. Then, we apply thresholding and NMS on the merged PLs to obtain the final PLs for SSOD.</p><p>To validate the effectiveness of this fusion strategy, we consider the following  <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b45">46]</ref> for SSOD and report the results in <ref type="table" target="#tab_14">Table 9</ref>. As shown, compared with SSL PLs only, VL-PLM w/o fusion is better on 1% and 2% COCO splits but worse on 5% and 10% COCO splits. A possible explanation is that V&amp;L models provide more useful information that boosts the performance when the amount of annotated data is smaller. Moreover, VL-PLM outperforms SSL PLs only and VL-PLM w/o fusion on all splits. This clearly demonstrates that our PLs from VL-PLM are better than the PLs from the teacher model. Our fusion method successfully improves the quality of the final PLs. Since putting PLs from the teacher and V&amp;L model together brings about the best results even for 5% and 10% COCO splits, we believe that PLs from the teacher and the V&amp;L model are complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Analysis on the Quality of PLs</head><p>In this section, we provide a more detailed analysis and discussion for Understanding the quality of PLs of Sec. 4.3 in the main paper.</p><p>Quality of PLs and performance of final detectors: In this section, we provide more analysis for <ref type="table" target="#tab_3">Table 10</ref> which is also present in the main paper. We recall our 5 baselines as follows, (1) PL v1 : We take the raw region proposals from region proposal network (RPN) without RoI refinement in our pseudo label  <ref type="figure">Fig. 5</ref>. Overview of pseudo labels (PLs) fusion for semi-supervised object detection (SSOD). We fuse our PLs with those from the semi-supervised teacher model before the thresholding and NMS.   <ref type="table" target="#tab_3">Table 10</ref>, the evaluations are conducted on the zero-shot splits <ref type="bibr" target="#b3">[4]</ref> on COCO <ref type="bibr" target="#b31">[32]</ref> (COCO-ZS). We report the AP 50 (AP@PL) and the number (#@PL) on novel categories for different PLs with the performance of detection models trained with corresponding PLs. Novel AP, Base AP, and Overall AP are provided to indicate the performance of detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VL-PLM</head><p>Based on <ref type="table" target="#tab_3">Table 10</ref>, we have the following findings. First, compared with PL v4, PL v1 shares similar AP@PL but has much more pseudo annotations. The final detector of PL v4 significantly outperforms that of PL v1. Second, compared with PL v4, PL v2 has nearly the same amount of PLs with a lower AP@PL. In terms of Novel AP, the final detector of PL v4 outperforms that of PL v1 by a large margin. Based on those facts, we conclude that neither AP@PL nor #@PL alone can decide the quality of PLs. We need to consider both AP@PL and #@PL. Good PLs come with high AP@PL and low #@PL. Third, based on the comparison between PL v4 and PL v5, we find that an extremely high threshold ? harms the predictions of novel categories but results in a slightly better performance on base categories. Empirically, we find a reasonable ? ? [0.6, 0.95] and set ? = 0.8 as default. Fourth, comparing PL v2 and PL v4, we find that with RoI head refinement, our PLs gain a significant improvement. For the final detector, PL v4 achieves similar performance on base categories as PL v2 and much better results on novel categories, which clearly demonstrate the effectiveness of using RoI head as box refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Modeling Background in PL Generation</head><p>Background is a latent category for the detection task and should be considered in our pseudo label generation, as well. In this section, we demonstrate how different ways of modeling the background affects the quality of PLs for OVD on COCO-ZS. Since there may be region proposals for base categories, we generalize the concept of background as categories that are not in the target categories, and consider 5 category spaces with different backgrounds as 1. Novel : The label space for pseudo labels only contains novel categories, no explicit modeling of background 2. Novel+BG The text "background" is used as one additional background category 3. Novel+Base: Both novel and base categories are used in the label space of PLs, where base categories should model the background (since those are annotated in the OVD setting) 4. Novel+Base+BG: Same as Novel+Base, but with the additional category of "background" 5. Novel+OV : We remove novel categories in COCO from the 1203 categories in LVIS <ref type="bibr" target="#b16">[17]</ref>. The remaining categories are used to model background. We name this background set as OV. <ref type="table" target="#tab_3">Table 11</ref> provides AP@PL and #@PL on novel categories for different category spaces of the background. As shown, Novel+BG is slightly better than Novel with higher AP@PL and lower #@PL. Novel+Base and Novel+Base+BG result in the same observation. BG does improve the quality but the improvement is not significant. Moreover, Novel+Base gains a clear improvement over Novel, likely because it helps V&amp;L models to identify objects of base categories which will be removed as the background, improving the quality of PLs for novel categories. Third, OV as the background decreases the quality of PLs based on the comparison betweenNovel+OV and Novel+Base. This is reasonable because V&amp;L models may be influenced by the large amount of OV categories that are absent in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Generalization ability of the proposal generator</head><p>For OVD, we need to identify the objects of novel categories in the unlabeled data. Similar to <ref type="bibr" target="#b15">[16]</ref>, we study if the two-stage class-agnostic detector trained on base categories generalizes to novel ones. Basing our experiments on COCO and LVIS datasets, we train a Faster R-CNN with either base or all categories (base+novel). <ref type="table" target="#tab_3">Table 12</ref> presents the top-N average recall (AR@N) of the RPN on novel categories. As shown, on COCO-ZS, RPN trained without novel categories suffers a clear performance drop. But it can still achieve a reasonable AR@1000, and we adopt top 1000 boxes from RPN in our pseudo label generation. On the contrary, models on LVIS achieve nearly the same average recall (AR) on novel categories with either base categories or base + novel categories as the training data. Possibly, there are more categories and instances in base categories of LVIS than in those of COCO. Thus, the proposal generator is able to learn a better concept about the objects in LVIS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Visualization of Our Pseudo Labels</head><p>We illustrate good cases and failure cases of our PLs for OVD on COCO in <ref type="figure" target="#fig_5">Fig. 6</ref> and <ref type="figure" target="#fig_6">Fig. 7</ref>. We only visualize boxes for target (novel) categories that are included in the scene. Good cases show that VL-PLM is able to locate multiple objects correctly. However, in the recent caption-based pseudo label generation method <ref type="bibr" target="#b13">[14]</ref>, it's a major issue to find multiple objects of the same categories. For failure cases, there are four major types, i.e., part domination, redundant boxes, missing instances, and grouped instances. We believe that part domination and redundant boxes are mainly caused by the poor localization ability of the adopted V&amp;L model CLIP <ref type="bibr" target="#b37">[38]</ref>. Missing and grouped instances usually happen when multiple instances are close to each other or only part of instances appears.</p><p>Possibly, the major reason is that the proposal generator cannot provide correct region proposals, leading to poor quality of PLs. In this sense, an improvement on either the V&amp;L models or the proposal generator in VL-PLM will boost PLs' quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Visualization of Our OVD Detector</head><p>This section visualizes the good and failure cases of the final detector for OVD. <ref type="figure" target="#fig_7">Fig. 8</ref> illustrates those cases on novel and base categories, respectively. As shown, the detector trained with our PLs is able to detect objects of novel categories. Moreover, unlike PLs, the results of the final detector mainly include three types of failure cases, i.e., missing instances, redundant boxes, and grouped instances. Possibly, the part domination that degrades the quality of PLs is reduced during the training with both ground truth and PLs.  The major failure cases belong to three types, i.e., missing instances, redundant boxes, or grouped instances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Overview of leveraging the semantic knowledge contained in vision and language models for mining unlabeled data to improve object detection systems for open-vocabulary and semi-supervised tasks. (b) Illustration of the weak localization ability when applying CLIP [38] on raw object proposals (top), compared with our improvements (bottom). The left images show the pseudo label with the highest score. The right images show all pseudo labels with scores greater than 0.8. The proposed scoring gives much cleaner pseudo labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. Overview of the proposed VL-PLM to mine unlabeled images with vision &amp; language models to generate pseudo labels for object detection. The top part illustrates our class-agnostic proposal generator, which improves the pseudo label localization by using the class-agnostic proposal score and the repeated application of the RoI head. The bottom part illustrates the scoring of cropped regions with the V&amp;L model based on the target category names. The chosen category names can be adjusted for the desired downstream task. After thresholding and NMS, we get the final pseudo labels. For some tasks like SSOD, we will merge external pseudo labels for a teacher model with ours before thresholding and NMS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>generation and set ? = 0.05. (2) PL v2 : The same as PL v1 but with ? = 0.95. (3) PL v3 : VL-PLM with ? = 0.05. (4) PL v4 : VL-PLM with ? = 0.95. (5) PL v5 : VL-PLM with ? = 0.99. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>supplemental material first provides additional experiments on open-vocabulary detection (OVD) on the LVIS dataset in Sec. A.1, a faster version of VL-PLM to speed-up pseudo label (PL) extraction in Sec. A.2, and additional experiments on semi-supervised detection (SSOD) in Sec. A.3. Then, we give additional analysis on fusing pseudo labels in SSOD in Sec. B.1, the quality of PLs in Sec. B.2, how to model the background category in PL generation in Sec. B.3, and the generalization ability of the proposal generator in Sec. B.4. Finally, qualitative results of PLs and the final OVD detector are given in Sec. C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>generation and set ? = 0.05. (2) PL v2 : The same as PL v1 but with ? = 0.95. (3) PL v3 : VL-PLM with ? = 0.05. (4) PL v4 : VL-PLM with ? = 0.95. (5) PL v5 : VL-PLM with ? = 0.99. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization of pseudo labels. Only novel categories in the image are shown. Top: Good cases with multiple instances. Bottom: Failure cases with missing instances, grouped instances, part domination and redundant annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Visualizations of the pseudo labels (PLs) from VL-PLM. Only boxes for target categories in the scene are shown. (a) Good cases. All target objects are located with appropriate boxes. (b) The most common types of failure cases in our PLs, i.e., part domination, redundant boxes, missing instances, and grouped instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of the final detection results. Only boxes for target categories in the scene are shown. (a) Novel categories as the target. (b) Base categories as the target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2207.08954v1 [cs.CV] 18 Jul 2022</figDesc><table><row><cell>Task Specific</cell><cell>Pseudo Label Generation</cell><cell></cell><cell cols="2">Unlabeled Data</cell><cell>Top 1 box</cell><cell>Box scores &gt; 0.8</cell></row><row><cell>Pseudo Labels</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>V&amp;L Models</cell><cell cols="2">Proposal Generators</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Task Specific</cell><cell></cell><cell>CLIP on raw region proposals</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Category Space</cell><cell></cell><cell>Top 1 box</cell><cell>Box scores &gt; 0.8</cell></row><row><cell></cell><cell cols="2">Downstream Tasks</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Existing Labeled Datasets</cell><cell cols="2">Open Vocabulary Object Detection</cell><cell>Semi-Supervised Object Detection</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLIP on enhanced region proposals</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1) Generating robust proposals for novel categories, required by open-vocabulary detection, and (2) overcoming RPN scores indicate localization quality. Top: Top 50 boxes from RPN in an image which correctly locates nearly all objects. Bottom: A positive correlation between RPN and IoU scores for RPN boxes of 50 randomly sampled COCO images.</figDesc><table><row><cell>RoI head x0</cell><cell>RoI head x1</cell><cell>RoI head x6</cell></row><row><cell>RoI head x10</cell><cell>RoI head x20</cell><cell>RoI head x50</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell></cell></row><row><cell>Fig. 3. (a)</cell><cell></cell><cell></cell></row></table><note>The correlation coefficient is 0.51. (b) Box refinement by repeating RoI head. "?N" indicates how many times we repeat the RoI head.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Evaluations for open vocabulary detection on the COCO 2017<ref type="bibr" target="#b31">[32]</ref>. Region-CLIP* indicates a model without refinement using image-caption pairs.</figDesc><table><row><cell>Method</cell><cell>Training Source</cell><cell cols="3">Novel AP Base AP Overall AP</cell></row><row><cell>Bansal et al. [4]</cell><cell></cell><cell>0.31</cell><cell>29.2</cell><cell>24.9</cell></row><row><cell>Zhu et al. [63]</cell><cell>instance-level labels in SB</cell><cell>3.41</cell><cell>13.8</cell><cell>13.0</cell></row><row><cell>Rahman et al. [40]</cell><cell></cell><cell>4.12</cell><cell>35.9</cell><cell>27.9</cell></row><row><cell>OVR-CNN [56]</cell><cell>image-caption pairs in SB ? SN instance-level labels in SB</cell><cell>22.8</cell><cell>46.0</cell><cell>39.9</cell></row><row><cell>Gao et al. [14] RegionCLIP [59]</cell><cell>raw image-text pairs via Internet image-caption pairs in SB ? SN instance-level labels in SB</cell><cell>30.8 31.4</cell><cell>46.1 57.1</cell><cell>42.1 50.4</cell></row><row><cell>RegionCLIP* [59] ViLD [16] VL-PLM (Ours)</cell><cell>raw image-text pairs via Internet instance-level labels in SB</cell><cell>14.2 27.6 34.4</cell><cell>52.8 59.5 60.2</cell><cell>42.7 51.3 53.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Open-vocabulary models trained with base categories from COCO are evaluated on unseen datasets. The evaluation protocol follows<ref type="bibr" target="#b13">[14]</ref> and reports AP50</figDesc><table><row><cell>PLs</cell><cell cols="4">Iterations?Batch size VOC 2007 Object365 LVIS</cell></row><row><cell>Gao et al. [14]</cell><cell>150K?64</cell><cell>59.2</cell><cell>6.9</cell><cell>8.0</cell></row><row><cell>VL-PLM</cell><cell>180K?16</cell><cell>67.4</cell><cell>10.9</cell><cell>22.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Experimental setup: Following previous studies<ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b61">62]</ref>, we conduct experiments on COCO<ref type="bibr" target="#b31">[32]</ref> with 1, 2, 5, and 10% of the training images selected as the labeled data and the rest as the unlabeled data, respectively. In the supplement, we provide more results for varying numbers of unlabeled data. To demonstrate how VL-PLM improves PLs for SSOD, we mainly compare our method with the following baselines. (1) Supervised : A vanilla teacher model trained on the labeled set I L .(2) Supervised +PLs: We apply the vanilla teacher model on the unlabeled set I U to generate PLs and train a student model with both ground truth and PLs. To compare with Supervised +PLs, VL-PLM generates PLs for all categories on I U . Then, those PLs are merged into the PLs from the vanilla teacher as the</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Evaluation of pseudo labels for semi-supervised object detection on COCO<ref type="bibr" target="#b31">[32]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="4">1% COCO 2% COCO 5% COCO 10% COCO</cell></row><row><cell>Supervised</cell><cell>9.25</cell><cell>12.70</cell><cell>17.71</cell><cell>22.10</cell></row><row><cell>Supervised +PLs</cell><cell>11.18</cell><cell>14.88</cell><cell>21.20</cell><cell>25.98</cell></row><row><cell cols="2">Supervised +VL-PLM 15.35</cell><cell>18.60</cell><cell>23.70</cell><cell>27.23</cell></row><row><cell>STAC [46]</cell><cell>13.97</cell><cell>18.25</cell><cell>24.38</cell><cell>28.64</cell></row><row><cell>STAC+VL-PLM</cell><cell>17.71</cell><cell>21.20</cell><cell>26.21</cell><cell>29.61</cell></row><row><cell cols="5">final PLs to train a student model named as Supervised +VL-PLM. (3) STAC [46]:</cell></row></table><note>A popular SSOD baseline. To compare with STAC, we only replace its PLs with ours that are used to train Supervised +VL-PLM. The new STAC student model is denoted as STAC+VL-PLM.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Relationship between the quality of pseudo labels and the performance of the final open vocabulary detectors.</figDesc><table><row><cell>PL Setting</cell><cell cols="5">Pseudo Labels AP@PL #@PL Base AP Novel AP Overall AP Final Detector</cell></row><row><cell>PL v1 No RoI, ? = 0.05</cell><cell>17.4</cell><cell>89.92</cell><cell>33.3</cell><cell>14.6</cell><cell>28.4</cell></row><row><cell>PL v2 No RoI, ? = 0.95</cell><cell>14.6</cell><cell>2.88</cell><cell>56.1</cell><cell>26.0</cell><cell>48.2</cell></row><row><cell cols="2">PL v3 VL-PLM, ? = 0.05 20.6</cell><cell>85.15</cell><cell>29.7</cell><cell>19.3</cell><cell>27.0</cell></row><row><cell cols="2">PL v4 VL-PLM, ? = 0.95 18.0</cell><cell>2.93</cell><cell>55.4</cell><cell>31.3</cell><cell>49.1</cell></row><row><cell cols="2">PL v5 VL-PLM, ? = 0.99 11.1</cell><cell>1.62</cell><cell>56.7</cell><cell>27.2</cell><cell>49.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>The quality of pseudo labels generated from different region proposals. The threshold ? is tuned to ensure a similar #@PL for each method.</figDesc><table><row><cell></cell><cell cols="4">Selective Search [48] RoI Head RPN RPN+RoI (Ours)</cell></row><row><cell>?</cell><cell>0.99</cell><cell>0.55</cell><cell>0.88</cell><cell>0.82</cell></row><row><cell>AP@PL</cell><cell>5.7</cell><cell>8.8</cell><cell>19.7</cell><cell>25.3</cell></row><row><cell>#@PL</cell><cell>34.92</cell><cell>5.01</cell><cell>4.70</cell><cell>4.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .</head><label>6</label><figDesc>Evaluations for open vocabulary detection on LVIS-v1<ref type="bibr" target="#b16">[17]</ref>.</figDesc><table><row><cell>Method</cell><cell>Training data</cell><cell>APr</cell><cell>APc</cell><cell>AP f</cell><cell>mAP</cell></row><row><cell>Supervised</cell><cell>Base + Novel</cell><cell>12.3</cell><cell>24.3</cell><cell>32.4</cell><cell>25.4</cell></row><row><cell>ViLD [16]</cell><cell>Base</cell><cell>16.6</cell><cell>21.1</cell><cell>31.6</cell><cell>24.4</cell></row><row><cell>VL-PLM (Ours)</cell><cell>Base</cell><cell>17.2</cell><cell>23.7</cell><cell>35.1</cell><cell>27.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 .</head><label>7</label><figDesc>Average time to get pseudo labels per image and their quality.</figDesc><table><row><cell></cell><cell cols="4">CLIP Backbone Time (s) AP@PL #@PL</cell></row><row><cell>Original VL-PLM</cell><cell>ResNet50</cell><cell>0.5413</cell><cell>15.9</cell><cell>7.31</cell></row><row><cell>Fast VL-PLM</cell><cell>ResNet50</cell><cell>0.1199</cell><cell>13.5</cell><cell>7.39</cell></row><row><cell>Multiscale Fast VL-PLM</cell><cell>ResNet50</cell><cell>0.1685</cell><cell>15.4</cell><cell>7.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 .</head><label>8</label><figDesc>Detection accuracy in mAP for Faster R-CNN on 5% of labeled COCO images with varied amounts of unlabeled images. Fusing Pseudo Labels from SSOD teacher with VL-PLM This section provides more details for Sec. 3.3 of the main paper on how we merge PLs for SSOD. As illustrated in</figDesc><table><row><cell># of labeled images</cell><cell>5764</cell><cell>5764</cell><cell>5764</cell><cell>5764</cell><cell>5764</cell></row><row><cell># of unlabeled images</cell><cell>0</cell><cell>5764</cell><cell>28820</cell><cell>57640</cell><cell>115280</cell></row><row><cell>Ratio</cell><cell>1:0</cell><cell>1:1</cell><cell>1:5</cell><cell>1:10</cell><cell>1:20</cell></row><row><cell>mAP</cell><cell>17.7</cell><cell>21.1</cell><cell>22.7</cell><cell>23.1</cell><cell>23.7</cell></row><row><cell>mAP increase</cell><cell>-</cell><cell>+3.4</cell><cell>+5.0</cell><cell>+5.4</cell><cell>+6.0</cell></row><row><cell cols="4">B Additional Analysis and Discussion</cell><cell></cell><cell></cell></row><row><cell>B.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 .</head><label>9</label><figDesc>Pseudo label fusion for semi-supervised object detection on COCO 2017<ref type="bibr" target="#b31">[32]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="4">1% COCO 2% COCO 5% COCO 10% COCO</cell></row><row><cell>SSL PLs only</cell><cell>11.18</cell><cell>14.88</cell><cell>21.20</cell><cell>25.98</cell></row><row><cell>VL-PLM w/o fusion</cell><cell>13.27</cell><cell>15.97</cell><cell>20.64</cell><cell>24.20</cell></row><row><cell>VL-PLM</cell><cell>15.35</cell><cell>18.60</cell><cell>23.70</cell><cell>27.23</cell></row><row><cell cols="5">baselines. (1) SSL PLs only: We only adopt the PLs from the semi-supervised</cell></row><row><cell cols="5">teacher as the final PLs. (2) VL-PLM w/o fusion: We only pick the PLs from the</cell></row><row><cell cols="5">vision and language model. (3) VL-PLM: The fused PLs. We base our experiments</cell></row><row><cell cols="2">on 1%, 2%, 5% and 10% COCO splits</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 .Table 11 .</head><label>1011</label><figDesc>Relationship between the quality of pseudo labels and the performance of the final open vocabulary detectors on COCO 2017<ref type="bibr" target="#b31">[32]</ref>. The quality of pseudo labels generated with different ways to model the background. ? is tuned to keep similar #@PL. See text for more details.</figDesc><table><row><cell></cell><cell cols="2">PL Setting</cell><cell cols="4">Pseudo Labels AP@PL #@PL Base AP Novel AP Overall AP Final Detector</cell></row><row><cell cols="3">PL v1 No RoI, ? = 0.05</cell><cell>17.4</cell><cell>89.92</cell><cell>33.3</cell><cell>14.6</cell><cell>28.4</cell></row><row><cell cols="3">PL v2 No RoI, ? = 0.95</cell><cell>14.6</cell><cell>2.88</cell><cell>56.1</cell><cell>26.0</cell><cell>48.2</cell></row><row><cell cols="4">PL v3 VL-PLM, ? = 0.05 20.6</cell><cell>85.15</cell><cell>29.7</cell><cell>19.3</cell><cell>27.0</cell></row><row><cell cols="4">PL v4 VL-PLM, ? = 0.95 18.0</cell><cell>2.93</cell><cell>55.4</cell><cell>31.3</cell><cell>49.1</cell></row><row><cell cols="4">PL v5 VL-PLM, ? = 0.99 11.1</cell><cell>1.62</cell><cell>56.7</cell><cell>27.2</cell><cell>49.0</cell></row><row><cell></cell><cell cols="6">Novel Novel+BG Novel+Base Novel+Base+BG Novel+OV set</cell></row><row><cell>?</cell><cell>0.80</cell><cell>0.80</cell><cell></cell><cell>0.59</cell><cell>0.59</cell><cell>0.501</cell></row><row><cell cols="2">AP@PL 25.5</cell><cell>25.7</cell><cell></cell><cell>26.3</cell><cell>26.4</cell><cell>25.1</cell></row><row><cell cols="2">#@PL 4.86</cell><cell>4.18</cell><cell></cell><cell>4.36</cell><cell>4.22</cell><cell>4.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 .</head><label>12</label><figDesc>The generalization ability of RPN of the proposal generator on COCO 2017<ref type="bibr" target="#b31">[32]</ref> and LVIS-v1<ref type="bibr" target="#b16">[17]</ref>. We report the average recall (AR) on novel categories.</figDesc><table><row><cell cols="6">Dataset Training Data AR@100 AR@300 AR@500 AR@1000</cell></row><row><cell>COCO</cell><cell>Base Base + Novel</cell><cell>34.5 54.2</cell><cell>43.4 59.3</cell><cell>47.2 61.1</cell><cell>51.7 62.8</cell></row><row><cell>LVIS</cell><cell>Base Base + Novel</cell><cell>33.3 33.7</cell><cell>42.3 43.0</cell><cell>45.9 46.6</cell><cell>50.5 50.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research has been partially funded by research grants to D. Metaxas from NEC Labs America through NSF IUCRC CARTA-1747778, NSF: 1951890, 2003874, 1703883, 1763523  and ARO MURI SCAN.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<title level="m">nocaps: novel object captioning at scale. In: ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">UNITER: UNiversal Image-TExt Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Embodied Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Boosting weakly supervised object detection via learning bounding box adjusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">From Captions to Visual Concepts and Back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<title level="m">Towards open vocabulary object detection without human-provided bounding boxes</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note>In: ICLR (2022) 1, 3, 4, 5, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">UniT: Multimodal Multitask Learning with a Unified Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning by Abstraction: The Neural State Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>In: ICML (2021) 2, 3</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MDETR -Modulated Detection for End-to-End Multi-Modal Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep Visual-Semantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to Objects in Photographs of Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>and visual relationship detection at scale. IJCV (2020</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Language-driven Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common Objects in Context</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<meeting><address><addrLine>Roberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Exploring the Limits of Weakly Supervised Pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Generation and Comprehension of Unambiguous Object Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Dynamic Fusion with Intra-and Inter-Modality Attention Flow for Visual Question Answering</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note>In: ICML (2021) 2, 3, 5, 7, 8, 20</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Data Distillation: Towards Omni-Supervised Learning</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improved visual-semantic alignment for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Denseclip: Language-guided dense prediction with context-aware prompting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Objects365: A Large-scale, High-quality Dataset for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Proposalclip: Unsupervised open-category object proposal generation via exploiting clip cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Localizing objects with self-supervised transformers and no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sim?oni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roburin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>BMVC</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A simple semisupervised learning framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning Deep Structure-Preserving Image-Text Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">End-to-end semi-supervised object detection with soft teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A simple baseline for zero-shot semantic segmentation with pre-trained vision-language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unsupervised Domain Adaptation for Object Detection via Cross-Domain Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karianakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>WACV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">MAttNet: Modular Attention Network for Referring Expression Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Modeling Context in Referring Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Open-Vocabulary Object Detection Using Captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 1, 4, 5</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">VinVL: Revisiting Visual Representations in Vision-Language Models</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Object Detection with a Unified Label Space from Multiple Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">Regionclip: Region-based language-image pretraining (2021) 2, 4</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Boosting weakly supervised object detection with progressive knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<title level="m">Denseclip: Extract free dense labels from clip</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Instant-Teaching: An End-to-End Semi-Supervised Object Detection Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 1, 4, 5</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Don&apos;t even look once: Synthesizing features for zero-shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
