<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CTC-Segmentation of Large Corpora for German End-to-end Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-17">17 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>K?rzinger</surname></persName>
							<email>ludwig.kuerzinger@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Institute for Human-Machine Communication</orgName>
								<orgName type="institution" key="instit2">Technische Universit?t M?nchen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Winkelbauer</surname></persName>
							<email>dominik.winkelbauer@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Institute for Human-Machine Communication</orgName>
								<orgName type="institution" key="instit2">Technische Universit?t M?nchen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Institute for Human-Machine Communication</orgName>
								<orgName type="institution" key="instit2">Technische Universit?t M?nchen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Watzel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Institute for Human-Machine Communication</orgName>
								<orgName type="institution" key="instit2">Technische Universit?t M?nchen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Institute for Human-Machine Communication</orgName>
								<orgName type="institution" key="instit2">Technische Universit?t M?nchen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CTC-Segmentation of Large Corpora for German End-to-end Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-17">17 Jul 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: German speech dataset</term>
					<term>End-to-end automatic speech recogni- tion</term>
					<term>hybrid CTC/attention</term>
					<term>CTC-segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent end-to-end Automatic Speech Recognition (ASR) systems demonstrated the ability to outperform conventional hybrid DNN/ HMM ASR. Aside from architectural improvements in those systems, those models grew in terms of depth, parameters and model capacity. However, these models also require more training data to achieve comparable performance. In this work, we combine freely available corpora for German speech recognition, including yet unlabeled speech data, to a big dataset of over 1700h of speech data. For data preparation, we propose a twostage approach that uses an ASR model pre-trained with Connectionist Temporal Classification (CTC) to boot-strap more training data from unsegmented or unlabeled training data. Utterances are then extracted from label probabilities obtained from the network trained with CTC to determine segment alignments. With this training data, we trained a hybrid CTC/attention Transformer model that achieves 12.8% WER on the Tuda-DE test set, surpassing the previous baseline of 14.4% of conventional hybrid DNN/HMM ASR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conventional speech recognition systems combine Deep Neural Networks (DNN) with Hidden Markov Models (HMM). The DNN serves as an acoustic model that infers classes, or their posterior probabilities respectively, originating from handcrafted HMMs and complex linguistic models. Hybrid DNN/HMM models also require multiple processing steps during training to refine frame-wise acoustic model labels. In comparison to hybrid DNN/HMM systems, end-to-end ASR simplifies training and decoding by directly inferring sequences of letters, or tokens, given a speech signal. For training, end-to-end systems only require the raw text corresponding to an utterance. Connectionist Temporal Classification (CTC) is a popular loss function to train end-to-end ASR architectures <ref type="bibr" target="#b4">[5]</ref>. In principle, its concept is similar to a HMM, the label sequence is modeled as sequence of states, and during training, a slightly modified forward-backward algorithm is used in the calculation of CTC loss. Another popular approach for end-to-end ASR is to directly infer letter sequences, as employed in attentionbased encoder-decoder architectures <ref type="bibr" target="#b2">[3]</ref>. Hybrid CTC/attention ASR architectures combine these two approaches <ref type="bibr" target="#b17">[18]</ref>.</p><p>End-to-end models also require more training data to learn acoustic representations. Many large corpora, such as Librispeech or TEDlium, are provided as large audio files partitioned into segments that contain speech with transcriptions. Although end-to-end systems do not need frame-wise temporal alignment or segmentation, an utterance-wise alignment between audio and text is necessary. To reduce training complexity, previous works used frameworks like sphinx <ref type="bibr" target="#b7">[8]</ref> or MAUS <ref type="bibr" target="#b14">[15]</ref> to partition speech data into sentence-length segments, each containing an utterance. Those frameworks determine the start and the end of a sentence from acoustic models (often HMMs) and the Viterbi algorithm. However, there are three disadvantages in using these for end-to-end ASR: <ref type="bibr" target="#b0">(1)</ref> As only words in the lexicon can be detected, the segmentation tool needs a strategy for out-of-vocabulary words. (2) Scaling the Viterbi algorithm to generate alignments within larger audio files requires additional mitigations. (3) As the the algorithms provide forced alignments, they assume that the audio contains only the text which should be aligned; but for most public domain audio this is not the case. So do for example all audio files from the Librivox dataset contain an additional prologue and epilogue where the speaker lists his name, the book title and the license. It might also be the case that the speaker skips some sentences or adds new ones due to different text versions. Therefore, aligning segments of large datasets, such as TEDlium <ref type="bibr" target="#b13">[14]</ref> is done in multiple iterations that often include manual examination. Unfortunately, this process is tedious and error prone; for example, by inspection of the SWC corpus, some of those automatically generated transcriptions are missing words in the transcription.</p><p>We aim for a method to extract labeled utterances in the form of correctly aligned segments from large audio files. To achieve this, we propose CTCsegmentation, an algorithm to correctly align start and end of utterance segments, supported by a CTC-based end-to-end ASR network 1 . Furthermore, we demonstrate additional data cleanup steps for German language orthography. Our contributions are:</p><p>-We extended and refined the existing recipe from the ASR toolkit kaldi with a collection of open source German corpora by two additional corpora, namely Librivox and CommonVoice, and ported it to the end-to-end ASR toolkit ESPnet. -We propose CTC-segmentation, a scalable method to extract utterance segments from speech corpora. In comparison to other automated segmentation <ref type="bibr" target="#b0">1</ref> The source code underlying this work is available at https://github.com/cornerfarmer/ctc_segmentation tools, alignments generated with CTC-segmentation were observed to more closely correspond to manually segmented utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Milde et al. <ref type="bibr" target="#b9">[10]</ref> proposed to combine freely available German language speech corpora into an open source German speech recognition system. A more detailed description of the German datasets can be found in <ref type="bibr" target="#b9">[10]</ref>, of which we give a short summary:</p><p>-The Tuda-DE dataset <ref type="bibr" target="#b12">[13]</ref> combines recordings of multiple sentences concerning various topics spoken by 180 speakers using five microphones. -The Spoken Wikipedia Corpus (SWC, <ref type="bibr" target="#b1">[2]</ref>) is an open source summary of recordings of different Wikipedia articles made by volunteers. The transcription already includes alignment notations between audio and text, but as these alignments were often incorrect, Milde et al. re-aligned utterance segments using the Sphinx speech recognizer <ref type="bibr" target="#b7">[8]</ref>.</p><p>-The M-AILABS Speech Dataset <ref type="bibr" target="#b15">[16]</ref> mostly consists of utterances extracted from political speeches and audio books from Librivox. Audio and text has been aligned by using synthetically generated audio (TTS) based on the text and by manually removing intro and outro.</p><p>In this work, we additionally combine the following German speech corpora:</p><p>-Common Voice dataset <ref type="bibr" target="#b0">[1]</ref> consists of utterances recorded and verified by volunteers; therefore, an utterance-wise alignment already exists. Milde et al. <ref type="bibr" target="#b9">[10]</ref> mainly used a conventional DNN/HMM model, as provided by the kaldi toolkit <ref type="bibr" target="#b11">[12]</ref>. Denisov et al. <ref type="bibr" target="#b3">[4]</ref> used a similar collection of German language corpora that additionally includes non-free pre-labeled speech corpora. Their ASR tool IMS Speech is based on a hybrid CTC/attention ASR architecture using the BLSTM model with location-aware attention as proposed by Watanabe et al. <ref type="bibr" target="#b17">[18]</ref>. The architecture used in our work also is based on the hybrid CTC/attention ASR of the ESPnet toolkit <ref type="bibr" target="#b18">[19]</ref>, however, in combination with the Transformer architecture <ref type="bibr" target="#b16">[17]</ref> that uses self-attention. As we only give a short description of its architecture, an in-detail description of the Transformer model is given by Karita et al. <ref type="bibr" target="#b6">[7]</ref>. There are several tools to extract labeled utterance segments from speech corpora. The Munich Automatic Segmentation (MAUS) system <ref type="bibr" target="#b14">[15]</ref> first transforms the given transcript into a graph representing different sequences of phones by applying predefined rules. Afterwards, the actual alignment is estimated by finding the most probable path using a set of HMMs and pretrained acoustic models. Gentle works in a similar way, but while MAUS uses HTK <ref type="bibr" target="#b19">[20]</ref>, Gentle is built on top of Kaldi <ref type="bibr" target="#b11">[12]</ref>. Both methods yield phone-wise alignments. Aeneas <ref type="bibr" target="#b10">[11]</ref> uses a different approach: It first converts the given transcript into audio by using text-to-speech (TTS) and then uses the Dynamic Time Warping (DTW) algorithm to align the synthetic and the actual audio by warping the time axis. In this way it is possible to estimate begin and end of given utterance within the audio file.</p><p>We propose to use a CTC-based network for segmentation. CTC was originally proposed as a loss function to train RNNs on unsegmented data. At the same time, using CTC as a segmentation algorithm was also proposed by Graves et al. <ref type="bibr" target="#b4">[5]</ref>. However, to the best knowledge of the authors, while the CTC algorithm is widely used for end-to-end speech recognition, there is not yet a segmentation tool for speech audio based on CTC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CTC-Segmentation of Utterances</head><p>The following paragraphs describe CTC-segmentation, an algorithm to extract proper audio-text alignments in the presence of additional unknown speech sections at the beginning or end of the audio recording. It uses a CTC-based endto-end network that was trained on already aligned data beforehand, e.g., as provided by a CTC/attention ASR system. For a given audio recording the CTC network generates frame-based character posteriors p(c|t, x 1:T ). From these probabilities, we compute via dynamic programming all possible maximum joint probabilities k t,j for aligning the text until character index j ? [1; M ] to the audio up to frame t ? [1; T ]. Probabilities are mapped into a trellis diagram by the following rules:</p><formula xml:id="formula_0">k t,j = ? ? ? ? ? max(k t?1,j ? p(blank|t), k t?1,j?1 ? p(c j |t)) if t &gt; 0 ? j &gt; 0 0 if t = 0 ? j &gt; 0 1 if j = 0<label>(1)</label></formula><p>The maximum joint probability at a point is computed by taking the most probable of the two possible transitions: Either only a blank symbol or the next character is consumed. The transition cost for staying at the first character is set to zero, to align the transcription start to an arbitrary point of the audio file. The character-wise alignment is then calculated by backtracking, starting off the most probable temporal position of the last character in the transcription, i.e, t = arg max t ? k t ? ,M . Transitions with the highest probability then determine the alignment a t of the audio frame t to its corresponding character from the text, such that</p><formula xml:id="formula_1">a t = ? ? ? ? ? M ? 1 if t arg max t ? (k t ? ,M?1 ) a t+1 if k t,at+1 ? p(blank|t + 1) &gt; k t,at+1?1 ? p(c j |t + 1) a t+1 ? 1 else .<label>(2)</label></formula><p>As this algorithm yields a probability ? t for every audio frame being aligned in a given way, a confidence score s seg for each segment is derived to sort out utterances with deviations between speech and corresponding text, that is calculated as</p><formula xml:id="formula_2">s seg = min j m j with m j = 1 L (j+1)L t=jL ? t .<label>(3)</label></formula><p>Here, audio frames that were segmented to correspond to a given utterance are first split into parts of length L. For each of these parts, a mean value m j based on the frame-wise probabilities ? t is calculated. The total probability s seg for a given utterance is defined as the minimum of these probabilities per part m j . This method inflicts a penalty on the confidence score on mismatch, e.g., even if a single word is missing in the transcription of a long utterance. The complexity of the alignment algorithm is reduced from O(M ?N ) to O(M ) by using the heuristic that the ratio between the aligned audio and text position is nearly constant. Instead of calculating all probabilities k t,j , for every character position j one only considers the audio frames in the interval [t ? W/2, t + W/2] with t = jN/M as the audio position proportional to a given character position and the window size W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data cleaning and text preparation</head><p>The ground truth text from free corpora, such as Librivox or the SWC corpus, is often not directly usable for ASR and has therefore to be cleaned. To maximize generalization to the Tuda-DE test dataset, this is done in a way to match the style of the ground truth text used in Tuda-DE, which only consists of letters, i.e. a-z and umlauts (?, ?, ?, ?). Punctuation characters are removed and all sentences with different letters are taken out of the dataset. All abbreviations and units are replaced with their full spoken equivalent. Furthermore, all numbers are replaced by their full spoken equivalent. Here it is also necessary to consider different cases, as this might influence the suffix of the resulting word. Say, "1800 Soldaten" needs to be replaced by "eintausendachthundert Soldaten", whereas "Es war 1800" is replaced according to its pronunciation by "Es war achtzehnhundert". The correct case can be determined from neighboring words with simple heuristics. For this, the NLP tagger provided by the spacy framework <ref type="bibr" target="#b6">[7]</ref> is used.</p><p>Another issue arised due to old German orthography. Texts obtained from Librivox is due to its expired copyright usually at least 70 years old and uses old German spelling rules. For an automated transition to the reformed German orthography, we implemented a self-updating lookup-table of letter replacements. This list was compiled based on a list of known German words from correctly spelled text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Alignment evaluation</head><p>In this section, we evaluate how well the proposed CTC-segmentation algorithm aligns utterance-wise text and audio. Evaluation is done on the dev and test set of the TEDlium v2 dataset <ref type="bibr" target="#b13">[14]</ref>, that consist of recordings from 19 unique speakers that talk in front of an audience. This corpus contains labeled sentencelength utterances, each with the information of start and end of its segment in the audio recording. As these alignments have been done manually, we use them as reference for the evaluation of the forced alignment algorithms. The comparison is done based on three parameters: the mean deviation of the predicted start or end from ground truth, its standard deviation and the ratio of predictions which are at maximum 0.5 seconds apart from ground truth. To evaluate the impact of the ASR model on CTC-segmentation, we include both BLSTM as well as Transformer models in the comparison. The pre-trained models were provided by the ESPnet toolkit <ref type="bibr" target="#b18">[19]</ref>. We compare our approach with three existing forced alignment methods from literature: MAUS, Gentle and Aeneas. To get utterance-wise from phone-wise alignments, we determine the begin time of the first phone and the end time of the last phone of the given utterance. As can be seen in Tab. 1, segment alignments generated by CTC-segmentation correspond significantly closer to ground truth compared to the segments generated by all other tested alignment algorithms. <ref type="figure">Fig. 1</ref> visualizes the density of segmentation timing deviations across all predictions. We thereby compare our approach using the LSTM-based model trained on TEDlium v2 with the Gentle alignment tool. It can be seen that both approaches have timing deviations smaller than one second for most predictions. Apart from that, our approach has a higher density in deviations between 0 and 0.5 seconds, while it is the other way around in the interval from 0.5 to 1 second.  <ref type="figure">Fig. 1</ref>: Relative deviation, denoted in seconds, of segments generated by Gentle and our CTC-segmentation compared to manually labeled segments from TEDlium 2. CTC-segmentation exhibited a greater accuracy to the start of the segment (left side) in comparison with Gentle; an also was observed to be slightly more accurate towards the end of the segments (right side). The y axis denotes density in a histogram with 60 bins. This indicates that our approach generates more accurately aligned segments when compared to Viterbi-or DTW based algorithms.</p><p>As explained in section 3.1, one of the main motivations for CTC-segmentation is to determine utterance segments in a robust manner, regardless of preambles or deviating transcriptions. To simulate such cases using the TEDlium v2 dev and test set, we prepended the last N seconds of every audio file before its start and appended the first M seconds to its end. Hereby, N and M are randomly sampled from the interval <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">30]</ref>s. <ref type="table" target="#tab_1">Table 2</ref> shows how the same algorithms perform on this altered dataset. Especially the accuracy of the alignment tools MAUS and Aeneas drops drastically when additional unknown parts of the audio recording are added. Gentle and our method however are able to retain their alignment abilities in such cases.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Composition of German Corpora for Training</head><p>Model evaluation is performed on multiple combinations of datasets, listed in Tab.3. Thereby we build upon the corpora collection used by Milde et al. <ref type="bibr" target="#b9">[10]</ref>, namely, Tuda-DE, SWC and M-AILABS. As <ref type="bibr" target="#b9">[10]</ref>, we also neglect recordings made by the Realtek microphone due to bad quality. Additional to these three corpora, we train our model on Common Voice and Librivox. Data preparation of the Common Voice dataset only required to post-process the ground truth text by replacing all numbers by their full spoken equivalent. As the Viterbialignment provided by <ref type="bibr" target="#b9">[10]</ref> for SWC is not perfect, with some utterances missing its first words in the transcription, we realign and clean the data using CTCsegmentation, as in Sec. 3.1. To perform CTC-segmentation on the Librivox corpus, we combined the audio files with the corresponding text pieces from Project Gutenberg-DE <ref type="bibr" target="#b5">[6]</ref>. Comparable evaluation results were obtained from decoding the Tuda-DE dev and test sets, as also used in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ASR configuration</head><p>For all experiments, the hybrid CTC/attention architecture with the Transformer is used. It consists of a 12 layer encoder and a 6 layer decoder, both with 2048 units in each layer; attention blocks contain 4 heads to each 256 units 2 . All models were trained for 23 epochs using the noam optimizer. We did not use data augmentation, such as SpecAugment. At inference time the decoding of the test and dev set is done using beam search with beam size of 16. To further improve the results on the test and dev set, a language model was used to guide the beam search. Language models with two sizes were used in decoding. The RNNLM language models were trained on the same text corpus as used in <ref type="bibr" target="#b9">[10]</ref> for 20 epochs. The first RNNLM has two layers with 650 LSTM units per layer. It achieves a perplexity of 8.53. The second RNNLM consists of four layer of each 1024 units, with a perplexity of 6.46. The benchmark results are listed in Tab. 4. First, the effects of using different dataset combinations are inspected. By using the CommonVoice dataset in addition to Tuda, SWC and M-AILABS the test WER decreases by 2%. Further replacing SWC and M-AILABS by the custom aligned SWC and Librivox dataset decreased the test set WER down to 12.8%. The second observation is that using a language model with a better perplexity also increased ASR performance. The significant improvement in WER of 2% can be explained by the better ability of the big RNNLM in detection and prediction of German words and grammar forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>End-to-end ASR models require more training data as conventional DNN/HMM ASR systems, as those models grow in terms of depth, parameters and model capacity. In order to compile a large dataset from yet unlabeled audio recordings, we proposed CTC-segmentation. This algorithm uses a CTC-based end-to-end neural network to extract utterance segments with exact time-wise alignments.</p><p>Evaluation of our method is two-fold: As evaluated on the hand-labeled dev and test datasets from TEDlium v2, alignments generated by CTC-segmentation were more accurate compared to those obtained from Viterbi-or DTW-based approaches. In terms of ASR performance, we build on a composition of German speech corpora <ref type="bibr" target="#b9">[10]</ref> and trained a end-to-end ASR model with CTC-segmented training data; the best model achieved 12.8% WER on the Tuda-DE test set, an improvement of 1.6% WER absolute in comparison with the conventional hybrid DNN/HMM ASR system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>-</head><label></label><figDesc>Librivox [9]  is a platform for volunteers to publish their recordings of reading public domain books. All recordings are published under a Creative Common license. The corresponding texts are retrieved from Project Gutenberg-DE<ref type="bibr" target="#b5">[6]</ref> that hosts a database of books in the public domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy of different alignment methods on the dev and test set of TEDlium v2, compared via the mean deviation from ground truth, its standard deviation and the ratio of predictions which are at maximum 0.5 seconds apart from ground truth.</figDesc><table><row><cell>Mean Std &lt;0.5s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Different alignment methods on the augmented dev and test set of TEDlium v2. Similar to the evaluation procedure as in Tab. 1, but the audio samples are augmented by adding random speech parts to their start and end. In this the robustness of the different approaches is evaluated.</figDesc><table><row><cell></cell><cell>Mean Std &lt;0.5s</cell></row><row><cell>Existing methods</cell><cell></cell></row><row><cell>MAUS (HMM-based using HTK)</cell><cell>3.18s 18.97 66.9 %</cell></row><row><cell>Aeneas (DTW-based)</cell><cell>10.91s 40.50 62.2 %</cell></row><row><cell>Gentle (HMM-based using kadi)</cell><cell>0.46s 2.40 81.7 %</cell></row><row><cell>Ours</cell><cell></cell></row><row><cell>LSTM trained on TEDlium v2</cell><cell>0.40s 1.63 89.3 %</cell></row><row><cell cols="2">Transformer trained on TEDlium v2 0.35s 1.38 89.2 %</cell></row><row><cell cols="2">Transformer trained on librispeech 0.40s 1.21 84.2 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Datasets used for training.</figDesc><table><row><cell>Datasets</cell><cell>length</cell><cell>Datasets</cell><cell>length</cell></row><row><cell cols="2">Tuda-DE train TD 127h</cell><cell cols="2">Common Voice CV 319h</cell></row><row><cell cols="2">SWC [10] SW 285h</cell><cell cols="2">CTC-segmented SWC SW* 210h</cell></row><row><cell cols="2">M-ailabs MA 237h</cell><cell cols="2">CTC-segmented Librivox LV* 804h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>A comparison of using different dataset combinations. Word error rates are in percent and evaluated on the Tuda-DE test and dev set, comprising of 9h of speech, or 10h respectively.</figDesc><table><row><cell cols="4">Datasets TD SW MA CV SW* LV*</cell><cell>ASR model</cell><cell>LM</cell><cell>Tuda-DE dev test</cell></row><row><cell></cell><cell>-</cell><cell>--</cell><cell>-</cell><cell cols="2">TDNN-HMM [10] 4-gram KN</cell><cell>15.3 16.5</cell></row><row><cell></cell><cell>-</cell><cell>--</cell><cell>-</cell><cell cols="2">TDNN-HMM [10] LSTM (2 ? 1024) 13.1 14.4</cell></row><row><cell></cell><cell></cell><cell>--</cell><cell>-</cell><cell cols="2">TDNN-HMM [10] 4-gram KN</cell><cell>14.8 15.9</cell></row><row><cell></cell><cell></cell><cell>--</cell><cell>-</cell><cell>Transformer</cell><cell>RNNLM (2 ? 650) 16.4 17.2</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>Transformer</cell><cell>RNNLM (2 ? 650) 16.0 17.1</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>Transformer</cell><cell>RNNLM (4 ? 1024) 14.1 15.2</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>Transformer</cell><cell>RNNLM (2 ? 650) 14.3 14.9</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>Transformer</cell><cell>RNNLM (4 ? 1024) 12.3 12.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? These authors contributed equally to this work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The default configuration of the Transformer model at ESPnet v.0.5.3</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06670</idno>
		<title level="m">Common voice: A massively-multilingual speech corpus</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?hn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hennig</surname></persName>
		</author>
		<title level="m">The spoken wikipedia corpus collection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ims-speech: A speech to text tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denisov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="170" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gutenberg</surname></persName>
		</author>
		<ptr target="https://gutenberg.spiegel.de" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Projekt gutenberg-de</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06317</idno>
		<title level="m">A comparative study on transformer vs rnn in speech applications</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cmu sphinx-4 speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gouvea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Librivox</surname></persName>
		</author>
		<ptr target="https://librivox.org/" />
		<title level="m">Librivox: Free public domain audiobooks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Open source automatic speech recognition for german</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Milde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?hn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Communication; 13th ITG-Symposium</title>
		<imprint>
			<publisher>VDE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pettarin</surname></persName>
		</author>
		<ptr target="https://www.readbeyond.it/aeneas/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>iEEE Catalog</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open source german distant speech recognition: Corpus and acoustic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radeck-Arneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Milde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gouv?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radomski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?hlh?user</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Text, Speech, and Dialogue</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="480" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhancing the ted-lium corpus with selected data for language modeling and more ted talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Del?glise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3935" to="3939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic Phonetic Transcription of Non-Prompted Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICPhS</title>
		<meeting>of the ICPhS<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-08" />
			<biblScope unit="page" from="607" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The m-ailabs speech dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Solak</surname></persName>
		</author>
		<ptr target="https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hybrid ctc/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTSP.2017.2763455</idno>
		<ptr target="https://doi.org/10.1109/JSTSP.2017.2763455" />
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Espnet: End-to-end speech processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Enrique Yalta Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-1456</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2018-1456" />
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The HTK hidden Markov model toolkit: Design and philosophy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>England</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Department of Engineering Cambridge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
