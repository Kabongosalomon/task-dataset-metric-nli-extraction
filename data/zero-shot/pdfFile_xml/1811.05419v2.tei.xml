<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
							<email>1zhangfengwcy@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Vision Semantics Limited</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing human pose estimation approaches often only consider how to improve the model generalisation performance, but putting aside the significant efficiency problem. This leads to the development of heavy models with poor scalability and cost-effectiveness in practical use. In this work, we investigate the under-studied but practically critical pose model efficiency problem. To this end, we present a new Fast Pose Distillation (FPD) model learning strategy. Specifically, the FPD trains a lightweight pose neural network architecture capable of executing rapidly with low computational cost. It is achieved by effectively transferring the pose structure knowledge of a strong teacher network. Extensive evaluations demonstrate the advantages of our FPD method over a broad range of state-of-the-art pose estimation approaches in terms of model cost-effectiveness on two standard benchmark datasets, MPII Human Pose and Leeds Sports Pose.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation has gained remarkable progress from the rapid development of various deep CNN models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. This is because deep neural networks are strong at approximating complex and non-linear mapping functions from arbitrary person images to the joint locations even at the presence of unconstrained human body appearance, viewing conditions and background noises. Nevertheless, the model performance advantages come with the cost of training and deploying resource-intensive networks with large depth and width. This causes inefficient model inference, requiring per-image computing cost at tens of FLoating point OPerations (FLOPs) therefore poor scalability particularly on resource-limited devices such as smart phones and robots. There is a recent attempt that binarises the network parameters for model execution speedup <ref type="bibr" target="#b6">[7]</ref>, which however suffers significantly weak model generalisation capacity.</p><p>In this study, we consider the problem of improving the pose estimation efficiency without model performance degradation but preserving comparable accuracy results. We observe that the basic CNN building blocks for state-ofthe-art human pose networks such as Hourglass <ref type="bibr" target="#b18">[19]</ref> are not cost-effective in establishing small networks due to a high number of channels per layer and being more difficult to train. To overcome these barriers, we design a lightweight variant of Hourglass network and propose a more effective training method of small pose networks in a knowledge distillation fashion <ref type="bibr" target="#b12">[13]</ref>. We call the proposed method Fast Pose Distillation (FPD). Compared with the top-performing alternative pose approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10]</ref>, the proposed FPD approach enables much faster and more cost-effective model inference with extremely smaller model size while simultaneously reaching the same level of human pose prediction performance. We summarise our contributions in follows:</p><p>(i) We investigate the under-studied human pose model efficiency problem, opposite to the existing attempts mostly focusing on improving the accuracy performance alone at high costs of model inference at deployment. This is a critical problem to be addressed for scaling up the existing deep pose estimation methods to real applications.</p><p>(ii) We propose a Fast Pose Distillation (FPD) model training method enabling to more effectively train extremely small human pose CNN networks. This is based on an idea of knowledge distillation that have been successfully exploited in inducing object image categorisation deep models. In particular, we derive a pose knowledge distillation learning objective to transfer the latent knowledge from a pre-trained larger teacher model to a tiny target pose model (to be deployed in test time). This aims to pursue the best model performance given very limited computational budgets using only a small fraction (less than 20%) of cost required by similarly strong alternatives.</p><p>(iii) We design a lightweight Hourglass network capable of constructing more cost-effective pose estimation CNN models while retaining sufficient learning capacity for allowing satisfactory accuracy rates. This is achieved by extensively examining the redundancy degree of existing state-of-the-art pose CNN architecture designs.</p><p>In the evaluations, we have conducted extensive empirical comparisons to validate the efficacy and superiority of the proposed FPD method over a wide variety of state-ofthe-art human pose estimation approaches in the balance of model inference efficiency and prediction performance on two commonly adopted benchmark datasets, MPII Human Pose <ref type="bibr" target="#b0">[1]</ref> and Leeds Sports Pose <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human Pose Estimation The past five years have witnessed a huge progress of human pose estimation in the deep learning regime <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>. Despite the clear performance increases, these prior works focus only on improving the pose estimation accuracy by using complex and computationally expensive models whilst largely ignoring the model inference cost issue. This significantly restricts their scalability and deployability in realworld applications particularly with very limited computing budgets available.</p><p>In the literature, there are a few recent works designed to improve model efficiency. For example, Bulat and Tzimiropoulos built parameter binarised CNN models to accommodate resource-limited platforms <ref type="bibr" target="#b6">[7]</ref>. But this method leads to dramatic performance drop therefore not satisfied for reliable utilisation. In most cases, high accuracy rates are required. Rafi et al. exploited good general purpose practices to improve model efficiency without presenting a novel algorithm <ref type="bibr" target="#b23">[24]</ref>. Further, this method does not provide quantitative evaluation on the trade-off between model efficiency and effectiveness.</p><p>In contrast to these previous methods, we systematically study the pose estimation efficiency problem under the condition of preserving the model performance rate so that the resulted model is more usable and reliable in real-world application scenarios.</p><p>Knowledge Distillation The objective of knowledge distillation is concerned with information transfer between different neural networks with distinct capacities <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3]</ref>. For instance, Hinton et al. successfully employed a well trained large network to help train a small network <ref type="bibr" target="#b12">[13]</ref>. The rationale is an exploitation of extra supervision from a teacher model, represented in form of class probabilities <ref type="bibr" target="#b12">[13]</ref>, feature representations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>, or an inter-layer flow <ref type="bibr" target="#b34">[35]</ref>. This principle has also been recently applied to accelerate the model training process of large scale distributed neural networks <ref type="bibr" target="#b1">[2]</ref>, to transfer knowledge between multiple layers <ref type="bibr" target="#b16">[17]</ref> or between multiple training states <ref type="bibr" target="#b17">[18]</ref>. Beyond the conventional two stage training based offline dis-tillation, one stage online knowledge distillation has been attempted with added merits of more efficient optimisation <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b15">16]</ref> and more effective learning <ref type="bibr" target="#b15">[16]</ref>. Besides, knowledge distillation has been exploited to distil easy-to-train large networks into harder-to-train small networks <ref type="bibr" target="#b24">[25]</ref>.</p><p>While these past works above transfer category-level discriminative knowledge, our method transfers richer structured information of dense joint confidence maps. A more similar work is the latest radio signals based pose model that also adopts the idea of knowledge distillation <ref type="bibr" target="#b38">[38]</ref>. However, this method targets at using wireless sensors to tackle the occlusion problem, rather than the model efficiency issue as we confider here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fast Human Pose Estimation</head><p>Human pose estimation aims to predict the spatial coordinates of human joints in a given image. To train a model in a supervised manner, we often have access to a training dataset {I i , G i } N i=1 of N person images each labelled with K joints defined in the image space as:</p><formula xml:id="formula_0">G i = {g i 1 , .., g i K } ? R K?2 ,<label>(1)</label></formula><p>where H and W denotes the image height and width, respectively. Generally, this is a regression problem at the imagery pixel level.</p><p>Objective Loss Function For pose model training, we often use the Mean-Squared Error (MSE) based loss function <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19]</ref>. To represent the ground-truth joint labels, we generate a confidence map m k for each single joint k (k ? {1, ? ? ? , K}) by centring a Gaussian kernel around the labelled position z k = (x k , y k ).</p><p>More specifically, a Gaussian confidence map m k for the k-th joint label is written as:</p><formula xml:id="formula_1">m k (x, y) = 1 2?? 2 exp ?[(x ? x k ) 2 + (y ? y k ) 2 ] 2? 2<label>(2)</label></formula><p>where (x, y) specifies a pixel location and the hyperparameter ? denotes a pre-fixed spatial variance. The MSE loss function is then obtained as:</p><formula xml:id="formula_2">L mse = 1 K K k=1 m k ?m k 2 2<label>(3)</label></formula><p>wherem k refers to the predicted confidence map for the k-th joint. The standard SGD algorithm can then be used to optimise a deep CNN pose model by back-propagating MSE errors on training data in a mini-batch incrementally.</p><p>Existing pose methods rely heavily on large deep neural networks for maximising the model performance, whilst neglecting the inference efficiency. We address this limitation for higher scalability by establishing lightweight CNN architectures and proposing an effective model learning strategy detailed below. The computationally expensive teacher model is abandoned finally, since its discriminative knowledge transferred already into the target model therefore used in deployment (rather than wasted).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage</head><p>Building Block 1, 2, 3, 4 Hourglass with 128 channels per layer <ref type="table">Table 1</ref>. The structure of a small pose CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Compact Pose Network Architecture</head><p>Human pose CNN models typically consist of multiple repeated building blocks with the identical structure <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>. Among these, Hourglass is one of the most common building block units <ref type="bibr" target="#b18">[19]</ref>. However, we observe that existing designs are not cost-effective, due to deploying a large number of both channels and blocks in the entire architecture therefore leading to a suboptimal trade-off between the representation capability and the computational cost. For example, <ref type="bibr" target="#b18">[19]</ref> suggested a CNN architecture of 8 Hourglass stages each having 9 Residual blocks with 256 channels within every layer.</p><p>We therefore want to minimise the expense of existing CNN architectures for enabling faster model inference. With careful empirical examination, we surprisingly revealed that a half number of stages (i.e. 4 Hourglass modules) suffice to achieve over 95% model generalisation capacity on the large scale MPII benchmark. Moreover, the per-layer channels are also found highly redundant and reducing a half number (128) only results in less than 1% performance drop <ref type="table">(Table 5</ref>). Based on these analysis, we construct a very light CNN architecture for pose estimation with only one sixth computational cost of the original design. See <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_0">Figure 1</ref> for the target CNN architecture specifications.</p><p>Remarks Whilst it is attractive to deploy tiny pose networks that run cheaply and fast, it is empirically non-trivial to train them although theoretically shallow networks have the similar representation capacities to approximate the target functions as learned by deeper counterparts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. A similar problem has been occurred and investigated in object image classification through the knowledge distillation strategy, i.e. let the target small network mimic the prediction of a larger teacher model <ref type="bibr" target="#b12">[13]</ref>. However, it remains unclear how well such a similar method will work in addressing structured human pose estimation in dense pixel space. To answer this question, in the following we present a pose structure knowledge distillation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervision Enhancement by Pose Distillation</head><p>Model Training Pipeline We adopt the generic model training strategy of knowledge distillation:</p><p>1. We first train a large teacher pose model. In our experiments, by default we select the original Hourglass model <ref type="bibr" target="#b18">[19]</ref> due to its clean design and easy model training. Other stronger models can be considered without any restrictions.</p><p>2. We then train a target student model with the assistance of knowledge learned by the teacher model. Knowledge distillation happens in this step. The structure of the student model is presented in <ref type="table">Table 1</ref>.</p><p>An overview of the whole training procedure is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. The key to distilling knowledge is to design a proper mimicry loss function that is able to effectively extract and transfer the teacher's knowledge to the training of the student model. The previous distillation function is designed for single-label based softmax cross-entropy loss in the context of object categorisation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> and unsuitable to transfer the structured pose knowledge in 2D image space.</p><p>To address this aforementioned problem, we design a joint confidence map dedicated pose distillation loss function formulated as:</p><formula xml:id="formula_3">L pd = 1 K K k=1 m s k ? m t k 2 2<label>(4)</label></formula><p>where m s k and m t k specify the confidence maps for the kth joint predicted by the pre-trained teacher model and the in-training student target model, respectively. We choose the MSE function as the distillation quantity to measure the divergence between the student and teacher models in order to maximise the comparability with the pose supervised learning loss (Eqn <ref type="formula" target="#formula_2">(3)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Loss Function</head><p>We formulate the overall FPD loss function for pose structure knowledge distillation during training as:</p><formula xml:id="formula_4">L fpd = ?L pd + (1 ? ?)L mse<label>(5)</label></formula><p>where ? is the balancing weight between the two loss terms, estimated by cross-validation. As such, the target network learns both to predict the labelled ground-truth annotations of training samples by L mse and to match the prediction structure of the stronger teacher model by L pd .</p><p>Further Remarks Why does the proposed pose distillation loss function probably help to train a more generalisable target model, as compared to training only on the labelled data? A number of reason may explain this in the context of pose estimation.</p><p>1. The body joint labels are likely to be erroneous due to the high difficulty of locating the true positions in the manual annotation process. In such cases, the teacher model may be able to mitigate some errors through statistical learning and reasoning therefore reducing the misleading effect of wrongly labelled training samples <ref type="figure" target="#fig_2">(Figure 3</ref> Row (A)).</p><p>2. Given difficult training cases say with confusing/cluttered background and random occlusion situations, the teacher prediction may provide softened learning tasks by explained away these hard samples with model inference <ref type="figure" target="#fig_2">(Figure 3 Row (B)</ref>).</p><p>3. The teacher model may provide more complete joint labels than the original annotation therefore not only providing additional more accurate supervision but also mitigating the misleading of missing joint labels <ref type="figure" target="#fig_2">(Figure 3</ref> Row (C)).</p><p>4. Learning to match the ground-truth confidence map can be harder in comparison to aligning the teacher's prediction. This is because the teacher model has spread some reasoning uncertainty for each training sample either hard or easy to process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>On the other hand, the teacher's confidence map encodes the abstract knowledge learned from the entire training dataset in advance, which may be beneficial to be considered in learning every individual training sample during knowledge distillation.</p><p>In summary, the proposed model is capable of handling wrong pose joint annotations, e.g. when the pre-trained teacher predicts more accurate joints than manual wrong and missing labels. Due to a joint use of the ground-truth labels and the teacher model's prediction, our model is tolerant to either error but not co-occurring ones. This alleviates the harm of label errors in the training data, in contrast to existing methods that often blindly trust all given labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Training and Deployment</head><p>The proposed FPD model training method consists of two stages: (i) We train a teacher pose model by the conventional MSE loss (Eqn (3)), and (ii) train a target student model by the proposed loss (Eqn (5)), with the knowledge distillation from the teacher model to the target model being conducted in each mini-batch and throughout the entire training process. At test time, we only use the small target model for efficient and cost-effective deployment whilst throwing away the heavy teacher network. The target model already extracts the teacher's knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Datasets We utilised two human pose benchmark datasets, MPII <ref type="bibr" target="#b0">[1]</ref> and Leeds Sports Pose (LSP) <ref type="bibr" target="#b14">[15]</ref>. The MPII dataset is collected from YouTube videos with a wide range of human activities and events. It has 25K scene images and 40K annotated persons (29K for training and 11K for test). Each person has 16 labelled body joints. We adopted the standard train/valid/test data split <ref type="bibr" target="#b27">[28]</ref>. Following <ref type="bibr" target="#b28">[29]</ref>, we randomly sampled 3K samples from the training set for model validation.</p><p>The LSP benchmark contains natural person images from many different sports scenes. Its extended version provides 11K training samples and 1K test samples. Each person in LSP has 14 labelled joints.</p><p>Performance Metrics We used the standard Percentage of Correct Keypoints (PCK) measurement that quantifies the fraction of correct predictions within an error threshold ? <ref type="bibr" target="#b33">[34]</ref>. Specifically, the quantity ? is normalised against the size of either torso (? = 0.2 for LSP, i.e. PCK@0.2) or head (? = 0.5 for MPII, i.e. PCKh@0.5). We measured each individual joint respectively and took their average as an overall metric. Using different ? values, we yielded a PCK curve. Therefore, the Area Under Curve (AUC) can be obtained as a holistic measurement across different de-  Training Details We implemented all the following experiments in Torch. We cropped all the training and test images according to the provided positions and scales, and resized them to 256?256 in pixels. As typical, random scaling (0.75-1.25), rotating (?30 degrees) and horizontal flipping were performed to augment the training data. We adopted the RMSProp optimisation algorithm. We set the learning rate to 2.5 ? 10 ?4 , the mini-batch size to 4, and the epoch number to 130 and 70 for MPII and LSP benchmarks, respectively. For the network architecture, we used the original Hourglass as the teacher model and the customised Hourglass with less depth and width <ref type="table">(Table 1</ref>) as the target model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons to State-Of-The-Art Methods</head><p>We evaluated the proposed FPD method by extensively comparing against recent human pose estimation deep methods on MPII and LSP. <ref type="table" target="#tab_0">Table 2</ref> compares the PCKh@0.5 accuracy results of state-of-the-art methods and the proposed FPD on the test dataset of MPII. It is clearly observed that the proposed FPD model is significantly efficient and compact therefore achieving a much cheaper deployment cost. Importantly, this advantage is obtained without clearly compromising the model generalisation capability, e.g. achieving as high as 91.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on MPII</head><p>Specifically, compared with the best performer <ref type="bibr" target="#b19">[20]</ref>, the FPD model only requires 14.3% (9/63) computational cost but gaining 96.4% (63.5/65.9) performance in mean PCKh accuracy. This leads to a 6.7%? (96.4/14.3) cost-effective advantage. When compared to the most efficient alternative competitor <ref type="bibr" target="#b23">[24]</ref>, our model is 2.9? (26/9) more efficient whilst simultaneously achieving a mean PCKh gain     is bigger than other state-of-the-art gains, e.g. +0.3% in 91.2% <ref type="bibr" target="#b20">[21]</ref> vs 90.9% <ref type="bibr" target="#b18">[19]</ref>; further +0.3% in 91.5% <ref type="bibr" target="#b22">[23]</ref>.</p><p>More specifically, given all 163,814 test joints, each 0.1% gain means correcting 163 joints. <ref type="table" target="#tab_1">Table 3</ref> compares the PCK@0.2 rates of our FPD model and existing methods with top reported performances on the LSP test data. Compared to MPII, this benchmark has been less evaluated by deep learning models, partly due to a smaller size of training data. Overall, we observed the similar comparisons. For example, our FPD runs more efficiently than the most competitive alternative <ref type="bibr" target="#b23">[24]</ref> and consumes much less training energy, in addition to achieving the best pose prediction accuracy rate among all compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on LSP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Examination</head><p>To provide visual test, <ref type="figure">Figure  2</ref> shows qualitative pose estimation evaluations on LSP and MPII. It is observed that such a small FPD model can still achieve reliable and robust pose estimation in arbitrary inthe-wild images with various background clutters, different human poses and viewing conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We carried out detailed component analysis and discussion on the validation set of MPII.</p><p>FPD generalisation evaluation Besides using the stateof-the-art Hourglass as the backbone network, we also tested the more recent model <ref type="bibr" target="#b31">[32]</ref> when integrated into the proposed FPD framework. In particular, we adopted the original network as the teacher model and constructed a lightweight variant as the student (target) model. The lightweight model was constructed similarly as in <ref type="table">Table 1</ref> because it is based on the Hourglass design too: reducing the number of stages to 4 and the number of channels in each module to 128. The results in <ref type="table">Table 4</ref> show that our FPD approach achieves 1.0% mean PCKh@0.5 gain, similar to the Hourglass case. This suggests the good generalisation capability of the proposed approach in yielding costeffective pose estimation deep models.</p><p>Cost-effectiveness analysis of Hourglass We extensively examined the architecture design of the state-of-theart Hourglass neural network model <ref type="bibr" target="#b18">[19]</ref> in terms of costeffectiveness. To this end, we tested two dimensions in design: depth (the layer number) and width (the channel number). Interestingly, we revealed in <ref type="table">Table 5</ref> that removing half stages (layers) and half channels only leads to quite limited performance degradation. This indicates that the original Hourglass design is highly redundant with poor cost-effectiveness. However, this is largely ignored in previous works due to their typical focus on pursuing the model accuracy performance alone whilst overlooking the important model efficiency problem. This series of CNN architecture examinations helps us to properly formulate a lightweight pose CNN architecture with only 16% (9/55) computational cost but obtaining 98% (90.1/91.9) model performance as compared to the state-of-the-art design, laying a good foundation towards building compact yet strong human pose deep models. methods, the model <ref type="bibr" target="#b22">[23]</ref> additionally benefits from an auxiliary dataset MPII in model training. <ref type="table">Table 6</ref> shows that teacher knowledge transfer brings in 0.8% (90.9-90.1) mean PCKh accuracy boost. This suggests that the generic principle of knowledge distillation is also effective in the structured pose estimation context, beyond object categorisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of pose knowledge distillation</head><p>To further validate how on earth this happens, we visualise three pose structure transfer examples in <ref type="figure" target="#fig_2">Figure 3</ref>. It is shown that the proposed mimicry loss against the teacher prediction is likely to pose extra information in cases of error labelling, hard training images, and missing annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose distillation loss function</head><p>We finally evaluated the effect of loss function choice for pose knowledge distillation. To that end, we further tested a Cross-Entropy measurement based loss. Specifically, we first normalise the entire confidence map so that the sum of all pixel confidence scores is equal to 1, i.e. L1 normalisation. We then measure the divergence between the predicted and ground-truth confidence maps using the Cross-Entropy criterion. The results in <ref type="table" target="#tab_4">Table 7</ref> show that the MSE is a better choice in comparison to Cross-Entropy. The plausible reason is that MSE is also the formulation of the conventional supervision loss (Eqn (3)) therefore more compatible.</p><p>Parameter analysis of loss balance We evaluated the balance importance between the conventional MSE loss and the proposed pose knowledge distillation loss, as controlled by ? in Eqn <ref type="bibr" target="#b4">(5)</ref>. <ref type="table">Table 8</ref> shows that equal importance (when ? = 0.5) is the optimal setting. This suggests that the two loss terms are similarly significant with the same numerical scale. On the other hand, we found that this parameter setting is not sensitive with a wide range of satisfactory values. This indicates that the teacher signal is not far away from the ground-truth labels (see <ref type="figure" target="#fig_2">Figure 3</ref> Column (4)), possibly providing an alternative supervision as a replacement of the original joint confidence map labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present a novel Fast Pose Distillation (FPD) learning strategy. In contrast to most existing human pose estimation methods, the FPD aims to address the under-studied and practically significant model costeffectiveness quality in order to scale the human pose estimation models to large deployments in reality. This is made possible by developing a lightweight human pose CNN architecture and designing an effective pose structure knowledge distillation method from a large teacher model to a lightweight student model. Compared with existing model compression techniques such as network parameter binarisation, the proposed method achieves highly efficient human pose models without accuracy performance compromise. We have carried out extensive comparative evaluations on two human pose benchmarking datasets. The results suggests the superiority of our FPD approach in comparison to a wide spectrum of state-of-the-art alternative methods. Moreover, we have also conducted a sequence of ablation study on model components to provide detailed analysis and insight about the gains in model costeffectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An overview of the proposed Fast Pose Distillation model learning strategy. To establish a highly cost-effective human pose estimation model, We need to build a compact backbone such as (a) a lightweight Hourglass network. To more effectively train a small target network, we adopt the principle of knowledge distillation in the pose estimation context. This requires to (b) pre-train a strong teacher pose model, such as the state-of-the-art Hourglass network or other existing alternatives. The teacher model is used to provide extra supervision guidance in the (c) pose knowledge distillation procedure via the proposed mimicry loss function. At test time, the small target pose model enables a fast and cost-effective deployment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>of 4 .Figure 2 .</head><label>42</label><figDesc>8% (91.1-86.3). These evidences clearly suggest the cost-effectiveness advantages of our method over other alternative approaches. In pose estimation, an improvement of 0.8% indicates a significant gain particularly on the challenging MPII with varying poses against cluttered background. This boost LSP MPII Example of human pose estimation on LSP and MPII.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Pose estimation examples on MPII by the proposed FPD model. Column (1): The input images. Column (2): Ground-truth joint confidence maps. Column (3): Joint confidence maps predicted by the teacher model. Column (4): The difference between ground-truth and teacher's confidence map. Each row represents a type of pose knowledge transfer. Row (A): Error labelling of the right leg ankle in the "ground-truth" annotations, which is corrected by the teacher model. Row (B): A softened teacher confidence map with larger uncertainty than the ground-truth due to the highly complex human posture. Row (C): Missing joint labels are discovered by the teacher model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FPD</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Method Head Sho. Elbo. Wri. Hip Knee Ank. Mean AUC # Param Deployment Cost Rafi et al., BMVC'16[24] 97.2 93.9 86.4 81.3 86.8 80.6 73.4 86.3 57.3 56M 28G Belagiannis&amp;Zisserman, FG'17[4] 97.7 95.0 88.2 83.0 87.9 82.6 78.4 88.1 58.8 17M 95G Insafutdinov et al., ECCV'16[14] 96.8 95.2 89.3 84.4 88.4 83.4 78.0 88.5 60.8 66M 286G Wei et al., CVPR'16[31] 97.8 95.0 88.7 84.0 88.4 82.8 79.4 88.5 61.PCKh@0.5 and AUC (%) rates on the MPII test dataset. M/G: 10 6 /10 9 .</figDesc><table><row><cell>4</cell><cell>31M</cell><cell>351G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>PCK@0.2 and AUC (%) rates on the LSP test dataset. M/G: 10 6 /10 9 .</figDesc><table /><note>cision thresholds. To measure the model efficiency both in training and test, we used the FLOPs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Generalisation evaluation of the proposed FPD approach. Metric: Mean PCKh@0.5 and AUC. Cost-effectiveness analysis of the Hourglass model. Metric: PCKh@0.5 and AUC. M/G: 10 6 /10 9 .</figDesc><table><row><cell># Stage</cell><cell># Channel</cell><cell>Mean</cell><cell>AUC</cell><cell># Param</cell><cell>Deployment Cost</cell></row><row><cell>8</cell><cell>256</cell><cell>91.9</cell><cell>63.7</cell><cell>26M</cell><cell>55G</cell></row><row><cell>4</cell><cell>256</cell><cell>91.4</cell><cell>63.9</cell><cell>13M</cell><cell>30G</cell></row><row><cell>2</cell><cell>256</cell><cell>90.5</cell><cell>63.0</cell><cell>7M</cell><cell>17G</cell></row><row><cell>1</cell><cell>256</cell><cell>86.4</cell><cell>58.3</cell><cell>3M</cell><cell>10G</cell></row><row><cell>4</cell><cell>256</cell><cell>91.4</cell><cell>63.9</cell><cell>13M</cell><cell>30G</cell></row><row><cell>4</cell><cell>128</cell><cell>90.1</cell><cell>62.4</cell><cell>3M</cell><cell>9G</cell></row><row><cell>4</cell><cell>64</cell><cell>87.9</cell><cell>59.5</cell><cell>0.95M</cell><cell>4.5G</cell></row><row><cell>4</cell><cell>32</cell><cell>83.4</cell><cell>54.9</cell><cell>0.34M</cell><cell>3.1G</cell></row><row><cell>Pose Distillation</cell><cell>Mean</cell><cell>AUC</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90.1</cell><cell>62.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90.9</cell><cell>63.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Table 6. Effect of the proposed pose knowledge distillation. Met-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ric: Mean PCKh@0.5 and AUC (%).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>We tested the effect of using our pose knowledge distillation on the lightweight Hourglass network. In contrast to all other Pose knowledge distillation by different types of loss function. Metric: Mean PCKh@0.5 and AUC.</figDesc><table><row><cell></cell><cell cols="2">Loss Function</cell><cell>Head</cell><cell cols="2">Sho.</cell><cell>Elbo.</cell><cell>Wri.</cell><cell>Hip</cell><cell>Knee</cell><cell>Ank.</cell><cell>Mean</cell><cell>AUC</cell></row><row><cell></cell><cell>MSE</cell><cell></cell><cell>97.7</cell><cell cols="2">96.4</cell><cell>91.8</cell><cell>87.6</cell><cell>89.7</cell><cell>86.6</cell><cell>83.9</cell><cell>90.9</cell><cell>63.3</cell></row><row><cell></cell><cell cols="2">Cross-Entropy</cell><cell>97.6</cell><cell cols="2">96.2</cell><cell>91.5</cell><cell>87.6</cell><cell>89.0</cell><cell>86.5</cell><cell>83.6</cell><cell>90.7</cell><cell>63.0</cell></row><row><cell>?</cell><cell>0</cell><cell>0.05</cell><cell>0.1</cell><cell>0.5</cell><cell cols="2">0.95 0.99</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell cols="6">90.1 90.8 90.8 90.9 90.7 90.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AUC</cell><cell cols="6">62.4 63.2 63.2 63.3 63.0 63.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Table 8. Performance analysis of the learning importance param-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">eter of pose distillation. Metric: Mean PCKh@0.5 and AUC (%).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China (61773093), National Key R&amp;D Program of China (2018YFC0831800), Important Science and Technology Innovation Projects in Chengdu (2018-YF08-00039-GX) and Research Programs of Sichuan Science and Technology Department (2016JY0088, 17ZDYF3184). Mao Ye is the major corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale distributed neural network training through online distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ormandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuewei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge distillation by on-the-fly native ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7528" to="7538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Person search by multi-scale matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-referenced deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human pose estimation with parsing induced learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge-guided deep fractal neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umer</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Fitnets: Hints for thin deep nets. arXiv e-print</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conversational speech transcription using context-dependent deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelfth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pose proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiki</forename><surname>Sekii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep deformation network for object landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="52" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Through-wall human pose estimation using radio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Abu Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
