<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boundless: Generative Adversarial Networks for Image Extension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
							<email>pteterwak@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
							<email>sarna@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
							<email>dilipkay@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
							<email>amaschinot@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
							<email>dbelanger@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
							<email>celiu@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
							<email>wfreeman@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boundless: Generative Adversarial Networks for Image Extension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image extension models have broad applications in image editing, computational photography and computer graphics. While image inpainting has been extensively studied in the literature, it is challenging to directly apply the state-of-the-art inpainting methods to image extension as they tend to generate blurry or repetitive pixels with inconsistent semantics. We introduce semantic conditioning to the discriminator of a generative adversarial network (GAN), and achieve strong results on image extension with coherent semantics and visually pleasing colors and textures. We also show promising results in extreme extensions, such as panorama generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Across many disparate disciplines there exists a strong need for high quality image extensions. In virtual reality, for example, it is often necessary to simulate different camera extrinsics than were actually used to capture an image, which generally requires filling in content outside of the original image bounds <ref type="bibr" target="#b19">[19]</ref>. Panorama stitching generally requires cropping the jagged edges of stitched projections to achieve a rectangular panorama, but high quality image extension could enable filling in the gaps instead <ref type="bibr" target="#b23">[23]</ref>. Similarly, extending videos has been shown to create more immersive experiences for viewers <ref type="bibr" target="#b2">[3]</ref>. As televisions transition to the 16:9 HDTV aspect ratio, it is appealing to display videos filmed at a different aspect ratio than the screen. <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b33">33]</ref>.</p><p>We desire a seamless blending between the original and extended image regions. Moreover, the extended region should match the original at the textural, structural and semantic levels, while appearing a plausible extension. Boundary conditions are only available on one side of the extended region. This is in contrast to the image inpaint-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>DeepFill PConv Ours <ref type="figure">Figure 1</ref>. Some examples of image extension: Our method (right column) generates better object shapes (top/middle rows) and produce good textures (middle/bottom rows), compared with two state of the art inpainting methods: DeepFill <ref type="bibr" target="#b48">[48]</ref> and PConv <ref type="bibr" target="#b26">[26]</ref>. The input image is extended onto the masked area (shown in gray).</p><p>ing problem <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b48">48]</ref>, where the region to be filled in is surrounded in all directions by original image data, significantly constraining the problem. Therefore, inpainting algorithms tend to have more predictable and higher quality results than image extension algorithms. In fact, we demonstrate in this paper that using inpainting algorithms with no modifications leads to poor results for image extension.</p><p>In the literature, image extension has been studied using both parametric and non-parametric methods <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b5">5]</ref>. While these methods generally do a good job of blending the extended and original regions, they have significant drawbacks. They either require the use of a carefully chosen guide image from which patches are borrowed, or they mostly extend texture, without taking into account larger scale structure or the semantics of an image. These models are only applicable in a narrow range of use cases and can-arXiv:1908.07007v1 [cs.CV] <ref type="bibr" target="#b19">19</ref> Aug 2019 not learn from a diverse data set. In practice, we would like image extension models that work on diverse data and can extend structure.</p><p>Fast progress in deep neural networks has brought the advent of powerful new classes of image generation models, the most prominent of which are generative adversarial networks (GANs) <ref type="bibr" target="#b14">[14]</ref> and variational autoencoders <ref type="bibr" target="#b21">[21]</ref>. GANs in particular have demonstrated the ability to generate high quality samples. In this paper, we use GANs, modified as described below, to learn plausible image extensions from large datasets of natural images using self-supervision, similar in spirit to the use of GANs in applications such as inpainting <ref type="bibr" target="#b16">[16]</ref> and image superresolution <ref type="bibr" target="#b24">[24]</ref>.</p><p>For the image extension problem, while state-of-the-art inpainting models <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b47">47]</ref> provide us a good starting point, we find that the results quickly degrade as we extend further from the image border. We start by pruning the components that do not apply to our setting and then adopt some techniques from the broader study of GANs. Finally, we introduce a novel method, derived from <ref type="bibr" target="#b29">[29]</ref>, of providing the model with semantic conditioning, that substantially improves the results. In summary, our contributions are:</p><p>1. We are one of the first to use GAN's effectively to learn image extensions, and do so reliably for large extrapolations (up to 3 times the width of the original). 2. We introduce a stabilization scheme for our training, based on using semantic information from a pretrained deep network to modulate the behavior of the discriminator in a GAN. This stabilization scheme is useful for any adversarial model which has a ground truth sample for each generator input. 3. We show empirically that several architectural components are important for good image extension. We present ablation studies that show the effect of each of these components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Prior work in image inpainting can be fairly neatly divided into two subcategories: classical methods, which use non-parametric computer vision and texture synthesis approaches to address the problem, and learning-based methods, which attack the problem using parametric machine learning, generally in the form of deep convolutional neural networks. Classical methods, such as <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b12">12]</ref> typically rely on patch similarity and diffusion to borrow information from the known regions of the image to fill in the hole. These methods work best when inpainting small holes in stationary textures and generally lack semantic understanding of the image. Perhaps the most successful of these methods are the Bidirectional Similarity <ref type="bibr" target="#b35">[35]</ref> and PatchMatch algorithms <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b23">23]</ref>. Other non-parametric approaches that specifically target image extension rely on image patches from images other than the one to be extrapolated. <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b36">36]</ref> rely on having large databases of photos available during the extrapolation process, while others, such as <ref type="bibr" target="#b50">[50]</ref>, depend on a carefully selected guide image.</p><p>In recent years, deep learning based approaches have made great strides in overcoming the weaknesses of the classical methods. The first significant learning-based approach to inpainting was the Context Encoder <ref type="bibr" target="#b30">[30]</ref>, which trained an encoder-decoder model to fill in a central square hole in an image, using a combination of 2 regression on pixel values, and an adversarial loss <ref type="bibr" target="#b14">[14]</ref>. <ref type="bibr" target="#b45">[45]</ref> minimizes the difference of nearest neighbor activation patches in deep layers of a pretrained ImageNet classification network, for improved synthesis of highly textured content. <ref type="bibr" target="#b17">[17]</ref> improve on the results of <ref type="bibr" target="#b30">[30]</ref> by adding a local discriminator loss to the original global discriminator loss; the local discriminator focuses on the realism of the synthetic content, while the global discriminator encourages global semantic coherence. <ref type="bibr" target="#b48">[48]</ref> improves on <ref type="bibr" target="#b17">[17]</ref> further by introducing a coarse-to-fine approach. Their model has two chained encoder-decoder sections, the second of which contains a contextual attention layer, which learns the optimal locations in the unmasked regions from which the model should borrow texture patches. Other similar approaches include <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b47">47]</ref>, while <ref type="bibr" target="#b46">[46]</ref> train an unconditioned GAN to generate complete images from the target distribution and perform an inference-time optimization to search for the latent code that would produce the closest match to the known pixels of the masked image. The only previous fullyparametric approach to image extension that we are aware of is <ref type="bibr" target="#b40">[40]</ref>, which showed impressive results using an autoregressive model to extend 32x32 pixel images, including the ability to output multiple plausible completions. These are, however, too small for practical applications. Concurrent to our work is <ref type="bibr" target="#b44">[44]</ref>, which is similar to ours but does not condition the discriminator with pre-trained features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>Our model uses a Wasserstein GAN framework <ref type="bibr" target="#b28">[28]</ref> comprising a generator network that is trained with the assistance of a concurrently-trained discriminator network.</p><p>Our generator network, G has an input consisting of the image z with pixel values in the range [?1, 1], which is to be extended, and a binary mask M . These are the same dimensions spatially and are concatenated channel-wise. Both z and M consist of a region of known pixels and a region of unknown pixels. In contrast to inpainting frameworks, the unknown region shares a boundary with the known region on only one side. z is set to 0 in the unknown region, while M is set to 1 in the unknown region and 0 in the known region. At training time, where x is sampled from a natural image distribution X and is the element-wise multiplication operator. The output G(z, M ) of G has the same dimensions as z and a pixel loss during training uses this full output. However, the last stage before feeding into the discriminator D is to replace what G synthesized in the unmasked regions with the known input pixels:</p><formula xml:id="formula_0">z = x (1 ? M )<label>(1)</label></formula><formula xml:id="formula_1">x = G(z, M ) M + z<label>(2)</label></formula><p>D is also a deep network, which transforms a real sample from X or a generated samplex to a single scalar value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generator</head><p>G generally follows the same fully convolutional encoder-decoder architecture as used by <ref type="bibr" target="#b47">[47]</ref> (see <ref type="figure" target="#fig_0">Figure 2</ref>). Each layer in the generator except the last one uses gated convolutions <ref type="bibr" target="#b47">[47]</ref> to enable the model to learn to select the contributing features for each spatial location and channel. Following the inpainting guidance in <ref type="bibr" target="#b48">[48]</ref>, each layer except the last uses an ELU activation function <ref type="bibr" target="#b10">[10]</ref>, and the final layer clips its outputs to the range [?1, 1]. As in <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48]</ref>, the innermost layers utilize dilated convolutions to increase their receptive field size.</p><p>To address the image extension problem, we deviated from the generator architecture proposed by <ref type="bibr" target="#b47">[47]</ref> in a few crucial ways. We eliminated the refinement network, including the contextual attention layer, since this layer is biased towards copying patches from the unmasked portion of the input. While borrowing patches is a useful property for inpainting of images <ref type="bibr" target="#b7">[7]</ref>, in the case of image extension, it is less likely that repeated patterns will result in convincing extension. <ref type="figure" target="#fig_1">Figure 3</ref> shows the effect of the contextual attention layer of <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b47">47]</ref>. We also compare to Adobe Photoshop's PatchMatch-based <ref type="bibr" target="#b7">[7]</ref> Content Aware Fill tool, which generates similar artifacts due to copying patches. These copying artifacts occur on a large fraction of the output images.</p><p>We also introduced skip connections [32] between the non-dilated layers, since we found that they improved the network's ability to synthesize high frequency information. In <ref type="figure" target="#fig_4">Figure 6</ref>, we show the typical benefit of using skip connections. We additionally added instance normalization <ref type="bibr" target="#b39">[39]</ref> after every generator layer besides the output layer, finding that it significantly reduces the number of artifacts in the generated images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discriminator</head><p>The objective of the discriminator network (see <ref type="figure" target="#fig_0">Figure 2</ref>) is determining whether an image is generator-produced or real. In our problem setup, the concern is not just whether the output of G appears real, but also that it is a plausible extension of G's inputs. To this end, we design our discriminator to be conditioned on the specific generator inputs when evaluating whether what is fed into the discriminator is real or fake. We condition the discriminator in two ways.</p><p>First, when a generated image is input, we copy the known pixels from z to overwrite the corresponding generated pixels, as described in eq. 2, and we additionally input the mask M itself. This on its own provides a major advantage to the discriminator in the adversarial game, since it can focus in on the area right around the seam at the edge of the real content and easily determine that an image is fake if there is any abrupt change in image statistics along that seam. We see this play out during training, as the generated image content close to the seam is the first to improve and the quality improvement gradually spreads towards the opposite edge of the image as training progresses. On its own, this form of conditioning produces seamless results, but the quality of generated content still deteriorates as it moves further from the real content.</p><p>To address this, we add another form of conditioning, which is a modified version of the conditional projection discriminator (cGAN) <ref type="bibr" target="#b29">[29]</ref>. In the original cGAN paper, a one-hot class label y is passed into the discriminator in addition to the image x * to be classified as real or fake. The discriminator output is then</p><formula xml:id="formula_2">D (x * , y) = f ? (? (x * )) + ? (x * ) , f y (y)<label>(3)</label></formula><p>where ? is a learned function mapping an image to a vector, f ? is a learned fully-connected layer that maps that vector to a scalar, f y is a learned fully-connected layer mapping y to a vector of the same size as the output of ?, and ?, ? denotes an inner product. The cGAN paper shows that this parameterization of the GAN objective enables the model to simultaneously learn the distributions p(x) and p(y|x).</p><p>In our setting we don't necessarily have class labels available, and we also want our conditioning vectors to contain more information than class labels would provide. To this end, we were inspired by previous work on perceptual metrics <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b49">49]</ref> to replace y with the activations of a pretrained image classification network, C, when applied to x (the ground truth image). We chose to instantiate C as an InceptionV3 <ref type="bibr" target="#b37">[37]</ref> network trained on ImageNet <ref type="bibr" target="#b11">[11]</ref> with the final softmax removed. We found that it helps to normalize these activations by subtracting the mean activation over the dataset and then dividing the result by its 2 norm. Note that since the discriminator is only used during training, we can condition on the full unmasked image (x), which also means that these activations can be precomputed before training. This conditioning encourages the generated content to semantically match the target image, which especially helps avoid semantic drift in larger extensions. Formally, we replace eq. 3 with</p><formula xml:id="formula_3">D (x * , M , x) =f ? (? (x * , M )) + ? (x * , M ) , f C (C (x))<label>(4)</label></formula><p>The architecture of ? is based on <ref type="bibr" target="#b47">[47]</ref> and consists of six strided convolutional layers, followed by a fully connected layer. Each convolutional layer uses a leaky ReLU activation function <ref type="bibr" target="#b27">[27]</ref> and all layers apply spectral normalization <ref type="bibr" target="#b28">[28]</ref> to satisfy the Lipschitz constraints of Wasserstein GANs <ref type="bibr" target="#b4">[4]</ref>. The output dimensions of ? and f C are both 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>The model is trained via a combination of a reconstruction loss and an adversarial loss. The reconstruction loss optimizes for coarse image agreement and is implemented as an 1 loss imposed on the full output of G. The full equation is below:</p><formula xml:id="formula_4">L rec = x ? G (z, M ) 1<label>(5)</label></formula><p>For the adversarial loss, which refines the coarse prediction, we use a Wasserstein GAN hinge loss <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b38">38]</ref>:</p><formula xml:id="formula_5">L adv,D = E x?P X (x) [ ReLU (1 ? D (x, M , x)) + ReLU (1 + D (x, M , x)) ] L adv,G = E x?P X (x) [ ?D (x, M , x) ]<label>(6)</label></formula><p>where ReLU is the rectified linear unit function. The total loss on the generator is L total = L rec + ?L adv,G (7) In all our experiments we set ? = 10 ?2 .</p><p>Our model is implemented in TensorFlow <ref type="bibr" target="#b0">[1]</ref>. The generator and discriminator are trained jointly using the Adam optimizer <ref type="bibr" target="#b20">[20]</ref> with parameters ? = 10 ?4 , ? 1 = 0.5, ? 2 = 0.9; (the discriminator has a slightly larger ? = 10 ?3 , but other parameters are the same). Unlike many previous papers, we did not see improvement from training the discriminator for multiple steps per each generator step. Based on the findings of <ref type="bibr" target="#b9">[9]</ref> on the benefits of training GANs with large batch sizes, we trained on 32 cores of a Google Cloud TPUv3 Pod with batch size 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>For all experiments we train our model on a dataset composed of the top 50 classes (measured by number of samples in the training set) of the Places365-Challenge dataset <ref type="bibr" target="#b51">[51]</ref>, producing a training set of just under 2 million images, which we scaled to the spatial dimensions of 257x257 FID PSNR FID PSNR FID PSNR FID PSNR (25%) (25%) (50%) (50%) (75%) (75%) (Inp) (Inp) DF 1.87 7.11 11.65 6.69 31. <ref type="bibr" target="#b21">21</ref>   <ref type="table">Table 2</ref>. Comparisons with other methods for stabilizing GAN training. We provide PSNR for reference, but found that FID correlates with perceptual quality best. Based on FID on 25% and 50% extensions, feature matching and perceptual losses outperform our conditioning, but the difference is fairly small. On 75% extensions, our conditioning provides the best results and the difference is large. pixels. The use of 50 classes allows us to test how well our model can generalize to multiple categories. We used a held-out set of 500 images from the same set of classes in the Places365 dataset, approximately 10 images per class, to compute quantitative scores and to visualize the image extension results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Extension</head><p>We compare our model (which we call Ours) with various baselines both qualitatively and quantitatively on the task of image extension. Specifically, we evaluate each algorithm's ability to fill in masked out image content for three different image extension tasks (where the rightmost 25%, 50% and 75% of pixels in the image are respectively masked) and one inpainting task (a central square mask comprising 25% of image pixels). For each experiment, our model is retrained with masks of the appropriate position, shape and size. The baselines we compare against are: No-Cond: A model that is identical to "Ours," but without discriminator conditioning. DeepFill: Our re-implementation of DeepFillv2 <ref type="bibr" target="#b47">[47]</ref>, which is a state of the art inpainting model. We confirmed that our reimplementation achieves inpainting results nearly identical to that of the original papers (see <ref type="figure" target="#fig_4">Figure 6</ref>). For each experiment, we retrain the model with the same masks and data as ours. We follow the authors' guidance of training for 5 days on an NVIDIA P100 GPU. We note that this results in the model being trained for many fewer steps than "Ours" because it trains much more slowly (0.8 steps/sec vs 4.7 steps/sec for "Ours"). Comparison against this model shows the benefits of our approach compared to simply repurposing an architecture suited for inpainting tasks. PConv: The authors of another state of the art inpainting work <ref type="bibr" target="#b26">[26]</ref> generated results for us based on provided masks, but the models were not retrained specifically for these tasks. The model was trained on the full Places2 dataset, which is a superset of our training set. They use a database of free-form masks, some of which are very large (up to 50% of the image size), but are often non-contiguous and non-convex, which means that at training time the model may not have needed to generate pixels that were very far from known context. While our comparisons to this paper are not exactly apples-to-apples, we believe that this still provides a strong baseline against which to compare our performance. CAF: Results from Adobe Photoshop's content aware fill, which is based on the PatchMatch algorithm <ref type="bibr" target="#b7">[7]</ref>. Content aware fill is a very powerful tool used for image extension, and reprents a strong classical baseline. However, due to the use of only patch level information, it does not provide semantically meaningful extensions.</p><p>We provide quantitative performance metrics for each mask-type and each algorithm in <ref type="table" target="#tab_0">Table 1</ref>. We agree with the authors of <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b26">26]</ref> that there are really no good metrics that capture the goals of these experiments, but we nonetheless report the best approximations. Specifically, we report Fr?chet Inception Distance (FID) <ref type="bibr" target="#b15">[15]</ref> on the full output image and PSNR of the masked regions only. For FID we used a diagonal covariance matrix, due to having few samples. Based on our own qualitative evaluations, we feel that FID of the entire output image best correlates with what we perceive as quality image extension. We additionally performed a qualitative analysis. We show results on a few images from our test set in <ref type="figure" target="#fig_3">Figures 4 and 6</ref>. We show many more results, including on free-form masks, in the Supplementary Material. Overall we see that all methods, other than "CAF," perform admirably for inpainting. On the extension tasks, as we move towards larger extensions with smaller context, "CAF" and "DeepFill" degrade into just repeating textures, while "PConv" gets blurrier and more artifact-filled the further away from the context it gets. The "NoCond" version of our model maintains higher quality for the larger extensions but does show some blurring and semantic drift. Meanwhile, our full model remains semantically consistent, with mostly photorealistic and seamless synthesis.</p><p>Furthermore, we experiment with replacing our conditioning with perceptual <ref type="bibr" target="#b18">[18]</ref> and feature matching <ref type="bibr" target="#b42">[42]</ref> losses, see <ref type="table">Table 2</ref> and <ref type="figure">Figure 5</ref>. In the perceptual loss,  the generator is optimized to produce images that are close to the ground truth images in the activation space of a pretrained classification network. Similar to our conditioning, perceptual loss uses a pre-trained network to guide the training towards plausible extensions. Unlike our conditioning, the perceptual loss modifies only the generator objective to bias it towards semantic coherence, whereas our conditioning figures into both the generator and discriminator objectives and adding semantic information to the whole adversarial optimization game. Feature matching is similar in principle to a perceptual loss, but it minimizes the distance between activations in a hidden layer of the discriminator, rather than a pre-trained classification network, and similarly only figures into the generator objective.</p><p>In our experiments on 75% extensions, we found that our conditioning performs significantly better than feature matching and perceptual loss, while on smaller extensions they perform similarly. Our perceptual loss implementation tries to match pre-softmax logits, while our feature matching follows <ref type="bibr" target="#b42">[42]</ref> and tries to match convolutional feature maps in the discriminator at multiple scales. Preliminary experiments indicate that combinations of all the three result in even better results, chiefly with fewer GAN-style artifacts, but we leave a more thorough analysis to future work.</p><p>We also qualitatively compare against PixelCNN in <ref type="figure">Figure 5</ref>. It is clear PixelCNN performs much worse than our method. Furthermore, it takes about 12 minutes to do inference for a single 64x64x3 image, on a Tesla P100. On our higher resolution 256x256 images, this would translate to over 2.5 hours. In contrast, our method takes 0.1 seconds/image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ours Perceptual FM PixelCNN <ref type="figure">Figure 5</ref>. Although FM and Perceptual give slightly better results on short range extensions, our model far outperforms the others in the 75% case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablations</head><p>In order to validate the contributions of each aspect of our model, we trained models each ablating a single feature of our network. We experimented with removing skip connections in the generator, removing feature conditioning from the discriminator, removing instance norm from the generator and reducing batch size to 64.</p><p>We show an example of the results in <ref type="figure" target="#fig_4">Figure 6</ref>. We see that without skip connections, the model has a hard time synthesizing high frequency textures, which often results in a perceptible seam between the real and generated content. Without discriminator conditioning the quality degrades more rapidly as we move further away from the edge with known context. Removing instance norm causes an increase in white and over-saturated artifacts. The smaller batch model generally produces blurrier results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Panorama Generation</head><p>We evaluate our model on its ability to generate panoramic images from a much narrower seed image. We use the same dataset and models as in Section 4.1, but whereas in those experiments we ran each model's forward pass only once, this time we apply the model recursively, using the synthesized content from the previous step as the known content for the current step in a sliding window approach. More specifically, we take a 257x192 image, pad it with 65 columns of zeros, and extend it using our our model trained for 25% extension. We then extend the image again by selecting the right most 192 pixels of the image, padding with zeros, and feeding it to the extension model. This is repeated 5 times, ultimately extending the image 2.7 times out to a width of 582 pixels. We show some results in <ref type="figure" target="#fig_5">Figure  7</ref> and more results in the Supplementary Material. While the results are for the most part plausible and aesthetically pleasing, we do see some degradation and semantic drift as we move away from the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Exploring the Space of Plausible Extensions</head><p>To visually explore the space of results generated by our extension model, we took sample videos from the YouTube8m dataset <ref type="bibr" target="#b1">[2]</ref> and applied our model to extend both the right and left sides horizontally. Videos are a natural domain to test our model since consecutive frames are closely related to each other, and the small variations can generate interesting plausible outcomes. This allows us to verify that our model has not memorized a fixed completion for closely related images or collapses with small natural variations in the input. We scaled the videos to have a height of 257 pixels and respectively selected the right and leftmost 192 columns. We then padded each of them with 65 columns of zeros on the side to be extended. We ran the resulting images, each frame independently, through our model trained with 25% masks. We then took the model output for the extended region and concatenated it back on the original video frame. Please refer to the supplementary video https://drive.google.com/open?id= 1x6FCYPmoqSuCdeLJTD0UpQ_MQhBPv7_e.  . Our models can also be used generate image panoramas. This can be viewed as a stress test for image extension tasks. We recursively apply the 25% model to create a very large output image of about 3 times the original width. <ref type="table">Table 3</ref>. The generator architecture. Act. stands for activation type, K stands for kernel size, S for stride, D for dilation, Out for number of channels in convolutional layers and number of units in fully connected units, and Skip represents the layer-id which is concatenated into the output of the given layer. All resize operations use bilinear interpolation. In the Generator, all convolutional layers use 'Same' padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Discriminator Network D</head><p>The discriminator applies spectral normalization <ref type="bibr" target="#b28">[28]</ref> at all layers, and consists of the the common tower (D N , Table 4), which feeds into the non-conditional branch (f N , <ref type="table" target="#tab_3">Table 5</ref>) and projection discriminator branch (f C , <ref type="table">Table  6</ref>). These two branches produce scalars, which are then summed to produce a single network output. We invite the reader to see Section 3 of the main paper for more in depth discussion of the model.</p><p>The scalar outputs of the main and projection discriminator are summed and passed to the adversarial loss. Flatten n/a n/a n/a n/a n/a  Inner Product w/Common Tower None 1 <ref type="table">Table 6</ref>. The projection discriminator <ref type="bibr" target="#b29">[29]</ref> branch of the network. The input is logits of a pretrained classification network, for which we used an InceptionV3 <ref type="bibr" target="#b37">[37]</ref> network trained on ImageNet <ref type="bibr" target="#b11">[11]</ref>. The output is a single scalar, which is summed with the output of the non-conditional branch and passed to the hinge loss.</p><p>Before passing the training image into the generator we resize the image to 257 x 257, and also concatenate the mask channel. The mask size is randomly sampled from a uniform distribution, which is the target size plus/minus 4 pixels, so the model doesn't overfit to a specific mask size.</p><p>Following the code of DeepFill <ref type="bibr" target="#b48">[48]</ref>, we concatenate a channel of 1's to the input of the generator. This enables the generator to see the edge of the image after 0 padding the inputs, although we do not verify this in this work.</p><p>We take generator and discriminator steps in a 1:1 ratio, with the steps executed jointly.</p><p>Please see Section 3 of the main paper for more discussion of the loss and optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Qualitative Results</head><p>We show additional samples from on the 25%, 50%, and 75% mask image extension experiments, and refer the reader to Figures 9, 10, and 11. We also show additional results from in-painting experiment in <ref type="figure" target="#fig_0">Figure 12</ref> and more panorama results in <ref type="figure" target="#fig_1">Figure 13</ref>.We also demonstrate the suitability of our method on freeform masks in <ref type="figure">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Our method Ground Truth <ref type="figure">Figure 8</ref>. Results on freeform masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Exploring the Space of Plausible Extensions</head><p>We invite the reader to view the accompanying video derived from a sample from the YouTube8m dataset <ref type="bibr" target="#b1">[2]</ref> at https://drive.google.com/file/d/ 1x6FCYPmoqSuCdeLJTD0UpQ_MQhBPv7_e/view? usp=sharing. Please refer to the main paper for details on how it was created. We encourage the reader to pause the video at arbitrary frames to see how the model produces different plausible completions as the result of tiny perturbations of the original frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Failure Cases</head><p>In <ref type="figure" target="#fig_3">Figure 14</ref> we examine some of the failure modes of our image extension model. We note that our model is much better at textures than objects; for example vehicles, people, and furniture are challenging for the model. Addressing this is left to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAF</head><p>DeepFill PConv NoCond Ours GT <ref type="figure">Figure 9</ref>. Extending images from masks which are 25% of the image width. We note that edges and structure are better defined in our method. For instance, edge of the roof in the second row. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Model Architecture: this architecture is used for all our models. See text for further details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The contextual attention layer from DeepFill<ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b47">47]</ref> tends to repeat patches and structures. The original image (top left) is extended to the right (top-middle and top-right). DeepFill creates a copy of the door handle, whereas our extension extends the structure in a semantically and geometrically more plausible manner. Similarly, Photoshop's Content Aware Fill (bottom row) often creates artifacts since it is based on PatchMatch<ref type="bibr" target="#b7">[7]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Extending images from masks of (a) 25%, (b) 50% and (c) 75% of the image width using multiple algorithms. From left to right: DeepFill<ref type="bibr" target="#b48">[48]</ref>, PConv<ref type="bibr" target="#b26">[26]</ref>, Photoshop Content Aware Fill, our model with no conditioning, our full model and ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Further analysis: (a) Comparing the different models on inpainting tasks; our conditioned model performs on par with the state of the art models such as PConv and DeepFill for the inpainting problem. (b) Ablation tests: we remove (from second column to fifth column) only one of the following: discriminator conditioning, instance norm, skip connections, and reduce batch size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7</head><label>7</label><figDesc>Figure 7. Our models can also be used generate image panoramas. This can be viewed as a stress test for image extension tasks. We recursively apply the 25% model to create a very large output image of about 3 times the original width.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .Figure 11 .Figure 14 .</head><label>101114</label><figDesc>Extending images from masks which are 50% of the image width. Extending images from masks which are 75% of the image width. Failure cases. The network struggles with objects; especially cars, humans, and furniture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>9.74 4.96 14.31 PC 1.40 11.10 11.20 6.63 31.83 8.94 3.70 13.78 NCnd 0.85 8.96 5.01 7.55 19.17 9.08 2.73 14.24 Ours 0.79 10.17 3.46 8.63 8.79 8.07 2.53 14.17 Quantitative metrics on 500 test images.</figDesc><table><row><cell>The mask types</cell></row><row><cell>are: 25% extension (3:1 ratio of context to mask), 50% exten-</cell></row><row><cell>sion (1:1 ratio), 75% (1:3 ratio) and inpainting a central square</cell></row><row><cell>mask comprising 25% of image pixels(Inp). We compare Deep-</cell></row><row><cell>Fill(DF), PartialConv(PC), ours without conditioning(NCnd), and</cell></row><row><cell>our model.</cell></row><row><cell>FID PSNR FID PSNR FID PSNR</cell></row><row><cell>(25%) (25%) (50%) (50%) (75%) (75%)</cell></row><row><cell>Prcptl 0.40 9.95 2.32 8.31 14.15 9.65</cell></row><row><cell>FM 0.75 9.42 3.14 8.97 14.74 8.87</cell></row><row><cell>Ours 0.79 10.17 3.46 8.63 8.79 8.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The base of the discriminator. It takes generated and ground truth images as input. Act. stands for activation type, K stands for kernel size, S for stride, Out for number of channels in convolutional layers and number of units in fully connected units.</figDesc><table><row><cell></cell><cell cols="2">Non-Conditional Branch f N</cell></row><row><cell>Layer ID</cell><cell>Type</cell><cell cols="2">Act. Out Size</cell></row><row><cell>1</cell><cell cols="2">Fully Connected No Bias None</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>The non-conditional branch of the discriminator, taking the common tower fromTable 2as input and outputting a single scalar value. Act. stands for activation type.</figDesc><table><row><cell></cell><cell cols="2">Projection Discriminator Branch f C</cell><cell></cell></row><row><cell>Layer ID</cell><cell>Type</cell><cell cols="2">Act. Out Size</cell></row><row><cell>1</cell><cell>Normalize</cell><cell>None</cell><cell>1000</cell></row><row><cell>2</cell><cell>Fully Connected No Bias</cell><cell>None</cell><cell>256</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors would like to acknowledge Amol Kapoor and Dr. Huiwen Chang for helpful discussion and sharing code, and Dr. Guilin Liu for helping with running Partial Convolution comparison experiments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Resize (2x) n/a n/a n/a n/a n/a n/a 14 <ref type="table">Gated Conv  ELU  3  1  1 64  3  15  Gated Conv  ELU  3  1  1 64  2  16</ref> Resize (2x) n/a n/a 1 n/a n/a n/a 17 <ref type="table">Gated Conv  ELU  3  1  1 32  1  18  Gated Conv  ELU  3  1  1 16 None  19  Conv  None  3  1  1  3 None  20</ref> Clip n/a n/a n/a n/a n/a n/a</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A largescale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Panoramic video textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">821</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">ACM</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Seam carving for contentaware image resizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Filling-in by joint interpolation of vector fields and gray levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coloma</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verdera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON IMAGE PROCESSING</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coloma</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alexei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alexei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas K</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh IEEE international conference on computer vision</title>
		<meeting>the seventh IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Globally and Locally Consistent Image Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno>107:1-107:14</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH 2017)</title>
		<meeting>of SIGGRAPH 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Infinite images: Creating and exploring a large photorealistic virtual space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biliana</forename><surname>Kaneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aspect ratio problems in television today and some new solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Knee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename><surname>Piroddi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SMPTE Motion Imaging Journal</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quality prediction for image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolf</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved seam carving for video retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on graphics (TOG)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven M</forename><surname>Seitz</surname></persName>
		</author>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="16" to="31" />
		</imprint>
	</monogr>
	<note type="report_type">Photo uncrop</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Summarizing visual data using bidirectional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Creating and exploring a large photorealistic virtual space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biliana</forename><surname>Kaneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6924" to="6932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Biggerpicture: data-driven image extrapolation using graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><forename type="middle">Robert</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image inpainting via generative multi-column convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Widecontext semantic image extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting using multiscale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6721" to="6729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Teck Yian Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schwing</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03589</idno>
		<title level="m">Free-form image inpainting with gated convolution</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Framebreak: Dramatic image extrapolation by guided shiftmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Training details: We take the training set of Places365-Challenge dataset [51], select the top 50 classes by number of samples, and create a holdout validation set from this. This creates about 39,000 training and 930 test samples per class, for a total training set size of 1,953,624 and testing set size of 46376</title>
		<imprint/>
	</monogr>
	<note>The classes selected are</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
