<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Ordinal Regression Network for Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SIT, FEIT</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Philosophy</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">LIGM (UMR 8049)</orgName>
								<orgName type="institution" key="instit1">Universit? Paris-Est</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">ENPC</orgName>
								<orgName type="institution" key="instit4">ESIEE Paris</orgName>
								<orgName type="institution" key="instit5">UPEM</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
							<email>kayhan@pitt.educhaohui.wang@u-pem.fr</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.aumig73</email>
							<affiliation key="aff0">
								<orgName type="department">SIT, FEIT</orgName>
								<orgName type="institution" key="instit1">UBTECH Sydney AI Centre</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Ordinal Regression Network for Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed problem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural networks (DC-NNs). These methods model depth estimation as a regression problem and train the regression networks by minimizing mean squared error, which suffers from slow convergence and unsatisfactory local solutions. Besides, existing depth estimation networks employ repeated spatial pooling operations, resulting in undesirable low-resolution feature maps. To obtain high-resolution depth maps, skipconnections or multi-layer deconvolution networks are required, which complicates network training and consumes much more computations. To eliminate or at least largely reduce these problems, we introduce a spacing-increasing discretization (SID) strategy to discretize depth and recast depth network learning as an ordinal regression problem. By training the network using an ordinary regression loss, our method achieves much higher accuracy and faster convergence in synch. Furthermore, we adopt a multi-scale network structure which avoids unnecessary spatial pooling and captures multi-scale information in parallel.</p><p>The method described in this paper achieves state-ofthe-art results on four challenging benchmarks, i.e., KITTI [18], ScanNet [10], Make3D [51], and NYU Depth v2 [43], and win the 1st prize in Robust Vision Challenge 2018.</p><p>Abstract Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed problem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural networks (DC-NNs). These methods model depth estimation as a regression problem and train the regression networks by minimizing mean squared error, which suffers from slow convergence and unsatisfactory local solutions. Besides, existing depth estimation networks employ repeated spatial pooling operations, resulting in undesirable low-resolution feature maps. To obtain high-resolution depth maps, skipconnections or multi-layer deconvolution networks are required, which complicates network training and consumes much more computations. To eliminate or at least largely reduce these problems, we introduce a spacing-increasing discretization (SID) strategy to discretize depth and recast depth network learning as an ordinal regression problem. By training the network using an ordinary regression loss, our method achieves much higher accuracy and faster convergence in synch. Furthermore, we adopt a multi-scale network structure which avoids unnecessary spatial pooling and captures multi-scale information in parallel.</p><p>The method described in this paper achieves state-ofthe-art results on four challenging benchmarks, i.e., KITTI [17], ScanNet [9], Make3D [50], and NYU Depth v2 <ref type="bibr" target="#b41">[42]</ref>, and win the 1st prize in Robust Vision Challenge 2018. Code has been made available at: https://github. com/hufu6371/DORN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating depth from 2D images is a crucial step of scene reconstruction and understanding tasks, such as 3D object recognition, segmentation, and detection. In this pa-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating depth from 2D images is a crucial step of scene reconstruction and understanding tasks, such as 3D object recognition, segmentation, and detection. In this pa- per, we examine the problem of Monocular Depth Estimation from a single image (abbr. as MDE hereafter). Compared to depth estimation from stereo images or video sequences, in which significant progresses have been made <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">45]</ref>, the progress of MDE is slow. MDE is an ill-posed problem: a single 2D image may be produced from an infinite number of distinct 3D scenes. To overcome this inherent ambiguity, typical methods resort to exploiting statistically meaningful monocular cues or features, such as perspective and texture information, object sizes, object locations, and occlusions <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Recently, some works have significantly improved the MDE performance with the use of DCNN-based models <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b2">3]</ref>, demonstrating that deep features are superior to handcrafted features. These methods address the MDE problem by learning a DCNN to estimate the continuous depth map. Since this problem is a standard regression problem, mean squared error (MSE) in log-space or its variants are usually adopted as the loss function. Al- per, we examine the problem of Monocular Depth Estimation from a single image (abbr. as MDE hereafter).</p><p>Compared to depth estimation from stereo images or video sequences, in which significant progresses have been made <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b45">46]</ref>, the progress of MDE is slow. MDE is an ill-posed problem: a single 2D image may be produced from an infinite number of distinct 3D scenes. To overcome this inherent ambiguity, typical methods resort to exploiting statistically meaningful monocular cues or features, such as perspective and texture information, object sizes, object locations, and occlusions <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Recently, some works have significantly improved the MDE performance with the use of DCNN-based models <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b2">3]</ref>, demonstrating that deep features are superior to handcrafted features. These methods address the MDE problem by learning a DCNN to estimate the continuous depth map. Since this problem is a standard regression problem, mean squared error (MSE) in log-space or its variants are usually adopted as the loss function. Although optimizing a regression network can achieve a rea-sonable solution, we find that the convergence is rather slow and the final solution is far from satisfactory.</p><p>In addition, existing depth estimation networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b58">59]</ref> usually apply standard DCNNs designed initially for image classification in a full convolutional manner as the feature extractors. In these networks, repeated spatial pooling quickly reduce the spatial resolution of feature maps (usually stride of 32), which is undesirable for depth estimation. Though high-resolution depth maps can be obtained by incorporating higher-resolution feature maps via multi-layer deconvolutional networks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref>, multiscale networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b10">11]</ref> or skip-connection <ref type="bibr" target="#b58">[59]</ref>, such a processing would not only require additional computational and memory costs, but also complicate the network architecture and the training procedure.</p><p>In contrast to existing developments for MDE, we propose to discretize continuous depth into a number of intervals and cast the depth network learning as an ordinal regression problem, and present how to involve ordinal regression into a dense prediction task via DCNNs. More specifically, we propose to perform the discretization using a spacing-increasing discretization (SID) strategy instead of the uniform discretization (UD) strategy, motivated by the fact that the uncertainty in depth prediction increases along with the underlying ground-truth depth, which indicates that it would be better to allow a relatively larger error when predicting a larger depth value to avoid over-strengthened influence of large depth values on the training process. After obtaining the discrete depth values, we train the network by an ordinal regression loss, which takes into account the ordering of discrete depth values.</p><p>To ease network training and save computational cost, we introduce a network architecture which avoids unnecessary subsampling and captures multi-scale information in a simpler way instead of skip-connections. Inspired by recent advances in scene parsing <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b63">64]</ref>, we first remove subsampling in the last few pooling layers and apply dilated convolutions to obtain large receptive fields. Then, multi-scale information is extracted from the last pooling layer by applying dilated convolution with multiple dilation rates. Finally, we develop a full-image encoder which captures image-level information efficiently at a significantly lower cost of memory than the fully-connected full-image encoders <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30]</ref>. The whole network is trained in an end-to-end manner without stage-wise training or iterative refinement. Experiments on four challenging benchmarks, i.e., KITTI <ref type="bibr" target="#b17">[18]</ref>, ScanNet <ref type="bibr" target="#b9">[10]</ref>, Make3D <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b49">50]</ref> and NYU Depth v2 <ref type="bibr" target="#b42">[43]</ref>, demonstrate that the proposed method achieves state-of-the-art results, and outperforms recent algorithms by a significant margin.</p><p>The remainder of this paper is organized as follows. After a brief review of related literatures in Sec. 2, we present in Sec. 3 the proposed method in detail. In Sec. 4, be-sides the qualitative and quantitative performance on those benchmarks, we also evaluate multiple basic instantiations of the proposed method to analyze the effects of those core factors. Finally, we conclude the whole paper in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Depth Estimation is essential for understanding the 3D structure of scenes from 2D images. Early works focused on depth estimation from stereo images by developing geometry-based algorithms <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref> that rely on point correspondences between images and triangulation to estimate the depth. In a seminal work <ref type="bibr" target="#b49">[50]</ref>, Saxena et al. learned the depth from monocular cues in 2D images via supervised learning. Since then, a variety of approaches have been proposed to exploit the monocular cues using handcrafted representations <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b60">61]</ref>. Since handcrafted features alone can only capture local information, probabilistic graphic models such as Markov Random Fields (MRFs) are often built based on these features to incorporate long-range and global cues <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b40">41]</ref>. Another successful way to make use of global cues is the DepthTransfer method <ref type="bibr" target="#b27">[28]</ref> which uses GIST global scene features <ref type="bibr" target="#b44">[45]</ref> to search for candidate images that are "similar" to the input image from a database containing RGBD images.</p><p>Given the success of DCNNs in image understanding, many depth estimation networks have been proposed in recent years <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29]</ref>. Thanks to multi-level contextual and structural information from powerful very deep networks (e.g., VGG <ref type="bibr" target="#b55">[56]</ref> and ResNet <ref type="bibr" target="#b23">[24]</ref>), depth estimation has been boosted to a new accuracy level <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b58">59]</ref>. The main hurdle is that the repeated pooling operations in these deep feature extractors quickly decrease the spatial resolution of feature maps (usually stride 32). Eigen et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref> applied multi-scale networks which stage-wisely refine estimated depth map from low spatial resolution to high spatial resolution via independent networks. Xie et al. <ref type="bibr" target="#b58">[59]</ref> adopted the skip-connection strategy to fuse low-spatial resolution depth map in deeper layers with high-spatial resolution depth map in lower layers. More recent works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref> apply multi-layer deconvolutional networks to recover coarse-to-fine depth. Rather than solely relying on deep networks, some methods incorporate conditional random fields to further improve the quality of estimated depth maps <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b39">40]</ref>. To improve efficiency, Roy and Todorovic <ref type="bibr" target="#b47">[48]</ref> proposed the Neural Regression Forest method which allows for parallelizable training of "shallow" CNNs.</p><p>Recently, unsupervised or semi-supervised learning is introduced to learn depth estimation networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref>. These methods design reconstruction losses to estimate the disparity map by recovering a right view with a left view. Also, some weakly-supervised methods considering The ASPP module consists of 3 dilated convolutional layers with kernel size of 3 ? 3 and dilated rate of 6, 12 and 18 respectively <ref type="bibr" target="#b5">[6]</ref>. The supervised information of our network is discrete depth values output by the discretization using the SID strategy. The whole network is optimized by our ordinal regression training loss in an end-to-end fashion.</p><p>pair-wise ranking information were proposed to roughly estimate and compare depth <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Ordinal Regression <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23]</ref> aims to learn a rule to predict labels from an ordinal scale. Most literatures modify wellstudied classification algorithms to address ordinal regression algorithms. For example, Shashua and Levin <ref type="bibr" target="#b52">[53]</ref> handled multiple thresholds by developing a new SVM. Cammer and Singer <ref type="bibr" target="#b8">[9]</ref> generalized the online perceptron algorithms with multiple thresholds to do ordinal regression. Another way is to formulate ordinal regression as a set of binary classification subproblems. For instance, Frank and Hall <ref type="bibr" target="#b14">[15]</ref> applied some decision trees as binary classifiers for ordinal regression. In computer vision, ordinal regression has been combined with DCNNs to address the age estimation problem <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>This section first introduces the architecture of our deep ordinal regression network; then presents the SID strategy to divide continuous depth values into discrete values; and finally details how the network parameters can be learned in the ordinal regression framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, the divised network consists of two parts, i.e., a dense feature extractor and a scene understanding modular, and outputs multi-channel dense ordinal labels given an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Dense Feature Extractor</head><p>Previous depth estimation networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b58">59]</ref> usually apply standard DCNNs originally designed for image recognition as the feature extractor. However, the re-peated combination of max-pooling and striding significantly reduces the spatial resolution of the feature maps. Also, to incorporate multi-scale information and reconstruct high-resolution depth maps, some partial remedies, including stage-wise refinement <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref>, skip connection <ref type="bibr" target="#b58">[59]</ref> and multi-layer deconvolution network <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref> can be adopted, which nevertheless not only requires additional computational and memory cost, but also complicates the network architecture and the training procedure. Following some recent scene parsing network <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b63">64]</ref>, we advocate removing the last few downsampling operators of DC-NNs and inserting holes to filters in the subsequent conv layers, called dilated convolution, to enlarge the field-ofview of filters without decreasing spatial resolution or increasing number of parameters. The scene understanding modular consists of three parallel components, i.e., an atrous spatial pyramid pooling (ASPP) module <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, a cross-channel leaner, and a fullimage encoder. ASPP is employed to extract features from multiple large receptive fields via dilated convolutional operations. The dilation rates are 6, 12 and 18, respectively. The pure 1 ? 1 convolutional branch can learn complex cross-channel interactions. The full-image encoder captures global contextual information and can greatly clarify local confusions in depth estimation <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Scene Understanding Modular</head><formula xml:id="formula_0">!" !" !" # (&amp;???)) ? (&amp;???,) - - .//0123 (4?4) COPY CONV ? (&amp;???,) # (&amp;???)) # ; ((&amp;/4)?(?/4)?)) , 1?1?, 456250 7185: 1?1 !" 1?1?,</formula><p>Though previous methods have incorporated full-image encoders, our full-image encoder contains fewer parameters. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, to obtain global feature F with dimension C ? h ? w from F with dimension C ? h ? w, a common fc-fashion method accomplishes this by using fully-connected layers, where each element in F connects to all the image features, implying a global understanding of the entire image. However, this method contains a prohibitively large number of parameters, which is difficult to train and is memory consuming. In contrast, we first make use of an average pooling layer with a small kernel size and stride to reduce the spatial dimensions, followed by a f c layer to obtain a feature vector with dimension C. Then, we treat the feature vector as C channels of feature maps with spatial dimensions of 1 ? 1, and add a conv layer with the kernel size of 1 ? 1 as a cross-channel parametric pooling structure. Finally, we copy the feature vector to F along spatial dimensions so that each location of F share the same understanding of the entire image.</p><p>The obtained features from the aforementioned components are concatenated to achieve a comprehensive understanding of the input image. Also, we add two additional convolutional layers with the kernel size of 1 ? 1, where the former one reduces the feature dimension and learns complex cross-channel interactions, and the later one transforms the features into multi-channel dense ordinal labels. To quantize a depth interval [?, ?] into a set of representative discrete values, a common way is the uniform discretization (UD). However, as the depth value becomes larger, the information for depth estimation is less rich, meaning that the estimation error of larger depth values is generally larger. Hence, using the UD strategy would induce an over-strengthened loss for the large depth values. To this end, we propose to perform the discretization using the SID strategy (as shown in <ref type="figure">Fig. 4</ref>), which uniformed discretizes a given depth interval in log space to down-weight the training losses in regions with large depth values, so that our depth estimation network is capable to more accurately predict relatively small and medium depth and to rationally estimate large depth values. Assuming that a depth interval [?, ?] needs to be discretized into K sub-intervals, UD and SID can be formulated as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spacing-Increasing Discretization</head><formula xml:id="formula_1">UD: t i = ? + (? ? ?) * i/K,</formula><formula xml:id="formula_2">SID: t i = e log(?)+ log(?/?) * i K ,<label>(1)</label></formula><p>where t i ? {t 0 , t 1 , ..., t K } are discretization thresholds. In our paper, we add a shift ? to both ? and ? to obtain ? * and ? * so that ? * = ? + ? = 1.0, and apply SID on [? * , ? * ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning and Inference</head><p>After obtaining the discrete depth values, it is straightforward to turn the standard regression problem into a multiclass classification problem, and adopts softmax regression loss to learn the parameters in our depth estimation network. However, typical multi-class classification losses ignore the ordered information between the discrete labels, while depth values have a strong ordinal correlation since they form a well-ordered set. Thus, we cast the depth estimation problem as an ordinal regression problem and develop an ordinal loss to learn our network parameters.</p><p>Let ? = ?(I, ?) denote the feature maps of size W ? H ?C given an image I, where ? is the parameters involved in the dense feature extractor and the scene understanding modular. Y = ?(?, ?) of size W ? H ? 2K denotes the ordinal outputs for each spatial locations, where ? = (? 0 , ? 1 , ..., ? 2K?1 ) contains weight vectors. And l (w,h) ? {0, 1, ..., K ? 1} is the discrete label produced by SID at spatial location (w, h). Our ordinal loss L(?, ?) is defined as the average of pixelwise ordinal loss ?(h, w, ?, ?) over the entire image domain:</p><formula xml:id="formula_3">L(?, ?) = ? 1 N W ?1 w=0 H?1 h=0 ?(w, h, ?, ?), ?(h, w, ?, ?) = l (w,h)?1 k=0 log(P k (w,h) ) + K?1 k=l (w,h) (log(1 ? P k (w,h) )), P k (w,h) = P (l (w,h) &gt; k|?, ?),<label>(2)</label></formula><p>Image Ground Truth Eigen <ref type="bibr" target="#b10">[11]</ref> LRC [18] DORN <ref type="figure" target="#fig_4">Figure 5</ref>: Depth Prediction on KITTI. Image, ground truth, Eigen <ref type="bibr" target="#b10">[11]</ref>, LRC <ref type="bibr" target="#b17">[18]</ref>, and our DORN. Ground truth has been interpolated for visualization. Pixels with distance &gt; 80m in LRC are masked out.</p><p>where N = W ? H, andl (w,h) is the estimated discrete value decoding from y (w,h) . We choose softmax function to compute P k (w,h) from y (w,h,2k) and y (w,h,2k+1) as follows:</p><formula xml:id="formula_4">P k (w,h) = e y (w,h,2k+1) e y (w,h,2k) + e y (w,h,2k+1) ,<label>(3)</label></formula><p>where y (w,h,i) = ? T i x (w,h) , and x (w,h) 2 . Minimizing L( , ?) ensures that predictions farther from the true label incur a greater penalty than those closer to the true label.</p><p>The minimization of L( , ?) can be done via an iterative optimization algorithm. Taking derivate with respect to ? i , the gradient takes the following form:</p><formula xml:id="formula_5">@L( , ?) @? i = 1 N W 1 X w=0 H 1 X h=0 @ (w, h, , ?) @? i , @ (w, h, , ?) @? 2k+1 = @ (w, h, , ?) @? 2k , @ (w, h, , ?) @? 2k = x (w,h) ?(l (w,h) &gt; k)(P k (w,h) 1) + x (w,h) ?(l (w,h) ? k)P k (w,h) ,<label>(4)</label></formula><p>where k 2 {0, 1, ..., K 1}, and ?(?) is an indicator function such that ?(true) = 1 and ?(false) = 0. We the can optimize our network via backpropagation.</p><p>In the inference phase, after obtaining ordinal labels for each position of image I, the predicted depth valued (w,h) is decoded as:d</p><formula xml:id="formula_6">(w,h) = tl (w,h) + tl (w,h) +1 2 ?, l (w,h) = K 1 X k=0 ?(P k (w,h) &gt;= 0.5).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To demonstrate the effectiveness of our depth estimator, we present a number of experiments examining different aspects of our approach. After introducing the implementation details, we evaluate our methods on three challenging outdoor datasets, i.e. KITTI <ref type="bibr" target="#b16">[17]</ref>, Make3D <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> and NYU Depth v2 <ref type="bibr" target="#b41">[42]</ref>. The evaluation metrics are following previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39]</ref>. Some ablation studies based on KITTI are discussed to give a more detailed analysis of our method. Implementation Details We implement our depth estimation network based on the public deep learning platform Caffe <ref type="bibr" target="#b25">[26]</ref>. The learning strategy applies a polynomial decay with a base learning rate of 0.0001 and the power of 0.9. Momentum and weight decay are set to 0.9 and 0.0005 respectively. The iteration number is set to 300K for KITTI, 50K for Make3D, and 3M for NYU Depth v2, and batch size is set to 3. We find that further increasing the iteration number can only slightly improve the performance. We adopt both VGG-16 <ref type="bibr" target="#b54">[55]</ref> and ResNet-101 <ref type="bibr" target="#b22">[23]</ref> as our feature extractors, and initialize their parameters via the pretrained classification model on ILSVRC <ref type="bibr" target="#b47">[48]</ref>. Since features in first few layers only contain general low-level information, we fixed the parameters of conv1 and conv2 blocks in ResNet after initialization. Also, the batch normalization parameters in ResNet are directly initialized and fixed during training progress. Data augmentation strategies are following <ref type="bibr" target="#b10">[11]</ref>. In the test phase, we split each image to some overlapping windows according the cropping method in the training phase, and obtain the predicted depth values in overlapped regions by averaging the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark Perfomance</head><p>KITTI The KITTI dataset <ref type="bibr" target="#b16">[17]</ref> contains outdoor scenes with images of resolution about 375 ? 1241 captured by  <ref type="bibr" target="#b11">[12]</ref>, LRC <ref type="bibr" target="#b18">[19]</ref>, and our DORN. Ground truth has been interpolated for visualization. Pixels with distance &gt; 80m in LRC are masked out.</p><p>where N = W ? H, andl (w,h) is the estimated discrete value decoding from y (w,h) . We choose softmax function to compute P k (w,h) from y (w,h,2k) and y (w,h,2k+1) as follows:</p><formula xml:id="formula_7">P k (w,h) = e y (w,h,2k+1) e y (w,h,2k) + e y (w,h,2k+1) ,<label>(3)</label></formula><p>where y (w,h,i) = ? T i x (w,h) , and x (w,h) ? ?. Minimizing L(?, ?) ensures that predictions farther from the true label incur a greater penalty than those closer to the true label.</p><p>The minimization of L(?, ?) can be done via an iterative optimization algorithm. Taking derivate with respect to ? i , the gradient takes the following form:</p><formula xml:id="formula_8">?L(?, ?) ?? i = ? 1 N W ?1 w=0 H?1 h=0 ??(w, h, ?, ?) ?? i , ??(w, h, ?, ?) ?? 2k+1 = ? ??(w, h, ?, ?) ?? 2k , ??(w, h, ?, ?) ?? 2k = x (w,h) ?(l (w,h) &gt; k)(P k (w,h) ? 1) + x (w,h) ?(l (w,h) ? k)P k (w,h) ,<label>(4)</label></formula><p>where k ? {0, 1, ..., K?1}, and ?(?) is an indicator function such that ?(true) = 1 and ?(false) = 0. We the can optimize our network via backpropagation. In the inference phase, after obtaining ordinal labels for each position of image I, the predicted depth valued (w,h) is decoded as:d </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To demonstrate the effectiveness of our depth estimator, we present a number of experiments examining different aspects of our approach. After introducing the implementation details, we evaluate our methods on three challenging outdoor datasets, i.e. KITTI <ref type="bibr" target="#b17">[18]</ref>, Make3D <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> and NYU Depth v2 <ref type="bibr" target="#b42">[43]</ref>. The evaluation metrics are following previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref>. Some ablation studies based on KITTI are discussed to give a more detailed analysis of our method. Implementation Details We implement our depth estimation network based on the public deep learning platform Caffe <ref type="bibr" target="#b26">[27]</ref>. The learning strategy applies a polynomial decay with a base learning rate of 0.0001 and the power of 0.9. Momentum and weight decay are set to 0.9 and 0.0005 respectively. The iteration number is set to 300K for KITTI, 50K for Make3D, and 3M for NYU Depth v2, and batch size is set to 3. We find that further increasing the iteration number can only slightly improve the performance. We adopt both VGG-16 <ref type="bibr" target="#b55">[56]</ref> and ResNet-101 <ref type="bibr" target="#b23">[24]</ref> as our feature extractors, and initialize their parameters via the pretrained classification model on ILSVRC <ref type="bibr" target="#b48">[49]</ref>. Since features in first few layers only contain general low-level information, we fixed the parameters of conv1 and conv2 blocks in ResNet after initialization. Also, the batch normalization parameters in ResNet are directly initialized and fixed during training progress. Data augmentation strategies are following <ref type="bibr" target="#b11">[12]</ref>. In the test phase, we split each image to some overlapping windows according the cropping method in the training phase, and obtain the predicted depth values in overlapped regions by averaging the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmark Perfomance</head><p>KITTI The KITTI dataset <ref type="bibr" target="#b17">[18]</ref>    cameras and depth sensors in a driving car. All the 61 scenes from the "city", "residential", "road" and "Campus" categories are used as our training/test sets. We test on 697 images from 29 scenes split by Eigen et al. <ref type="bibr" target="#b11">[12]</ref>, and train on about 23488 images from the remaining 32 scenes. We train our model on a random crop of size 385 ? 513. For some other details, we set the maximal ordinal label for KITTI as 80, and evaluate our results on a pre-defined center cropping following <ref type="bibr" target="#b11">[12]</ref> with the depth ranging from 0m to 80m and 0m to 50m. Note that, a single model is trained on the full depth range, and is tested on data with different depth ranges.</p><p>Make3D The Make3D dataset <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> contains 534 outdoor images, 400 for training, and 134 for testing, with the resolution of 2272 ? 1704, and provides the ground truth depth map with a small resolution of 55 ? 305. We reduce the resolution of all images to 568 ? 426, and train our model on a random crop of size 513?385. Following previous works, we report C1 (depth range from 0m to 80m) and C2 (depth range from 0m to 70m) error on this dataset using three commonly used evaluation metrics <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b39">40]</ref>. For the VGG model, we train our DORN on a depth range of 0m to 80m from scratch (ImageNet model), and evaluate results using the same model for C1 and C2 . However, for ResNet, we learn two separate models for C1 and C2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYU Depth v2</head><p>The NYU Depth v2 <ref type="bibr" target="#b42">[43]</ref>    cameras and depth sensors in a driving car. All the 61 scenes from the "city", "residential", "road" and "Campus" categories are used as our training/test sets. We test on 697 images from 29 scenes split by Eigen et al. <ref type="bibr" target="#b10">[11]</ref>, and train on about 23488 images from the remaining 32 scenes. We train our model on a random crop of size 385 ? 513. For some other details, we set the maximal ordinal label for KITTI as 80, and evaluate our results on a pre-defined center cropping following <ref type="bibr" target="#b10">[11]</ref> with the depth ranging from 0m to 80m and 0m to 50m. Note that, a single model is trained on the full depth range, and is tested on data with different depth ranges.</p><p>Make3D The Make3D dataset <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> contains 534 outdoor images, 400 for training, and 134 for testing, with the resolution of 2272 ? 1704, and provides the ground truth depth map with a small resolution of 55 ? 305. We reduce the resolution of all images to 568 ? 426, and train our model on a random crop of size 513?385. Following previous works, we report C1 (depth range from 0m to 80m) and C2 (depth range from 0m to 70m) error on this dataset using three commonly used evaluation metrics <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref>. For the VGG model, we train our DORN on a depth range of 0m to 80m from scratch (ImageNet model), and evaluate results using the same model for C1 and C2 . However, for ResNet, we learn two separate models for C1 and C2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYU Depth v2</head><p>The NYU Depth v2 <ref type="bibr" target="#b41">[42]</ref> dataset contains 464 indoor video scenes taken with a Microsoft Kinect camera. We train our DORN using all images (about 120K) from the 249 training scenes, and test on the 694-image test set following previous works. To speed up training, all the images are reduced to the resolution of 288 ? 384 from 480 ? 640. And the model are trained on random crops of size 257?353. We report our scores on a pre-defined center cropping by Eigen <ref type="bibr" target="#b10">[11]</ref>. ScanNet The ScanNet <ref type="bibr" target="#b8">[9]</ref> dataset is also a challenging benchmark which contains various indoor scenes. We train our model on the officially provided 24353 training and validation images with a random crop size of 385 ? 513, and evaluate our method on the ScanNet online test server. Performance Tab. 3 and Tab. 4 give the results on two outdoor datasets, i.e., KITTI and Make3D. It can be seen that our DORN improves the accuracy by 5% s 30% in terms of all metrics compared with previous works in all settings. Some qualitative results are shown in <ref type="figure" target="#fig_4">Fig. 5</ref> and <ref type="figure" target="#fig_6">Fig. 6</ref>. In Tab. 5, our DORN outperforms other methods on NYU Depth v2, which is one of the largest indoor benchmarks. The results suggest that our method is applicable to both indoor and outdoor data. We evaluate our method on the online KITTI evaluation server and the online ScanNet evaluation server. As shown in Tab. 2 and 1, our DORN significantly outperforms the officially provided baselines. Performance Tab. 3 and Tab. 4 give the results on two outdoor datasets, i.e., KITTI and Make3D. It can be seen that our DORN improves the accuracy by 5% ? 30% in terms of all metrics compared with previous works in all settings. Some qualitative results are shown in <ref type="figure" target="#fig_4">Fig. 5</ref> and <ref type="figure" target="#fig_6">Fig. 6</ref>. In Tab. 5, our DORN outperforms other methods on NYU Depth v2, which is one of the largest indoor benchmarks. The results suggest that our method is applicable to both indoor and outdoor data. We evaluate our method on the online KITTI evaluation server and the online ScanNet evaluation server. As shown in Tab. 2 and 1, our DORN significantly outperforms the officially provided baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>We conduct various ablation studies to analyze the details of our approach. Results are shown in Tab. <ref type="bibr" target="#b5">6</ref>    <ref type="figure" target="#fig_0">Fig. 1, and Fig. 7</ref>, and discussed in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Depth Discretization</head><p>Depth discretization is critical to performance improvement, because it allows us to apply classification and ordinal regression losses to optimize the network parameters. According to scores in Tab. 6, training by regression on continuous depth seems to converge to a poorer solution than the other two methods, and our ordinal regression network achieves the best performance. There is an obvious gap between approaches where depth is discretized by SID and UD, respectively. Besides, when replacing our ordinal regression loss by an advantage regression loss (i.e. BerHu), our DORN still obtain much higher scores. Thus, we can  conclude that: (i) SID is important and can further improve the performance compared to UD; (i) discretizing depth and training using a multi-class classification loss is better than training using regression losses; (iii) exploring the ordinal correlation among depth drives depth estimation networks to converge to even better solutions.</p><p>Furthermore, we also train the network using RMSE log on discrete depth values obtained by SID, and report the results in Tab. 6. We can see that MSE-SID performs slightly better than MSE, which demonstrates that quantization errors are nearly ignorable in depth estimation. The benefits of discretization through the use of ordinal regression losses far exceeds the cost of depth discretization.   From Tab. 7, a full-image encoder is important to further boost the performance. Our full-image encoder yields a little higher scores than fc type encoders <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30]</ref>, but significantly reduce the number of parameters. For example, we set C to 512 (VGG), C to 512, m to 2048 (Eigen <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref>), and k to 4 in <ref type="figure" target="#fig_3">Fig. 3</ref>. Because of limited computation resources, when implementing the fc-fashion encoder, we downsampled the resolution of F using the stride of 3, and upsampled F to the required resolution. With an input image of size 385 ? 513, h and w will be 49 and 65 respectively in our network. The number of parameters in f c-fashion encoder and our encoder is 1 9 * m * w * h * C + m 2 + 1 9 * w * h * C * m ? 753M , and is C * w 4 * h 4 * C + C * C ? 51M , respectively. From the experimental results and parameter analysis, it can be seen that our full-image encoder performs better while requires less computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">How Many Intervals</head><p>To illustrate the sensitivity to the number of intervals, we discretizing depth into various number of intervals via SID. As shown in <ref type="figure">Fig. 7</ref>, with a range of 40 to 120 intervals, our DORN has a score in [0.908, 0.915] regarding ? &lt; 1.25, and a score in [3.056, 3.125] in terms of RMSE, and is thereby robust to a long range of depth interval numbers. We can also see that neither too few nor too many depth intervals are rational for depth estimation: too few depth intervals cause large quantization error, while too many depth intervals lose the advantage of discretization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have developed an deep ordinal regression network (DORN) for monocular depth estimation MDE from a single image, consisting of a clean CNN architecture and some effective strategies for network optimization. Our method is motivated by two aspects: (i) to obtain high-resolution depth map, previous depth estimation networks require incorporating multi-scale features as well as full-image features in a complex architecture, which complicates network training and largely increases the computational cost; (ii) training a regression network for depth estimation suffers from slow convergence and unsatisfactory local solutions. To this end, we first introduced a simple depth estimation network which takes advantage of dilated convolution technique and a novel full-image encoder to directly obtain a high-resolution depth map. Moreover, an effective depth discretization strategy and an ordinal regression training loss were intergrated to improve the training of our network so as to largely increase the estimation accuracy. The proposed method achieves the state-of-theart performance on the KITTI, ScanNet, Make3D and NYU Depth v2 datasets. In the future, we will investigate new approximations to depth and extend our framework to other dense prediction problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Estimated Depth by DORN. MSE: Training our network via MSE in log space, where ground truths are continuous depth values. DORN: The proposed deep ordinal regression network. Depth values in the black part are not provided by KITTI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Figure 1 :</head><label>11</label><figDesc>Estimated Depth by DORN. MSE: Training our network via MSE in log space, where ground truths are continuous depth values. DORN: The proposed deep ordinal regression network. Depth values in the black part are not provided by KITTI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the network architecture. The network consists of a dense feature extractor, multi-scale feature learner (ASPP), cross channel information learner (the pure 1 ? 1 convolutional branch), a full-image encoder and an ordinal regression optimizer. The Conv components here are all with kernel size of 1 ? 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Full-Image Encoders. Top: the full-image encoder implemented by pure f c layers [12, 11, 2] (? &lt; 1.25: 0.910); Bottom: Our proposed encoder (? &lt; 1.25: 0.915).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Depth Prediction on KITTI. Image, ground truth, Eigen</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>k (w,h) &gt;= 0.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Depth Prediction on Make3D. Image, ground truth, and our DORN. Pixels with distance &gt; 70m are masked out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Depth Prediction on Make3D. Image, ground truth, and our DORN. Pixels with distance &gt; 70m are masked out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4 RMSEFigure 7 :</head><label>47</label><figDesc>Performance Ranging Different Intervals via SID. Left: accuracy on ? &lt; 1.25. Right: evaluation errors on RMSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Discrete Intervals. Illustration of UD (middle) and SID (bottom) to discretize depth interval [?, ?] into five subintervals. See Eq. 1 for details.</figDesc><table><row><cell>F</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>G</cell></row><row><cell>H B</cell><cell>H C</cell><cell>H L</cell><cell>H K</cell><cell>H J</cell><cell>H I</cell></row><row><cell>H B H C</cell><cell>H L</cell><cell>H K</cell><cell>H J</cell><cell></cell><cell>H I</cell></row><row><cell>Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>contains outdoor scenes with images of resolution about 375 ? 1241 captured by Method abs rel. imae irmse log mae log rmse mae rmse scale invar. sq. rel.</figDesc><table><row><cell>Official Baseline DORN</cell><cell>0.25 0.14</cell><cell>0.17 0.10</cell><cell>0.21 0.13</cell><cell>0.24 0.13</cell><cell>0.29 0.17</cell><cell>0.42 0.53 0.22 0.29</cell><cell>0.05 0.02</cell><cell>0.14 0.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Scores on the online ScanNet evaluation server. See https://goo.gl/8keUQN.</figDesc><table><row><cell>Method</cell><cell cols="4">SILog sqErrorRel absErrorRel iRMSE</cell></row><row><cell cols="2">Official Baseline 18.19 DORN 11.77</cell><cell>7.32 2.23</cell><cell>14.24 8.78</cell><cell>18.50 12.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Scores on the online KITTI evaluation server. See https://goo.gl/iXuhiN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>dataset contains 464 indoor video scenes taken with a Microsoft Kinect camera. We train our DORN using all images (about 120K) from the 249 training scenes, and test on the 694-image test set following previous works. To speed up training, all the images are reduced to the resolution of 288 ? 384 from 480 ? 640. And the model are trained on random crops of size 257?353. We report our scores on a pre-defined center cropping by Eigen<ref type="bibr" target="#b11">[12]</ref>. and evaluate our method on the ScanNet online test server.Method   abs rel. imae irmse log mae log rmse mae rmse scale invar. sq. rel.</figDesc><table><row><cell>Official Baseline DORN</cell><cell>0.25 0.14</cell><cell>0.17 0.10</cell><cell>0.21 0.13</cell><cell>0.24 0.13</cell><cell>0.29 0.17</cell><cell>0.42 0.53 0.22 0.29</cell><cell>0.05 0.02</cell><cell>0.14 0.06</cell></row></table><note>ScanNet The ScanNet [10] dataset is also a challenging benchmark which contains various indoor scenes. We train our model on the officially provided 24353 training and validation images with a random crop size of 385 ? 513,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Scores on the online ScanNet evaluation server. See https://goo.gl/8keUQN.</figDesc><table><row><cell>Method</cell><cell cols="4">SILog sqErrorRel absErrorRel iRMSE</cell></row><row><cell cols="2">Official Baseline 18.19 DORN 11.77</cell><cell>7.32 2.23</cell><cell>14.24 8.78</cell><cell>18.50 12.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Scores on the online KITTI evaluation server. See https://goo.gl/iXuhiN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>.25 ? &lt; 1.25 2 ? &lt; 1.25 3 Abs Rel Squa Rel RMSE RMSE log</figDesc><table><row><cell>Method ? &lt; 1Make3D [51] cap 0 -80 m 0.601 Eigen et al. [12] 0 -80 m 0.692 Liu et al. [40] 0 -80 m 0.647 LRC (CS + K) [19] 0 -80 m 0.861 Kuznietsov et al. [33] 0 -80 m 0.862 DORN (VGG) 0 -80 m 0.915 DORN (ResNet) 0 -80 m 0.932 Garg et al. [17] 0 -50 m 0.740 LRC (CS + K) [19] 0 -50 m 0.873 Kuznietsov et al. [33] 0 -50 m 0.875 DORN (VGG) 0 -50 m 0.920 DORN (ResNet) 0 -50 m 0.936</cell><cell>higher is better 0.820 0.899 0.882 0.949 0.960 0.980 0.984 0.904 0.954 0.964 0.982 0.985</cell><cell>0.926 0.967 0.961 0.976 0.986 0.993 0.994 0.962 0.979 0.988 0.994 0.995</cell><cell>0.280 0.190 0.217 0.114 0.113 0.081 0.072 0.169 0.108 0.108 0.079 0.071</cell><cell>lower is better 3.012 8.734 1.515 7.156 1.841 6.986 0.898 4.935 0.741 4.621 0.376 3.056 0.307 2.727 1.080 5.104 0.657 3.729 0.595 3.518 0.324 2.517 0.268 2.271</cell><cell>0.361 0.270 0.289 0.206 0.189 0.132 0.120 0.273 0.194 0.179 0.128 0.116</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>, Tab. 7,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Performance on KITTI. All the methods are evaluated on the test split by Eigen et al.</figDesc><table><row><cell>[12]. LRC (CS + K): LRC pre-train their</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Performance on Make3D.</figDesc><table><row><cell>LRC-Deep3D [59] is adopt-</cell></row><row><cell>ing LRC [19] on Deep3D model [59].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Performance on NYU Depth v2. ?i: ? &lt; 1.25 i . ?:</figDesc><table><row><cell>ResNet based model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Depth Discretization and Ordinal Regression. MSE: mean squared error in log space. MCC: multi-class classification. DORN: proposed ordinal regression. Note that training by MSE for 1M iterations only slightly improve the performance compared with 0.5M (about 0.001 on ? &lt; 1.25). berHu: the reverse Huber loss. ? : ResNet based model.</figDesc><table><row><cell cols="2">4.2.2 Full-image Encoder</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Variant</cell><cell cols="4">? &lt; 1.25 Abs Rel RMSE log Params</cell></row><row><cell>w/o full-image encoder fc-fashion our encoder</cell><cell>0.906 0.910 0.915</cell><cell>0.092 0.085 0.081</cell><cell>0.143 0.137 0.132</cell><cell>0M 753M 51M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>Full-image Encoder. Parameters here is computed by some common settings in Eigen<ref type="bibr" target="#b11">[12]</ref> and our DORN.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was supported by Australian Research Council Projects FL-170100117 and DP-180103424. This work was partially supported by SAP SE and CNRS INS2I-JCJC-INVISANA. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research. This research was partially supported by research grant from Pfizer titled "Developing Statistical Method to Jointly Model Genotype and High Dimensional Imaging Endophenotype." We were also grateful for the computational resources provided by Pittsburgh Super Computing grant number TG-ASC170024.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coupled depth learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth from a single image by harmonizing overcomplete local network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Matterport3D: Learning from RGB-D data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth analogy: Data-driven approach for single image depth estimation using gradient samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5953" to="5966" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pranking with ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Computer Vision: a Modern Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A simple approach to ordinal classification. ECML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Depth estimation using structured light flow -analysis of projected pattern flow on an object&apos;s surface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The kitti dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Highquality depth from uncalibrated small motion clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Direction matters: Depth estimation with a surface normal classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Regression modeling strategies: with applications to linear models, logistic and ordinal regression, and survival analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Harrell</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Support vector learning for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recovering surface layout from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unified depth prediction and intrinsic image decomposition from a single image via joint convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Intrinsic depth: Improving depth transfer with intrinsic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning-based, automatic 2d-to-3d image and video conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3485" to="3496" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dept: depth estimation by parameter transfer for single still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning lightness from human judgement on relative reflectance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Narihira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ordinal regression with multiple output cnn for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Depth estimation and image restoration using defocused stereo pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mudenagudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1521" to="1525" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dense monocular depth estimation in complex dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ranking with large margin principle: Two approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scene intrinsics and depth from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Break ames room illusion: depth from general single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">225</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multiscale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Local metric learning for exemplar-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1265" to="1276" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Indoor scene structure analysis for single image depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning ordinal relationships for mid-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
