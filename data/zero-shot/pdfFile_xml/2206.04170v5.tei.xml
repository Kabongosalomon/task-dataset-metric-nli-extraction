<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CASS: Cross Architectural Self-Supervision for Medical Image Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tandon School of Engineering New York University New York</orgName>
								<address>
									<postCode>11202</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Sizikova</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10011</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Cirrone</surname></persName>
							<email>cirrone@courant.nyu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Center for Data Science New York University and Colton Center for Autoimmunity NYU Grossman School of Medicine New York</orgName>
								<address>
									<postCode>10011</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CASS: Cross Architectural Self-Supervision for Medical Image Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in deep learning and computer vision have reduced many barriers to automated medical image analysis, allowing algorithms to process label-free images and improve performance. However, existing techniques have extreme computational requirements and drop a lot of performance with a reduction in batch size or training epochs. This paper presents Cross Architectural -Self Supervision (CASS), a novel self-supervised learning approach that leverages Transformer and CNN simultaneously. Compared to the existing state of the art self-supervised learning approaches, we empirically show that CASS-trained CNNs and Transformers across four diverse datasets gained an average of 3.8% with 1% labeled data, 5.9% with 10% labeled data, and 10.13% with 100% labeled data while taking 69% less time. We also show that CASS is much more robust to changes in batch size and training epochs. Notably, one of the test datasets comprised histopathology slides of an autoimmune disease, a condition with minimal data that has been underrepresented in medical imaging. The code is open source and is available on GitHub Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, medical image analysis has seen tremendous growth due to the availability of powerful computational modelling tools, such as neural networks, and the advancement of techniques capable of learning from partial annotations. Medical imaging is a field characterized by minimal data availability. First, data labeling typically requires domain-specific knowledge. Therefore, the requirement of large-scale clinical supervision may be cost and time prohibitive. Second, due to patient privacy, disease prevalence, and other limitations, it is often difficult to release imaging datasets for secondary analysis, research, and diagnosis. Third, due to an incomplete understanding of diseases. This could be either because the disease is emerging or because no mechanism is in place to systematically collect data about the prevalence and incidence of the diseases. An example of the latter is autoimmune diseases. Statistically, autoimmune diseases affect 3% of the US population or 9.9 million US citizens. On the other hand, 4% of the US population or 13.6 million US citizens are affected by cancer. Cancer has been widely studied in medical science as well as at the intersection of medical science and artificial intelligence. However, the application of artificial intelligence in medical image analysis for autoimmune diseases has received much less attention. Furthermore, there are still major outstanding research questions for autoimmune diseases regarding the presence of different cell types and their role in inflammation at the tissue level. Their study is critical not only because they affect a large part of society but also because they have been on the rise recently <ref type="bibr" target="#b0">Galeotti and Bayry [2020]</ref>, <ref type="bibr" target="#b1">Lerner et al. [2015]</ref>, <ref type="bibr" target="#b2">Ehrenfeld et al. [2020]</ref>.</p><p>To overcome these limitations, we turn to self-supervised learning, a learning paradigm that allows for learning useful data representations label-free. Models extracting these representations can later be fine-tuned with a small amount of labeled data for each downstream task <ref type="bibr" target="#b3">Sriram et al. [2021]</ref>. As a result, this learning approach avoids the relatively expensive and human-intensive task of data annotation and makes it an effective tool for the image analysis of emerging diseases that often have limited data availability (e.g., dermatomyositis, an autoimmune disease, or COVID-19, the cause of a recent worldwide pandemic). Existing approaches in the field of self-supervised learning rely on Convolutional Neural Networks (CNNs) or Transformers as the feature extraction backbone and learn feature representations by teaching the network to compare the extracted representations. Instead, we propose to combine a CNN and Transformer in a response-based contrastive method. In CASS, the extracted representations of each input image are compared across two branches representing each architecture (see <ref type="figure">Figure 1</ref>). By transferring features sensitive to translation equivariance and locality from CNN to Transformer, CASS learns more predictive data representations in limited data scenarios where a Transformer-only model cannot find them. We studied this qualitatively and quantitatively in Section 5. Our contributions are as follows:</p><p>? We introduce Cross Architectural -Self Supervision (CASS), a hybrid CNN-Transformer approach for learning improved data representations in a self-supervised setting in limited data availability problems in the medical image analysis domain 1 .</p><p>? We propose the use of CASS for analysis of autoimmune diseases such as dermatomyositis and demonstrate an improvement of 2.55% in comparison to the existing state of the art self-supervised approaches and 25% over supervised approaches for this problem.</p><p>? We evaluate CASS on three challenging medical image analysis problems (autoimmune disease cell classification, brain tumor classification, and skin lesion classification) on three public datasets (Dermofit Project <ref type="bibr">Dataset Fisher and Rees [2017]</ref>, brain tumor MRI Dataset <ref type="bibr" target="#b5">Cheng [2017]</ref>, <ref type="bibr">Kang et al. [2021]</ref> and ISIC 2019 <ref type="bibr" target="#b7">Tschandl et al. [2018]</ref>, <ref type="bibr" target="#b8">Gutman et al. [2018]</ref>, <ref type="bibr" target="#b9">Combalia et al. [2019]</ref>) and find that CASS outperforms existing state of the art self-supervised techniques by an average of 3.8% using 1% label fractions, 5.9 % with 10% label fractions and 10.13% with 100% label fractions.</p><p>? Existing methods also suffer a severe drop in performance when trained for a reduced number of epochs or batch size <ref type="bibr">(Caron et al. [2021]</ref>, <ref type="bibr" target="#b11">Grill et al. [2020a]</ref>, <ref type="bibr" target="#b12">Chen et al. [2020a]</ref>). We show that CASS is robust to these changes in Section 5.6.</p><p>? New state of the art self-supervised techniques often require significant computational requirements. This is a major hurdle as these methods can take around 20 GPU days to train <ref type="bibr" target="#b13">Azizi et al. [2021a]</ref>. This makes them inaccessible in limited computational resource settings and increase triage in medical image analysis. CASS, on average, takes 69% less time as opposed to the existing state of the art methods. We further expand on this result in Section 5.1 and further analyze this in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background 2.1 Neural Network Architectures for Image Analysis</head><p>CNNs are a popular architecture of choice for many image analysis applications <ref type="bibr" target="#b14">Khan et al. [2020]</ref>. CNNs learn more abstract visual concepts with a gradually increasing receptive field. They have two favorable inductive biases: (i) translation equivariance resulting in the ability to learn equally well with shifted object positions, and (ii) locality resulting in the ability to capture pixel-level closeness in the input data. CNNs have been used for many medical image analysis applications, such as disease diagnosis Yadav and Jadhav <ref type="bibr">[2019]</ref> or semantic segmentation <ref type="bibr" target="#b16">Ronneberger et al. [2015]</ref>. To address the requirement of additional context for a more holistic image understanding, the Vision Transformer (ViT) architecture <ref type="bibr" target="#b17">Dosovitskiy et al. [2020]</ref> has been adapted to images from language-related tasks and recently gained popularity <ref type="bibr" target="#b18">Liu et al. [2021a</ref><ref type="bibr" target="#b19">Liu et al. [ , 2022a</ref>, . In a ViT, the input image is split into patches, that are treated as tokens in a self-attention mechanism. In comparison to CNNs, ViTs can capture additional image context, but lack ingrained inductive biases of translation and location. As a result, ViTs typically outperform CNNs on larger datasets d'Ascoli et al. <ref type="bibr">[2021]</ref>.</p><p>ConViT d'Ascoli et al. <ref type="bibr">[2021]</ref> combines CNNs and ViTs using gated positional self-attention (GPSA) to create a soft-convolution similar to inductive bias and improve upon the capabilities of Transformers alone. More recently, the training regimes and inferences from ViTs have been used to design a new family of convolutional architectures -ConvNext <ref type="bibr" target="#b22">Liu et al. [2022b]</ref>, outperforming benchmarks set by ViTs in classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Supervised Learning for Medical Imaging</head><p>Self-supervised learning allows for the learning of useful data representations without data labels <ref type="bibr" target="#b23">Grill et al. [2020b]</ref>, and is particularly attractive for medical image analysis applications where data labels are difficult to find <ref type="bibr" target="#b13">Azizi et al. [2021a]</ref>. Recent developments have made it possible for self-supervised methods to match and improve upon existing supervised learning methods <ref type="bibr" target="#b24">Hendrycks et al. [2019]</ref>.</p><p>However, existing self-supervised techniques typically require large batch sizes and datasets. When these conditions are not met, a marked reduction in performance is demonstrated <ref type="bibr">Caron et al. [2021]</ref>, <ref type="bibr" target="#b12">Chen et al. [2020a]</ref>, <ref type="bibr" target="#b25">Caron et al. [2020]</ref>, <ref type="bibr" target="#b11">Grill et al. [2020a]</ref>. Self-supervised learning approaches have been shown to be useful in big data medical applications <ref type="bibr" target="#b26">Ghesu et al. [2022]</ref>, <ref type="bibr" target="#b27">Azizi et al. [2021b]</ref>, such as analysis of dermatology and radiology imaging. In more limited data scenarios (3,662 images -25,333 images), <ref type="bibr" target="#b28">Matsoukas et al. [2021]</ref> reported that ViTs outperform their CNN counterparts when self-supervised pre-training is followed by supervised fine-tuning. Transfer learning favors ViTs when applying standard training protocols and settings. Their study included running the <ref type="bibr">DINO Caron et al. [2021]</ref> self-supervised method over 300 epochs with a batch size of 256. However, questions remain about the accuracy and the efficiency of using existing self-supervised techniques when using them on datasets whose entire size is smaller than their peak performance batch size. Also, viewing this from the general practitioner's perspective with limited computational power raises the question of how we can make practical self-supervised approaches more accessible? Adoption and faster development of self-supervised paradigms will only be possible when they become easy to plug-and-play with limited computational power.</p><p>In this work, we explore these questions by designing CASS, a novel approach developed with the core values of efficiency and effectiveness. In simple terms, we are combining CNN and Transformer in a response-based contrastive method by reducing similarity to combine the abilities of CNNs and Transformers. This approach was originally designed for a 198 image dataset for muscle biopsies of inflammatory lesions from patients who have dermatomyositis -an autoimmune disease. The benefits of this approach are illustrated by challenges in the diagnosis of autoimmune diseases due to their rarity, limited data availability, and heterogeneous features. As a consequence, misdiagnoses are common, and the resulting diagnostic delay plays a major factor in their high mortality rate. Autoimmune diseases also share commonalities with COVID-19 in terms of clinical manifestations, immune responses and pathogenic mechanisms. Moreover, some patients have developed autoimmune diseases after COVID-19 infection <ref type="bibr" target="#b29">Liu et al. [2020]</ref>. Despite this increasing prevalence, the representation of autoimmune diseases in medical imaging and deep learning is limited. Furthermore, developing effective and efficient techniques such as CASS will aid in their widespread adoption, further expanding the work in multiple domains and resulting in a multi-fold improvement in quality of life.</p><p>Recent self-supervised methods define the inputs as two augmentations of one image and maximize the similarity between the two representations, by passing them through a pair of feature extractors. These feature extractors are similar in structure and only differ in their weights/parameters. Methods like Momentum Contrast (MoCo) <ref type="bibr" target="#b30">He et al. [2020]</ref> and SiMCLR <ref type="bibr" target="#b31">Chen et al. [2020b]</ref> maintain negative samples in a memory queue. The core idea in such scenarios is to bring the positive pairs together while repulsing the negative sample pairs. Recently, Bootstrap Your Own Latent (BYOL) <ref type="bibr" target="#b11">Grill et al. [2020a]</ref> and <ref type="bibr">DINO Caron et al. [2021]</ref> have improved upon this approach by eliminating the memory banks. The premise of using negative pairs is to avoid collapse. Several strategies have been developed with BYOL using a momentum encoder, Simple Siamese (SimSiam) Chen and He [2021] a stop gradient, and DINO applying the counterbalancing effects of sharpening and centering to avoid collapse. DINO is the first self-supervised training approach extended for Transformers. As described on the right side of <ref type="figure">Figure 1</ref>, DINO augments an image to produce two versions of the image; these are then passed through the student and teacher networks, which are essentially the same encoder with different parameters. Their similarity is then measured with a cross-entropy loss. A stop-gradient (sg) operator is applied to the teacher network to propagate gradients only through the student network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We start by motivating our method before explaining in detail (in Section 3.1). Self-supervised methods until now have been using different augmentations of the same image to create positive pairs. These were then passed through same architectures but with different set of parameters <ref type="bibr" target="#b11">Grill et al. [2020a]</ref>.</p><p>In <ref type="bibr">Caron et al. [2021]</ref> they introduced image cropping of different sizes to add local and global information. They also used different operators and techniques to avoid collapse as described in Section 2.2. <ref type="bibr" target="#b33">Raghu et al. [2021]</ref> in their study suggested that for the same input, Transformers and CNNs extract different representations. They conducted their study by analyzing the CKA (Centered Kernel Alignment) for CNNs and Transformer using ResNet <ref type="bibr" target="#b34">He et al. [2016]</ref> and ViT (Vision Transformer) <ref type="bibr" target="#b17">Dosovitskiy et al. [2020]</ref> family of encoders respectively. They found that Transformers have a more uniform representation across all layers as compared to CNNs. They also have self-attention, enabling global information aggregation from shallow layers and skip connections that connect lower layers to higher layers, promising information transfer. Hence, lower and higher layers in Transformers show much more similarity than in CNNs. The receptive field of lower layers for Transformers is more extensive than in CNNs. While this receptive field gradually grows for CNNs, it becomes global for Transformers around the midway point. Transformers don't attend locally in their earlier layers, while CNNs do. Using local information earlier is important for strong performance. CNNs have a more centered receptive field as opposed to a more globally spread receptive field of Transformers. Hence, representations drawn from the same input, will be different for Transformers and CNNs. Until now self-supervised techniques have used only one kind of architecture either a CNN or Transformer. But differences in the representations learned with CNN and Transformers, inspired us to create positive pairs by different architectures or feature extractors rather than using different set of augmentations. This by design avoids collapse as the two architectures will never give the same representation as output. By contrasting their extracted features at the end we hope to help the Transformer learn representations from CNN and vice versa. This should help both the architectures to learn better representations. We verify this, by studying attention maps and feature maps from supervised and CASS trained CNN and Transformers in Appendix D. We observed that CASS trained CNN and Transformer were able to retain a lot more detail about the input image which pure CNN and Transformers lacked. Furthermore, cross-architecture knowledge distilled models have shown encouraging improvements over supervised-knowledge distillation <ref type="bibr" target="#b35">Gong et al. [2022]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Description of CASS</head><p>CASS' goal is to extract and learn representations in a self-supervised way. To achieve this, an image is passed through a common set of augmentations. The augmented image is then simultaneously passed though a CNN and Transformer to create positive pairs. The output logits from the CNN and Transformer are then used to find cosine similarity loss (equation 1). Since, the two architectures give different output representations as mentioned in <ref type="bibr" target="#b33">Raghu et al. [2021]</ref>, the model doesn't collapse.</p><p>We also report results for CASS using different set of CNNs and Transformers in Appendix B and C, and not a single case of model collapse was registered.</p><formula xml:id="formula_0">loss = 2 ? 2 ? F(R) ? F(T) (1) where, F(x) = N i=1 x (max ( x 2 ) , )</formula><p>We use same parameters for optimizer and learning schedule for both the architectures. We also use stochastic weigh averaging (SWA) <ref type="bibr" target="#b36">Izmailov et al. [2018]</ref> with Adam optimizer and a learning rate of 1e-3. For learning rate we use cosine schedule with a maximum of 16 iterations and a minimum value of 1e-6. ResNets are typically trained with Stochastic Gradient Descent (SGD) and our use of Adam optimizer is quite unconventional. Furthermore, unlike existing self-supervised techniques there is no parameter sharing between the two architectures.</p><p>In <ref type="figure">Figure 1</ref>, we show CASS on top and <ref type="bibr">DINO Caron et al. [2021]</ref> at the bottom. Comparing the two, CASS does not use any extra mathematical treatment used in DINO to avoid collapse such as centring and applying softmax function on the output of its student and teacher networks. After training CASS and DINO for one cycle, DINO yields only one kind of trained architecture, while CASS provides two trained architectures (1 -CNN and 1 -Transformer). CASS trained architectures provide better performance than DINO trained architectures in most cases as further elaborated in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: (Top)</head><p>In our proposed self-supervised architecture -CASS, R represents ResNet-50, a CNN and T in the other box represents the Transformer used (ViT); X is the input image, which becomes X' after applying augmentations. Note that CASS applies only one set of augmentations to create X'. X' is then passed through both the arms to compute loss as mentioned in Equation 1. This is different from DINO, which passes different augmentation of the same image through networks with the same architecture but different parameters. The output of the teacher network is centred on a mean computed over a batch. Another key difference is that in CASS, loss is computed over logits meanwhile in DINO it is computed over softmax output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Self-supervised learning</head><p>We studied and compared results between DINO and CASS trained self-supervised CNNs and Transformers. For the same, we trained from ImageNet initialization for 100 epochs with a batch size of 16. We ran these experiments on an internal cluster with single GPU unit (NVIDIA RTX8000) with 48 GB video RAM, 2 CPU cores and 64 GB system RAM.</p><p>For DINO, we used the hyper parameters and augmentations mentioned in the original implementation. For CASS, we describe the experimentation details in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">End-to-end fine-tuning</head><p>In order to evaluate the utility of the learned representations, we use the self-supervised pre-trained weights for downstream classification task. While performing the downstream fine tuning we perform entire model (E2E fine-tuning). The test set metrics were used as proxies for representation quality. We trained the entire model for a maximum of 50 epochs with an early stopping patience of 5 epochs. For supervised fine tuning we used Adam optimizer with a cosine annealing learning rate starting at 3e-04. Since almost all medical datasets have some class imbalance we applied class distribution normalized Focal Loss <ref type="bibr" target="#b37">Lin et al. [2017]</ref> to navigate class imbalance. We fine tune the models with different label fractions during training i.e 1%, 10% and 100% label fractions. For example, if a model is trained with 10% label fraction then that model will have access only to 10% of the training dataset labels, which was used during self-supervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Datasets</head><p>We split the datasets into three splits -training, validation and testing following the 70/10/20 split strategy unless specified otherwise. We futher expand upon our thought process for choosing datasets in Appendix E.</p><p>? Autoimmune diseases biopsy slides (Van Buren et al. <ref type="bibr">[2022]</ref>) consists of slides cut from muscle biopsies of dermatomyositis patients stained with different proteins and imaged to generate a dataset of 198 TIFF image set from 7 patients. The presence or absence of these cells helps to diagnose dermatomyositis. Multiple cell classes can be present per image; therefore this is a multi-label classification problem. Our task here was to classify cells based on their protein staining into TFH-1, TFH-217, TFH-Like, B cells, and others. We used F1 score as our metric for evaluation, as employed in the original work by Van Buren et al. <ref type="bibr">[2022]</ref>. These RGB images have a consistent size of 352 by 469.</p><p>? Dermofit dataset <ref type="bibr" target="#b4">Fisher and Rees [2017]</ref> contains normal RGB images captured through SLR camera indoors with ring lightning. There are 1300 image samples, classified into 10 classes: Actinic Keratosis (AK), Basal Cell Carcinoma (BCC), Melanocytic Nevus / Mole (ML), Squamous Cell Carcinoma (SCC), Seborrhoeic Keratosis (SK), Intraepithelial carcinoma (IEC), Pyogenic Granuloma (PYO), Haemangioma (VASC), Dermatofibroma (DF) and Melanoma (MEL). This dataset comprises of images of different sizes and no two images are of same size. They range from 205?205 to 1020?1020 in size. We pretext task is multi-class classification and we use F1 score as our evaluation metric on this dataset.</p><p>? Brain tumor MRI dataset <ref type="bibr" target="#b5">Cheng [2017]</ref>, <ref type="bibr">Amin et al. [2022]</ref> 7022 images of human brain MRI that are classified into four classes: glioma, meningioma, no tumor, and pituitary. We used the dataset from https://www.kaggle.com/datasets/masoudnickparvar/ brain-tumor-mri-dataset that combines Br35H: Brain tumor Detection 2020 dataset used in "Retrieval of Brain tumors by Adaptive Spatial Pooling and Fisher Vector Representation" and Brain tumor classification curated by Navoneel Chakrabarty and Swati Kanchan. Out of these, the dataset curator created the training and testing splits. We followed their splits, 5,712 images for training and 1,310 for testing. Since this was a combination of multiple datasets, size of images vary throughout the dataset from 512?512 to 219?234. The pretext of the task is multi-class classification, and we used the F1 score as the metric.</p><p>? ISIC 2019 <ref type="bibr" target="#b7">(Tschandl et al. [2018]</ref>, <ref type="bibr" target="#b8">Gutman et al. [2018]</ref>, <ref type="bibr" target="#b9">Combalia et al. [2019]</ref>) consists of 25,331 images across eight different categories -melanoma (MEL), melanocytic nevus (NV), Basal cell carcinoma (BCC), actinic keratosis(AK), benign keratosis(BKL) , dermatofibroma(DF), vascular lesion (VASC) and Squamous cell carcinoma(SCC). This dataset contains images of size 600 ? 450 and 1024 ? 1024. The distribution of these labels is unbalanced across different classes. For evaluation, we followed the metric followed in the official competition i.e balanced multi-class accuracy value, which is semantically equal to recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Compute and Time analysis Analysis</head><p>We ran all the experiments on a single NVIDIA RTX8000 GPU with 48GB video memory. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Autoimmune Diseases Biopsy Slides Dataset</head><p>We did not perform 1% training for the autoimmune diseases biopsy slides of 198 images because using 1% images would be too small number to learn anything meaningful and the results would be highly randomized.</p><p>Following the self-supervised training and fine tuning procedure as described in Section 4.1 and 4.2, we observed that using CASS with the ViT B/16 backbone and ResNet50 improved upon existing result of 0.63 Van Buren et al.</p><p>[2022] to 0.8894. All kinds of Transformers consistently outperformed CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Techniques Backbone</head><p>Testing <ref type="formula">F1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dermofit Dataset</head><p>We did not perform 1% training for this dataset as the training set was too small to draw meaningful results with just 10 samples. We observe that CASS outperforms both supervised and existing state of the art self-supervised methods for all label fractions. We present the F1 score for different label fractions in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Techniques</head><p>Testing <ref type="formula">F1</ref>  We observed that CASS outperformed the existing state-of-art self-supervised method using for all label fractions and for both the architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Brain tumor MRI dataset</head><p>We observed that supervised CNNs performed better than Transformers on this dataset. Similarly, for DINO, CNNs performed better than Transformers by a margin. We observed that this trend is followed by CASS as well. The difference between CNN and Transformer performance is smaller for CASS as compared to the difference in performance for CNN and Transformer with supervised and DINO training. We report these results in <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">ISIC 2019 Dataset</head><p>The ISIC-2019 dataset is an incredibly challenging dataset, not only because of the class imbalance issue but because it is made of partially processed and inconsistent images with hard-to-classify classes. From <ref type="table">Table 5</ref> it is clear that CASS outperforms DINO for all label fractions for both CNN and Transformer by a margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Change in batch size</head><p>To gauge the change in performance with the change in batch size, we ran experiments with CASS with a batch size of 8, 16, and 32. We present these results in Appendix B.1 and C.1. Based on them we concluded that CASS is robust to change in batch size. Interestingly, instead of dropping performance like existing methods, CASS-trained Transformers with smaller batch sizes performed better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Training Epochs</head><p>We report the performance of CASS trained for 50, 100, 200 and 300 epochs in Appendix B.2 and C.2. On reducing the epochs to 50, a performance drop of 2% was observed for CNN, while the performance of Transformer remained almost constant. Similarly, there is a 2% gain when we double the number of self-supervised training epochs. However, after that, the gain is minimal on changing epochs from 200 to 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.3">Effect of augmentation change</head><p>CASS does not use hard augmentations like DINO or BYOL. We study the effect of adding BYOL/DINO-like augmentations in Appendix B.3. Although, Gaussian blur helps in converging the CNN and Transformer for CASS. We find that adding BYOL-like hard augmentations costs performance. CASS has global-local cropping inbuilt due to difference in the receptive field of CNN and Transformer, unlike DINO, where it was added with augmentation.   <ref type="table">Table 5</ref>: Results for the ISIC-2019 dataset. Comparable to the official metrics used in the challenge https://challenge.isic-archive.com/landing/2019/. We use balanced multi-class accuracy as our metric, which is semantically equal to recall value. We observed that CASS consistently outperforms DINO by approximately 4% for all label fractions with CNN and Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.4">Change in architecture</head><p>We provide intuition for changing the architecture of CNNs and Transformers in Appendix B.5 and C.3. As a baseline we started with ResNet-50 and ViT Base/16 Transformer for our experiments. But we also expand these results to other CNN and Transformer families. Furthermore, we also report results of using two CNNs or two Transformers on the brain MRI classification dataset in Appendix C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.5">Optimization</head><p>For CASS, we used Adam optimizer for both CNN and Transformer. Traditionally, CNNs and more specifically ResNets have been used with a SGD optimizer, but in our case we use Adam optimiser. This choice is fairly unconventional and we further expand upon this in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.6">Studying the attention and feature maps</head><p>Our motivation to combine CNN and Transformer was to help the two architectures learn meaningful representations from each other. As mentioned already in Section 3, the two architectures focus on different parts of the image and hence create positive pairs without differential augmentation. In this section we study the feature maps and attention maps of CNN and Transformer respectively to see this gain qualitatively. Quantitative gains have been summarized in Section 5. We present the attention and feature maps for a given input image 2 in this section and expand the study of feature and attention maps in Appendix D.</p><p>? Feature maps. We studied the feature maps after Conv2d layer of ResNet-50. In this section, we focus on the features extracted after the first layer of ResNet-50 trained with CASS and supervised technique in Appendix D.2. We observed that CASS trained Resnet-50 was able to retain a lot more detail/information about the input as compared to the supervised ResNet-50. We present the extracted features in <ref type="figure" target="#fig_2">Figure 4</ref>. Furthermore, we expand this  Transformer (on the right). We pass the same image as input through both of them; the image used is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. We observed that the CASS-trained Transformer's attention is more spread as compared to supervised Transformer. This can be easily inferred from the right-hand side bottom portion of both the attention maps. study to include feature maps from the first five layers of CASS and supervised ResNet-50 in Appendix D.</p><p>? Class attention maps. Similar to feature maps of CNNs, for Transformers we study their class attention maps. We extracted the attention maps after the first attention block. For the sample image in 2, we present the attnetion maps resized to the image size in 3. We observed that CASS trained Transformer is able to pay more attention to the intricate details in the input image as compared to the supervised Transformer. Furthermore, It is able to pay attention to areas that are missed by a supervised Transformer. We also expand this study to study attention maps averaged over 30 samples on different datasets in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Although CASS' performance for larger and non-biological data can be hypothesized based on inferences, a complete study on large-sized natural datasets hasn't been conducted. In this study, we focused extensively on studying the effects and performance of our proposed method for small dataset sizes and limited computational resources. Furthermore, all the datasets used in our experimentation are restricted to academic and research use only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Potential negative societal impact</head><p>The autoimmune dataset is limited to a geographic institution. Hence the study is specific to a disease variant. Inferences drawn may or may not hold true for other variants. Also, the results produced are dependent on a set of markers. Medical practitioners often require multiple tests before finalising diagnosis; medical history and existing health conditions also play an essential role. We haven't incorporated the aforementioned meta-data in CASS. Finally, application on a wider scale -real life scenarios should only be trusted after taking clearance form the concerned health and safety governing bodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Based on our experimentation on four diverse medical imaging datasets, we qualitatively and empirically conclude that CASS gained an average of 3.8% with 1% labeled data, 5.9% with 10% labeled data, and 10.13% with 100% labeled data and trained in 69% less time than the existing state of the art self-supervised method. Furthermore, we saw that CASS is robust to batch size changes and training epochs reduction. To conclude, for medical image analysis, CASS is computationally efficient, performs better, and overcomes some of the shortcomings of existing self-supervised techniques. This ease of accessibility and better performance will catalyze medical imaging research to help us improve healthcare solutions and develop new solutions for underrepresented and emerging diseases.</p><p>Algorithm 1: Herein we describe CASS self-supervised training algorithm Input: Unlabeled same augmented images from the training set x Output: Logits from each network. Data: Images from a given dataset 1 for x in train loader: do 2 R = cnn(x ) // taking logits output from CNN 3 T = vit(x ) // taking logits output from ViT</p><formula xml:id="formula_1">4 loss = 2 ? 2 * N i=1 R (man( R 2), ) ? N i=1</formula><p>T man( T 2 , )) // taking cosine similarity between the logits outputs from CNN and ViT 5 Calculate the mean value of all elements of the loss tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>Compute gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Batch size</head><p>As mentioned, with CASS, we aim to overcome the shortcomings of existing self-supervised methods where they drop much performance with a reduction in batch size. To study this effect, we ran CASS with three different batch sizes on the autoimmune dataset -8, 16, and 32 and reported the results in 6. We observed that the performance of CASS-trained CNN improved with a reduction in batch size while that of the Transformer remained almost constant. We followed the standard set of protocols mentioned in Appendix E for training. As standard, we used 16 as our batch size. We also conducted this experiment on the brain MRI classification dataset and reported the results in Appendix C.1.</p><p>Batch Size CNN F1 Score Transformer F1 Score 8 0.88285?0011 0.8844?0.0009 16 0.8650?0.0001 0.8894?0.005 32 0.8648?0.0005 0.889?0.0064 <ref type="table">Table 6</ref>: F1 metric comparison between the two arms of CASS trained over 100 epochs, following the protocols and procedure listed in Appendix E. We only change the batch size during self-supervised training. Based on these we observed that while CASS trained Transformer gains 1% with reduction in batch size from 16 to 8, CASS trained CNN gains almost 2% for the same change. Although there is a diminishing gain in performance as we increase the batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Change in training epochs</head><p>As standard, we trained CASS for 100 epochs in all cases. However, existing self-supervised techniques are plagued with a loss in performance with a decrease in the number of training epochs. To test this effect for CASS, we ran it for 50, 100, 200, and 300 epochs on the autoimmune dataset and reported the results in <ref type="table" target="#tab_12">Table 7</ref>. We observed a slight gain in performance when we increased the epochs from 100 to 200 but minimal gain beyond that. We also ran the same experiment on the brain MRI dataset and reported the results in Appendix C.2.</p><p>Epochs CNN F1 Score Transformer F1 Score 50 0.8521?0.0007 0.8765? 0.0021 100 0.8650?0.0001 0.8894?0.005 200 0.8766?0.001 0.9053?0.008 300 0.8777?0.004 0.9091?8.2e-5 <ref type="table" target="#tab_12">Table 7</ref>: Performance comparison over varied number of epochs, from 50 to 300 epochs, the downstream training procedure and the CNN-Transformer combination is kept constant across all the four experiments, only the number of self-supervised epochs have been changed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Augmentations</head><p>Contrastive learning techniques are known to be highly dependent on augmentations. Recently, most self-supervised techniques have adopted BYOL <ref type="bibr" target="#b11">Grill et al. [2020a]</ref>-like a set of augmentations. <ref type="bibr">DINO Caron et al. [2021]</ref> uses the same set of augmentations as BYOL, along with adding local-global cropping. We use a reduced set of BYOL augmentations for CASS, along with a few changes. For instance, we do not use solarize and Gaussian blur. Instead, we use affine transformations and random perspectives. In this section, we study the effect of adding BYOL-like augmentations to CASS. We report these results in <ref type="table">Table 8</ref>. We observed CASS trained CNN is robust to changes in augmentations. On the other hand, the Transformer drops performance with change in augmentations. A possible solution to regain this loss in performance for Transformer with change in augmentation is by using Gaussian blur, which has a converging effect on the results of CNN and Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmentation Set</head><p>CNN F1 Score Transformer F1 Score CASS only 0.8650?0.0001 0.8894?0.005 CASS + Solarize 0.8551?0.0004 0.81455?0.002 CASS + Gaussian blur 0.864?4.2e-05 0.8604?0.0029 CASS + Gaussian blur + Solarize 0.8573?2.59e-05 0.8513?0.0066 <ref type="table">Table 8</ref>: We report the F1 metric of CASS trained with a different set of augmentations for 100 epochs. While CASS-trained CNN fluctuates within a percent of its peak performance, CASS-trained Transformer drops performance with the addition of solarization and Gaussian blur. Interestingly, the two arms converged with the use of Gaussian blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Optimization</head><p>In CASS we use Adam optimizer for both CNN and Transformer. This is a shift from the traditional use of SGD or stochastic gradient descent for CNNs. In this <ref type="table">Table 9</ref> we report the performance of CASS trained CNN and Transformer with the CNN using SGD and Adam optimizer. We observed that while the performance of CNN remained almost constant, the performance of Transformer dropped by almost 6% with CNN using SGD.</p><p>Optimiser for CNN CNN F1 Score Transformer F1 Score Adam 0.8650?0.0001 0.8894?0.005 SGD 0.8648?0.0005 0.82355?0.0064 <ref type="table">Table 9</ref>: We report the F1 metric of CASS trained with a different set of optimizers for the CNN arm for 100 epochs. While there is no change in CNN's performance, the Transformer's performance drops around 6% with SGD.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Results</head><p>We conducted some further experimentation to check for collapse and ablation studies. The model did not report collapse in any case. The following results are calculated following the protocols in Appendix E on the brain MRI classification dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Changing Batch Size</head><p>This section presents the results of varying the batch size in the brain MRI classification dataset. In the standard implementation of CASS, we used a batch size of 16; here, we showed results for batch sizes 8 and 32. The largest batch size we could run was 34 on a single GPU of 48 GB video memory. Hence 32 was the biggest batch size we showed in our results. We present these results in <ref type="table" target="#tab_4">Table 14</ref>. However, the performance of CNN remained nearly constant with a change of less than a percentage. The performance of the Transformer drops as we increase the batch size. Since CASS was developed with the requirements of running on small datasets, with an overall size smaller than the batch size of the current state of the art techniques, its peak performance for small batch size justifies its development. Furthermore, from <ref type="table">Table 6</ref> and Section 5.2, we observed that Transformer is the better performing architecture out of the two and that with batch size change, it was unaffected. Similarly, from <ref type="table" target="#tab_4">Table 14</ref> and Section 5.4, we observed that CNN is the better performing architecture, and with batch size change, its performance is unaffected. Hence, we conclude that batch size change does not affect the leading architecture on a given dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Size CNN F1 Score</head><p>Transformer F1 Score 8 0.9895?0.0025 0.93158?0.0109 16 0.9909? 0.0032 0.9279? 0.0213 32 0.9848?0.011 0.9176?0.006 <ref type="table" target="#tab_4">Table 14</ref>: This table represents the results for different batch sizes on the brain MRI classification dataset. We maintain the downstream batch size constant for all the three self-supervised batch sizes, following the standard experimental setup as mentioned in Appendix E. These results are for 100% label fraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Effect of the Number of Training Epochs</head><p>We saw that there was an incremental gain in performance as we increased the number of selfsupervised training epochs. For the opposite scenario, there wasn't a steep drop in performance like the existing self-supervised techniques when we reduce the number of self-supervised epochs. For this experiment we use ResNet family of CNNs along with ViT base/16 as our Transformer. We use ImageNet initialization for ResNet 18 and 50, while random initialization for ResNet-200. We present these results in <ref type="table" target="#tab_9">Table 16</ref>. We observed that increase in performance of ResNet correlates to increase in performance of Transformer, hence implying that there is information transfer between the two.   <ref type="table" target="#tab_12">Table 17</ref>: For the same number of Transformer parameters, DEiT-base with ResNet-50 performed much better than ResNet-50 with ViT-base. The difference in their CNN arm is 0.10%. On ImageNet DEiT-base has a top1% accuracy of 83.106 while ViT-base has an accuracy of 86.006. We use both the Transformers with 16 patches. [ResNet-50 has an accuracy of 80.374]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.3 Using CNN in both arms</head><p>Until now we have experimented by using a CNN and a Transformer in CASS. In this section we present results for using two CNNs in CASS. We pair ResNet-50 with DenseNet-161. We observe that both the CNNs fail to reach the benchmark set by ResNet-50 and ViT-B/16 combination. Although training the ResNet-50-DenseNet-161 pair takes 5 hour 24 minutes which is less than the 7 hours 11 minutes taken by the ResNet-50-ViT-B/16 combination to be trained with CASS. We compare these results in <ref type="table">Table 18</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN</head><p>Architecture in arm 2 F1 Score of ResNet-50 arm F1 Score of arm 2</p><p>ResNet-50 ViT Base/16 0.9909?0.0032 0.9279? 0.0213 DenseNet-161 0.9743?8.8e-5 0.98365?9.63e-5 <ref type="table">Table 18</ref>: We observed that for the ResNet-50-DenseNet-161 pair, we train 2 CNNs instead of 1 in our standard setup of CASS. Furthermore, none of these CNNs could match the performance of ResNet-50 trained with the ResNet-50-ViT base/16 combination. Hence, by adding a Transformer-CNN combination, we transfer information between the two architectures that would have been missed otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.4 Using Transformer in both arms</head><p>Similar, to the above section, for this section we use a Transformer-Transformer combination instead of a CNN-Transformer combination. For this we use Swin-Transformer patch-4/window-12 <ref type="bibr" target="#b40">Liu et al. [2021b]</ref> alongside ViT-B/16 Transformer. We observe that the performance for ViT/B-16 improves by around 1.3% when we use Swin Transformer. Although this comes at computational cost. Swin-ViT combination took 10 hours to train as opposed to 7 hours 11 minutes taken by the ResNet-50-ViT-B/16 combination to be trained with CASS. Even with the increase in time taken to train the Swin-ViT combination, it is still almost 50% less than DINO. We present these results in <ref type="table" target="#tab_11">Table 19</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Effect of Initialization</head><p>We use ImageNet initialized CNN and Transformers for CASS, DINO and supervised training. We use Timm's library for these initialization Wightman <ref type="bibr">[2019]</ref>. ImageNet initialization is preferred for transfer learning in medical image analysis not because of feature reuse but becuase ImageNet weights allow for faster convergence through better weight scaling <ref type="bibr" target="#b41">Raghu et al. [2019]</ref>. But sometimes pretrained weights might be hard to find, so we study CASS' performance with random and ImageNet initialization in this section. We observed that performance almost remained the same with minor gains when the initialization was altered for the two networks. <ref type="table">Table 20</ref> presents the results for this experimentation.</p><p>Initialisation CNN F1 Score Transformer F1 Score Random 0.9907?0.009 0.9316?0.027 Imagenet 0.9909?0.0032 0.9279? 0.0213 <ref type="table">Table 20</ref>: We observe that the Transformer gains some performance with random initialization, although performance has more variance when used with random initialization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Result Analysis E.1 Time complexity analysis</head><p>In section 5.1, we observed that CASS takes 69% less time as compared to DINO. This reduction in time could be attributed to the follwoing reasons:</p><p>1. In DINO, augmentations are applied twice as opposed to just once in CASS. Furthermore, we per application CASS uses less number of augmentations as compared to DINO. 2. Since the architectures, used are different, there is no scope of parameter sharing between them. A major chunk of time is saved in just updating the two architectures, after each epoch instead of re-initializing architectures with lagging parameters.</p><p>To qualitatively understand the effect of training CNN with Transformer, we study the feature maps of CNN and attention maps of Transformers trained using CASS and supervised techniques. To reinstate, based on the study by <ref type="bibr" target="#b33">Raghu et al. [2021]</ref> since CNN and Transformer extract different kinds of features from the same input, combing the two of them would help us create positive pairs for self-supervised learning. In doing so, we would transfer between the two architectures, which is not innate. We have already seen that this yield better performance in most cases over four different datasets and with three different label fractions. In this section, we study this gain qualitatively with the help of feature maps and class attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Feature maps</head><p>In this section, we study the feature maps from the first five layers of the ResNet-50 model trained with CASS and supervision. We extracted feature maps after the Conv2d layer of ResNet-50. We present the extracted features in <ref type="figure">Figure 6</ref>. We observed that CASS-trained CNN could retain much more detail about the input image than supervised CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Class attention maps</head><p>We have already studied the class attention maps over a single image in Section 5.6. This section will study the average class attention maps for all four datasets. We studied the attention maps averaged over 30 random samples for autoimmune, dermofit, and brain MRI datasets. Since the ISIC 2019 dataset is highly unbalanced, we averaged the attention maps over 100 samples so that each class <ref type="figure">Figure 6</ref>: At the top we have features extracted from top 5 layers of supervised ResNet-50 while at the bottom we have features extracted from top 5 layers of CASS trained ResNet-50. We supplied both the networks with the same input ( shown in <ref type="figure" target="#fig_3">Figure 5</ref>).</p><p>may have an example in our sample. We maintained the same distribution as the test set, which has the same class distribution as the overall training set. We observed that CASS-trained Trasformers were better able to map global and local attention maps due to Transformers ability to map global dependencies and by learning features sensitive to translation equivariance and locality from CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.1 Autoimmune dataset</head><p>We study the class attention maps averaged over 30 test samples for the autoiimune dataset in <ref type="figure">Figure 7</ref>. We observed that the CASS-trained Transformer has much more attention in the center as compared to the supervised Transformer. This extra attention could be attributed that a Transformer on its own couldn't map out is due to the information transfer from CNN. Another, feature to observe is that the attention map of CASS-trained Transformer is much more connected than that of supervised Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.2 Dermofit dataset</head><p>We present the average attention maps for the dermofit dataset in <ref type="figure" target="#fig_6">Figure 9</ref>. We observed that the CASS-trained Transformer is able to pay a lot more attention to the center part of the image. Furthermore, the attention map of CASS-trained Transformer is much more connected as compared to the supervised Transformer. So, overall with CASS, the Transformer is not only able to map long-range dependencies which are innate to Transformers but is also able to make more local connections with the help of features sensitive to translation equivariance and locality from CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.3 Brain tumor MRI classification dataset</head><p>We present the results for the average class attention maps in <ref type="figure" target="#fig_0">Figure 12</ref>. We observed that a CASS-trained Transformer could better capture long and short-range dependencies than a supervised Transformer. Furthermore, we observed that a CASS-trained Transformer's attention map is much more centered than a supervised Transformer's. From <ref type="figure">Figure 11</ref> we can observe that most MRI images are center localized, so having a more centered attention map is advantageous in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.4 ISIC 2019 dataset</head><p>The ISIC-2019 dataset is one of the most challenging datasets out of the four datasets. ISIC 2019 consists of images from the HAM10000 and BCN_20000 datasets <ref type="bibr" target="#b42">Cassidy et al. [2022]</ref>, <ref type="bibr" target="#b43">Gessert et al. [2020]</ref>. For the HAM1000 dataset, it is difficult to classify between 4 classes (melanoma and melanocytic nevus), (actinic keratosis and benign keratosis). HAM10000 dataset contains images of size 600?450, centered and cropped around the lesion. Histogram corrections have been applied to only a few images. The BCN_20000 dataset contains images of size 1024?1024. This dataset is particularly challenging as many images are uncropped, and lesions are in difficult and uncommon locations. Hence, in this case, having more spread-out attention maps would be advantageous instead of a more centered one. From <ref type="figure" target="#fig_1">Figure 13</ref>, we observed that CASS-trained Transformer has a lot more spread attention map than a supervised Transformer. Furthermore, CASS-trained Transformer is also able to attend the corners far better than supervised Transformer. We chose four medical imaging datasets with diverse sample sizes ranging from 198 to 25,336 and diverse modalities to study the performance of existing self-supervised techniques and CASS. Most of the existing self-supervised techniques have been studied on million image datasets, but medical imaging datasets, on average, are much smaller than a million images. We expand this to include datasets of emerging and underrepresented diseases with only a few hundred samples, like the autoimmune dataset in our case (198 samples). To the best of our knowledge, no existing literature studies the effect of self-supervised learning on such a small dataset. Furthermore, we chose the dermofit dataset because all the images are taken using an SLR camera, and no two images are the same size. Image size in dermofit varies from 205?205 to 1020?1020. MRI images constitute a large part of medical imaging; hence we included this dataset in our study. So, to incorporate them in our study, we included the Brain tumor MRI classification dataset. Furthermore, it is our study's only black and white dataset; the other three datasets are RGB. The ISIC 2019 is a unique dataset as it contains multiple pairs of hard-to-classify classes (Melanoma -melanocytic nevus and actinic keratosis -benign keratosis) and different image sizes -out of which only a few have been prepossessed. It is a highly imbalanced dataset containing samples with lesions in difficult and uncommon locations.  F.2 Self-supervised training F.2.1 Protocols</p><p>? Self-supervised learning was only done on the training data and not on the validation data. We used https://github.com/PyTorchLightning/pytorch-lightning to set the pseudo-random number generators in PyTorch, NumPy and (python.random). ? We run training over five different seed values, and report mean results with variance in each table. We don't perform a seed value sweep to extract anymore performance <ref type="bibr" target="#b44">Picard [2021]</ref>. ? For DINO implementation we use Phil Wang's implementation: https://github.com/ lucidrains/vit-pytorch.    ? Color jittering: change the brightness, contrast, saturation and hue of an image or apply random perspective with a given probability. We set the degree of distortion to 0.2 (between 0 and 1) and use bilinear interpolation, with an application probability of 0.3.</p><p>? Color jittering or apply random affine transformation of the image keeping center invariant with degree 10, with an application probability of 0.3.</p><p>? Horizontal and Vertical flip. Each with an application probability of 0.3.</p><p>? Channel normalisation with mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.3 Hyper-parameters</head><p>? Optimization: We use stochastic weighted averaging over Adam optimiser with learning rate (LR) set to 1e-3 for both CNN and vision transformer (ViT). This is a shift from SGD which is usally used for CNNs.</p><p>? Learning Rate: Cosine annealing learning rate is used with 16 iterations and a minimum learning rate of 1e-6. Unless mentioned otherwise, this setup was trained over 100 epochs. These were then used as initialisation for the downstream supervised learning. The standard batch size is 16.</p><p>F.3 Supervised training</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3.1 Augmentations</head><p>We use the same set of augmentations used in self-supervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3.2 Hyper-parameters</head><p>? We use Adam optimiser with lr set to 3e-4 and a cosine annealing learning schedule.</p><p>? Since, all medical datasets have class imbalance we address it by using focal loss <ref type="bibr" target="#b37">Lin et al. [2017]</ref> as our choice of loss function with the alpha value set to 1 and the gamma value to 2. In our case it uses minimum-maximum normalised class distribution as class weights for focal loss. ? We train for 50 epochs. We also use a five epoch patience on validation loss to check for early stopping. This downstream supervised learning setup is kept the same for CNN and Transformers.</p><p>We repeat all the experiments five time with different seed values and then present the average results in all the tables.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Sample image used from the test set of the autoimmune dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overall attention maps from supervised Transformer (on the left) and CASS trained</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>This figure shows the feature map extracted after the first layer of ResNet-50 for CASS (on the left) and supervised CNN (on the right). From the four circles, it is clear that CASS-trained CNN can retain much more intricate detail about the input image(Figure 2) that the supervised CNN misses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>C. 5</head><label>5</label><figDesc>Change in architecture C.5.1 Changing Transformer and keeping the CNN sameFrom</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Sample image used from the test set of the autoimmune dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Sample of autofluorescence slide images from the muscle biopsy of patients with dermatomyositis -a type of autoimmune disease.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Class attention maps averaged over 30 samples of the dermofit dataset for supervised Transformer (on the left), and CASS trained Transformer (on the right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Sample images from the Dermofit dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Sample images of brain tumor MRI dataset, Each image corresponds to a prediction class in the data set glioma (Left), meningioma (Center) and No tumor (Right) Class attention maps averaged over 30 samples of the brain tumor MRI classification dataset for supervised Transformer (on the left) and CASS trained Transformer (on the right).? For implementation of CNNs and Transformers we use timm's library Wightman[2019].? For all experiments, ImageNet<ref type="bibr" target="#b45">Deng et al. [2009]</ref> initialised CNN and Transformers were used.F.2.2 Augmentations? Resizing: Resize input images to 384?384 with bilinear interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Class attention maps avergaed over 100 samples form the ISIC-2019 dataset for supervised Transformer (on the left) and CASS trained Transformer (on the right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Sample images from the ISIC-2019 challenge dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 ,Table 1 :</head><label>11</label><figDesc>we compare the cumulative training times for self-supervised training of a CNN and Transformer with DINO and CASS. We observed that CASS took an average of 69% less time compared to DINO. Another point to note is that, CASS trained two architectures at the same time or in a single pass. While to train a CNN and Transformer with DINO it would take two separate passes. Self-supervised training time comparison for 100 epochs on a single RTX8000 GPU.</figDesc><table><row><cell>Dataset</cell><cell>DINO</cell><cell>CASS</cell></row><row><cell cols="2">Autoimmune 1 Hour 13 Mins</cell><cell>21 Mins</cell></row><row><cell>Dermofit</cell><cell>3 Hours 9 mins</cell><cell>1 Hour 11 Mins</cell></row><row><cell>Brain MRI</cell><cell>26 Hours 21 Mins</cell><cell>7 Hours 11 Mins</cell></row><row><cell>ISIC-2019</cell><cell cols="2">109 Hours 21 Mins 29 Hours 58 Mins</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Results for autoimmune biopsy slides dataset. In this table we compare the F1 score on test set. We observed that CASS outperformed the existing state-of-art self-supervised method using 100% labels for CNN as well as for Transformers. Although DINO outperforms CASS for CNN with 10% labeled fraction. Overall CASS outperforms DINO by 2.2% for 100% labeled training for CNN and Transformer. For Transformers in 10% labeled training CASS' performance was 2.7% better than DINO.</figDesc><table><row><cell></cell><cell></cell><cell>score</cell><cell></cell></row><row><cell></cell><cell></cell><cell>10%</cell><cell>100%</cell></row><row><cell>DINO</cell><cell cols="2">Resnet-50 0.8237?0.001</cell><cell>0.84252?0.008</cell></row><row><cell>CASS</cell><cell cols="3">Resnet-50 0.8158?0.0055 0.8650?0.0001</cell></row><row><cell cols="4">Supervised Resnet-50 0.82095?0.007 0.819?0.0216</cell></row><row><cell>DINO</cell><cell>ViT B/16</cell><cell cols="2">0.8445?0.0008 0.8639? 0.002</cell></row><row><cell>CASS</cell><cell>ViT B/16</cell><cell>0.8717?0.005</cell><cell>0.8894?0.005</cell></row><row><cell cols="2">Supervised ViT B/16</cell><cell>0.8356?0.007</cell><cell>0.8420?0.009</cell></row><row><cell>Table 2:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Results for the dermofit dataset. Parenthesis next to the techniques represent the architecture used, for example DINO(ViT B/16) represents ViT B/16 trianed with DINO. In this table we compare the F1 score on test set.</figDesc><table><row><cell></cell><cell>score</cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell>100%</cell></row><row><cell>DINO (Resnet-50)</cell><cell cols="2">0.3749?0.0011 0.6775?0.0005</cell></row><row><cell>CASS (Resnet-50)</cell><cell cols="2">0.4367?0.0002 0.7132?0.0003</cell></row><row><cell cols="2">Supervised (Resnet-50) 0.33?0.0001</cell><cell>0.6341?0.0077</cell></row><row><cell>DINO (ViT B/16)</cell><cell cols="2">0.332? 0.0002 0.4810?0.0012</cell></row><row><cell>CASS (ViT B/16)</cell><cell cols="2">0.3896?0.0013 0.6667?0.0002</cell></row><row><cell>Supervised (ViT B/16)</cell><cell>0.299?0.002</cell><cell>0.456?0.0077</cell></row><row><cell>Table 3:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>While DINO outperformed CASS for 1% and 10% labeled training for CNN, CASS maintained its superiority for 100% labeled training, albeit by just 0.09%. Similarly, CASS outperformed DINO for all data regimes for Transformers, incrementally 1.34% in for 1%, 3.04% for 10%, and 4.38% for 100% labeled training. We observe that this margin is more significant than for biopsy images. Such results could be ascribed to the increase in dataset size and increasing learnable information.</figDesc><table><row><cell cols="2">Techniques Backbone</cell><cell cols="3">Testing Balanced multi-class accuracy 1% 10% 100%</cell></row><row><cell>DINO</cell><cell cols="2">Resnet-50 0.328?0.0016</cell><cell cols="2">0.3797?0.0027 0.493?3.9e-05</cell></row><row><cell>CASS</cell><cell cols="3">Resnet-50 0.3617?0.0047 0.41?0.0019</cell><cell>0.543?2.85e-05</cell></row><row><cell cols="3">Supervised Resnet-50 0.2640?0.031</cell><cell cols="2">0.3070?0.0121 0.35?0.006</cell></row><row><cell>DINO</cell><cell>ViT B/16</cell><cell>0.3676? 0.012</cell><cell>0.3998?0.056</cell><cell>0.5408?0.001</cell></row><row><cell>CASS</cell><cell>ViT B/16</cell><cell cols="3">0.3973? 0.0465 0.4395?0.0179 0.5819?0.0015</cell></row><row><cell cols="2">Supervised ViT B/16</cell><cell cols="3">0.3074?0.0005 0.3586?0.0314 0.42?0.007</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 10</head><label>10</label><figDesc>and 11, we observed that CASS trained ViT Transformer with the same CNN consistently gained approximately 4.7% over its supervised counterpart. Furthermore, fromTable 11we observed that although ViT L/16 performs better than ViT B/16 on ImageNet ( Wightman [2019]'s results), we observed that the trend is opposite on the autoimmune dataset. Hence, the supervised performance of architecture must be considered before pairing it with CASS. In this table we show performance of CASS for ViT large/16 with ResNet-50 and ViT base/16 with ResNet-50. We observed that CASS trained Transformers on average performed 4.7% better than their supervised counterparts.Architecture Testing F1 Score ResnNet-50 0.819?0.0216 ViT Base/16 0.8420?0.009 ViT large/16 0.80495?0.0077Table 11: Supervised performance of ViT family on the autoimmune dataset. We observed that as opposed to ImageNet performance, ViT large/16 performs worse than ViT Base/16 on the autoimmune dataset.C.5.2 Changing CNN and keeping the Transformer sameTable 12 and 13 we observed that similar to changing Transformer while keeping CNN same, CASS trained CNNs gained an average of 3% over their supervised counterparts. For ResNet-200 Wightman [2019] doesn't have ImageNet initialization hence used random initialization.</figDesc><table><row><cell>Transformer</cell><cell cols="2">CNN F1 Score Transformer F1 Score</cell></row><row><cell>ViT Base/16</cell><cell>0.8650?0.001</cell><cell>0.8894? 0.005</cell></row><row><cell cols="2">ViT Large/16 0.8481?0.001</cell><cell>0.853?0.004</cell></row><row><cell>Table 10:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 12 :Table 13 :</head><label>1213</label><figDesc>F1 metric comparison between the two arms of CASS trained over 100 epochs, following the protocols and procedure listed in Appendix E. The numbers in parentheses show the parameters learned by the network. We use Wightman [2019] implementation of CNN and transformers, with ImageNet initialisation except for ResNet-200. Supervised performance of the ResNet CNN family on the autoimmune dataset.</figDesc><table><row><cell cols="2">Architecture Testing F1 Score</cell></row><row><cell>ResnNet-18</cell><cell>0.8299?0.0004</cell></row><row><cell>ResnNet-50</cell><cell>0.831?0.0216</cell></row><row><cell cols="2">ResnNet-200 0.823?0.0005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 15</head><label>15</label><figDesc>Performance comparison over varied number of epochs, from 50 to 300 epochs, the downstream training procedure and the CNN-transformer combination is kept constant across all the four experiments, only the number of self-supervised epochs has been changed.</figDesc><table><row><cell cols="2">displays results for this experimentation.</cell><cell></cell></row><row><cell cols="2">Epochs CNN F1 Score</cell><cell>Transformer F1 Score</cell></row><row><cell>50</cell><cell cols="2">0.9795?0.0109 0.9262?0.0181</cell></row><row><cell>100</cell><cell cols="2">0.9909? 0.0032 0.9279? 0.0213</cell></row><row><cell>200</cell><cell>0.9864?0.008</cell><cell>0.9476?0.0012</cell></row><row><cell>300</cell><cell>0.9920?0.001</cell><cell>0.9484?0.017</cell></row><row><cell cols="2">Table 15: D.3 Effect of Changing the ViT/CNN branch</cell><cell></cell></row><row><cell cols="3">D.3.1 Changing CNN while keeping Transformer same</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 16 :</head><label>16</label><figDesc>F1 metric comparison between the two arms of CASS trained over 100 epochs, following the protocols and procedure listed in Appendix E. The numbers in parentheses show the parameters learned by the network. We use Wightman [2019] implementation of CNN and transformers, with ImageNet initialisation except for ResNet-200.D.3.2 Changing Transformer while keeping CNN sameFor this experiment we keep the CNN as constant and study the effect of changing the Transformer. For this experiment we use ResNet as our choice of CNN and ViT base and large Transformers with 16 patches. Additionally we also report performance for DeiT-B with ResNet-50. We report these results inTable 17. Similar toTable 10we observe that changing Transformer from ViT Base to Large while keeping the number of tokens same at 16, performance drops. Additionally, for approximately the same size, out of DEiT base and ViT base Trasnformers, DEiT performs much better than ViT base.</figDesc><table><row><cell>CNN</cell><cell>Transformer</cell><cell>CNN F1 Score</cell><cell>Transformer F1 Score</cell></row><row><cell>ResNet-50 (25.56M)</cell><cell cols="3">DEiT Base/16 (86.86M) 0.9902?0.0025 ViT Base/16 (86.86M) 0.9909?0.0032 ViT Large/16 (304.72M) 0.98945?2.45e-5 0.8896?0.0009 0.9844?0.0048 0.9279? 0.0213</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 19 :</head><label>19</label><figDesc>We present the results for using Transformers in both the arms and compare the results with CNN-Transformer combination.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Figure 7 :</head><label>7</label><figDesc>To ensure the consistency of our study, we studied average attention maps over 30 sample images from the autoimmune dataset. The top image is the overall attention map averaged over 30 samples for supervised Transformer, while the one at the bottom is for CASS trained Transformer.</figDesc><table><row><cell>F Expansion on experimentation details</cell></row><row><cell>F.1 Datasets</cell></row><row><cell>F.1.1 Choice of Datasets</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is open source and available at: github.com/pranavsinghps1/CASS</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Appendix B Self-supervised Algorithm</p><p>The core self-supervised algorithm, used to train CASS with a CNN (R) and a Transformer (T), is described in Algorithm 1. Here, num_epochs represents the number of self-supervised epochs to run. CNN and Transformer represents the respective architecture we use, for example CNN could be a ResNet50 and Transformer can be ViT Base/16. The Loss used in line 5, is described in Equation 1. Finally, after training for downstream fine tuning we save the CNN and Transformer at the defined 'path' mentioned in lines 12 and 13.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Autoimmune and inflammatory diseases following covid-19</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Galeotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Bayry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Rheumatology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="413" to="414" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The world incidence and prevalence of autoimmune diseases is increasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Jeremias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Matthias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Celiac Dis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="151" to="156" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Covid-19 and autoimmunity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ehrenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Tincani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Andreoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cattalini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darja</forename><surname>Kanduc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaume</forename><surname>Alijotas-Reig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vsevolod</forename><surname>Zinserling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Semenova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Amital</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autoimmunity reviews</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">102597</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Covid-19 prognosis via self-supervised representation learning and multi-image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Muckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koustuv</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farah</forename><surname>Shamout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yindalon</forename><surname>Azour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nafissa</forename><surname>Aphinyanaphongs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Yakubova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04909</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dermofit project datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Rees</surname></persName>
		</author>
		<ptr target="https://homepages.inf.ed.ac.uk/rbf/DERMOFIT/datasets.htm" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.6084/m9.figshare.1512427.v5</idno>
		<ptr target="https://figshare.com/articles/dataset/brain_tumor_dataset/1512427" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mri-based brain tumor classification using ensemble of deep features and machine learning classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyong</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahid</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghwan</forename><surname>Gwak</surname></persName>
		</author>
		<idno type="DOI">10.3390/s21062222</idno>
		<ptr target="https://www.mdpi.com/1424-8220/21/6/2222" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Armando</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabin</forename><forename type="middle">K</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">C</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Combalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veronica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ver?nica</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Vilaplana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">C</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halpern</surname></persName>
		</author>
		<title level="m">Susana Puig, and Josep Malvehy. Bcn20000: Dermoscopic lesions in the wild. ArXiv, abs</title>
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch&amp;apos;e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Bernardo ?vila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valko</surname></persName>
		</author>
		<idno>abs/2006.07733</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Big self-supervised models advance medical image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiona</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Von Freyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3458" to="3468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Umme Zahoora, and Aqsa Saeed Qureshi. A survey of the recent architectures of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asifullah</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anabia</forename><surname>Sohail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5455" to="5516" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network based medical image classification for disease diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shivajirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jadhav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bootstrap your own latenta new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Self-supervised learning from 100 million medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awais</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pragneshkumar</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Balter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grbic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01283</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Big self-supervised models advance medical image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiona</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Freyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3478" to="3488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Is it time to replace cnns with transformers for medical images? ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><forename type="middle">Fredin</forename><surname>Haslum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><forename type="middle">P</forename><surname>Soderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/2108.09038</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Covid-19 and autoimmune diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Amr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianjin</forename><surname>Sawalha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Rheumatology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="155" to="162" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/2002.05709</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15745" to="15753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Do vision transformers see like convolutional neural networks? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cmkd: Cnn/transformerbased cross-model knowledge distillation for audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<idno>abs/2203.06760</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dmitrii Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<idno>abs/1803.05407</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Artificial intelligence and deep learning to map immune cell types in inflamed human tissue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayla</forename><surname>Van Buren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanghao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrutesh</forename><surname>Puranik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><forename type="middle">A</forename><surname>Loomis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">B</forename><surname>Niewold</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jim.2022.113233.URLhttps:/www.sciencedirect.com/science/article/pii/S0022175922000205</idno>
		<idno>0022-1759. doi</idno>
		<ptr target="https://doi.org/10.1016/j.jim.2022.113233.URLhttps://www.sciencedirect.com/science/article/pii/S0022175922000205" />
	</analytic>
	<monogr>
		<title level="j">Journal of Immunological Methods</title>
		<imprint>
			<biblScope unit="volume">505</biblScope>
			<biblScope unit="page">113233</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A new model for brain tumor detection using ensemble transfer learning and quantum variational classifier. Computational Intelligence and Neuroscience, 2022, 2022. Ross Wightman. Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javeria</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Almas</forename><surname>Anjum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saima</forename><surname>Jabeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seifedine</forename><surname>Kadry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Moreno</forename><surname>Ger</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Transfusion: Understanding transfer learning for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Analysis of the isic image datasets: usage, benchmarks and recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connah</forename><surname>Kendrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Brodzicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Jaworek-Korjakowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moi Hoon</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102305</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Skin lesion classification using ensembles of multi-resolution efficientnets with meta data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Gessert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsin</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schlaefer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Torch. manual_seed (3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
