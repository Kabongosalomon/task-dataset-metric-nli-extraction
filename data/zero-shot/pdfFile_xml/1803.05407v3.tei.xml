<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Averaging Weights Leads to Wider Optima and Better Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Higher School of Economics</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Samsung-HSE Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Samsung AI Center in Moscow</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Lomonosov Moscow State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Higher School of Economics</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Samsung-HSE Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Averaging Weights Leads to Wider Optima and Better Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With a better understanding of the loss surfaces for multilayer networks, we can accelerate the convergence, stability, and accuracy of training procedures in deep learning. Recent work <ref type="bibr" target="#b4">[Garipov et al., 2018</ref><ref type="bibr" target="#b3">, Draxler et al., 2018</ref> shows that local optima found by SGD can be connected by simple curves of near constant loss. Building upon this insight, <ref type="bibr" target="#b4">Garipov et al. [2018]</ref> also developed Fast Geometric Ensembling (FGE) to sample multiple nearby points in weight space to create high performing ensembles in the time required to train a single DNN.</p><p>FGE uses a high frequency cyclical learning rate with SGD to select networks to ensemble. In <ref type="figure">Figure 1</ref> (left) * Equal contribution. we see that the weights of the networks ensembled by FGE are on the periphery of the most desirable solutions. This observation suggests it is promising to average these points in weight space, and use a network with these averaged weights, instead of forming an ensemble by averaging the outputs of networks in model space. Although the general idea of maintaining a running average of weights traversed by SGD dates back to <ref type="bibr" target="#b18">Ruppert [1988]</ref>, this procedure is not typically used to train neural networks. It is sometimes applied as an exponentially decaying running average in combination with a decaying learning rate (where it is called an exponential moving average), which smooths the trajectory of conventional SGD but does not perform very differently. However, we show that an equally weighted average of the points traversed by SGD with a cyclical or high constant learning rate, which we refer to as Stochastic Weight Averaging (SWA), has many surprising and promising features for training deep neural networks, leading to a better understanding of the geometry of their loss surfaces. Indeed, SWA with cyclical or constant learning rates can be used as a drop-in replacement for standard SGD training of multilayer networks -but with improved generalization and essentially no overhead. In particular:</p><p>? We show that SGD with cyclical [e.g., <ref type="bibr" target="#b14">Loshchilov and Hutter, 2017]</ref> and constant learning rates traverses regions of weight space corresponding to high-performing networks. We find that while these models are moving around this optimal set they never reach its central points. We show that we can move into this more desirable space of points by averaging the weights proposed over SGD iterations.</p><p>? While FGE ensembles <ref type="bibr" target="#b4">[Garipov et al., 2018]</ref> can be trained in the same time as a single model, test predictions for an ensemble of k models requires k times more computation. We show that SWA can be interpreted as an approximation to FGE ensembles but with the test-time, convenience, and inter- pretability of a single model.</p><p>? We demonstrate that SWA leads to solutions that are wider than the optima found by SGD. Keskar et al.</p><p>[2017] and <ref type="bibr" target="#b10">Hochreiter and Schmidhuber [1997]</ref> conjecture that the width of the optima is critically related to generalization. We illustrate that the loss on the train is shifted with respect to the test error ( <ref type="figure">Figure 1</ref>, middle and right panels, and sections 3, 4). We show that SGD generally converges to a point near the boundary of the wide flat region of optimal points. SWA on the other hand is able to find a point centered in this region, often with slightly worse train loss but with substantially better test error.</p><p>? We show that the loss function is asymmetric in the direction connecting SWA with SGD. In this direction, SGD is near the periphery of sharp ascent. Part of the reason SWA improves generalization is that it finds solutions in flat regions of the training loss in such directions.</p><p>? SWA achieves notable improvement for training a broad range of architectures over several consequential benchmarks. In particular, running SWA for just 10 epochs on ImageNet we are able to achieve 0.8% improvement for ResNet-50 and DenseNet-161, and 0.6% improvement for ResNet-150. We achieve improvement of over 1.3% on CIFAR-100 and of over 0.4% on CIFAR-10 with Preactivation ResNet-164, VGG-16 and Wide ResNet-28-10. We also achieve substantial improvement for the recent Shake-Shake Networks and PyramidNets.</p><p>? SWA is extremely easy to implement and has virtually no computational overhead compared to the conventional training schemes.</p><p>? We provide an implementation of SWA at https://github.com/timgaripov/swa.</p><p>We emphasize that SWA is finding a solution in the same basin of attraction as SGD, as can be seen in <ref type="figure">Figure 1</ref>, but in a flatter region of the training loss. SGD typically finds points on the periphery of a set of good weights. By running SGD with a cyclical or high constant learning rate, we traverse the surface of this set of points, and by averaging we find a more centred solution in a flatter region of the training loss. Further, the training loss for SWA is often slightly worse than for SGD suggesting that SWA solution is not a local optimum of the loss. In the title of this paper, optima is used in a general sense to mean solutions (converged points of a given procedure), rather than different local minima of the same objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This paper is fundamentally about better understanding the geometry of loss surfaces and generalization in deep learning. We follow the trajectory of weights traversed by SGD, leading to new geometric insights and the intuition that SWA will lead to better results than standard training. Empirically, we make the discovery that SWA notably improves training of many state-of-the-art deep neural networks over a range of consequential benchmarks, with essentially no overhead.</p><p>The procedures for training neural networks are constantly being improved. New methods are being proposed for architecture design, regularization and optimization. The SWA approach is related to work in both optimization and regularization.</p><p>In optimization, there is great interest in how differ-1 Suppose we have three weight vectors w1, w2, w3. We set</p><formula xml:id="formula_0">u = (w2 ?w1), v = (w3 ?w1)? w3 ? w1, w2 ? w1 / w2 ? w1 2 ? (w2 ? w1)</formula><p>. Then the normalized vectors? = u/ u , v = v/ v form an orthonormal basis in the plane containing w1, w2, w3. To visualize the loss in this plane, we define a Cartesian grid in the basis?,v and evaluate the networks corresponding to each of the points in the grid. A point P with coordinates (x, y) in the plane would then be given by P = w1 + x ?? + y ?v. ent types of local solutions affect generalization in deep learning. <ref type="bibr" target="#b13">Keskar et al. [2017]</ref> claim that SGD is more likely to converge to broad local optima than batch gradient methods, which tend to converge to sharp optima. Moreover, they argue that the broad optima found by SGD are more likely to have good test performance, even if the training loss is worse than for the sharp optima. On the other hand <ref type="bibr" target="#b2">Dinh et al. [2017]</ref> argue that all the known definitions of sharpness are unsatisfactory and cannot on their own explain generalization. <ref type="bibr" target="#b0">Chaudhari et al. [2017]</ref> propose the Entropy-SGD method that explicitly forces optimization towards wide valleys. They report that although the optima found by Entropy-SGD are wider than those found by conventional SGD, the generalization performance is still comparable.</p><p>The SWA method is based on averaging multiple points along the trajectory of SGD with cyclical or constant learning rates. The general idea of maintaining a running average of weights proposed by SGD was first considered in convex optimization by <ref type="bibr" target="#b18">Ruppert [1988]</ref> and later by Polyak and Juditsky <ref type="bibr">[1992]</ref>. However, this procedure is not typically used to train neural networks. Practitioners instead sometimes use an exponentially decaying running average of the weights found by SGD with a decaying learning rate, which smooths the trajectory of SGD but performs comparably.</p><p>SWA is making use of multiple samples gathered through exploration of the set of points corresponding to high performing networks. To enforce exploration we run SGD with constant or cyclical learning rates. <ref type="bibr" target="#b15">Mandt et al. [2017]</ref> show that under several simplifying assumptions running SGD with a constant learning rate is equivalent to sampling from a Gaussian distribution centered at the minimum of the loss, and the covariance of this Gaussian is controlled by the learning rate. Following this explanation from <ref type="bibr" target="#b15">[Mandt et al., 2017]</ref>, we can interpret points proposed by SGD as being constrained to the surface of a sphere, since they come from a high dimensional Gaussian distribution. SWA effectively allows us to go inside the sphere to find higher density solutions.</p><p>In a procedure called Fast Geometric Ensembling (FGE), <ref type="bibr" target="#b4">Garipov et al. [2018]</ref> showed that using a cyclical learning rate it is possible to gather models that are spatially close to each other but produce diverse predictions. They used the gathered models to train ensembles with no computational overhead compared to training a single DNN model. In recent work <ref type="bibr" target="#b16">Neklyudov et al. [2018]</ref> also discuss an efficient approach for model averaging of Bayesian neural networks. SWA was inspired by following the trajectories of FGE proposals, in order to find a single model that would approximate an FGE ensemble, but provide greater interpretability, convenience, and test-time scalability.</p><p>Dropout <ref type="bibr">[Srivastava et al., 2014]</ref> is an extremely popular approach to regularizing DNNs. Across each minibatch used for SGD, a different architecture is created by randomly dropping out neurons. The authors make analogies between dropout, ensembling, and Bayesian model averaging. At test time, an ensemble approach is proposed, but then approximated with similar results by multiplying each connection by the dropout rate. At a high level, SWA and Dropout are both at once regularizers and training procedures, motivated to approximate an ensemble. Each approach implements these high level ideas quite differently, and as we show in our experiments, can be combined for improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STOCHASTIC WEIGHT AVERAGING</head><p>We present Stochastic Weight Averaging (SWA) and analyze its properties. In section 3.1, we consider trajectories of SGD with a constant and cyclical learning rate, which helps understand the geometry of SGD training for neural networks, and motivates the SWA procedure. Then in section 3.2 we present the SWA algorithm in detail, in section 3.3 we derive its complexity, and in section 3.4 we analyze the width of solutions found by SWA versus conventional SGD training. In section 3.5 we then examine the relationship between SWA and the recently proposed Fast Geometric Ensembling <ref type="bibr" target="#b4">[Garipov et al., 2018]</ref>. Finally, in section 3.6 we consider SWA from the perspective of stochastic convex optimization.</p><p>We note the name SWA has two meanings: on the one hand, it is an average of SGD weights. On the other, with a cyclical or constant learning rate, SGD proposals are approximately sampling from the loss surface of the DNN, leading to stochastic weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ANALYSIS OF SGD TRAJECTORIES</head><p>SWA is based on averaging the samples proposed by SGD using a learning rate schedule that allows exploration of the region of weight space corresponding to high-performing networks. In particular we consider cyclical and constant learning rate schedules.</p><p>The cyclical learning rate schedule that we adopt is inspired by <ref type="bibr" target="#b4">Garipov et al. [2018]</ref> and Smith and Topin <ref type="bibr">[2017]</ref>. In each cycle we linearly decrease the learning rate from ? 1 to ? 2 . The formula for the learning rate at iteration i is given by The base learning rates ? 1 ? ? 2 and the cycle length c are the hyper-parameters of the method. Here by iteration we assume the processing of one batch of data. <ref type="figure">Figure</ref> 2 illustrates the cyclical learning rate schedule and the test error of the corresponding points. Note that unlike the cyclical learning rate schedule of <ref type="bibr" target="#b4">Garipov et al. [2018]</ref> and Smith and Topin <ref type="bibr">[2017]</ref>, here we propose to use a discontinuous schedule that jumps directly from the minimum to maximum learning rates, and does not steadily increase the learning rate as part of the cycle. We use this more abrupt cycle because for our purposes exploration is more important than the accuracy of individual proposals. For even greater exploration, we also consider constant learning rates ?(i) = ? 1 .</p><formula xml:id="formula_1">?(i) = (1 ? t(i))? 1 + t(i)? 2 , t(i) = 1 c (mod(i ? 1, c) + 1) .</formula><p>We run SGD with cyclical and constant learning rate schedules starting from a pretrained point for a Preactivation ResNet-164 on CIFAR-100. We then use the first, middle and last point of each of the trajectories to define a 2-dimensional plane in the weight space containing all affine combinations of these points. In <ref type="figure">Figure 3</ref> we plot the loss on train and error on test for points in these planes. We then project the other points of the trajectory to the plane of the plot. Note that the trajectories do not generally lie in the plane of the plot, except for the first, last and middle points, showed by black crosses in the figure. Therefore for other points of the trajectories it is not possible to tell the value of train loss and test error from the plots.</p><p>The key insight from <ref type="figure">Figure 3</ref> is that both methods explore points close to the periphery of the set of highperforming networks. The visualizations suggest that both methods are doing exploration in the region of space corresponding to DNNs with high accuracy. The main difference between the two approaches is that the individual proposals of SGD with a cyclical learning rate schedule are in general much more accurate than the proposals of a fixed-learning rate SGD. After making a large step, SGD with a cyclical learning rate spends several epochs fine-tuning the resulting point with a decreasing learning rate. SGD with a fixed learning rate on the other hand is always making steps of relatively large sizes, exploring more efficiently than with a cyclical learning rate, but the individual proposals are worse.</p><p>Another important insight we can get from <ref type="figure">Figure 3</ref> is that while the train loss and test error surfaces are qualitatively similar, they are not perfectly aligned. The shift between train and test suggests that more robust central points in the set of high-performing networks can lead to better generalization. Indeed, if we average several proposals from the optimization trajectories, we get a more robust point that has a substantially higher test performance than the individual proposals of SGD, and is essentially centered on the shifted mode for test error. We further discuss the reasons for this behaviour in sections 3.4, 3.5, 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SWA ALGORITHM</head><p>We now present the details of the Stochastic Weight Averaging algorithm, a simple but effective modification for training neural networks, motivated by our observations in section 3.1.</p><p>Following <ref type="bibr" target="#b4">Garipov et al. [2018]</ref>, we start with a pretrained model?. We will refer to the number of epochs required to train a given DNN with the conventional training procedure as its training budget and will denote it by B. The pretrained model? can be trained with the conventional training procedure for full training budget or reduced number of epochs (e.g. 0.75B). In the latter case we just stop the training early without modifying the learning rate schedule. Starting from? we continue training, using a cyclical or constant learning rate schedule. When using a cyclical learning rate we capture the models w i that correspond to the minimum values of the learning rate (see <ref type="figure" target="#fig_0">Figure 2</ref>), following <ref type="bibr" target="#b4">Garipov et al. [2018]</ref>. For constant learning rates we capture models at each epoch. Next, we average the weights of all the captured networks w i to get our final model w SWA .</p><p>Note that for cyclical learning rate schedule, the SWA algorithm is related to FGE <ref type="bibr" target="#b4">[Garipov et al., 2018]</ref>, except that instead of averaging the predictions of the models, we average their weights, and we use a different type of learning rate cycle. In section 3.5 we show how SWA can approximate FGE, but with a single model.  </p><formula xml:id="formula_2">? w for i ? 1, 2, . . . , n do ? ? ?(i) {Calculate LR for the iteration} w ? w ? ??L i (w) {Stochastic gradient update} if mod(i, c) = 0 then n models ? i/c {Number of models} w SWA ? wSWA?nmodels+w nmodels+1</formula><p>{Update average} end if end for {Compute BatchNorm statistics for w SWA weights} Batch normalization. If the DNN uses batch normalization <ref type="bibr" target="#b12">[Ioffe and Szegedy, 2015]</ref>, we run one additional pass over the data, as in <ref type="bibr" target="#b4">Garipov et al. [2018]</ref>, to compute the running mean and standard deviation of the activations for each layer of the network with w SWA weights after the training is finished, since these statistics are not collected during training. For most deep learning libraries, such as PyTorch or Tensorflow, one can typically collect these statistics by making a forward pass over the data in training mode.</p><p>The SWA procedure is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">COMPUTATIONAL COMPLEXITY</head><p>The time and memory overhead of SWA compared to conventional training is negligible. During training, we need to maintain a copy of the running average of DNN weights. Note however that the memory consumption in storing a DNN is dominated by its activations rather than its weights, and thus is only slightly increased by the SWA procedure, even for large DNNs (e.g., on the order of 10%). After the training is complete we only need to store the model that aggregates the average, leading to the same memory requirements as standard training.</p><p>During training extra time is only spent to update the aggregated weight average. This operation is of the form w SWA ? w SWA ? n models + w n models + 1 , and it only requires computing a weighted sum of the weights of two DNNs. As we apply this operation at most once per epoch, SWA and SGD require practically the same amount of computation. Indeed, a similar operation is performed as a part of each gradient step, and each epoch consists of hundreds of gradient steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SOLUTION WIDTH</head><p>Keskar et al. <ref type="bibr">[2017]</ref> and <ref type="bibr" target="#b0">Chaudhari et al. [2017]</ref> conjecture that the width of a local optimum is related to generalization. The general explanation for the importance of width is that the surfaces of train loss and test error are shifted with respect to each other and it is thus desirable to converge to the modes of broad optima, which stay approximately optimal under small perturbations. In this section we compare the solutions found by SWA and SGD and show that SWA generally leads to much wider solutions.</p><p>Let w SWA and w SGD denote the weights of DNNs trained using SWA and conventional SGD, respectively. Consider the rays</p><formula xml:id="formula_3">w SWA (t, d) = w SWA + t ? d, w SGD (t, d) = w SGD + t ? d,</formula><p>which follow a direction vector d on the unit sphere, starting at w SWA and w SGD , respectively. In <ref type="figure">Figure 4</ref> we plot train loss and test error of w SWA (t, d i ) and w SGD (t, d i ) as a function of t for 10 random directions d i , i = 1, 2, . . . , 10 drawn from a uniform distribution on the unit sphere. For this visualization we use a Preactivation ResNet-164 on CIFAR-100.</p><p>First, while the loss values on train for w SGD and w SWA are quite similar (and in fact w SGD has a slightly lower train loss), the test error for w SGD is lower by 1.5% (at the converged value corresponding to t = 0). Further, the shapes of both train loss and test error curves are considerably wider for w SWA than for w SGD , suggesting that SWA indeed converges to a wider solution: we have to step much further away from w SWA to increase error by a given amount. We even see the error curve for SGD has an inflection point that is not present for these distances with SWA.</p><p>Notice that in <ref type="figure">Figure 4</ref> any of the random directions from w SGD increase test error. However, we know that the direction from w SGD to w SWA would decrease test error, since w SWA has considerably lower test error than w SGD . In other words, the path from w SGD to w SWA is qualitatively different from all directions shown in <ref type="figure">Figure 4</ref>, because along this direction w SGD is far from optimal. We therefore consider the line segment connecting w SGD and w SWA :</p><formula xml:id="formula_4">w(t) = t ? w SGD + (1 ? t) ? w SWA .</formula><p>In <ref type="figure">Figure 5</ref> we plot the train loss and test error of w(t) as a function of signed distance from w SWA for Preactivation ResNet-164 and VGG-16 on CIFAR-100.</p><p>We can extract several key insights about w SWA and w SGD from <ref type="figure">Figure 5</ref>. First, the train loss and test error plots are indeed substantially shifted, and the point obtained by minimizing the train loss is far from optimal on test.</p><p>Second, w SGD lies near the boundary of a wide flat region of the train loss. Further, the loss is very steep near w SGD .</p><p>Keskar et al. <ref type="bibr">[2017]</ref> argue that the loss near sharp optima found by SGD with very large batches are actually flat in most directions, but there exist directions in which the optima are extremely steep. They conjecture that because of this sharpness the generalization performance of large batch optimization is substantially worse than that of solutions found by small batch SGD. Remarkably, in our experiments in this section we observe that there exist directions of steep ascent even for small batch optima, and that SWA provides even wider solutions (at least along random directions) with better generalization. Indeed, we can see clearly in <ref type="figure">Figure 5</ref> that SWA is not finding a different minima than SGD, but rather a flatter region in the same basin of attraction. We can also see clearly that the significant asymmetry of the loss function in certain directions, such as the direction SWA to SGD, has a role in understanding why SWA provides better generalization than SGD. In these directions SWA finds a much flatter solution than SGD, which can be near the periphery of sharp ascent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CONNECTION TO ENSEMBLING</head><p>Garipov et al. <ref type="bibr">[2018]</ref> proposed the Fast Geometric Ensembling (FGE) procedure for training ensembles in the time required to train a single model. Using a cyclical learning rate, FGE generates a sequence of points that are close to each other in the weight space, but produce diverse predictions. In SWA instead of averaging the predictions of the models we average their weights. However, the predictions proposed by FGE ensembles and SWA models have similar properties.</p><p>Let f (?) denote the predictions of a neural network parametrized by weights w. We will assume that f is a scalar (e.g. the probability for a particular class) twice continuously differentiable function with respect to w.</p><p>Consider points w i proposed by FGE. These points are close in the weight space by design, and concentrated around their average w SWA = 1 n n i=1 w i . We denote</p><formula xml:id="formula_5">? i = w i ? w SWA . Note n i=1 ? i = 0.</formula><p>Ensembling the networks corresponds to averaging the function values</p><formula xml:id="formula_6">f = 1 n n i=1 f (w i ).</formula><p>Consider the linearization of f at w SWA .</p><formula xml:id="formula_7">f (w j ) = f (w SWA ) + ?f (w SWA ), ? j + O( ? j 2 ),</formula><p>where ?, ? denotes the dot product. Thus, the difference between averaging the weights and averaging the predictions</p><formula xml:id="formula_8">f ? f (w SWA ) = 1 n n i=1 ?f (w SWA ), ? i + O( ? i 2 ) = ?f (w SWA ), 1 n n i=1 ? i + O(? 2 ) = O(? 2 ),</formula><p>where ? = max n i=1 ? i . Note that the difference between the predictions of different perturbed networks is</p><formula xml:id="formula_9">f (w i ) ? f (w j ) = ?f (w SWA ), ? i ? ? j + O(? 2 ),</formula><p>and is thus of the first order of smallness, while the difference between averaging predictions and averaging weights is of the second order of smallness. Note that for the points proposed by FGE the distances between proposals are relatively small by design, which justifies the local analysis.</p><p>To analyze the difference between ensembling and averaging the weights of FGE proposals in practice, we run FGE for 20 epochs and compare the predictions of different models on the test dataset with a Preactivation ResNet-164 <ref type="bibr" target="#b9">[He et al., 2016]</ref> on CIFAR-100. The norm of the difference between the class probabilities of consecutive FGE proposals averaged over the test dataset is 0.126. We then average the weights of the proposals and compute the class probabilities on the test dataset.</p><p>The norm of difference of the probabilities for the SWA model and the FGE ensemble is 0.079, which is substantially smaller than the difference between the probabilities of consecutive FGE proposals. Further, the fraction of objects for which consecutive FGE proposals output the same labels is not greater than 87.33%. For FGE and SWA the fraction of identically labeled objects is 95.26%.</p><p>The theoretical considerations and empirical results presented in this section suggest that SWA can approximate the FGE ensemble with a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">CONNECTION TO CONVEX MINIMIZATION</head><p>Mandt et al. <ref type="bibr">[2017]</ref> showed that under strong simplifying assumptions SGD with a fixed learning rate approximately samples from a Gaussian distribution centered at the minimum of the loss. Suppose this is the case when we run SGD with a fixed learning rate for training a DNN.</p><p>Let us denote the dimensionality of the weight space of the neural network by d. Denote the samples produced by SGD by w i , i = 1, 2, . . . , k. Assume the points w i are concentrated around the local optimum?. The SWA solution is given by w SWA = 1 n k i=1 w i . The points w i are samples from a multidimensional Gaussian N (?, ?) for some covariance matrix ? defined by the curvature of the loss, batch size and the learning rate. Note that the samples from a multidimensional Gaussian are concentrated on the ellipsoid</p><formula xml:id="formula_10">z ? R d | ? ? 1 2 (z ??) = ? d ,</formula><p>and the probability mass for a sample to end up inside the ellipsoid near? is negligible. On the other hand, w SWA is guaranteed to converge to? as k ? ?.</p><p>Moreover, Polyak and Juditsky <ref type="bibr">[1992]</ref> showed that averaging SGD proposals achieves the best possible convergence rate among all stochastic gradient algorithms. The proof relies on the convexity of the underlying problem and in general there are no convergence guarantees if the loss function is non-convex [see e.g. <ref type="bibr" target="#b6">Ghadimi and Lan, 2013]</ref>. While DNN loss functions are known to be nonconvex [e.g. <ref type="bibr" target="#b1">Choromanska et al., 2015]</ref>, over the trajectory of SGD these loss surfaces are approximately convex [e.g. <ref type="bibr" target="#b7">Goodfellow et al., 2015]</ref>. However, even when the loss is locally non-convex, SWA can improve generalization. For example, in <ref type="figure">Figure 5</ref> we see that SWA converges to a central point of the training loss.</p><p>In other words, there are a set of points that all achieve low training loss. By running SGD with a high constant or cyclical schedule, we traverse over the surface of this set. Then by averaging the corresponding iterates, we get to move inside the set. This observation explains both convergence rates and generalization. In deep learning we mostly observe benefits in generalization from averaging. Averaging can move to a more central point, which means one has to move further from this point to increase the loss by a given amount, in virtually any direction. By contrast, conventional SGD with a decaying schedule will converge to a point on the periphery of this set. With different initializations conventional SGD will find different points on the boundary, of solutions with low training loss, but it will not move inside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We compare SWA against conventional SGD training on CIFAR-10, CIFAR-100 and ImageNet ILSVRC-2012 <ref type="bibr" target="#b19">[Russakovsky et al., 2012]</ref>. We also compare to Fast Geometric Ensembling (FGE) <ref type="bibr" target="#b4">[Garipov et al., 2018]</ref>, but we note that FGE is an ensemble whereas SWA corresponds to a single model. Conventional SGD training uses a standard decaying learning rate schedule (details in the Appendix) until convergence. We found an exponentially decaying average of SGD to perform comparably to conventional SGD at convergence. We release the code for reproducing the results in this paper at https://github.com/timgaripov/swa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CIFAR DATASETS</head><p>For the experiments on CIFAR datasets we use VGG-16 <ref type="bibr" target="#b20">[Simonyan and Zisserman, 2014]</ref>, a 164-layer Preactivation-ResNet <ref type="bibr" target="#b9">[He et al., 2016]</ref> and Wide ResNet-28-10 [Zagoruyko and Komodakis, 2016] models. Additionally, we experiment with the recent Shake-Shake-2x64d <ref type="bibr" target="#b5">[Gastaldi, 2017]</ref> on CIFAR-10 and PyramidNet-272 (bottleneck, ? = 200) <ref type="bibr" target="#b8">[Han et al., 2016]</ref> on CIFAR-100. All models are trained using L 2 -regularization, and VGG-16 also uses dropout.</p><p>For each model we define budget as the number of epochs required to train the model until convergence with conventional SGD training, such that we do not see improvement with SGD beyond this budget. We use the same budgets for VGG, Preactivation ResNet and Wide ResNet models as <ref type="bibr" target="#b4">Garipov et al. [2018]</ref>. For Shake-Shake and PyramidNets we use the budgets indicated by the papers that proposed these models <ref type="bibr" target="#b5">[Gastaldi, 2017</ref><ref type="bibr" target="#b8">, Han et al., 2016</ref>. We report the results of SWA training within 1, 1.25 and 1.5 budgets of epochs.</p><p>For VGG, Wide ResNet and Preactivation-ResNet models we first run standard SGD training for ? 75% of the training budget, and then use the weights at the last epoch as an initialization for SWA with a fixed learning rate schedule. We ran SWA for 0.25, 0.5 and 0.75 budget to complete the training within 1, 1.25 and 1.5 budgets respectively.</p><p>For Shake-Shake and PyramidNet architectures we do not report the results in one budget. For these models we use a full budget to get an initialization for the procedure, and then train with a cyclical learning rate schedule for 0.25 and 0.5 budgets. We used long cycles of small learning rates for Shake-Shake, because this architecture already involves many stochastic components.</p><p>We present the details of the learning rate schedules for each of these models in the Appendix.</p><p>For each model we also report the results of conventional SGD training, which we denote by SGD. For VGG, Preactivation ResNet and Wide ResNet we also provide the results of the FGE method with one budget reported in <ref type="bibr" target="#b4">Garipov et al. [2018]</ref>. Note that for FGE we report the accuracy of an ensemble of 6 to 12 networks, while for SWA we report the accuracy of a single model.</p><p>We summarize the experimental results in <ref type="table" target="#tab_2">Table 1</ref>. For all models we report the mean and standard deviation of test accuracy over 3 runs. In all conducted experiments SWA substantially outperforms SGD in one budget, and improves further, as we allow more training epochs. Across different architectures we see consistent improvement by ? 0.5% on CIFAR-10 (excluding Shake-Shake, for which SGD performance is already extremely high) and by 0.75-1.5% on CIFAR-100. Amazingly, SWA is able to achieve comparable or better performance than FGE ensembles with just one model. On CIFAR-100 SWA usually needs more than one budget to get results comparable with FGE ensembles, but on CIFAR-10 even with 1 budget SWA outperforms FGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IMAGENET</head><p>On ImageNet we experimented with <ref type="bibr">ResNet-50, ResNet-152 [He et al., 2016]</ref> and DenseNet-161 <ref type="bibr" target="#b11">[Huang et al., 2017]</ref>. For these architectures we used pretrained models from PyTorch.torchvision. For each of the models we ran SWA for 10 epochs with a cyclical learning rate schedule with the same parameters for all models (the details can be found in the Appendix), and report the mean and standard deviation of test error averaged over 3 runs. The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. For all 3 architectures SWA provides consistent improvement by 0.6-0.9% over the pretrained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EFFECT OF THE LEARNING RATE SCHEDULE</head><p>In this section we explore how the learning rate schedule affects the performance of SWA. We run experiments on Preactivation ResNet-164 on CIFAR-100. For all schedules we use the same initialization from a model trained for 125 epochs using the conventional SGD training. As a baseline we use a fully-trained model trained with conventional SGD for 150 epochs.</p><p>We consider a range of constant and cyclical learning rate schedules. For cyclical learning rates we fix the cycle length to 5, and consider the pairs of base learning rate parameters (? 1 , ? 2 ) ? {(10 ?1 , 10 ?3 ), (5 ? 10 ?2 , 5 ? 10 ?4 ), (10 ?2 , 10 ?4 ), (5 ? 10 ?3 , 5 ? 10 ?5 )}. Among the constant learning rates we consider ? 1 ? {10 ?1 , 5 ? 10 ?2 , 10 ?2 , 10 ?3 }.</p><p>We plot the test error of the SWA procedure for different learning rate schedules as a function of the number of training epochs in <ref type="figure">Figure 6</ref>.</p><p>We find that in general the more aggressive constant learning rate schedule leads to faster convergence of SWA. In our experiments we found that setting the learning rate to some intermediate value between the largest and the smallest learning rate used in the annealing scheme in conventional training usually gave us the best results. The approach is however universal and can work well with different learning rate schedules tailored for particular tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">DNN TRAINING WITH A FIXED LEARNING RATE</head><p>In this section we show that it is possible to train DNNs from scratch with a fixed learning rate using SWA. We run SGD with a fixed learning rate of 0.  <ref type="figure">Figure 7</ref>: Test error as a function of training epoch for constant (green) and decaying (blue) learning rate schedules for a Wide ResNet-28-10 on CIFAR-100. In red we average the points along the trajectory of SGD with constant learning rate starting at epoch 140.</p><p>the optimum, but SWA converges.</p><p>While being able to train a DNN with a fixed learning rate is a surprising property of SWA, for practical purposes we recommend initializing SWA from a model pretrained with conventional training (possibly for a reduced number of epochs), as it leads to faster and more stable convergence than running SWA from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>We have presented Stochastic Weight Averaging (SWA) for training neural networks. SWA is extremely easy to implement, architecture-agnostic, and improves generalization performance at virtually no additional cost over conventional training.</p><p>There are so many exciting directions for future research. SWA does not require each weight in its average to correspond to a good solution, due to the geometry of weights traversed by the algorithm. It therefore may be possible to develop SWA for much faster convergence than standard SGD. One may also be able to combine SWA with large batch sizes while preserving generalization performance, since SWA discovers much broader optima than conventional SGD training. Furthermore, a cyclic learning rate enables SWA to explore regions of high posterior density over neural network weights. Such learning rate schedules could be developed in conjunction with stochastic MCMC approaches, to encourage exploration while still providing high quality samples. One could also develop SWA to average whole regions of good solutions, using the high-accuracy curves discovered in <ref type="bibr" target="#b4">Garipov et al. [2018]</ref>.</p><p>A better understanding of the loss surfaces for multilayer networks will help continue to unlock the potential of these rich models. We hope that SWA will inspire further progress in this area.</p><p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 EXPERIMENTAL DETAILS</head><p>For the experiments on CIFAR datasets (section 4.1) we used the following implementations (embedded links):</p><p>? Shake-Shake-2x64d</p><p>? PyramidNet-272</p><p>? VGG-16</p><p>? Preactivation-ResNet-164</p><p>? Wide ResNet-28-10</p><p>Models for ImageNet are from here. Pretrained networks can be found here.</p><p>SWA learning rates. For PyramidNet SWA uses a cyclic learning rate with ? 1 = 0.05 and ? 2 = 0.001 and cycle length 3. For VGG and Wide ResNet we used constant learning ? 1 = 0.01. For ResNet we used constant learning rates ? 1 = 0.01 on CIFAR-10 and 0.05 on CIFAR-100.</p><p>For Shake-Shake Net we used a custom cyclic learning rate based on the cosine annealing used when training Shake-Shake with SGD. Each of the cycles replicate the learning rates corresponding to epochs 1600 ? 1700 of the standard training and the cycle length c = 100 epochs. The learning rate schedule is depicted in <ref type="figure" target="#fig_3">Figure  8</ref> and follows the formula ?(i) = 0.1 ? 1 + cos ? ? 1600 + epoch(i) mod 100) 1800 ,</p><p>where epoch(i) is the number of data passes completed before iteration i.</p><p>For all experiments with ImageNet we used cyclic learning rate schedule with the same hyperparameters ? 1 = 0.001, ? 2 = 10 ?5 and c = 1.</p><p>SGD learning rates. For conventional SGD training we used SGD with momentum 0.9 and with an annealed learning rate schedule. For VGG, Wide ResNet and Preactivation ResNet we fixed the learning rate to ? 1 for the first half of epochs (0B-0.5B), then linearly decreased the learning rate to 0.01? 1 for the next 40% of epochs (0.5B-0.9B), and then kept it constant for the last 10% of epochs (0.9B -1B). For VGG we set ? 1 = 0.05, and for Preactivation ResNet and Wide ResNet we set ? 1 = 0.1. For Shake-Shake Net and PyramidNets we used the cosine and piecewise-constant learning rate schedules described in <ref type="bibr" target="#b5">Gastaldi [2017]</ref> and <ref type="bibr" target="#b8">Han et al. [2016]</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 TRAINING RESNET WITH A CONSTANT LEARNING RATE</head><p>In this section we present the experiment on training Preactivation ResNet-164 using a constant learning rate. The experimental setup is the same as in section 4.4. We set the learning rate to ? 1 = 0.1 and start averaging after epoch 200. The results are presented in <ref type="figure" target="#fig_4">Figure 9</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Top: cyclical learning rate as a function of iteration. Bottom: test error as a function of iteration for cyclical learning rate schedule with Preactivation-ResNet-164 on CIFAR-100. Circles indicate iterations corresponding to the minimum learning rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>(Left) Test error and (Right) L 2 -regularized cross-entropy train loss as a function of a point on a random ray starting at SWA (blue) and SGD (green) solutions for Preactivation ResNet-164 on CIFAR-100. Each line corresponds to a different random ray. L 2 -regularized cross-entropy train loss and test error as a function of a point on the line connecting SWA and SGD solutions on CIFAR-100. Left: Preactivation ResNet-164. Right: VGG-16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Cyclical learning rate used for Shake-Shake as a function of iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Test error as a function of training epoch for constant (green) and decaying (blue) learning rate schedules for a Preactivation ResNet-164 on CIFAR-100. In red we average the points along the trajectory of SGD with constant learning rate starting at epoch 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1803.05407v3[cs.LG] 25 Feb 2019Figure 1: Illustrations of SWA and SGD with a Preactivation ResNet-164 on CIFAR-100 1 . Left: test error surface for three FGE samples and the corresponding SWA solution (averaging in weight space). Middle and Right: test error and train loss surfaces showing the weights proposed by SGD (at convergence) and SWA, starting from the same initialization of SGD after 125 training epochs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Test error (%)</cell><cell></cell><cell></cell><cell>&gt; 50</cell><cell></cell><cell cols="4">Test error (%)</cell><cell></cell><cell></cell><cell>&gt; 50</cell><cell></cell><cell></cell><cell cols="2">Train loss</cell><cell></cell><cell></cell><cell></cell><cell>&gt; 0.8832</cell></row><row><cell>30</cell><cell></cell><cell></cell><cell>W2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>10</cell><cell>W SGD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>10</cell><cell>W SGD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8832</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4391</cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28.49</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>27.52</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2206</cell></row><row><cell>10</cell><cell></cell><cell></cell><cell cols="2">W SWA</cell><cell></cell><cell></cell><cell></cell><cell>24.5</cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>23.65</cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1131</cell></row><row><cell>0</cell><cell></cell><cell>W1</cell><cell></cell><cell></cell><cell>W3</cell><cell></cell><cell></cell><cell>21.24 22.38</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>W SWA</cell><cell></cell><cell>20.67 21.67</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>W SWA</cell><cell></cell><cell>0.03422 0.06024</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20.64</cell><cell></cell><cell>epoch 125</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20.15</cell><cell></cell><cell>epoch 125</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.02142</cell></row><row><cell>?10</cell><cell>?10</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>19.95</cell><cell>?5</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>19.62</cell><cell>?5</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>0.00903</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The L 2 -regularized cross-entropy train loss and test error surfaces of a Preactivation ResNet-164 on CIFAR-100 in the plane containing the first, middle and last points (indicated by black crosses) in the trajectories with (left two) cyclical and (right two) constant learning rate schedules.</figDesc><table><row><cell>Test error (%)</cell><cell>&gt; 50</cell><cell></cell><cell></cell><cell>Train loss</cell><cell></cell><cell></cell><cell></cell><cell>&gt; 1.7</cell><cell></cell><cell></cell><cell cols="2">Test error (%)</cell><cell></cell><cell></cell><cell>&gt; 50</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell></row><row><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>35.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.883</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37.03</cell></row><row><cell></cell><cell>28.49</cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5062</cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30.04</cell></row><row><cell></cell><cell>24.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3324</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26.28</cell></row><row><cell></cell><cell>22.38</cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2522</cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.26</cell></row><row><cell></cell><cell>21.24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2152</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>23.17</cell></row><row><cell></cell><cell>20.64</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1981</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>22.58</cell></row><row><cell></cell><cell>19.95</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>0.1835</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>21.9</cell></row><row><cell>Figure 3: Algorithm 1 Stochastic Weight Averaging</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Require:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>weights?, LR bounds ? 1 , ? 2 ,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>cycle length c (for constant learning rate c = 1), num-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ber of iterations n</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ensure: w</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>SWA w ?? {Initialize weights with?} w SWA</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Accuracies (%) of SWA, SGD and FGE methods on CIFAR-100 and CIFAR-10 datasets for different training budgets. Accuracies for the FGE ensemble are from<ref type="bibr" target="#b4">Garipov et al. [2018]</ref>.</figDesc><table><row><cell>SWA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Top-1 accuracies (%) on ImageNet for SWA and</cell></row><row><cell cols="3">SGD with different architectures.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>SWA</cell><cell></cell></row><row><cell>DNN</cell><cell>SGD</cell><cell>5 epochs</cell><cell>10 epochs</cell></row><row><cell cols="4">ResNet-50 ResNet-152 DenseNet-161 77.65 78.26 ? 0.09 78.44 ? 0.06 76.15 76.83 ? 0.01 76.97 ? 0.05 78.31 78.82 ? 0.01 78.94 ? 0.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>05 on a Wide ResNet-28-10 [Zagoruyko and Komodakis, 2016] for 300 epochs from a random initialization on CIFAR-100. We then averaged the weights at the end of each epoch from epoch 140 and until the end of training. The final test accuracy of this SWA model was 81.7.Figure 7illustrates the test error as a function of the number of training epochs for SWA and conventional training. The accuracy of the individual models with weights averaged by SWA stays at the level of ? 65% which is 16% less than the accuracy of the SWA model.</figDesc><table><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test error (%)</cell><cell>30 35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>25</cell><cell>SGD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell cols="2">Const LR SGD</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Const LR SWA</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epochs</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>These re-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sults correspond to our intuition presented in section 3.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>that SGD with a constant learning rate oscillates around</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The Journal of Machine Learning Research, 15 (1):1929-1958, 2014. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by NSF IIS-1563887, Samsung Research, Samsung Electronics and Russian Science Foundation grant 17-11-01027. We also thank Vadim Bereznyuk for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Entropy-sgd: Biasing gradient descent into wide valleys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="192" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sharp minima can generalize for deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Essentially no barriers in neural network energy landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kambis</forename><surname>Veschgini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Salmhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1308" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10026</idno>
		<title level="m">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxe</surname></persName>
		</author>
		<title level="m">Qualitatively characterizing neural network optimization problems. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02915</idno>
		<title level="m">Deep pyramidal residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mudigere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sgdr: stochastic gradient descent with restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent as approximate bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4873" to="4907" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Variance networks: When expectation does not meet your expectations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Neklyudov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03764</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient estimations from a slowly convergent robbins-monro process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ruppert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
		<respStmt>
			<orgName>Cornell University Operations Research and Industrial Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploring loss function topology with cyclical learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04283</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
