<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyu</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
							<email>dinggg@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<email>jungonghan77@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Aberystwyth University</orgName>
								<address>
									<postCode>SY23 3FL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In real-world scenarios, data tends to exhibit a long-tailed distribution, which increases the difficulty of training deep networks. In this paper, we propose a novel self-paced knowledge distillation framework, termed Learning From Multiple Experts (LFME). Our method is inspired by the observation that networks trained on less imbalanced subsets of the distribution often yield better performances than their jointly-trained counterparts. We refer to these models as 'Experts, and the proposed LFME framework aggregates the knowledge from multiple 'Experts' to learn a unified student model. Specifically, the proposed framework involves two levels of adaptive learning schedules: Self-paced Expert Selection and Curriculum Instance Selection, so that the knowledge is adaptively transferred to the 'Student'. We conduct extensive experiments and demonstrate that our method is able to achieve superior performances compared to state-of-the-art methods. We also show that our method can be easily plugged into state-of-the-art long-tailed classification algorithms for further improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep convolutional neural networks (CNNs) have achieved remarkable success in various computer vision applications such as image classification, object detection and face recognition. Training a CNN typically relies on carefully collected large-scale datasets, such as ImageNet <ref type="bibr" target="#b5">[6]</ref> and MS COCO <ref type="bibr" target="#b29">[30]</ref> with hundreds of examples for each class. However, collecting such a uniformly distributed dataset in real-world scenarios is usually difficult since the underlying natural data distribution tends to exhibit a long-tailed property with few majority classes (head) and large amount of minority classes (tail) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b35">36]</ref>. When deep models are trained under such imbalanced distribution, they are unlikely to achieve the expected performances which necessitates developing relevant algorithms.</p><p>Recent approaches tackle this problem mainly from two aspects. First is via re-sampling schemes or cost-sensitive loss functions to alleviate the negative impact of data imbalance. Second is by head-to-tail knowledge transfer, where prior knowledge or induction bias is learned from the richly annotated classes and generalize to the minority ones.</p><p>Orthogonal to the above two perspectives, we propose a novel self-paced knowledge distillation method which can be easily plugged into previous methods. Our method is motivated by an interesting observation that learning a more uniform distribution with fewer samples is sometimes easier than learning a long-tailed distribution with more samples <ref type="bibr" target="#b36">[37]</ref>. We first introduce four metrics to measure the 'longtailness' of a long-tailed distribution. We then show that if we sort all the categories according to their cardinality, then splitting the entire long-tailed dataset into subsets will lead to a smaller longtailness, which indicates that they suffer a less severe data imbalance problem. Therefore training a CNN on these subsets is expected to perform better than their jointly-trained counterparts. For clarity, we refer to such a subset as cardinality-adjacent subset, and the CNN trained on these subsets as Expert Models.</p><p>Once we acquire the well-trained expert models, they can be utilized as guidance to train a unified student model. If we take a look at human learning process as students, we can conclude two characteristics: 1) the student often takes various courses from easy to hard, 2) as the learning proceeds, the student acquires more knowledge from self-learning than from teachers and he/her may even exceed his/her teachers. Inspired by these findings, we propose a Learning From Multiple Experts (LFME) framework with two levels of adaptive learning schemes, termed as self-paced expert selection and curriculum instance selection. Specifically, the self-paced expert selection automatically controls the impact of knowledge distillation from each expert, so that the learned student model will gradually acquire the knowledge from the experts, and finally exceed the expert. The curriculum instance selection, on the other hand, designs a curriculum for the unified model where the training samples are organized from easy to hard, so that the unified student model will receive a less challenging learning schedule, and gradually learns from easy to hard samples. A schematic illustration of our LFME framework is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>To verify the effectiveness of our proposed framework, we conduct extensive experiments on three benchmark long-tailed classification datasets, and show that our method is able to yield superior performances compared to the state-ofthe-art methods. It is worth noting that our method can be easily combined with other state-of-the-art methods and achieve further improvements. Moreover, we conduct extensive ablation studies to verify the contribution of each component.</p><p>Our contributions can be summarized as follows: <ref type="formula">(1)</ref> We introduce four metrics for evaluating the 'longtailness' of a distribution and further propose a Learning From Multiple Experts knowledge distillation framework. (2) We propose two levels of adaptive learning schemes, i.e. model level and instance level, to learn a unified Student model. (3) Our proposed method achieves state-ofthe-art performances on three benchmark long-tailed classification datasets, and can be easily combined with state-of-art methods for further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Long-tailed, Data-imbalanced Learning. The long-tailed learning problem has been comprehensively studied due to the prevalence of data imbalance problem <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref>. Most previous methods tackle this problem using either resampling, re-weighting or 'head-to-tail' knowledge transfer. Re-sampling methods either adopt over-sampling on tail classes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref> or use under-sampling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b21">22]</ref> on head classes. On the other hand, various cost-sensitive loss functions have been proposed in the literature to re-weight majority and minority instances <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5]</ref>. Among them, Range Loss <ref type="bibr" target="#b50">[51]</ref> minimizes the range of each class to enhance the learning towards face recognition with long-tail while Focal Loss <ref type="bibr" target="#b28">[29]</ref> down-weights the loss assigned to well-classified examples to deal with class imbalance in object detection. Label-Distribution-Aware Margin Loss (LDAM) <ref type="bibr" target="#b1">[2]</ref>, on the other hand, encourages minority classes to have larger margins.</p><p>Researchers also try to employ head-to-tail knowledge transfer for data imbalance. In <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> a transformation from minority classes to majority classes regressors/classifiers is learned progressively while in <ref type="bibr" target="#b30">[31]</ref> a meta embedding equipped with a feature memory is proposed for such knowledge transfer.</p><p>Few-shot learning methods [?, <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">47</ref>] also try to generalize knowledge from a richly annotated dataset to a low-shot dataset. This is often achieved by training a meta-learner from the many-shot classes and then generalize to new fewshot classes. However, different from few-shot learning algorithms, we mainly focus on learning a continuous spectrum of data distribution jointly, rather than focus solely on the few-shot classes. Knowledge Distillation. The idea of knowledge distillation was first introduced in <ref type="bibr" target="#b18">[19]</ref> for the purpose of model compression where a student network is trained to mimic the behavior of a teacher network so that the knowledge is com-pressed to the compact student network. Then the distillation target is further extended to hidden layer features <ref type="bibr" target="#b40">[41]</ref> and visual attention <ref type="bibr" target="#b49">[50]</ref>, where attention map from the teacher model is transferred to the student. Apart from distilling for model compression, knowledge distillation is also proved to be effective when the teacher and the student have identical architecture. i.e. self-distillation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b10">11]</ref>. Knowledge distillation is also applied in other areas such as continual learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">39]</ref>, semi-supervised learning <ref type="bibr" target="#b31">[32]</ref> and neural style transfer <ref type="bibr" target="#b20">[21]</ref>. Curriculum and Self-paced Learning. The basic idea of curriculum learning is to organize samples or tasks in ascending order of difficulty, and it has been widely adopted for weakly supervised learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b13">14]</ref> and reinforcement learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Apart from designing easy-to-hard curriculums based on prior knowledge, efforts have also been made to incorporate learning process to dynamically adjust the curriculum. In <ref type="bibr" target="#b22">[23]</ref> a self-paced curriculum determined by both prior knowledge and learning dynamics is proposed. In <ref type="bibr" target="#b12">[13]</ref> a multi-armed bandit algorithm is used to determine a syllabus, where the rate of increase in prediction accuracy and network complexity are utilized as reward signals. In <ref type="bibr" target="#b39">[40]</ref>, meta learning is employed to assign weights to training samples based on gradient directions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation and Metrics for Evaluating Data Imbalance</head><p>The problem we address in this work is to train a CNN on a long-tailed classification task. Our method is inspired by an interesting empirical finding that, training a CNN on a balanced dataset with fewer samples sometimes leads to superior performances than on a long-tailed dataset with more samples. As the experiment in <ref type="bibr" target="#b36">[37]</ref> reveals that even when 40% of the positive samples are left out for the representation learning, the object detection performances can still be surprisingly improved a bit due to a more uniform distribution. This observation successfully emphasizes the importance of learning a balanced, uniform distribution. To learn a more balanced distribution, a natural question to ask would be, how to measure the data imbalance. Since almost every manually collected dataset more or less contains various number of samples per class. To this end, we first introduce four metrics for data imbalance measurement.</p><p>For a long-tailed dataset, if we denote N, N i , C to be the total number of samples, number of samples in class i, and number of classes respectively, then the four metrics for measuring data imbalance is introduced as follows: Imbalance Ratio <ref type="bibr" target="#b41">[42]</ref> is defined as the ratio between the largest and the smallest number of samples:</p><formula xml:id="formula_0">I Ratio = Ni max Ni min 60</formula><p>Comparison of CNN Accuracy on lmageNet_L T Imbalance Divergence is defined as the KL-Divergence between the longtailed distribution and the uniform distribution:</p><formula xml:id="formula_1">I KL = D(P ||Q) = i p i log p i q i</formula><p>where p i = Ni N is the class probability for class i, and q i = 1 C denotes the uniform probability. Imbalance Absolute Deviation <ref type="bibr" target="#b3">[4]</ref> is defined as the sum of aboslute distance between each long-tailed and uniform probability:</p><formula xml:id="formula_2">I Abs = i | 1 C ? N i N |</formula><p>Gini Coefficient is defined as</p><formula xml:id="formula_3">I Gini = C i=1 (2i ? C ? 1)N i C C i=1 N i where i</formula><p>is the class index when all classes are sorted by cardinality in ascending order.</p><p>The last three metrics all measure the distance between the current longtailed distribution and a uniform distribution. For all four metrics, smaller values indicate a more uniform distribution. Having these metrics at hand, we show that for a long-tailed dataset, if we sort the classes by their cardinality, i.e. number of samples, then a subset of the classes with adjacent cardinality will become less long-tailed under these imbalance measurements. As <ref type="table" target="#tab_1">Table 2</ref> shows that, if we split the long-tailed benchmark dataset ImageNet-LT into three splits (manyshot, medium-shot, few-shot) according to class cardinality (following <ref type="bibr" target="#b30">[31]</ref>), then all four metrics become smaller, which indicates that these subsets become less imbalanced than the original. Then the CNNs trained on these subsets are expected to perform better than their jointly-trained counterparts. We verify this assumption on two long-tailed benchmark datasets ImageNet-LT and Places-LT <ref type="bibr" target="#b30">[31]</ref>. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref> that CNNs trained on these subsets outperform the joint model by a large margin. This empirical result also accords with our intuition that a continuous spectrum subset of adjacent classes is more balanced in terms of cardinality, and the learning process involves less interference between the majority and the minority. In this section, we describe the proposed LFME framework in detail. Formally, given a long-tailed dataset D with C classes, we split the entire set of categories into L cardinality-adjacent subsets S 1 , S 2 , ..., S L using L ? 1 thresholds</p><formula xml:id="formula_4">T 1 , T 2 , ..., T L?1 such that S i = {c|T i ? N c ? T i+1 }, where N c is class c's cardinal- ity.</formula><p>Then we train L expert models on each of the cardinality-adjacent subset and denote them as M E1 , M E2 , ..., M E L . These expert models serve to 1) provide output logits' distribution for knowledge transfer 2) provide output confidence as instance difficulty cues. These information enables us to develop self-paced and curriculum learning schemes from both model level and instance level. From the perspective of self-paced expert selection, we adopt a weighted knowledge distillation between the output logits from the expert models and the student model. As the learning proceeds, the student will gradually approach the experts' performances. In such cases, we do not want the experts to limit the learning process of the student. To achieve this goal, we introduce a self-paced weighted scheme based on the performance gap on the validation set between the expert models and the student model. As the student model acquires knowledge from both data and the expert models, the importance weight of knowledge distillation will gradually decrease, and finally the unified student model is able to achieve comparable or even superior performance compared to the experts. From the perspective of curriculum instance selection, given the confidence scores from the Expert models, we re-organize the training data from easy to hard, i.e. from low-confidence to high-confidence. Then we exploit a soft weighted instance selection scheme to conduct such curriculum, so that easy samples are trained first, then harder samples are added to the training set gradually. This progressive learning curriculum has proved to be beneficial for training deep models <ref type="bibr" target="#b0">[1]</ref>. Finally, with the two levels of self-paced and curriculum learning schemes, the knowledge from the expert models will be gradually transferred to the unified student model. An overview of the LFME framework is shown in <ref type="figure">Fig 3.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Self-paced Expert Selection</head><p>Once we acquire the well-trained expert models, we feed the training data and obtain their output predictions. Then we employ knowledge distillation as an extra supervision signal to the student model. Specifically, for the expert M E l trained on l-th cardinality-adjacent subset S l , if we denote z (l) ,? to be the output logits of the current expert model and the current student model respectively, then the knowledge distillation loss for expert M E l is given by:</p><formula xml:id="formula_5">L KD l = ?H(? (z (l) ), ? (? (l) )) = ? |S l | i=1 ? (z (l) i ) log(? (? i (l) ))</formula><p>where? (l) =? c?S l is the student logits for classes in S l and</p><formula xml:id="formula_6">? (z (l) i ) = exp(z (l) i /T ) j exp(z (l) j /T ) , ? (? i (l) ) = exp(? i (l) /T ) j exp(? j (l) /T )</formula><p>are the output probabilities using Softmax with temperature T . T is usually set to be greater than 1 to increase the weight for smaller probabilities. In this way, for each expert model M E l we have its knowledge distillation loss to transfer its knowledge to the student model, and there are L losses in total, corresponding to L experts trained on L cardinality-adjacent subsets. The most straightforward way to aggregate these losses would be simply summing them up. However, this could be problematic, since as the learning process goes on, the student model's performance will gradually approach or even exceed the expert's. In such cases, we do not want the expert models become performance ceilings for the student model, and we wish to gradually weaken the guidance from the experts as the data-driven learning proceeds.</p><p>To achieve this goal, we propose a Self-paced Expert Selection scheme based on the performance gap between the student and the experts. In the experiments, we use the Top-1 Accuracy on the validation set after each training epoch as the measurement for performance gap. If we denote the Top-1 Accuracy of the student model M and the expert model M E l at epoch e to be Acc M and Acc E l respectively, then a weighting scheme is defined as follows:</p><formula xml:id="formula_7">w l = ? ? ? 1.0 if Acc M ? ?Acc E l Acc E l ? Acc M Acc E l (1 ? ?)</formula><p>if Acc M &gt; ?Acc E l and w l is updated at the end of each epoch. The weight scheduling threshold ? controls the knowledge distillation to switch from ordinary to a self-paced decaying schedule. With the self-paced weight scheduling weight w l , the knowledge transfer from the experts is automatically controlled by the student model's performance. The final knowledge distillation loss is the automatic weighted sum of knowledge distillation loss from all expert models:</p><formula xml:id="formula_8">L KD = L l=1 w l L KD l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Curriculum Instance Selection</head><p>Following the spirit of curriculum learning which mimics the human learning process, three questions need to be answered: 1) how to evaluate the difficulty of each instance, 2) how to select or unselect a sample, 3) how to design a curriculum so that samples are organized from easy to hard. For the first question, we use the expert models' output confidence for each instance as an indication for instance difficulty. Given a training instance (x i , y i ), suppose its ground-truth class y i falls into the l-th subset S l , i.e. y i ? S l , then we take the corresponding l-th expert model and use its output prediction for class y i as confidence score, denoted as p i . In this way, we can obtain the confidence score for all the instances in the training set.</p><p>For the second question, we adopt a soft selection method for instance selection. For instance (x i , y i ), we replace the cross entropy loss with a weighted version:</p><formula xml:id="formula_9">L CE = N i=1 v (k) i L CE (x i ) where v (k) i ? [0, 1]</formula><p>is the selection weight at k-th epoch. A higher value of v i (close to 1) indicates a soft selection of i-th instance, while a smaller value indicates a soft unselection of that instance.</p><p>Finally, to answer the third question, we design an automatic curriculum to determine the value of v (k) i , so that the instances are selected from easy to hard. The simplest approach is to sort the instances using their confidence score p i obtained by the expert models. However, different from traditional curriculum learning scenarios, our long-tailed classification problem involves both many-shot and low-shot categories, where low-shot instances tend to have lower confidence scores than many-shot instances. When sorted by the confidence score, the lowshot samples tend to be classified as hard examples and are not selected at first, which we do not wish to happen. To deal with such scenarios, instead of sorting across the whole training set, we sort instances according to their confidence scores within each cardinality subset. To be more specific, given the expert output confidence, v k i should be determined by three factors 1) the expert confidence p i , 2) current epoch k, 3) the cardinality-adjacent subset S l the i-th instance belongs to. Since the whole dataset is long-tailed, while we select samples from easy to hard, we also wish to select as uniform as possible across all subsets at the beginning of the training, and gradually add more hard samples as the epoch increases. In other words, at the first epoch we wish to select all the samples in the subset with lowest shots S min (i.e. classes in S min have the smallest number of samples) and same amount of samples in other subsets, and gradually add more samples until all the samples in all subsets are selected in the last epoch.</p><p>To achieve this goal, if we denote </p><formula xml:id="formula_10">N S l = 1 |S l | |S l | i=1 N i</formula><formula xml:id="formula_11">(v k i ) = (1 ? v (1) i ) e E + v (1) i where v (1) i = p i N S min N S l</formula><p>is the initial soft selection weight at epoch 1, and e, E are the current epoch and the total number of epochs respectively. It is worth noting that the scheduling function f can also be any convex or concave function as long as it is monotonically increasing within <ref type="bibr">[1, E]</ref>. The impact of choosing different f is further analyzed in the experimental section. A schematic illustration of w l and v i can be found in <ref type="figure" target="#fig_4">Fig 4.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>With the Self-paced Expert Selection and Curriculum Instance Selection, we obtain the final loss function:</p><formula xml:id="formula_12">L = N i=1 v i L CE (x i , y i ) + L l=1 N i=1 w l L KD l (M, M Exp ; x i )</formula><p>where N, L are the number of training instances and number of experts respectively, and v i , w i controls the two levels of adaptive learning schedules. In practice, we first train expert models using ordinary Instance-level Random Sampling, where each instance is sampled with equal probability. We then train the whole LFME using Class-level Random Sampling adopted in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25]</ref>, where each class is sampled with equal number of samples and probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Dataset We evaluate our proposed method on three benchmark long-tailed classification datasets: ImageNet-LT, Places-LT proposed in <ref type="bibr" target="#b30">[31]</ref> and CIFAR100-LT proposed in <ref type="bibr" target="#b1">[2]</ref>. ImageNet-LT is created by sampling a subset of ImageNet <ref type="bibr" target="#b5">[6]</ref> following the Pareto distribution with power value ? = 6. It contains 1000 categories with class cardinality ranging from 5 to 1280. Places-LT is created similarly from Places dataset <ref type="bibr" target="#b51">[52]</ref> and contains 365 categories with class cardinality ranging from 5 to 4980. CIFAR100-LT is created with exponential decay imbalance and controllable imbalance ratio.</p><p>Baselines For the first two datasets, similar to <ref type="bibr" target="#b30">[31]</ref>, our baseline methods include three re-weighting methods: Lifted Loss <ref type="bibr" target="#b34">[35]</ref>, Focal Loss <ref type="bibr" target="#b28">[29]</ref> and Range Loss <ref type="bibr" target="#b50">[51]</ref>, and one SOTA few-shot learning method FSLwF <ref type="bibr" target="#b11">[12]</ref>, as well as the recent SOTA method OLTR <ref type="bibr" target="#b30">[31]</ref>. For the CIFAR100-LT dataset, we mainly compare with the SOTA method LDAM proposed in <ref type="bibr" target="#b1">[2]</ref>. Implementation Details For the first two datasets, we choose the number of cardinality-adjacent subsets L = 3 with thresholds {20, 100} following the splits in <ref type="bibr" target="#b30">[31]</ref>. We refer to these subsets as many, medium and few-shot subsets. For CIFAR100-LT, We equally split the 100 classes into two subsets: many and few-shot. We use PyTorch <ref type="bibr" target="#b37">[38]</ref> to implement all experiments. For the first two datasets, we first train the experts using SGD for 90 epochs. Then the LFME model is trained with SGD with momentum 0.9, weight decay 0.0005 for 90 epochs, batch size 256, learning rate 0.1 and divide by 0.1 every 40 epochs. We use ResNet-10 <ref type="bibr" target="#b17">[18]</ref> training from scratch for ImageNet-LT and ImageNet pretrained Resnet-152 for Places-LT. During training, class-balanced sampling is adopted. For the CIFAR100-LT experiments, we first train the experts for 200 epochs and then train the LFME model using SGD with momentum 0.9, weight decay 2 ? 10 ?4 , batch size 128, epochs 200, initial learning rate 0.1 and decay by 0.01 at 160 and 180 epochs, as well as deferred class-balanced sampling, same as <ref type="bibr" target="#b1">[2]</ref>. The backbone network is ResNet-32. The distillation temperature T is set to 2, and the expert weight scheduling threshold ? is set to 0.6 during the experiments.  <ref type="table" target="#tab_1">Table 2</ref> shows the long-tailed classification results on ImageNet-LT and Places-LT dataset. As can be found that our method is able to achieve superior or at least comparable results to the state-of-the-art methods such as OLTR. We found that many-shot categories benefit most from our LFME framework, while few-shot classes also demonstrate improvements and perform similarly with the re-weighting methods. Moreover, we also demonstrate that our method can be easily incorporated with other state-of-the-art methods, and we show the result of LFME+Focal Loss and LFME+OLTR (where LFME is added in the second stage of OLTR). We observe that both methods benefit from our expert model on all three subsets, and the combination of our method and OLTR outperforms previous methods by a large margin. It is also worth noting that our expert models are trained using vanilla CNNs, and utilizing other techniques will further lead to superior expert models, and assumably, superior student model. To further demonstrate the statistical significance of the proposed method, we conduct experiments on CIFAR100-LT with imbalance ratio 100. The results in <ref type="table" target="#tab_2">Table 3</ref> show that LFME is able to achieve comparable performances with the SOTA method LDAM <ref type="bibr" target="#b1">[2]</ref> and combining them will further improve LDAM on both many and few-shot subsets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Each Component</head><p>We evaluate each part of our method and the result is shown in <ref type="table" target="#tab_3">Table 5</ref>. We compare with the following variants: 1) Instance-level Random Sampling (Ins.Samp.), where each instance is sampled with equal probability. 2) Instancelevel Random Sampling + Ordinary Knowledge distillation (Ins.Samp.+KD), where non-self-paced version knowledge distillation from the experts is added, i.e. w l = 1.0. 3) Class-level Random Sampling (Cls.Samp.), where each class is sampled with equal number of samples and probability. 4) Class-level Random Sampling + Ordinary Knowledge Distillation (Cls.Samp.+KD). 5) Classlevel Random Sampling + Knowledge Distillation + Self-paced Expert Selection (Cls.Samp.+ KD + SpES). 6) Curriculum Instance Selection + Self-paced Expert Selection (CurIS+KD+SpES), which constitute our LFME framework. From the results, we come up with the following observations: first, compared to the instance-random sampling, the adopted class-level random sampling is able to largely improve the few-shot performance while also decrease the manyshot performance slightly, since it samples more few-shot and less many-shot instances. Second, the introduction of knowledge distillation from experts can significantly improve the results, as it brings 11.3% and 3.2% to the Instance-level Sampling and Class-level Sampling baselines. However, while knowledge distillation brings improvements for many-shot classes, it will also decrease the few-shot accuracy slightly. Third, the Self-paced Expert Selection improves the knowledge distillation on all three subsets. It removes the performance ceiling brought from the experts and allows the student to exceed the experts. As the results show that SpES brings 0.4% and 1.3% overall performance gain respectively. Finally, the proposed Curriculum Instance Selection further improves on the few-shot categories with 1.0% in accuracy, so that the decrease on few-shot subset caused by the knowledge distillation is compensated.  Visualization of Self-paced Expert Selection Self-paced expert selection plays an important role in LFME for more efficient and effective knowledge transfer. <ref type="figure" target="#fig_6">Fig 5 (a)</ref>-(c) gives a visualization of the expert selection weights w l for many-shot, medium-shot, few-shot model. From the visualization, we observe that w l serves to automatically control the knowledge transfer, as for manyshot and medium-shot experts the knowledge is consistently distilled, while for few-shot experts, the student instantly exceeds the expert's performance, thus leading to a decay in w f ewshot . Moreover, we also visualize the impact of Selfpaced Expert Selection in terms of cross-entropy loss curves, shown in <ref type="figure" target="#fig_6">Fig 5 (d)</ref>, and we find that it leads to a lower cross-entropy loss, which also verifies the effectiveness of the proposed Self-paced Expert Selection. Effect of Learning Scheduler We also discuss the impact of different learning schedules for v i as shown in <ref type="table">Table 4</ref>. Given the initial instance confidence v <ref type="bibr" target="#b0">(1)</ref> , we test with the following scheduling functions:  The result shows that the linear growing function yields the best result, while the concave the convex function f also produce similar performances. The convex function yields the worst performances as it selects fewest instances at the start of the training which may not be beneficial for the training dynamics. Sensitivity Analysis of Hyperparameter ? <ref type="figure" target="#fig_7">Fig. 6(a)</ref> shows the sensitivity analysis of expert weight scheduling threshold ?. From the result, we observe that our model is robust to most ? values. When ? grows to 1.0, the Selfpaced Expert Selection becomes ordinary knowledge distillation, and result in a performance decline.</p><formula xml:id="formula_13">-f Linear = (1 ? v (1) ) e E + v (1) -f Convex = 1 ? (1 ? v (1) ) cos( e E ? 2 ) -f Concave = (1 ? v (1) )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Classification Weights</head><p>We visualize the classification weights of vanilla CNN and our LFME via T-SNE in <ref type="figure" target="#fig_7">Fig 6(b)</ref>. The results show that our method results in a more structured, compact feature manifold. It shows that without particular re-weighting, our method is also able to produce discriminative feature space and classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose a Learning From Multiple Experts framework for longtailed classification problem. By introducing the idea of cardinality-adjacent subset which is less long-tailed, we train several expert models and propose two levels of adaptive learning to distill the knowledge from the expert models to a unified student model. From the extensive experiments and visualizations, we verify the effectiveness of our proposed method as well as each of its component, and show that the LFME framework is able to achieve state-of-the-art performances on the long-tailed classification benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Schematic illustration of our proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison of CNN performances trained on cardinality-adjacent subsets and the entire dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 ?Fig. 3 .</head><label>13</label><figDesc>Overview of the LFME framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>as the average shot (average number of samples per class) in subset S l , then v (k) i is determined by p i N S min N S l at epoch 1, and grows gradually to 1 at the last epoch. Then at epoch 1, each subset softly selects its N Smin easiest samples, and harder samples are gradually softly added to the training process. Formally, we use a monotonically increasing function f as scheduling function, so that v (k) i will gradually grow from p i N S min N S l to 1. For simplicity, we choose the linear function in the experiments and f is defined as f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Weight scheduling function for (a) model level selection w l , and (b) instance level selection vi.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>(a)-(c): Visualization of self-paced expert selection scheduler w l for many-shot, medium-shot, few-shot expert model. (d): Loss curves before and after adding Selfpaced Expert Selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>(a) Sensitivity analysis of ?. (b) T-SNE visualization of classification weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of entire distribution and subsets under four metrics. Larger values indicate more longtailness.</figDesc><table><row><cell>Metric</cell><cell cols="2">Accuracy IRatio IKL I Abs IGini</cell></row><row><cell>Entire</cell><cell></cell><cell>256.0 0.707 0.769 0.524</cell></row><row><cell cols="2">Many-shot</cell><cell>12.8 0.278 0.481 0.322</cell></row><row><cell cols="2">Medium-shot</cell><cell>4.7 0.122 0.356 0.235</cell></row><row><cell>Low-shot</cell><cell></cell><cell>4.0 0.099 0.320 0.209</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Long-tailed classification results on ImageNet-LT and Place-LT. * denotes reproduced results, other results are from<ref type="bibr" target="#b30">[31]</ref>.</figDesc><table><row><cell>Acc.</cell><cell></cell><cell cols="2">ImageNet-LT</cell><cell></cell><cell></cell><cell cols="2">Places-LT</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Many Med. Few</cell><cell>All</cell><cell cols="3">Many Med. Few</cell><cell>All</cell></row><row><cell>Plain Model</cell><cell>40.9</cell><cell>10.7</cell><cell>0.4</cell><cell>20.9</cell><cell>45.9</cell><cell>22.4</cell><cell>0.36</cell><cell>27.2</cell></row><row><cell>Lifted Loss[35]</cell><cell>35.8</cell><cell>30.4</cell><cell>17.9</cell><cell>30.8</cell><cell>41.1</cell><cell>35.4</cell><cell>24</cell><cell>35.2</cell></row><row><cell>Focal Loss[29]</cell><cell>36.4</cell><cell>29.9</cell><cell>16.0</cell><cell>30.5</cell><cell>41.1</cell><cell>34.8</cell><cell>22.4</cell><cell>34.6</cell></row><row><cell>Range Loss[51]</cell><cell>35.8</cell><cell>30.3</cell><cell>17.6</cell><cell>30.7</cell><cell>41.1</cell><cell>35.4</cell><cell>23.2</cell><cell>35.1</cell></row><row><cell>FSLwF [12]</cell><cell>40.9</cell><cell>22.1</cell><cell>15.0</cell><cell>28.4</cell><cell>43.9</cell><cell>29.9</cell><cell>29.5</cell><cell>34.9</cell></row><row><cell>OLTR [31]</cell><cell>43.2</cell><cell>35.1</cell><cell>18.5</cell><cell>35.6</cell><cell>44.7</cell><cell>37.0</cell><cell>25.3</cell><cell>35.9</cell></row><row><cell>OLTR [31]*</cell><cell>40.7</cell><cell>33.2</cell><cell>17.4</cell><cell>33.8</cell><cell>42.2</cell><cell>38.1</cell><cell>17.8</cell><cell>35.3</cell></row><row><cell>Ours</cell><cell>47.1</cell><cell>35.0</cell><cell>17.5</cell><cell>37.2</cell><cell>38.4</cell><cell>39.1</cell><cell>21.7</cell><cell>35.2</cell></row><row><cell>Ours + Focal Loss</cell><cell>46.7</cell><cell>35.8</cell><cell>17.3</cell><cell>37.3</cell><cell>37.0</cell><cell>39.6</cell><cell>23.0</cell><cell>35.2</cell></row><row><cell>Ours + OLTR</cell><cell>47.0</cell><cell cols="3">37.9 19.2 38.8</cell><cell>39.3</cell><cell>39.6</cell><cell>24.2</cell><cell>36.2</cell></row><row><cell cols="8">5.2 Main Results on Long-tailed Classification Benchmarks</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on CIFAR100-LT.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Table 4. Effect of different scheduling func-</cell></row><row><cell></cell><cell></cell><cell>tions.</cell><cell></cell></row><row><cell>Methods</cell><cell>Many Few All</cell><cell></cell><cell></cell></row><row><cell>Plain CNN</cell><cell>59.0 18.2 38.6</cell><cell cols="2">Schedule Many Medium Few All</cell></row><row><cell>Ours</cell><cell>59.0 25.5 42.3</cell><cell>Linear 47.1</cell><cell>35.0 17.5 37.2</cell></row><row><cell>LDAM [2]</cell><cell>58.8 26.1 42.4</cell><cell>Convex 47.2</cell><cell>34.6 16.7 36.9</cell></row><row><cell cols="2">Ours+LDAM 59.5 28.0 43.8</cell><cell>Concave 47.5</cell><cell>34.7 17.0 37.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Effectiveness of each component.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Many Med. Few All</cell></row><row><cell>Ins.Samp.</cell><cell></cell><cell>40.9 10.7 0.4 20.9</cell></row><row><cell cols="2">Ins.Samp.+KD</cell><cell>55.7 22.2 0.02 32.2</cell></row><row><cell cols="2">Cls.Samp.</cell><cell>38.8 32.3 17.0 32.6</cell></row><row><cell cols="2">Cls.Samp.+KD</cell><cell>44.9 34.5 15.8 35.8</cell></row><row><cell cols="3">Cls.Samp.+KD+SpES 46.6 35.8 16.5 37.1</cell></row><row><cell cols="2">CurIS+KD+SpES</cell><cell>47.1 35.0 17.5 37.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by the National Natural Science Foundation of China (No.U1936202, 61925107). We also thank anonymous reviewers for their constructive comments. Corresponding author: Guiguang Ding.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07413</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Evolutionary data measures: Understanding the difficulty of text classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rozanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01910</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decode: Deep confidence network for robust image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3752" to="3765" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class rectification hard mining for imbalanced deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on learning from imbalanced datasets II</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Born-again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1602" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated curriculum learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1311" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zero-shot learning with transferred samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3277" to="3290" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep imbalanced learning for face recognition and attribute prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<title level="m">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Classification of imbalanced data by combining the complementary neural network and smote algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jeatrakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05055</idno>
		<title level="m">Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09217</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3573" to="3587" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03643</idno>
		<title level="m">Unifying distillation and privileged information</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Curriculum learning in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narvekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="5195" to="5196" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Autonomous task sequencing for customized curriculum design in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narvekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sinapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="2536" to="2542" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Imbalance problems in object detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00169</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="864" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2001" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09050</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Author identification: Using text sampling to handle the class imbalance problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="790" to="799" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic curriculum graph generation for reinforcement learning agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Svetlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leonetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sinapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inverse random under sampling for class imbalance problem and its application to multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Tahir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3738" to="3750" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Incremental few-shot learning for pedestrian attribute recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3912" to="3918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adaptive region embedding for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7314" to="7321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
