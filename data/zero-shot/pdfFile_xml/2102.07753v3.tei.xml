<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Intra-Batch Connections for Deep Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Seidenschwarz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Elezi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
						</author>
						<title level="a" type="main">Learning Intra-Batch Connections for Deep Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of metric learning is to learn a function that maps samples to a lower-dimensional space where similar samples lie closer than dissimilar ones. Particularly, deep metric learning utilizes neural networks to learn such a mapping. Most approaches rely on losses that only take the relations between pairs or triplets of samples into account, which either belong to the same class or two different classes. However, these methods do not explore the embedding space in its entirety. To this end, we propose an approach based on message passing networks that takes all the relations in a mini-batch into account. We refine embedding vectors by exchanging messages among all samples in a given batch allowing the training process to be aware of its overall structure. Since not all samples are equally important to predict a decision boundary, we use an attention mechanism during message passing to allow samples to weigh the importance of each neighbor accordingly. We achieve state-of-the-art results on clustering and image retrieval on the CUB-200-2011, Cars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate further research, we make available the code and the models at https: //github.com/dvl-tum/intra_batch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Metric learning is a widely popular technique that constructs task-specific distance metrics by learning the similarity or dissimilarity between samples. It is often used for object retrieval and clustering by training a deep neural network to learn a mapping function from the original samples into a new, more compact, embedding space. In that embedding space, samples coming from the same class should be closer than samples coming from different classes.</p><p>Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).</p><p>To learn the mapping function, current approaches utilize siamese networks <ref type="bibr" target="#b6">(Bromley et al., 1994)</ref>, typically trained using loss functions that measure distances between pairs of samples of the same class (positive) or different classes (negative). Contrastive loss <ref type="bibr" target="#b6">(Bromley et al., 1994)</ref> minimizes the distance of the feature embeddings for a positive pair, and maximizes their distance otherwise. Triplet loss <ref type="bibr" target="#b55">(Schultz &amp; Joachims, 2003;</ref><ref type="bibr" target="#b67">Weinberger &amp; Saul, 2009</ref>) takes a triplet of images and pushes the embedding distance between an anchor and a positive sample to be smaller than the distance between the same anchor and a negative sample by a given margin. While the number of possible image pairs and triplets in a dataset of size n is O(n 2 ) and O(n 3 ), respectively, the vast majority of these pairs (or triplets) are not informative and do not contribute to the loss. This leads to slow convergence and possible overfitting when the pairs (triplets) are not appropriately sampled. Perhaps more worryingly, because these losses are focused on pairs (triplets), they are unable to consider the global structure of the dataset resulting in lower clustering and retrieval performance. To compensate for these drawbacks, several works resort to training tricks like intelligent sampling <ref type="bibr" target="#b15">(Ge et al., 2018;</ref><ref type="bibr" target="#b34">Manmatha et al., 2017)</ref>, multi-task learning , or hard-negative mining <ref type="bibr" target="#b54">(Schroff et al., 2015;</ref><ref type="bibr" target="#b71">Xuan et al., 2020a)</ref>. Recently, researchers started exploring the global structure of the embedding space by utilizing rank-based <ref type="bibr" target="#b8">(? akir et al., 2019;</ref><ref type="bibr" target="#b20">He et al., 2018a;</ref><ref type="bibr" target="#b50">Revaud et al., 2019)</ref> or contextual classification loss functions <ref type="bibr" target="#b8">(? akir et al., 2019;</ref><ref type="bibr">Elezi et al., 2020;</ref><ref type="bibr" target="#b20">He et al., 2018a;</ref><ref type="bibr" target="#b50">Revaud et al., 2019;</ref><ref type="bibr" target="#b57">Sohn, 2016;</ref><ref type="bibr" target="#b58">Song et al., 2016;</ref><ref type="bibr" target="#b77">Zheng et al., 2019)</ref>. The Group Loss <ref type="bibr">(Elezi et al., 2020)</ref> explicitly considers the global structure of a mini-batch and refines class membership scores based on feature similarity. However, the global structure is captured using a handcrafted rule instead of learning, hence its refinement procedure cannot be adapted depending on the samples in the mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions</head><p>In this work, we propose a fully learnable module that takes the global structure into account by refining the embedding feature vector of each sample based on all intra-batch relations. To do so, we utilize message passing networks (MPNs) <ref type="bibr" target="#b16">(Gilmer et al., 2017)</ref>. MPNs allow the samples in a mini-batch to communicate with each other, and to arXiv:2102.07753v3 [cs.CV] 11 Jun 2021</p><p>Learning Intra-Batch Connections for Deep Metric Learning <ref type="figure">Figure 1</ref>. Overview of our proposed approach. Given a mini-batch consisting of N classes, each of them having P images, we initialize the embedding vectors using a backbone CNN. We then construct a fully connected graph that refines their initial embeddings by performing K message-passing steps. After each step, the embeddings of the images coming from the same class become more similar to each other and more dissimilar to the embeddings coming from images that belong to different classes. Finally, we apply Cross-Entropy loss and we backpropagate the gradients to update the network. refine their feature representation based on the information taken from their neighbors. More precisely, we use a convolutional neural network (CNN) to generate feature embeddings. We then construct a fully connected graph where each node is represented by the embedding of its corresponding sample. In this graph, a series of message passing steps are performed to update the node embeddings. Not all samples are equally important to predict decision boundaries, hence, we allow each sample to weigh the importance of neighboring samples by using a dot-product self-attention mechanism to compute aggregation weights for the message passing steps.</p><p>To draw a parallelism with the triplet loss, our MPN formulation would allow samples to choose their own triplets which are best to make a prediction on the decision boundary. Unlike the triplet loss though, we are not limited to triplets, as each sample can choose to attend over all other samples in the mini-batch. By training the CNN and MPN in an end-to-end manner, we can directly use our CNN backbone embeddings during inference to perform image retrieval and clustering. While this reaches state-of-the-art results without adding any computational overhead, we also show how to further boost the performance by using the trained MPN at test time, constructing the batches based on k-reciprocal nearest neighbor sampling <ref type="bibr" target="#b78">(Zhong et al., 2017)</ref>.</p><p>Our contribution in this work is three-fold:</p><p>? We propose an approach for deep metric learning that computes sample embeddings by taking into account all intra-batch relations. By leveraging message passing networks, our method can be trained end-to-end.</p><p>? We perform a comprehensive robustness analysis showing the stability of our module with respect to the choice of hyperparameters.</p><p>? We present state-of-the-art results on CUB-200-2011 <ref type="bibr" target="#b63">(Wah et al., 2011)</ref>, Cars196 <ref type="bibr" target="#b28">(Krause et al., 2013)</ref>, Stanford online Products <ref type="bibr" target="#b58">(Song et al., 2016)</ref> and In-Shop Clothes <ref type="bibr" target="#b32">(Liu et al., 2016)</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Metric Learning Losses. Siamese neural networks were first proposed for representation learning in <ref type="bibr" target="#b6">(Bromley et al., 1994)</ref>. The main idea is to use a CNN to extract a feature representation from an image and using that representation, or embedding, to compare it to other images. In <ref type="bibr" target="#b10">(Chopra et al., 2005)</ref>, the contrastive loss was introduced to train such a network for face verification. The loss minimizes the distance between the embeddings of image pairs coming from the same class and maximizes the distance between image pairs coming from different classes. In parallel, researchers working on convex optimization developed the triplet loss <ref type="bibr" target="#b55">(Schultz &amp; Joachims, 2003;</ref><ref type="bibr" target="#b67">Weinberger &amp; Saul, 2009</ref>) which was later combined with the expressive power of CNNs, further improving the solutions on face verification <ref type="bibr" target="#b54">(Schroff et al., 2015)</ref>. Triplet loss extends contrastive loss by using a triplet of samples consisting of an anchor, a positive, and a negative sample, where the loss is defined to make the distance between the anchor and the positive smaller than the distance between the anchor and the negative, up to a margin. The concept was later generalized to N-Pair loss <ref type="bibr" target="#b57">(Sohn, 2016)</ref>, where an anchor and a positive sample are compared to N ? 1 negative samples at the same time. In recent years, different approaches based on optimizing other qualities than the distance, such as clustering <ref type="bibr" target="#b29">(Law et al., 2017;</ref><ref type="bibr" target="#b35">McDaid et al., 2011)</ref> or angular distance <ref type="bibr" target="#b64">(Wang et al., 2017)</ref>, have shown to reach good results.</p><p>Sampling and Ensembles. Since computing the loss of all possible triplets is computationally infeasible even for moderately-sized datasets and, furthermore, based on the knowledge that the majority of them are not informative <ref type="bibr" target="#b54">(Schroff et al., 2015)</ref>, more researchers have given attention to intelligent sampling. The work of <ref type="bibr" target="#b34">(Manmatha et al., 2017)</ref> showed conclusive evidence that the design of smart sampling strategies is as important as the design of efficient loss functions. In <ref type="bibr" target="#b15">(Ge et al., 2018)</ref>, the authors propose a hierarchical version of triplet loss that embeds the sampling during the training process. More recent techniques continue this line of research by developing new sampling strategies <ref type="bibr" target="#b12">(Duan et al., 2019;</ref><ref type="bibr" target="#b71">Xuan et al., 2020a;</ref> while others introduce new loss functions <ref type="bibr" target="#b65">(Wang et al., 2019a;</ref><ref type="bibr" target="#b69">Xu et al., 2019)</ref>. In parallel, other researchers investigated the usage of ensembles for deep metric learning, unsurprisingly finding out that ensembles outperform single networks trained on the same loss <ref type="bibr" target="#b26">(Kim et al., 2018;</ref><ref type="bibr" target="#b44">Opitz et al., 2017;</ref><ref type="bibr" target="#b53">Sanakoyeu et al., 2019;</ref><ref type="bibr" target="#b70">Xuan et al., 2018;</ref><ref type="bibr" target="#b73">Yuan et al., 2017)</ref>.</p><p>Global Metric Learning Losses. Most of the mentioned losses do not consider the global structure of the mini-batch. The work of (Movshovitz-Attias et al., 2017) proposes to optimize the triplet loss on a space of triplets different from the one of the original samples, consisting of an anchor data point and similar and dissimilar learned proxy data points. These proxies approximate the original data points so that a triplet loss over the proxies is a tight upper bound of the loss over the original samples. The introduction of proxies adds additional contextual knowledge that shows to significantly improve triplet loss. The results of this approach were significantly improved by using training tricks <ref type="bibr">(Teh et al., 2020)</ref> or generalizing the concept of proxy triplets to multiple proxy anchors <ref type="bibr">(Kim et al., 2020;</ref><ref type="bibr">Zhu et al., 2020)</ref>.</p><p>In  the authors generate negative samples in an adversarial manner, while in  a deep variational metric learning framework was proposed to explicitly model the intra-class variance and disentangle the intra-class invariance. In the work of <ref type="bibr" target="#b66">(Wang et al., 2019b)</ref>, a non-proxy contextual loss function was developed. The authors propose a loss function based on a ranking distance that considers all the samples in the mini-batch</p><p>Classification Losses for Metric Learning. A recent line of work <ref type="bibr" target="#b74">(Zhai &amp; Wu, 2019;</ref><ref type="bibr" target="#b77">Zheng et al., 2019)</ref> is showing that a carefully designed classification loss function can rival, if not outperform, triplet-based functions in metric learning. This has already been shown for multiple tasks such as hashing (binary-embedding) <ref type="bibr" target="#b20">(He et al., 2018a)</ref>, landmark detection <ref type="bibr" target="#b21">(He et al., 2018b;</ref><ref type="bibr" target="#b50">Revaud et al., 2019)</ref>, few-shot learning <ref type="bibr">(? akir et al., 2019)</ref>, and person re-identification <ref type="bibr" target="#b0">(Alemu et al., 2019;</ref><ref type="bibr" target="#b76">Zhao et al., 2019)</ref>. In metric learning, SoftTriple loss <ref type="bibr" target="#b49">(Qian et al., 2019</ref>) develops a classification loss where each class is represented by K centers. In the same classification spirit, the Group Loss <ref type="bibr">(Elezi et al., 2020)</ref> replaces the softmax function with a contextual module that considers all the samples in the mini-batch at the same time.</p><p>Message Passing Networks. Recent works on message passing networks <ref type="bibr" target="#b16">(Gilmer et al., 2017)</ref> and graph neural networks <ref type="bibr" target="#b2">(Battaglia et al., 2018;</ref><ref type="bibr" target="#b27">Kipf &amp; Welling, 2017)</ref> have been successfully applied to problems such as human action recognition <ref type="bibr" target="#b18">(Guo et al., 2018)</ref>, visual question answering <ref type="bibr" target="#b43">(Narasimhan et al., 2018)</ref> or tracking <ref type="bibr">(Bras? &amp; Leal-Taix?, 2020)</ref>. Given a graph with some initial features for nodes and edges, the main idea behind these models is to embed nodes and edges into representations that take into account not only the node's own features but also those of its neighbors in the graph, as well as the graphs overall topology. The attention-based Transformers <ref type="bibr" target="#b61">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b68">Xu et al., 2015)</ref>, which can be seen as message passing networks, have revolutionized the field of natural language processing, and within the computer vision, have shown impressive results in object detection <ref type="bibr">(Carion et al., 2020)</ref>.</p><p>Closely related to message passing networks, <ref type="bibr">(Elezi et al., 2020)</ref> considered contextual information for metric learning based on the similarity (dissimilarity) between samples coming from the same class (respectively from different classes). However, they use a handcrafted rule as part of their loss function that only considers the label preferences <ref type="bibr" target="#b13">(Elezi et al., 2018)</ref>. In contrast, based on message passing networks, we develop a novel learnable model, where each sample uses learned attention scores to choose the importance of its neighbors, and based on this information, refines its own feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The goal of the message passing steps is to exchange information between all samples in the mini-batch and to refine the feature embeddings accordingly. Note that this approach is very different from label-propagation methods as used in <ref type="bibr">(Elezi et al., 2020)</ref>, where samples exchange information only on their label preferences, information which only implicitly affects the choice of their final feature vectors.</p><p>In our proposed method, each sample exchanges messages with all the other samples in the mini-batch, regardless of whether the samples belong to the same class or not. In this way, our method considers both the intra-class and inter-class relations between all samples in the mini-batch, allowing our network to receive information about the overall structure of the mini-batch. We can use cross-entropy loss to train our network since the information of the mini-batch is already contained in the refined individual feature embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>In <ref type="figure">Figure 1</ref>, we show an overview of our proposed approach. We compute feature vectors for each sample as follows:</p><p>1. Generate initial embedding feature vectors using a CNN and construct a fully connected graph, where each node represents a sample in the mini-batch.</p><p>2. Perform message-passing between nodes to refine the initial embedding feature vectors by utilizing dotproduct self-attention.</p><p>3. Perform classification and optimize both the MPN and the backbone CNN in an end-to-end fashion using cross-entropy loss on the refined node feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Initialization and Graph Construction</head><p>The global structure of the embedding space is modeled by a graph G = (V, E), where V represents the nodes, i.e., all images in the training dataset, and E the edges connecting them. An edge represents the importance of one image to the other, expressed, for example, by their similarity. During training, we would ideally take the graph of the whole dataset into account, but this is computationally infeasible. Therefore, we construct mini-batches consisting of n randomly sampled classes with p randomly chosen samples per class. Each sample in the mini-batch is regarded as a node in a mini-batch graph G B = (V B , E B ). Unlike CNNs that perform well on data with an underlying grid-like or Euclidean structure <ref type="bibr" target="#b7">(Bronstein et al., 2017)</ref>, graphs have a noneuclidean structure. Thus, to fully explore the graph-like structure, we model the mini-batch relations using MPNs.</p><p>More precisely, we use a backbone CNN to compute the initial embeddings f ? R d for all samples in a mini-batch, where d is their embedding dimension. To leverage all relations in the batch, we utilize a fully connected graph, where every node with initial node features h 0 i = f is connected to all the other nodes in the graph (see <ref type="figure">Figure 2</ref> in the upper left corner).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Message Passing Network</head><p>In order to refine the initial feature vectors based on the contextual information of the mini-batch, we use message passing to exchange information between single nodes, i.e., between samples of the mini-batch. To this end, we utilize MPNs with graph attention <ref type="bibr" target="#b62">(Velickovic et al., 2018)</ref> for deep metric learning. It should be noted that the following formulation is equivalent to the Transformers architecture <ref type="bibr" target="#b61">(Vaswani et al., 2017)</ref>, which can be seen as a fully connected graph attention network <ref type="bibr" target="#b62">(Velickovic et al., 2018)</ref>.</p><p>Passing Messages. We apply L message passing steps successively. In each step, we pass messages between all samples in a batch and obtain updated features h l+1 i of node i at message passing step l + 1 by aggregating the features h l j of all neighbouring nodes j ? N i at message passing step l:</p><formula xml:id="formula_0">h l+1 i = j?Ni W l h l j (1)</formula><p>where W l is the corresponding weight matrix of message passing step l. As we construct a fully connected graph, the neighboring nodes N i consist of all nodes in the given batch, thus each feature representation of an image is affected by all the other images in the mini-batch.</p><p>Attention Weights on the Messages. Not all samples of a mini-batch are equally informative to predict the decision boundaries between classes. Hence, we add an attention score ? to every message passing step (see <ref type="figure">Figure 2</ref> on Message Passing) to allow each sample to weigh the importance of the other samples in the mini-batch:</p><formula xml:id="formula_1">h l+1 i = j?Ni ? l ij W l h l j (2)</formula><p>where ? ij is the attention score between node i and node j. We utilize dot-product self-attention to compute the attention scores, leading to ? ij at step l defined as:</p><formula xml:id="formula_2">? l ij = W l q h l i (W l k h l j ) T ? d<label>(3)</label></formula><p>where W l q is the weight matrix corresponding to the receiving node and W l k is the weight matrix corresponding to the sending node on message passing step l. Furthermore, we apply the softmax function to all in-going attention scores (edges) of a given node i. To allow the MPN to learn a diverse set of attention scores, we apply M dot product self-attention heads in every message passing step and concatenate their results. To this end, instead of using single weight matrices W l q , W l k and W l , we now use different</p><formula xml:id="formula_3">weight matrices W l,m q ? R d M ?d , W l,m k ? R d M ?d and W l,m ? R d M ?d for each attention head: h l+1 i = cat( j?Ni ? l,1 ij W l,1 h l j , ..., j?Ni ? l,M ij W l,M h l j )</formula><p>(4) where cat represents the concatenation.</p><p>Note, by using the attention-head specific weight matrices, we reduce the dimension of all embeddings h l j by 1 M so that when we concatenate the embeddings generated by all attention heads the resulting embedding h l+1 i has the same dimension as the input embedding h l i . <ref type="figure">Figure 2</ref>. Left: To update the feature vectors in a message-passing step we first construct a fully connected graph and compute attention scores between all samples in a batch. We then pass messages between nodes and weigh them with the corresponding attention scores.</p><p>During the aggregation step, we sum the weighted messages to get updated node features. Right: Visualization of development of attention scores and feature vectors over two steps of message passing steps showing that feature vectors, as well as attention scores between samples from the same class, get more and more similar.</p><p>Adding Skip Connections. We add a skip connection around the attention block <ref type="bibr" target="#b19">(He et al., 2016)</ref> and apply layer normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> given by:</p><formula xml:id="formula_4">f (h l+1 i ) = LayerN orm(h l+1 i + h l i )<label>(5)</label></formula><p>where h l+1 i is the outcome of Equation 4. We then apply two fully connected layers, followed by another skip connection <ref type="bibr" target="#b19">(He et al., 2016)</ref> and a layer normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref>:</p><formula xml:id="formula_5">g(h l+1 i ) = LayerN orm(F F (f (h l+1 i )) + f (h l+1 i )) (6)</formula><p>where F F represents the two linear layers. Finally, we pass the outcome of Equation 6 to the next message passing step. For illustrative purposes, in <ref type="figure">Figure 2</ref>, we show how the attention scores and the feature vectors evolve over the message passing steps. As can be seen, the feature vectors of samples of the same class become more and more similar. Similar to <ref type="bibr" target="#b62">(Velickovic et al., 2018)</ref>, we indirectly address oversmoothing by applying node-wise attention scores ? i,j (Equation 3) during the feature aggregation step <ref type="bibr">(Min et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization</head><p>We apply a fully connected layer on the refined features after the last message passing step and then use cross-entropy loss. Even if cross-entropy loss itself does not take into account the relations between different samples, this information is already present in the refined embeddings, thanks to the message passing steps. As the MPN takes its initial feature vectors from the backbone CNN, we add an auxiliary cross-entropy loss to the backbone CNN, to ensure a sufficiently discriminative initialization. This loss is also needed since at test time we do not use the MPN, as described below. Both loss functions utilize label smoothing and low temperature scaling <ref type="bibr">(Teh et al., 2020;</ref><ref type="bibr" target="#b74">Zhai &amp; Wu, 2019)</ref> to ensure generalized, but discriminative, decision boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Inference</head><p>One disadvantage of using the MPN during inference is that in order to generate an embedding vector for a sample, we need to create a batch of samples to perform message passing as we do during the training. However, using the MPN during inference would be unfair to other methods that directly perform retrieval on the CNN embedding since we would be adding parameters, hence, expressive power, to the model. Therefore, we perform all experiments by directly using the embedding feature vectors of the backbone CNN unless stated differently. The intuition is that when optimizing the CNN and MPN together in an end-to-end fashion, the CNN features will have also improved with the information of sample relations. In the ablation studies, we show how the performance can be further improved with a simple batch construction strategy at test time. For more discussion on using MPN at test time, we refer the reader to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we compare our proposed approach to stateof-the-art deep metric learning approaches on four public benchmarks. To underline the effectiveness of our approach, we further present an extensive ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We implement our method in PyTorch <ref type="bibr" target="#b48">(Paszke et al., 2017)</ref> library. Following other works <ref type="bibr" target="#b5">(Brattoli et al., 2019;</ref><ref type="bibr" target="#b8">? akir et al., 2019;</ref><ref type="bibr" target="#b34">Manmatha et al., 2017;</ref><ref type="bibr" target="#b53">Sanakoyeu et al., 2019;</ref><ref type="bibr">Teh et al., 2020;</ref><ref type="bibr" target="#b72">Xuan et al., 2020b;</ref><ref type="bibr" target="#b74">Zhai &amp; Wu, 2019)</ref>, we present results using ResNet50 <ref type="bibr" target="#b19">(He et al., 2016)</ref> pretrained on ILSVRC 2012-CLS dataset <ref type="bibr" target="#b52">(Russakovsky et al., 2015)</ref> as backbone CNN. Like the majority of recent methods <ref type="bibr" target="#b15">(Ge et al., 2018;</ref><ref type="bibr">Kim et al., 2020;</ref><ref type="bibr" target="#b46">Park et al., 2019a;</ref><ref type="bibr" target="#b49">Qian et al., 2019;</ref><ref type="bibr" target="#b65">Wang et al., 2019a;</ref><ref type="bibr">Zhu et al., 2020)</ref>, we use embedding dimension of sizes 512 for all our experiments and low temperature scaling for the softmax cross-entropy loss function <ref type="bibr" target="#b17">(Guo et al., 2017)</ref>. Furthermore, we preprocess the images following <ref type="bibr">(Kim et al., 2020)</ref>. We resize the cropped image to 227?227, followed by applying a random horizontal flip. During test time, we resize the images to 256 ? 256 and take a center crop of size 227 ? 227. We train all networks for 70 epochs using RAdam optimizer <ref type="bibr">(Liu et al., 2020)</ref>. To find all hyperparameters we perform random search <ref type="bibr" target="#b3">(Bergstra &amp; Bengio, 2012)</ref>. For mini-batch construction, we first randomly sample a given number of classes, followed by randomly sampling a given number of images for each class as commonly done in metric learning <ref type="bibr">(Elezi et al., 2020;</ref><ref type="bibr" target="#b54">Schroff et al., 2015;</ref><ref type="bibr">Teh et al., 2020;</ref><ref type="bibr" target="#b74">Zhai &amp; Wu, 2019)</ref>. We use small mini-batches of size 50-100 and provide an analysis on different numbers of classes and samples on CUB-200-2011 and Cars196 in the supplementary. Our forward pass takes 73% of time for the backbone and the remaining for the MPN. All the training is done in a single TitanX GPU, i.e., the method is memory efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmark Datasets and Evaluation Metrics</head><p>Datasets:</p><p>We conduct experiments on 4 publicly available datasets using the conventional splitting protocols <ref type="bibr" target="#b58">(Song et al., 2016)</ref>:</p><p>? CUB-200-2011 <ref type="bibr" target="#b63">(Wah et al., 2011)</ref> consists of 200 classes of birds with each class containing 58 images on average. For training, we use the first 100 classes and for testing the remaining classes.</p><p>? Cars196 <ref type="bibr" target="#b28">(Krause et al., 2013)</ref> contains 196 classes representing different cars with each class containing on average 82 images. We use the first 98 classes for training and the remaining classes for testing.</p><p>? Stanford Online Products (SOP) <ref type="bibr" target="#b58">(Song et al., 2016)</ref> consists of 22,634 classes (5 images per class on average) of product images from ebay. We use 11, 318 classes for training and the remaining 11, 316 classes for testing.</p><p>? In-Shop Clothes <ref type="bibr" target="#b32">(Liu et al., 2016)</ref> contains 7, 982 classes of clothing items, with each class having 4 images on average. We use 3, 997 classes for training, while the test set, containing 3, 985 classes, is split into a query set and a gallery set.</p><p>Evaluation Metrics: For evaluation, we use the two commonly used evaluation metrics, Recall@K (R@K) <ref type="bibr" target="#b24">(J?gou et al., 2011)</ref>  DiVA <ref type="bibr">(Milbich et al., 2020)</ref>) scored the highest in at-least one metric, now our method reaches the best results in all Recall@1 and NMI metrics across all four datasets.</p><p>Qualitative Results. In <ref type="figure" target="#fig_3">Figure 6</ref>, we present qualitative results on the retrieval task for all four datasets. In all cases, the query image is given on the left, with the four nearest neighbors given on the right. Green boxes indicate cases where the retrieved image is of the same class as the query image, and red boxes indicate a different class. In supplementary material, we provide qualitative evaluations on the clustering performance using t-SNE (van der Maaten &amp; Hinton, 2012) visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies and Robustness Analysis</head><p>In this section, we use the CUB-200-2011 <ref type="bibr" target="#b63">(Wah et al., 2011)</ref> and Cars196 <ref type="bibr" target="#b28">(Krause et al., 2013)</ref> datasets to analyze the robustness of our method and show the importance of our design choices.</p><p>MPN Matters. To show the performance improvement when using the MPN during training, we conduct experi-ments by training the backbone CNN solely with the auxiliary loss, i.e., the cross-entropy loss on the backbone CNN, and without MPN (see the first row in <ref type="table" target="#tab_1">Table 3</ref>). For a fair comparison, we use the same implementation details as for the training with MPN. On CUB-200-2011, this leads to a performance drop of 2.8pp in Recall@1 (to 67.5) and 4.2pp in NMI (to 69.8). On Cars196, it leads to a more significant performance drop of 3.9pp in Recall@1 (to 84.2) and 6.1pp in NMI (to 68.7), showing the benefit of our proposed formulation.</p><p>To give an intuition of how the MPN evolves during the training process, we use GradCam <ref type="bibr" target="#b56">(Selvaraju et al., 2020)</ref> to observe which neighbors a sample relies on when computing the final class prediction after the MPN <ref type="bibr" target="#b56">(Selvaraju et al., 2020)</ref>. To do so, we compare the predictions of an untrained MPN to a trained one. As can be seen in the left part of <ref type="figure" target="#fig_4">Figure 7</ref>, the untrained MPN takes information from nearly all samples in the batch into account, where red, blue, and green represent different classes. The trained MPN (left part of <ref type="figure" target="#fig_4">Figure 7</ref>) only relies on the information of samples of the same class. This suggests that using the MPN with selfattention scores as edge weights enforces the embeddings of negative and positive samples to become more dissimilar and similar, respectively. In supplementary, we also provide and compare visualizations of the embedding vectors of a batch of samples after one epoch of training and of all test samples after the whole training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Message Passing Steps and Attention Heads.</head><p>In <ref type="figure" target="#fig_0">Figure 3</ref>, we investigate the robustness of the algorithm when we differ the number of message passing steps and    attention heads of our MPN. On CUB-200-2011 dataset, we reach the best results when we use a single message passing step, containing two attention heads. We see that increasing the number of message passing steps or the number of attention heads, for the most part, does not result in a large drop in performance. The biggest drop in performance happens when we use four message-passing steps, each having sixteen attention heads. In <ref type="figure" target="#fig_1">Figure 4</ref>, we do a similar robustness analysis for the Cars196 dataset. Unlike CUB-200-2011, the method performs best using two layers and eight attention heads. However, it again performs worst using four message passing steps. This observation is in line with <ref type="bibr" target="#b62">(Velickovic et al., 2018)</ref>, which also utilizes a few message passing steps when applying graph attention.</p><p>Embedding Dimension. In <ref type="figure" target="#fig_2">Figure 5</ref>, we measure the performance of the model as a function of the embedding size. We observe that the performance of the network increases on both datasets when we increase the size of the embedding layer. This is unlike <ref type="bibr" target="#b65">(Wang et al., 2019a)</ref>, which reports a drop in performance when the size of the embedding layer gets bigger than 512. While increasing the dimension of the embedding layer results in even better performance, for fairness with the other methods that do not use an embedding size larger than 512, we avoid those comparisons.</p><p>Auxiliary Loss Function. Considering that in the default scenario, we do not use the MPN during inference, we investigate the effect of adding the auxiliary loss function at the top of the backbone CNN embedding layer. On CUB-200-2011 dataset, we see that such a loss helps the network improve by 2.2pp in Recall@1. Without the loss, the performance of the network drops to 68.1 as shown in the second row of Implicit Regularization. We further investigate the training behavior of our proposed approach on CUB-200-2011. As already stated above, Group Loss <ref type="bibr">(Elezi et al., 2020)</ref> also utilized contextual classification, with the authors claiming that it introduces implicit regularization and thus less overfitting. However, their approach is based on a handcrafted label propagation rule, while ours takes into account the contextual information in an end-to-end learnable way.</p><p>Therefore, we present the training behavior of our approach and compare it to the behavior of the Group Loss <ref type="bibr">(Elezi et al., 2020)</ref>. As can be seen in <ref type="figure" target="#fig_5">Figure 8</ref>, Group Loss <ref type="bibr">(Elezi et al., 2020)</ref> shows higher overfitting on the training data, while our method is capable of better generalization on the test dataset and has a smaller gap between training and test performance. We argue that by taking into account the global structure of the dataset in an end-to-end learnable way, our approach is able to induce an even stronger implicit regularization. Using MPN During Test Time.</p><p>In <ref type="table" target="#tab_1">Table 3</ref>, we analyze the effect of applying message passing during inference (see row four). On CUB-200-2011 dataset, we improve by 0.5pp in Recall@1, and by 0.5pp on the NMI metric. On Cars196 dataset, we also gain 0.5pp in Recall@1 by using MPN during inference. More impressively, we gain 1.4pp in the NMI metric, putting our results 2.2pp higher than Normalized Softmax <ref type="bibr" target="#b74">(Zhai &amp; Wu, 2019)</ref>. We gain an improvement in performance in all cases, at the cost of extra parameters.</p><p>Note, our method does not require the usage of these extra parameters in inference. As we have shown, for a fair comparison, our method reaches state-of-the-art results even without using MPN during inference (see <ref type="table">Tables 1 and 2)</ref>. We consider the usage of MPN during inference a performance boost, but not a central part of our work.  Ensembles. The Group Loss <ref type="bibr">(Elezi et al., 2020)</ref> showed that the performance of their method significantly improves by using an ensemble at test time. The ensemble was built by simply concatenating the features of k independently trained networks. Similarly, we also conduct experiments on ensembles using 2 and 5 networks, respectively, and compare our ensemble with that of <ref type="bibr">(Elezi et al., 2020)</ref>.  <ref type="table">Table 4</ref>. Performance of our ensembles and comparisons with the ensemble models of <ref type="bibr">(Elezi et al., 2020)</ref>.</p><p>In <ref type="table">Table 4</ref>, we present the results of our ensembles. We see that when we use 2 networks, the performance increases by Compared to <ref type="bibr">(Elezi et al., 2020)</ref>, the performance increase of our approach from one network to an ensemble is higher. This is surprising, considering that our network starts from a higher point, and has less room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we propose a model that utilizes the power of message passing networks for the task of deep metric learning. Unlike classical metric learning methods, e.g., triplet loss, our model utilizes all the intra-batch relations in the mini-batch to promote similar embeddings for images coming from the same class, and dissimilar embeddings for samples coming from different classes. Our model is fully learnable, end-to-end trainable, and does not utilize any handcrafted rules. Furthermore, our model achieves state-ofthe-art results while using the same number of parameters, and compute time, during inference. In future work, we will explore the applicability of our model for the tasks of semisupervised deep metric learning and deep metric learning in the presence of only relative labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Different Embedding Sizes</head><p>To investigate the robustness of our approach concerning different embedding sizes, we present the performance for several embedding dimensions on Stanford Online Products and In Shop Clothes, which complement the results presented on CUB-200-2011 and Cars196 in the main paper (see <ref type="figure" target="#fig_3">Figure 6</ref> in the main paper). The results with embedding dimension up to 1024 can be found in <ref type="table">Table 5</ref>. As one can see, the performance on the smaller datasets increases with increasing embedding dimension similar to Proxy Anchor <ref type="bibr">(Kim et al., 2020)</ref> while the performance on Stanford Online Products remains the same. On the other hand, similar to the Multi-Similarity loss <ref type="bibr" target="#b65">(Wang et al., 2019a)</ref> the performance is shown to decrease on the In-Shop dataset if the size of the embedding layer becomes larger than 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Large Images and ProxyNCA++</head><p>Different from most approaches in the field of deep metric learning, <ref type="bibr">(Teh et al., 2020;</ref><ref type="bibr" target="#b23">Jacob et al., 2019)</ref> crop the images during training to size 256?256, while at test time they first resize to 288 ? 288 before again cropping to 256 ? 256. Naturally, larger images are expected to lead to an increased performance. In the main work, for fair comparison, we report the performance of <ref type="bibr">(Teh et al., 2020)</ref> on the typical image size, i.e., 227 ? 227. To obtain these numbers we ran their provided code 1 and compared the results to the Recall@1 given in their supplementary to validate the correctness, see <ref type="table">Table 6</ref>.</p><p>We also evaluate the performance of our approach on images of size 256 ? 256 to show that our performance also increases when using larger images. As can be seen in <ref type="table">Table 7</ref>, when we use larger images, our performance increases by 1.4pp Recall@1 and 0.3pp NMI on CUB-200-2011 and 1.9pp Recall@1 and 0.6pp NMI on Cars196. This leads to a even larger increase in performance compared to <ref type="bibr">(Teh et al., 2020;</ref><ref type="bibr" target="#b23">Jacob et al., 2019)</ref>. We also outperform <ref type="bibr">(Kim et al., 2020)</ref> who also gave additional results for larger-size images (we already showed in the main paper that we outperform <ref type="bibr">(Kim et al., 2020)</ref> on regular-sized images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Different Settings during Test Time</head><p>In this section, we detail three methods for batch construction when using the MPN during inference, as well as a teacher-student approach to avoid batch construction at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Batch Construction Based on Clustering</head><p>Ideally, we imitate the batch construction process that happens during training, i.e., sampling n classes and taking k samples for each. As we can not access ground truth labels during test time, we are not able to construct batches in this manner.</p><p>To this end, we first use randomly sampled batches to generate initial feature vectors using the backbone CNN. We then use an approximation of the ground truth class assignment by using a clustering algorithm. We compare 6 common clustering algorithms as can be seen in <ref type="table" target="#tab_6">Table 8</ref>. Based on these clusters, we construct batches by sampling from n clusters, k samples each, analogous to the training procedure. This way, we ensure that every sample in the mini-batch communicates with samples from its own cluster (similar) and other clusters (dissimilar). We call the k samples belonging to one cluster a chunk. Finally, we compute refined feature vectors using MPNs and use those features for retrieval and clustering.</p><p>In general, clustering algorithms can be divided into two groups: the ones that require a fixed number of clusters to be generated, and the density-based cluster algorithms that need a minimum number of samples per cluster as well as a maximum distance between two samples to be considered as neighbors.</p><p>As in theory, we do not know the ground truth number of clusters during test time, we cannot use it for the cluster construction in the first group. Therefore, we conduct experiments on several different numbers of clusters and report the results on the number that performs best. To be specific, the algorithms achieve the best performance if we set the number of clusters to 900. This number is significantly larger than the number of ground truth classes which is 100 and 98 for CUB-200-2011 and Cars196, respectively. Intuitively, using at least as many clusters as the number of classes is necessary since otherwise samples of different classes will be assigned to the same clusters. As overclustering constructs more clusters than the ground truth number of classes and, therefore, smaller clusters than the ground truth class sizes, the cluster assignment is less prone to outliers. The performance of the algorithms that need a fixed number of classes can be seen in <ref type="table" target="#tab_6">Table 8</ref>, indicated by ?. However, they do not lead to a performance increase compared to the performance using solely the backbone architecture.</p><p>While we can bypass the oversampling issue by using the density-based algorithms (indicated by the * in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB-200-2011 Cars196</head><p>Stanford Online Products In-Shop Clothes Our results on <ref type="bibr">(Teh et al., 2020)</ref> 66.3 84.9 79.8 90.4 Results in <ref type="bibr">(Teh et al., 2020)</ref> 64.7 ? 1.6 85.1 ? 0.3 79.6 ? 0.6 87.6 ? 1.0 <ref type="table">Table 6</ref>. Comparison of Recall@1 on images of size 227 ? 227 using ProxyNCA ++ <ref type="bibr">(Teh et al., 2020)</ref> of the results reported in <ref type="bibr">(Teh et al., 2020)</ref> and our results obtained by running their code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB-200-2011 Cars196</head><p>Stanford Online Products In-Shop Clothes R@1 NMI R@1 NMI R@1 NMI R@1 Horde 512 ? <ref type="bibr" target="#b23">(Jacob et al., 2019)</ref> 66. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Batch Construction Based on Nearest Neighbors</head><p>As another option, we sample chunks by randomly choosing one anchor and finding its k ? 1 nearest neighbors. Then we construct batches consisting of n of these chunks and feed them through the MPN. In every batch, we only update the feature vectors of the anchors, meaning that we build one chunk for each image in the test set. As these chunks again can be highly noisy, the performance after the MPN drops compared to simply taking the embeddings from the backbone (see "nearest neighbors" in <ref type="table" target="#tab_6">Table 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Reciprocal-kNN Batch Construction</head><p>Since imitating the training batch construction during test time and simply using a sample's k ? 1 nearest neighbors does not lead to a performance increase, we propose to construct batches more strictly during test time. To that end, we suggest constructing reciprocal k-nearest neighbor batches inspired by <ref type="bibr" target="#b78">(Zhong et al., 2017)</ref>. Different from <ref type="bibr" target="#b78">(Zhong et al., 2017)</ref> who use reciprocal k-nearest neighbor for evaluation, we use a similar idea for batch construction (we still do the final evaluation regularly, by simply evaluating Recall@K and NMI). Knowing that samples that are highly similar to the query sample are more likely to be of the same class as the query image c q than dissimilar samples, we first compute the k-nearest neighbor set N k q of a given query q (see the upper part in <ref type="figure" target="#fig_1">Figure C.4)</ref>. However, N k q still might contain noisy samples. Therefore, we reduce N k q to a reciprocal k-nn set N k r,q by only taking samples g ? N k q into account that also contain q in their own k-nn set N k g (indicated by the green frames in the middle part of <ref type="figure" target="#fig_1">Figure C.4</ref>). Up to this step N k r,q only contains samples that are already highly similar to the query image. Some gallery samples g of class c q might not be directly contained in N k r,q , but in N k r,g of some samples g ? N r q , k. Therefore, we expand N k r,q to? k r,q by the reciprocal 1 2 k-nn set N 0.5k r,g of the samples g ? N k r,q , if the following holds:</p><formula xml:id="formula_6">|N k r,q ? N 0.5k r,g | ? ?|N 0.5k r,g |<label>(7)</label></formula><p>where ? ? [0, 1], |? k r,q | = k r and k r is a constant. In the expansion step of <ref type="figure">Figure C</ref>.4, samples that fulfill the above mentioned constraint for ? = 2 3 are visualized by a green frame, those who do not by a red frame. If |? k r,q | &lt; k r , we add the closest samples to q that are not yet contained in? k r,q . Finally, we feed? k r,q into the Message Passing Network to refine the feature vector of q. As we have shown (see Tab. 3 in the main paper) this approach improved the performance during test time by 0.5pp Recall@1 and 0.5pp NMI on CUB-200-2011 dataset and 0.5pp (Recall@1) and 1.4pp (NMI) on the Cars196 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Teacher Student Approach</head><p>As the latter approach requires the usage of additional parameters during test time, we develop several teacherstudent approaches to transfer the knowledge of the MPN, acting as a teacher, to the backbone CNN, acting as a student.</p><p>Knowledge Distillation. As our approach is based on the cross-entropy loss function, we first use knowledge distillation <ref type="bibr" target="#b22">(Hinton et al., 2015)</ref>, where the class probability distributions of the teacher are imitated by the student. The advantage of this technique is that these probability distributions also contain information about the similarities between classes. However, the usage of this approach does not increase the performance compared to solely using the backbone CNN, but decreases it by 4.6pp Recall@1 on CUB-200-2011 and 2.5pp Recall@1 on Cars196 (see <ref type="table" target="#tab_6">Table 8)</ref> Feature Imitation. Since we are not directly interested in the class prediction quality of our network, but in the feature vectors themselves, our second approach forces the student to directly imitate the feature vectors. Further, those feature vectors of the training data after the MPN are highly discriminative. We apply the Huber Loss which is less sensitive to outliers. Again, the performance drops by 5pp Recall@1 on CUB-200-2011 and 2.5pp Recall@1 on Cars196 compared to solely using the backbone CNN.</p><p>Relational Teacher Student. Lastly, we utilize relational knowledge distillation <ref type="bibr" target="#b47">(Park et al., 2019b)</ref>, where the student does not directly imitate the feature vectors, but the relations between different feature vectors. This also supports the paradigm of our MPN-based approach, where we refine feature vectors by taking the relations between samples into account. The performance, though, does not increase but drops even more by 5.4pp Recall@1 on CUB-200-2011 and 3.4pp Recall@1 on Cars196.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>In the main work, we already showed several examples for our qualitative results. To show the robustness of our approach, we will now show several more samples of qualitative results on CUB-200-2011 ( <ref type="figure">Figure 10</ref>), Cars196 <ref type="figure">(Figure 11)</ref>, Stanford Online Products <ref type="figure">(Figure 12)</ref> and In-Shop Clothes <ref type="figure" target="#fig_0">(Figure 13</ref>). As can be seen, our approach is able to retrieve images of the same class even for harder examples, like in the first example of CUB-200-2011 ( <ref type="figure">Figure 10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. T-SNE</head><p>We now show a visualization of the embedding space obtained by our approach on CUB-200-2011 using the t-  <ref type="figure" target="#fig_16">Figure 20</ref>. Every scatter point represents a sample and different colors represent different classes. As can be seen, our approach is able to achieve representative clusters of many classes. We highlight several groups of samples, that can be best viewed when zoomed in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. MPN Matters 2.0 -Comparison of Performance of Training with and without MPN</head><p>In addition to the ablation studies in the main paper where we showed the performance increase of the backbone CNN trained with MPN over not using the MPN but solely crossentropy loss during training (see MPN matters) as well as some visualizations of how the class prediction after the MPN is influenced by other samples in the batch, we provide some more visualizations that support these findings.</p><p>Firstly, we visualize the difference between the embeddings  <ref type="bibr">, 2012)</ref>. As can be seen in <ref type="figure" target="#fig_1">Figure 14</ref>, the features after the backbone CNN are much more clustered than when training without MPN.</p><p>Secondly, we again utilize t-SNE (van der Maaten &amp; Hinton, 2012) to get low dimensional representations of the embeddings of all samples in a given test set after the whole training without as well as with MPN and sample 10 classes for the sake of clarity. In <ref type="figure" target="#fig_2">Figure 15</ref> and <ref type="figure" target="#fig_3">Figure 16</ref> we show three such subsets of classes for CUB-200-2011 and Cars196, respectively. The upper rows in both figures represent embeddings generated by the backbone CNN trained without MPN while the lower ones show embeddings generated by the backbone trained with MPN. The backbone CNN trained using solely cross-entropy loss performs well on many samples. However, our approach is able to better divide more difficult classes from the remaining classes in the embedding space as can be seen for example from the   dark blue class in the first column of <ref type="figure" target="#fig_2">Figure 15</ref>. Further, it is less prone to outliers as can be seen in the second column in <ref type="figure" target="#fig_3">Figure 16</ref>, where there are fewer outliers in all classes in the lower row that shows the visualizations of the embeddings trained with MPN. Finally, the embeddings of samples of the same class most often lie closer together and are further apart from other classes as can be seen from the red and pink classes in the central column of <ref type="figure" target="#fig_2">Figure 15</ref> or the dark blue, orange, and pink classes in the first column of <ref type="figure" target="#fig_3">Figure 16</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Analysis of Number of Message Passing Steps and Heads on Stanford Online Products</head><p>Additionally to the analysis of different numbers of message passing steps and attention heads on CUB-200-2011 and Cars196 in the main paper, we provide an equal analysis on Stanford Online Products. Investigating the results on CUB-200-2011 and Cars196 datasets one could assume that with increasing size of the dataset an increasing number of message passing steps is needed, as the best performing model on CUB-200-2011 utilizes only one message passing step while the best performing model on Cars196 utilizes two message passing steps and CUB-200-2011 is smaller than Cars196. However, as can be seen in <ref type="figure" target="#fig_4">Figure 17</ref> this is not the case, and the performance of our approach on Stanford Online Products drops with an increasing number of message passing steps and attention heads. As already mentioned in the main paper, this is in line with <ref type="bibr" target="#b62">(Velickovic et al., 2018)</ref>, who also utilize few message passing steps when applying graph attention.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Different Number of Classes</head><p>We also conduct experiments to investigate the impact of the composition of the batches concerning the number of classes and the number of samples per class. Therefore we vary the number of classes and samples on CUB-200-2011 and Cars196 between five and ten. As can be seen in <ref type="figure" target="#fig_5">Figure 18</ref>, the performance on CUB-200-2011 tends to go down with an increasing number of classes while on Cars196 (see <ref type="figure" target="#fig_6">Figure 19</ref>) the performance drops when only a few samples per class are used. However, it can be said that the performance is stable with the biggest drop in performance on CUB-200-2011 being 2.8pp and 3.4pp on Cars196.</p><p>I. Metric Learning Reality Check <ref type="bibr">(Musgrave et al., 2020)</ref> claim that the huge improvements of recent metric learning approaches over prior works is mainly caused by flaws in the experimental methodology like utilizing a more powerful backbone, unsuitable evaluation metrics or training with test set feedback. The authors show, that the performance of ResNet50 is worse on CUB-200-2011 and Cars196 than when using BN-Inception. To prove that our approach is robust to the hyperparameter choice we followed <ref type="bibr">(Musgrave et al., 2020)</ref> to find the hyperparameters (e.g., number of epochs) without feedback from the test set and report the results here. We get stateof-the-art results in Cars196, Stanford Online Products, and In-Shop datasets, and competitive results on CUB-200-2011 dataset compared to results mentioned in <ref type="bibr">(Musgrave et al., 2020)</ref> as can be seen in <ref type="table">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Large Number of Classes</head><p>Our method uses a fully connected layer to compute the final loss function. In cases where the number of classes  <ref type="table">Table 9</ref>. Performance of our approach following <ref type="bibr">(Musgrave et al., 2020)</ref> to find the hyperparameters (e.g., number of epochs) without feedback from the test set. increases, then the size of the classification layer increases too. While this typically is not a problem for metric learning datasets which contain from hundreds to tens of thousands of classes, it can become a problem for the closely related problem of face recognition, where datasets typically contain millions of classes. Unlike our method (and other classificationbased methods <ref type="bibr" target="#b74">(Zhai &amp; Wu, 2019;</ref><ref type="bibr" target="#b77">Zheng et al., 2019;</ref><ref type="bibr" target="#b49">Qian et al., 2019;</ref><ref type="bibr">Elezi et al., 2020)</ref>), methods that use a pairwise (e.g. contrastive/triplet) loss function do not have this problem.</p><p>Nevertheless, there are ways of facing the problem. Normalized Softmax <ref type="bibr" target="#b74">(Zhai &amp; Wu, 2019)</ref> tackles the problem by sampling a mini-batch only from a certain number of classes, a strategy proposed also in Group Loss <ref type="bibr">(Elezi et al., 2020)</ref>. Our method uses this sampling strategy in default mode (in each mini-batch, we sample only a certain number of classes). Consequently, we know in advance which units of the last layer need to be modified, and all the other units can easily get masked out for a more efficient tensor-tensor multiplication.</p><p>Dealing with datasets that contain a large number of classes is a problem that has been widely studied in natural language processing <ref type="bibr" target="#b36">(Mikolov et al., 2013)</ref>, typically solved by replacing the softmax layer with hierarchical-softmax <ref type="bibr" target="#b39">(Mnih &amp; Hinton, 2008)</ref>. Considering that the problem is similar, we could envision replacing softmax with hierarchical-softmax for our problem to have a more efficient method.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Relative difference to the best model with respect to Recall@1 on CUB-200-2011.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Relative difference to the best model with respect to Recall@1 on Cars196.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Performance for different embedding dimensions on CUB-200-2011 and Cars196.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Retrieval results on a set of images from CUB-200-2011 (top), Cars196 (second from top), Stanford Online Products (second from bottom), and In-Shop Clothes (bottom) datasets using our model. The most left column contains query images and the results are ranked by distance. Green frames indicate that the retrieved image is from the same class as the query image, while red frames indicate that the retrieved image is from a different class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Comparison of the embeddings of a given batch after one epoch of training without and with MPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Performance on training and test data of CUB-200-2011 compared to Group Loss(Elezi et al., 2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Reciprocal k-nearest neighbor batch sampling for MPN during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>More qualitative results on CUB-200-2011 More qualitative results on Cars196 distributed stochastic neighbor embedding (t-SNE) (van der Maaten &amp; Hinton, 2012) in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .Figure 13 .</head><label>1213</label><figDesc>More qualitative results on Stanford Online Products More qualitative results on In-Shop after the backbone CNN of a given batch after one epoch of training without and with MPN using t-distributed stochastic neighbor embedding (t-SNE) (van der Maaten &amp; Hinton</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 .</head><label>14</label><figDesc>Comparison of the embeddings of a given batch after one epoch of training without and with MPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 .</head><label>15</label><figDesc>Visualization of 10 sampled classes from CUB-200-2011 test dataset when trained without MPN (upper row) and with MPN (lower row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 .</head><label>16</label><figDesc>Visualization of 10 sampled classes from Cars196 test dataset when trained without MPN (upper row) and with MPN (lower row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 .</head><label>17</label><figDesc>Relative Difference with respect to Best Recall@1 on Stanford Online Products.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 18 .</head><label>18</label><figDesc>Relative Difference with respect to Best Recall@1 on CUB-200-2011.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 19 .</head><label>19</label><figDesc>Relative Difference with respect to Best Recall@1 on Cars196.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Learning Intra-Batch Connections for Deep Metric Learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 20 .</head><label>20</label><figDesc>t-SNE (van der Maaten &amp; Hinton, 2012) visualization of our embeddings on the CUB-200-2011 (Wah et al., 2011) dataset with some clusters highlighted. Best viewed on a monitor when zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Normalized Mutual Information (NMI)<ref type="bibr" target="#b35">(McDaid et al., 2011)</ref>. The first one evaluates the retrieval performance by computing the percentage of images whose K nearest neighbors contain at least one sample of the same class as the query image. To evaluate the clustering quality, we apply K-means clustering (MacQueen, 1967) on the embedding feature vectors of all test samples, and compute NMI based on this clustering. To be more specific, NMI evaluates how much the knowledge about the ground truth classes increases given the clustering obtained by the K-means algorithm.4.3. Comparison to state-of-the-artQuantitative Results. InTable 1, we present the results of our method and compare them with the results of other approaches on CUB-200-2011<ref type="bibr" target="#b63">(Wah et al., 2011)</ref>, Cars196<ref type="bibr" target="#b28">(Krause et al., 2013)</ref>, and Stanford Online Products<ref type="bibr" target="#b58">(Song et al., 2016)</ref>. On CUB-200-2011 dataset, our method reaches 70.3 Recall@1, an improvement of 0.6 percentage points (pp) over the state-of-the-art Proxy Anchor(Kim  et al., 2020)  using ResNet50 backbone. On the NMI metric, we outperform the highest scoring method, DiVA(Milbich  et al., 2020)  by 2.6pp. On Cars196, we reach 88.1 Recall@1, an improvement of 0.4pp over Proxy Anchor(Kim et al.,  2020)  with ResNet50 backbone. On the same dataset, we reach 74.8 on the NMI score, 0.8pp higher than the previous best-performing method, Normalized Softmax<ref type="bibr" target="#b74">(Zhai &amp; Wu, 2019)</ref>. On Stanford Online Products dataset, our method reaches 81.4 Recall@1 which is 1.3pp better than the previous best method, HORDE<ref type="bibr" target="#b23">(Jacob et al., 2019)</ref>. On the NMI metric, our method reaches the highest score, outperforming SoftTriple Loss<ref type="bibr" target="#b49">(Qian et al., 2019)</ref> by 0.6pp. Finally, we present the results of our method on the In-Shop Clothes dataset in Table 2. Our method reaches 92.8 CUB-200-2011 CARS196 Stanford Online Products Method BB R@1 R@2 R@4 R@8 NMI R@1 R@2 R@4 R@8 NMI R@1 R@10 R@100 NMI Triplet 64 (Schroff et al., 2015) CVPR15 Proxy-NCA 64 (Movshovitz-Attias et al., 2017) ICCV17</figDesc><table><row><cell></cell><cell></cell><cell>G</cell><cell>42.5</cell><cell>55</cell><cell>66.4</cell><cell>77.2</cell><cell>55.3</cell><cell>51.5</cell><cell>63.8</cell><cell>73.5</cell><cell>82.4</cell><cell>53.4</cell><cell>66.7</cell><cell>82.4</cell><cell>91.9</cell><cell>89.5</cell></row><row><cell>Npairs 64 (Sohn, 2016) NeurIPS16</cell><cell></cell><cell>G</cell><cell>51.9</cell><cell>64.3</cell><cell>74.9</cell><cell>83.2</cell><cell>60.2</cell><cell>68.9</cell><cell>78.9</cell><cell>85.8</cell><cell>90.9</cell><cell>62.7</cell><cell>66.4</cell><cell>82.9</cell><cell>92.1</cell><cell>87.9</cell></row><row><cell>Deep Spectral 512 (Law et al., 2017) ICML17</cell><cell></cell><cell cols="2">BNI 53.2</cell><cell>66.1</cell><cell>76.7</cell><cell>85.2</cell><cell>59.2</cell><cell>73.1</cell><cell>82.2</cell><cell>89.0</cell><cell>93.0</cell><cell>64.3</cell><cell>67.6</cell><cell>83.7</cell><cell>93.3</cell><cell>89.4</cell></row><row><cell>Angular Loss 512 (Wang et al., 2017) ICCV17</cell><cell></cell><cell>G</cell><cell>54.7</cell><cell>66.3</cell><cell>76</cell><cell>83.9</cell><cell>61.1</cell><cell>71.4</cell><cell>81.4</cell><cell>87.5</cell><cell>92.1</cell><cell>63.2</cell><cell>70.9</cell><cell>85.0</cell><cell>93.5</cell><cell>88.6</cell></row><row><cell></cell><cell></cell><cell cols="2">BNI 49.2</cell><cell>61.9</cell><cell>67.9</cell><cell>72.4</cell><cell>59.5</cell><cell>73.2</cell><cell>82.4</cell><cell>86.4</cell><cell>88.7</cell><cell>64.9</cell><cell>73.7</cell><cell>-</cell><cell>-</cell><cell>90.6</cell></row><row><cell>Margin Loss 128 (Manmatha et al., 2017) ICCV17</cell><cell></cell><cell cols="2">R50 63.6</cell><cell>74.4</cell><cell>83.1</cell><cell>90.0</cell><cell>69.0</cell><cell>79.6</cell><cell>86.5</cell><cell>91.9</cell><cell>95.1</cell><cell>69.1</cell><cell>72.7</cell><cell>86.2</cell><cell>93.8</cell><cell>90.7</cell></row><row><cell>Hierarchical triplet 512 (Ge et al., 2018) ECCV18</cell><cell></cell><cell cols="2">BNI 57.1</cell><cell>68.8</cell><cell>78.7</cell><cell>86.5</cell><cell>-</cell><cell>81.4</cell><cell>88.0</cell><cell>92.7</cell><cell>95.7</cell><cell>-</cell><cell>74.8</cell><cell>88.3</cell><cell>94.8</cell><cell>-</cell></row><row><cell>ABE 512 (Kim et al., 2018) ECCV18</cell><cell></cell><cell>G</cell><cell>60.6</cell><cell>71.5</cell><cell>79.8</cell><cell>87.4</cell><cell>-</cell><cell>85.2</cell><cell>90.5</cell><cell>94.0</cell><cell>96.1</cell><cell>-</cell><cell>76.3</cell><cell>88.4</cell><cell>94.8</cell><cell>-</cell></row><row><cell cols="2">Normalized Softmax 512 (Zhai &amp; Wu, 2019) BMVC19</cell><cell cols="2">R50 61.3</cell><cell>73.9</cell><cell>83.5</cell><cell>90.0</cell><cell>69.7</cell><cell>84.2</cell><cell>90.4</cell><cell>94.4</cell><cell>96.9</cell><cell>74.0</cell><cell>78.2</cell><cell>90.6</cell><cell>96.2</cell><cell>91.0</cell></row><row><cell>RLL-H 512 (Wang et al., 2019b) CVPR19</cell><cell></cell><cell cols="2">BNI 57.4</cell><cell>69.7</cell><cell>79.2</cell><cell>86.9</cell><cell>63.6</cell><cell>74</cell><cell>83.6</cell><cell>90.1</cell><cell>94.1</cell><cell>65.4</cell><cell>76.1</cell><cell>89.1</cell><cell>95.4</cell><cell>89.7</cell></row><row><cell>Multi-similarity 512 (Wang et al., 2019a) CVPR19</cell><cell></cell><cell cols="2">BNI 65.7</cell><cell>77.0</cell><cell>86.3</cell><cell>91.2</cell><cell>-</cell><cell>84.1</cell><cell>90.4</cell><cell>94.0</cell><cell>96.5</cell><cell>-</cell><cell>78.2</cell><cell>90.5</cell><cell>96.0</cell><cell>-</cell></row><row><cell cols="2">Relational Knowledge 512 (Park et al., 2019a) CVPR19</cell><cell>G</cell><cell>61.4</cell><cell>73.0</cell><cell>81.9</cell><cell>89.0</cell><cell>-</cell><cell>82.3</cell><cell>89.8</cell><cell>94.2</cell><cell>96.6</cell><cell>-</cell><cell>75.1</cell><cell>88.3</cell><cell>95.2</cell><cell>-</cell></row><row><cell cols="4">Divide and Conquer 1028 (Sanakoyeu et al., 2019) CVPR19 R50 65.9</cell><cell>76.6</cell><cell>84.4</cell><cell>90.6</cell><cell>69.6</cell><cell>84.6</cell><cell>90.7</cell><cell>94.1</cell><cell>96.5</cell><cell>70.3</cell><cell>75.9</cell><cell>88.4</cell><cell>94.9</cell><cell>90.2</cell></row><row><cell>SoftTriple Loss 512 (Qian et al., 2019) ICCV19</cell><cell></cell><cell cols="2">BNI 65.4</cell><cell>76.4</cell><cell>84.5</cell><cell>90.4</cell><cell>69.3</cell><cell>84.5</cell><cell>90.7</cell><cell>94.5</cell><cell>96.9</cell><cell>70.1</cell><cell>78.3</cell><cell>90.3</cell><cell>95.9</cell><cell>92.0</cell></row><row><cell>HORDE 512 (Jacob et al., 2019) ICCV19</cell><cell></cell><cell cols="2">BNI 66.3</cell><cell>76.7</cell><cell>84.7</cell><cell>90.6</cell><cell>-</cell><cell>83.9</cell><cell>90.3</cell><cell>94.1</cell><cell>96.3</cell><cell>-</cell><cell>80.1</cell><cell>91.3</cell><cell>96.2</cell><cell>-</cell></row><row><cell>MIC 128 (Brattoli et al., 2019) ICCV19</cell><cell></cell><cell cols="2">R50 66.1</cell><cell>76.8</cell><cell>85.6</cell><cell>-</cell><cell>69.7</cell><cell>82.6</cell><cell>89.1</cell><cell>93.2</cell><cell>-</cell><cell>68.4</cell><cell>77.2</cell><cell>89.4</cell><cell>95.6</cell><cell>90.0</cell></row><row><cell cols="2">Easy triplet mining 512 (Xuan et al., 2020b) WACV20</cell><cell cols="2">R50 64.9</cell><cell>75.3</cell><cell>83.5</cell><cell>-</cell><cell>-</cell><cell>82.7</cell><cell>89.3</cell><cell>93.0</cell><cell>-</cell><cell>-</cell><cell>78.3</cell><cell>90.7</cell><cell>96.3</cell><cell>-</cell></row><row><cell>Group Loss 1024 (Elezi et al., 2020) ECCV20</cell><cell></cell><cell cols="2">BNI 65.5</cell><cell>77.0</cell><cell>85.0</cell><cell>91.3</cell><cell>69.0</cell><cell>85.6</cell><cell>91.2</cell><cell>94.9</cell><cell>97.0</cell><cell>72.7</cell><cell>75.1</cell><cell>87.5</cell><cell>94.2</cell><cell>90.8</cell></row><row><cell>Proxy NCA++ 512 (Teh et al., 2020) ECCV20</cell><cell></cell><cell cols="2">R50 66.3</cell><cell>77.8</cell><cell>87.7</cell><cell>91.3</cell><cell>71.3</cell><cell>84.9</cell><cell>90.6</cell><cell>94.9</cell><cell>97.2</cell><cell>71.5</cell><cell>79.8</cell><cell>91.4</cell><cell>96.4</cell><cell>-</cell></row><row><cell>DiVA 512 (Milbich et al., 2020) ECCV20</cell><cell></cell><cell cols="2">R50 69.2</cell><cell>79.3</cell><cell>-</cell><cell>-</cell><cell>71.4</cell><cell>87.6</cell><cell>92.9</cell><cell>-</cell><cell>-</cell><cell>72.2</cell><cell>79.6</cell><cell>-</cell><cell>-</cell><cell>90.6</cell></row><row><cell>PADS 128 (Roth et al., 2020) CVPR20</cell><cell></cell><cell cols="2">R50 67.3</cell><cell>78.0</cell><cell>85.9</cell><cell>-</cell><cell>69.9</cell><cell>83.5</cell><cell>89.7</cell><cell>93.8</cell><cell>-</cell><cell>68.8</cell><cell>76.5</cell><cell>89.0</cell><cell>95.4</cell><cell>89.9</cell></row><row><cell>Proxy Anchor 512 (Kim et al., 2020) CVPR20</cell><cell></cell><cell cols="2">BNI 68.4</cell><cell>79.2</cell><cell>86.8</cell><cell>91.6</cell><cell>-</cell><cell>86.1</cell><cell>91.7</cell><cell>95.0</cell><cell>97.3</cell><cell>-</cell><cell>79.1</cell><cell>90.8</cell><cell>96.2</cell><cell>-</cell></row><row><cell>Proxy Anchor 512 (Kim et al., 2020) CVPR20</cell><cell></cell><cell cols="2">R50 69.7</cell><cell>80.0</cell><cell>87.0</cell><cell>92.4</cell><cell>-</cell><cell>87.7</cell><cell>92.9</cell><cell>95.8</cell><cell>97.9</cell><cell>-</cell><cell>80.0</cell><cell>91.7</cell><cell>96.6</cell><cell>-</cell></row><row><cell>Proxy Few 512 (Zhu et al., 2020) NeurIPS20</cell><cell></cell><cell cols="2">BNI 66.6</cell><cell>77.6</cell><cell>86.4</cell><cell>-</cell><cell>69.8</cell><cell>85.5</cell><cell>91.8</cell><cell>95.3</cell><cell>-</cell><cell>72.4</cell><cell>78.0</cell><cell>90.6</cell><cell>96.2</cell><cell>90.2</cell></row><row><cell cols="17">Ours 512 92.Method R50 70.3 80.3 87.6 92.7 74.0 88.1 93.3 96.2 98.2 74.8 81.4 91.3 95.9 BB R@1 R@10 R@20 R@40</cell></row><row><cell>FashionNet 4096 (Liu et al., 2016) CVPR16</cell><cell>V</cell><cell>53.0</cell><cell>73.0</cell><cell>76.0</cell><cell>79.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A-BIER 512 (Opitz et al., 2020) PAMI20</cell><cell>G</cell><cell>83.1</cell><cell>95.1</cell><cell>96.9</cell><cell>97.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ABE 512 (Kim et al., 2018) ECCV18</cell><cell>G</cell><cell>87.3</cell><cell>96.7</cell><cell>97.9</cell><cell>98.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Multi-similarity 512 (Wang et al., 2019a) CVPR19 BNI 89.7</cell><cell>97.9</cell><cell>98.5</cell><cell>99.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Learning to Rank 512 (? akir et al., 2019)</cell><cell cols="2">R50 90.9</cell><cell>97.7</cell><cell>98.5</cell><cell>98.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HORDE 512 (Jacob et al., 2019) ICCV19</cell><cell cols="2">BNI 90.4</cell><cell>97.8</cell><cell>98.4</cell><cell>98.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MIC 128 (Brattoli et al., 2019) ICCV19</cell><cell cols="2">R50 88.2</cell><cell>97.0</cell><cell>98.0</cell><cell>98.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Proxy NCA++ 512 (Teh et al., 2020) ECCV20</cell><cell cols="2">R50 90.4</cell><cell>98.1</cell><cell>98.8</cell><cell>99.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Proxy Anchor 512 (Kim et al., 2020) CVPR20</cell><cell cols="2">BNI 91.5</cell><cell>98.1</cell><cell>98.8</cell><cell>99.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Proxy Anchor 512 (Kim et al., 2020) CVPR20</cell><cell cols="2">R50 92.1</cell><cell>98.1</cell><cell>98.7</cell><cell>99.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours 512</cell><cell cols="2">R50 92.8</cell><cell>98.5</cell><cell>99.1</cell><cell>99.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>6 Table 1. Retrieval and Clustering performance on CUB-200-2011, CARS196 and Stanford Online Products datasets. Bold indicates best, red second best, and blue third best results. The exponents attached to the method name indicates the embedding dimension. BB=backbone, G=GoogLeNet, BNI=BN-Inception and R50=ResNet50.2 Table 2. Retrieval performance on In Shop Clothes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>On the other hand, removing the auxiliary loss function leads to a performance drop of only 0.9pp in Recall@1 on Cars196 to 87.2. However, the NMI performance drops by 2.7pp to 72.1 on Cars196 and 2.0pp on CUB-200-2011.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance of the network with and without MPN during training and testing time. We achieved all results using embedding dimension 512.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Using 5 networks, the performance increases by 2.8pp on CUB-200-2011, 3.4pp on Cars196, 0.7pp on Stanford Online Products, and 0.6pp on In-Shop Clothes compared to using a single network. NMI on CUB-200-2011 is improved by 0.4pp compared to a single network, on Cars196 it increases by 0.6pp more and on Stanford Online Products it increases by 0.6pp.</figDesc><table><row><cell>1.9pp on CUB-200-2011, 3.0pp on Cars196, and 0.4pp on</cell></row><row><cell>Stanford Online Products. Similarly, the NMI score also</cell></row><row><cell>improves by 0.3pp on CUB-200-2011, 0.1pp on Cars196,</cell></row><row><cell>and 0.1pp on Stanford Online Products. Unfortunately, the</cell></row><row><cell>Recall@1 performance on In-Shop Clothes only improves</cell></row><row><cell>by 0.1pp.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>),</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Performance of our approach using larger images compared to the approaches that only report their results on larger images. ? indicates results on larger images. Proxy Anchor(Kim et al., 2020)  presents the results both in regular (shown in the tables in the main paper) and large size images. Performance of different settings of using the MPN during test time as well as the performance of teacher student approaches.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>-</cell><cell>83.9</cell><cell>-</cell><cell>80.1</cell><cell>-</cell><cell>90.4</cell></row><row><cell cols="4">Proxy NCA++ 512 ? (Teh et al., 2020) 69.0</cell><cell>73.9</cell><cell>86.5</cell><cell>73.8</cell><cell>80.7</cell><cell>-</cell><cell>90.4</cell></row><row><cell cols="4">Proxy Anchor 512 ? (Kim et al., 2020) 71.1</cell><cell>-</cell><cell>88.3</cell><cell>-</cell><cell>80.3</cell><cell>-</cell><cell>92.6</cell></row><row><cell>Ours 512</cell><cell></cell><cell></cell><cell>70.3</cell><cell>74.0</cell><cell>88.1</cell><cell>74.8</cell><cell>81.4</cell><cell>92.6</cell><cell>92.8</cell></row><row><cell>Ours 512 ?</cell><cell></cell><cell></cell><cell>71.7</cell><cell>74.3</cell><cell>90.2</cell><cell>75.4</cell><cell>81.7</cell><cell>92.3</cell><cell>92.9</cell></row><row><cell></cell><cell cols="2">CUB-200-2011</cell><cell cols="2">CARS196</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>R@1</cell><cell>NMI</cell><cell cols="2">R@1 NMI</cell><cell></cell><cell></cell><cell></cell></row><row><cell>backbone only</cell><cell>70.3</cell><cell>74.0</cell><cell>88.1</cell><cell>74.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>K-means  ?</cell><cell>66.1</cell><cell>69.6</cell><cell>85.1</cell><cell>70.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ward clustering  ?</cell><cell>67.9</cell><cell>68.8</cell><cell>86.6</cell><cell>69.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Spectral Clustering  ?</cell><cell>63.1</cell><cell>69.6</cell><cell>85.1</cell><cell>69.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Birch  ?</cell><cell>65.5</cell><cell>69.5</cell><cell>86.1</cell><cell>70.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DBSCAN*</cell><cell>65.7</cell><cell>72.0</cell><cell>85.0</cell><cell>70.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Optics*</cell><cell>66.5</cell><cell>71.1</cell><cell>85.4</cell><cell>69.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Nearest neighbors</cell><cell>62.1</cell><cell>69.2</cell><cell>84.8</cell><cell>70.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Reciprocal kNN</cell><cell>70.8</cell><cell>74.5</cell><cell>88.6</cell><cell>76.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Knowledge Distillation 65.7</cell><cell>70.3</cell><cell>85.6</cell><cell>71.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Feature Imitation</cell><cell>65.3</cell><cell>70.1</cell><cell>85.6</cell><cell>70.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relational TS</cell><cell>64.9</cell><cell>68.7</cell><cell>84.7</cell><cell>70.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">? indicates clustering algorithms that need a fixed number of clus-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ters (900 clusters), * indicates density-based clustering algorithms</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(eps=0.9, min sample=5)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Ours reality check 67.1?0.69 86.7?0.48 81.1?0.13 92.5?0.11</figDesc><table><row><cell></cell><cell>CUB</cell><cell>CARS</cell><cell>SOP</cell><cell>In-Shop</cell></row><row><cell>Ours</cell><cell>70.3</cell><cell>88.1</cell><cell>81.4</cell><cell>92.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Department of Computer Science, Technical University of Munich, Munich, Germany. Correspondence to: Jenny Seidenschwarz &lt;j.seidenschwarz@tum.de&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Recall@1, an improvement of 0.7pp over the previous best method Proxy Anchor(Kim et al., 2020)  with ResNet50 backbone. In summary, while in the past, different methods (Proxy Anchor(Kim et al., 2020), ProxyNCA++(Teh et al.,  2020), Normalized Softmax<ref type="bibr" target="#b74">(Zhai &amp; Wu, 2019)</ref>, HORDE<ref type="bibr" target="#b23">(Jacob et al., 2019)</ref>, SoftTriple Loss<ref type="bibr" target="#b49">(Qian et al., 2019)</ref>,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/euwern/proxynca_pp</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research was partially funded by the Humboldt Foundation through the Sofia Kovalevskaja Award. We thank Guillem Bras? for useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep constrained dominant sets for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Alemu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Relational inductive biases, deep learning, and graph networks. CoRR, abs/1806.01261</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random search for hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bras?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MIC: mining interclass characteristics for improved metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep metric learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>? Akir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep adversarial metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep embedding learning with discriminative sampling policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transductive label augmentation for improved deep network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Elezi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torcinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The group loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Elezi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torchinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference in Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference in Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural graph matching networks for fewshot 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hashing as tie-aware learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local descriptors optimized for average precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition, (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Metric learning with HORDE: high-order regularizer for deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell. (tPAMI)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Proxy anchor loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep spectral clustering learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep variational metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference in Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Normalized mutual information to evaluate overlapping community finding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Mcdaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Hurley</surname></persName>
		</author>
		<idno>abs/1110.2515</idno>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Diverse visual feature aggregation for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bharadhwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference in Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scattering GCN: overcoming oversmoothness in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wenkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th</title>
		<editor>Vedaldi, A., Bischof, H., Brox, T., and Frahm, J.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Proceedings, Part XXV</title>
		<meeting>Part XXV<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12370</biblScope>
			<biblScope unit="page" from="681" to="699" />
		</imprint>
	</monogr>
	<note>European Conference</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Out of the box: Reasoning with graph convolution nets for factual visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">BIER -boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep metric learning with BIER: boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell. (tPAMI)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="276" to="290" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Softtriple loss: Deep metric learning without triplet sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tacoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning with average precision: Training image retrieval with a listwise loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">PADS: policy-adapted sampling for visual similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis. (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Divide and conquer the embedding space for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tschernezki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>B?chler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning a distance metric from relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis. (IJCV)</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="336" to="359" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visualizing non-metric similarities in multiple maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="33" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ranked list loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep asymmetric metric learning via rich relationship mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep randomized ensembles for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Hard negative examples are hard, but useful</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference in Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Improved embeddings with easy positive triplet mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Classification is a strong baseline for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Embedding label structures for fine-grained feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deep face recognition via exclusive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Regularface</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Towards optimal fine grained retrieval via decorrelated centralized loss with normalize-scale layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Fewer is more: A deep graph metric learning perspective using fewer proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
