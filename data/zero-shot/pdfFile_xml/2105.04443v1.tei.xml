<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liner</forename><surname>Yang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Beijing Language and Culture University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grammatical Error Correction (GEC) aims to correct writing errors and help language learners improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as beam search, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing models neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/ thunlp/VERNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grammatical Error Correction (GEC) systems primarily aim to serve second-language learners for proofreading. These systems are expected to detect grammatical errors, provide precise corrections, and guide learners to improve their language ability. With the rapid increase of second-language learners, GEC has drawn growing attention from numerous researchers of the NLP community. * Corresponding author: M. Sun (sms@tsinghua.edu.cn)  <ref type="figure">Figure 1</ref>: The Grammaticality of Generated Hypotheses. The hypotheses are generated by <ref type="bibr" target="#b22">Kiyono et al. (2019)</ref> with beam search decoding. The hypothesis is compared to the source sentence with a BERT based language model and classified into Win (the hypothesis is better), Tie (the hypothesis and source are same) and Loss (the source is better). The ratios of different classes are plotted with different beam search ranks.</p><p>Existing GEC systems usually inherit the seq2seq architecture <ref type="bibr" target="#b34">(Sutskever et al., 2014)</ref> to correct grammatical errors or improve sentence fluency. These systems employ beam search decoding to generate correction hypotheses and rerank hypotheses with quality estimation models from Kbest decoding <ref type="bibr" target="#b22">(Kiyono et al., 2019;</ref><ref type="bibr" target="#b20">Kaneko et al., 2020)</ref> or model ensemble <ref type="bibr" target="#b3">(Chollampatt and Ng, 2018a)</ref> to produce more appropriate and accurate grammatical error corrections. Such models thrive from edit distance and language models <ref type="bibr" target="#b3">(Chollampatt and Ng, 2018a;</ref><ref type="bibr" target="#b5">Chollampatt et al., 2019;</ref><ref type="bibr" target="#b41">Yannakoudakis et al., 2017;</ref><ref type="bibr" target="#b20">Kaneko et al., , 2020</ref>. <ref type="bibr" target="#b4">Chollampatt and Ng (2018b)</ref> further consider the GEC accuracy in quality estimation by directly predicting the official evaluation metric, F 0.5 score.</p><p>The K-best hypotheses from beam search usually derive from model uncertainty <ref type="bibr" target="#b31">(Ott et al., 2018)</ref>. These uncertainties of multi-hypotheses come from model confidence and potential ambiguity of lin-  <ref type="figure">Figure 2</ref>: The GEC Performance of Generated Hypotheses. The hypotheses generated by <ref type="bibr" target="#b22">Kiyono et al. (2019)</ref> are evaluated on the CoNLL2014 dataset. The average scores of Precision and Recall are calculated according to the two annotations of CoNLL2014. guistic variation <ref type="bibr" target="#b12">(Fomicheva et al., 2020)</ref>, which can be used to improve machine translation performance <ref type="bibr" target="#b37">(Wang et al., 2019b)</ref>. <ref type="bibr" target="#b12">Fomicheva et al. (2020)</ref> further leverage multi-hypotheses to make convinced machine translation evaluation, which is more correlated with human judgments. Their work further demonstrates that multi-hypotheses from well-trained neural models have the ability to provide more hints to estimate generation quality.</p><p>For GEC, the hypotheses from the beam search decoding of well-trained GEC models can provide some valuable GEC evidence. We illustrate the reasons as follows.</p><p>? Beam search can provide better GEC results.</p><p>The GEC performance of the top-ranked hypothesis and the best one has a large gap in beam search. For two existing GEC systems, <ref type="bibr" target="#b43">Zhao et al. (2019)</ref> and <ref type="bibr" target="#b22">Kiyono et al. (2019)</ref>, the F 0.5 scores of these systems are 58.99 and 62.03 on the CoNLL2014 dataset. However, the F 0.5 scores of the best GEC results of these systems can achieve 73.56 and 76.82.</p><p>? Beam search candidates are more grammatical. As shown in <ref type="figure">Figure 1</ref>, the hypotheses from well-trained GEC models with beam search usually win the favor of language models, even for these hypotheses ranked to the rear. It illustrates these hypotheses are usually more grammatical than source sentences.</p><p>? Beam search candidates can provide valuable GEC evidence. As shown in <ref type="figure">Figure 2</ref>, the hypotheses of different beam ranks have almost the same Recall score, which demonstrates all hypotheses in beam search can provide some valuable GEC evidence.</p><p>Existing quality estimation models <ref type="bibr" target="#b4">(Chollampatt and Ng, 2018b)</ref> for GEC regard hypotheses independently and neglect the potential GEC evidence from different hypotheses. To fully use the valuable GEC evidence from GEC hypotheses, we propose the Neural Verification Network (VERNet) to estimate the GEC quality with modeled interactions from multi-hypotheses. Given a source sentence and K hypothesis sentences from the beam search decoding of the basic GEC model, VERNet establishes hypothesis interactions by regarding source, hypothesis pairs as nodes, and constructing a fullyconnected reasoning graph to propagate GEC evidence among multi-hypotheses. Then VERNet proposes two kinds of attention mechanisms on the reasoning graph, node interaction attention and node selection attention, to summarize and aggregate necessary GEC evidence from other hypotheses to estimate the quality of tokens.</p><p>Our experiments show that VERNet can pick up necessary GEC evidence from multi-hypotheses provided by GEC models and help verify the quality of GEC hypotheses. VERNet helps GEC models to generate more accurate GEC results and benefits most grammatical error types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The GEC task is designed for automatically proofreading. Large-scale annotated corpora <ref type="bibr" target="#b26">(Mizumoto et al., 2011;</ref><ref type="bibr" target="#b9">Dahlmeier et al., 2013;</ref><ref type="bibr">Bryant et al., 2019)</ref> bring an opportunity for building fully datadriven GEC systems.</p><p>Existing neural models regard GEC as a natural language generation (NLG) task and usually use sequence-to-sequence architecture <ref type="bibr" target="#b34">(Sutskever et al., 2014)</ref> to generate correction hypotheses with beam search decoding <ref type="bibr" target="#b42">(Yuan and Briscoe, 2016;</ref><ref type="bibr" target="#b3">Chollampatt and Ng, 2018a)</ref>. Transformer-based architectures <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> show their effectiveness in NLG tasks and are also employed to achieve convinced correction results <ref type="bibr" target="#b14">(Grundkiewicz et al., 2019;</ref><ref type="bibr" target="#b22">Kiyono et al., 2019)</ref>. The copying mechanism is also introduced for GEC models <ref type="bibr" target="#b43">(Zhao et al., 2019)</ref> to better align tokens from source sentence to hypothesis sentence. To further accelerate the generation process, some work also comes up with non-autoregressive GEC models and leverages a single encoder to parallelly detect and correct grammatical errors <ref type="bibr" target="#b0">(Awasthi et al., 2019;</ref><ref type="bibr" target="#b24">Malmi et al., 2019;</ref><ref type="bibr" target="#b30">Omelianchuk et al., 2020)</ref>.</p><p>Recent research focuses on two directions to im-prove GEC systems. The first one treats GEC as a low-resource language generation problem and focuses on data augmentation for a grammar sensitive and language proficient GEC system <ref type="bibr" target="#b17">(Junczys-Dowmunt et al., 2018;</ref><ref type="bibr" target="#b22">Kiyono et al., 2019)</ref>. Various weak-supervision corpora have been leveraged, such as Wikipedia edit history <ref type="bibr" target="#b23">(Lichtarge et al., 2019)</ref>, Github edit history <ref type="bibr" target="#b15">(Hagiwara and Mita, 2020)</ref> and confusing word set <ref type="bibr" target="#b14">(Grundkiewicz et al., 2019)</ref>. Besides, lots of work generates grammatical errors through generation models or round-trip translation <ref type="bibr" target="#b13">(Ge et al., 2018;</ref><ref type="bibr" target="#b36">Wang et al., 2019a;</ref><ref type="bibr" target="#b39">Xie et al., 2018)</ref>. <ref type="bibr" target="#b22">Kiyono et al. (2019)</ref> further consider different data augmentation strategies to conduct better GEC pretraining. Reranking GEC hypotheses from K-best decoding or GEC model ensemble <ref type="bibr" target="#b16">(Hoang et al., 2016;</ref><ref type="bibr" target="#b4">Chollampatt and Ng, 2018b)</ref> with quality estimation models provides another promising direction to achieve better GEC performance. Some methods evaluate if hypotheses satisfy linguistic and grammatical rules. For this purpose, they employ language models <ref type="bibr" target="#b3">(Chollampatt and Ng, 2018a;</ref><ref type="bibr" target="#b5">Chollampatt et al., 2019)</ref> or grammatical error detection (GED) models to estimate hypothesis quality. GED models <ref type="bibr" target="#b32">(Rei, 2017;</ref><ref type="bibr" target="#b33">Rei and S?gaard, 2019)</ref> estimate the hypothesis quality on both sentence level  and token level <ref type="bibr" target="#b41">(Yannakoudakis et al., 2017)</ref>. <ref type="bibr" target="#b4">Chollampatt and Ng (2018b)</ref> further estimate GEC quality by considering correction accuracy. They establish sourcehypothesis interactions with the encoder-decoder architecture and learn to directly predict the official evaluation score F 0.5 .</p><p>The pre-trained language model BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> has proven its effectiveness in producing contextual token representations, achieving better quality estimation <ref type="bibr" target="#b5">Chollampatt et al., 2019)</ref> and improving GEC performance by fuse BERT representations <ref type="bibr" target="#b20">(Kaneko et al., 2020)</ref>. However, existing quality estimation models regard each hypothesis independently and neglect the interactions among multihypotheses, which can also benefit the quality estimation <ref type="bibr" target="#b12">(Fomicheva et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Verification Network</head><p>This section describes Neural Verification Network (VERNet) to estimate the GEC quality with multihypotheses, as shown in <ref type="figure">Figure 3</ref>.</p><p>Given a source sentence s and K correspond-</p><formula xml:id="formula_0">&lt; ", $ 1 &gt; $ ' ( (* ' ( ) , ' -?( , ' (?( , ' /?( Label 0 (1 or 0) Node Selection Attention Fine-grained Representation Verification Representation Node Interaction Attention ? -?( , ' ( " $ - $ / $ ( 2 - 2 / 2 ( * ' ( Node Representation ? /?(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Sentence:</head><p>": Do one who suffered from this disease ? Hypotheses from the beam search decoding of basic GEC : $ 1 : Does someone who suffered from this disease ? $ 3 : Does someone who suffers (4-th token) from this disease ? $ 5 : Does anyone who suffers from this disease ? 2 &lt; ", $ 5 &gt; <ref type="figure">Figure 3</ref>: The Architecture of Neural Verification Network (VERNet). The estimated token (c k p ) and potentially supporting evidence towards c k p are annotated.</p><p>ing hypotheses C = {c 1 , . . . , c k , . . . , c K } generated by a GEC model, we first regard each sourcehypothesis pair s, c k as a node and fully connect all nodes to establish multi-hypothesis interactions. Then VERNet leverages BERT to get the representation of each token in s, c k pairs (Sec. 3.1) and conducts two kinds of attention mechanisms to propagate and aggregate GEC evidence from other hypotheses to verify the token quality (Sec. 3.2). Finally, VERNet estimates hypothesis quality by aggregating token level quality estimation scores (Sec. 3.3). Our VERNet is trained end-to-end with supervisions from golden labels (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Initial Representations for Sentence Pairs</head><p>Pre-trained language models, e.g. BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, show their advantages of producing contextual token representations for various NLP tasks. Hence, given a source sentence s with m tokens and the k-th hypothesis c k with n tokens, we use BERT to encode the source-hypothesis pair s, c k and get its representation H k :</p><formula xml:id="formula_1">H k = BERT([CLS] s [SEP] c k [SEP]).<label>(1)</label></formula><p>The pair representation H k consists of token-level representations, that is,</p><formula xml:id="formula_2">H k = {H k 0 , . . . , H k m+n+2 }. H k 0 denotes the representation of "[CLS]" token.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Verify Token Quality with Multi-hypotheses</head><p>VERNet conducts two kinds of attention mechanisms, node interaction attention and node selection attention, to verify the token quality with the verification representation V k of k-th node, which learns the supporting evidence towards estimating token quality from multi-hypotheses. The node interaction attention first summarizes useful GEC evidence from the l-th node for the finegrained representation V l?k (Sec. 3.2.1). Then node selection attention further aggregates finegrained representation V l?k with score ? l according to each node's confidence (Sec. 3.2.2). Finally, we can calculate the verification representation V k to verify the token's quality of each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Fine-grained Node Representation with</head><p>Node Interaction Attention</p><p>The node interaction attention ? l?k attentively reads tokens in the l-th node and picks up supporting evidence towards the k-th node to build fine-grained node representations V l?k . For the p-th token in the k-th node, w k p , we first calculate the node interaction attention weight ? l?k q according to the relevance between w k p and the q-th token in the l-th node, w l q :</p><formula xml:id="formula_3">? l?k q = softmaxq((H k p ) T ? W ? H l q ),<label>(2)</label></formula><p>where W is a parameter. H k p and H l q are the representations of w k p and w l q . Then all token representations of l-th node are aggregated:</p><formula xml:id="formula_4">V l?k p = m+n+2 q=1 (? l?k q ? H l q ).<label>(3)</label></formula><p>Based on V l?k p , we further build the l-th node fine-grained representation towards the k-th node,</p><formula xml:id="formula_5">V l?k = {V l?k 1 , . . . , V l?k p , . . . , V l?k m+n+2 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Evidence Aggregation with Node Selection Attention</head><p>The node selection attention measures node importance and is used to aggregate supporting evidence from the fine-grained node representation V l?k of the l-th node. We leverage attention-over-attention mechanism <ref type="bibr" target="#b7">(Cui et al., 2017)</ref> to conduct source h ls and hypotheses h lh representations to calculate the l-th node selection attention score ? l . Then we get the node verification representation V k p with the node selection attention ? l .</p><p>To calculate the node selection attention ? l , we establish an interaction matrix M l between the source and hypothesis sentences of the l-th node. Each element M l ij in M l is calculated with the relevance between i-th source token and j-th hypothesis token (include "[SEP]" tokens):</p><formula xml:id="formula_6">M l ij = (H l i ) T ? W ? H l m+1+j ,<label>(4)</label></formula><p>where W is a parameter. Then we calculate attention scores ? ls i and ? lh j along the source dimension and hypothesis dimension, respectively:</p><formula xml:id="formula_7">? ls i = 1 n + 1 n+1 j=1 softmaxi(M l ij ),<label>(5)</label></formula><formula xml:id="formula_8">? lh j = 1 m + 1 m+1 i=1 softmaxj(M l ij ).<label>(6)</label></formula><p>Then the representations of source sentence and hypothesis sentence are calculated:</p><formula xml:id="formula_9">h ls = m+1 i=1 ? ls i ? H l i , h lh = n+1 j=1 ? lh j ? H l m+1+j .<label>(7)</label></formula><p>Finally, the node selection attention ? l of l-th node is calculated for the evidence aggregation:</p><formula xml:id="formula_10">? l = softmax l (Linear((h ls ? h lh ); h ls ; h lh )),<label>(8)</label></formula><p>where ? is the element-wise multiplication operator and ; is the concatenate operator. The node selection attention ? l aggregates evidence for the verification representation V k p of w k p :</p><formula xml:id="formula_11">V k p = K l=1 (? l ? V l?k p ),<label>(9)</label></formula><p>where</p><formula xml:id="formula_12">V k = {V k 1 , . . . , V k p , . . . , V k m+n+2 } is the k-th node verification representation.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hypothesis Quality Estimation</head><p>For the p-th token w k p in the k-th node, the probability P (y|w k p ) of quality label y is calculated with the verification representation V k p :</p><formula xml:id="formula_13">P (y|w k p ) = softmaxy(Linear((H k p ? V k p ); H k p ; V k p )),<label>(10)</label></formula><p>where ? is the element-wise multiplication and ; is the concatenate operator. We average all probability P (y = 1|w k p ) of token level quality estimation as hypothesis quality estimation score f (s, c k ) for the pair s, c k :</p><formula xml:id="formula_14">f (s, c k ) = 1 n + 1 m+n+2 p=m+2 P (y = 1|w k p ).<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">End-to-end Training</head><p>We conduct joint training with token-level supervision. The source labels and hypothesis labels are used, which denote the grammatical quality of source sentences and GEC accuracy of hypotheses. The cross entropy loss for the p-th token w k p in the k-th node is calculated:</p><formula xml:id="formula_15">L(w k p ) = CrossEntropy(y * , P (y|w k p )),<label>(12)</label></formula><p>using the ground truth token labels y * . Then the training loss of VERNet is calculated:</p><formula xml:id="formula_16">L = 1 K 1 m + n + 2 K k=1 m+n+2 p=1 L(w k p ).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Methodology</head><p>This section describes the datasets, evaluation metrics, baselines, and implementation details. Datasets. We use FCE <ref type="bibr" target="#b40">(Yannakoudakis et al., 2011)</ref>, BEA19 <ref type="bibr">(Bryant et al., 2019)</ref> and NU-CLE <ref type="bibr" target="#b9">(Dahlmeier et al., 2013)</ref> to construct training and development sets. Four testing scenarios, FCE, BEA19 (Restrict), <ref type="bibr">CoNLL-2014</ref><ref type="bibr" target="#b29">(Ng et al., 2014</ref> and JFLEG <ref type="bibr" target="#b28">(Napoles et al., 2017)</ref>, are leveraged to evaluate model performance. Detailed data statistics are presented in <ref type="table" target="#tab_0">Table 1</ref>. We do not incorporate additional training corpora for fair comparison.</p><p>Basic GEC Model. To generate correction hypotheses, we take one of the state-of-the-art autoregressive GEC systems <ref type="bibr" target="#b22">(Kiyono et al., 2019)</ref> as our basic GEC model and keep the same setting. The beam size of our baseline model is set to 5 <ref type="bibr" target="#b22">(Kiyono et al., 2019)</ref>, and all these beam search hypotheses are reserved in our experiments.</p><p>We generate quality estimation labels for tokens in both source sentences and hypothesis sentences with ERRANT <ref type="bibr" target="#b2">(Bryant et al., 2017;</ref><ref type="bibr" target="#b11">Felice et al., 2016)</ref>, which indicate grammatical correctness and GEC accuracy, respectively. As shown in <ref type="table" target="#tab_1">Table 2</ref>, ERRANT annotates edit operations (delete, insert, and replace) towards the ground truth corrections. In terms of such annotations, each token is labeled with correct (1) or incorrect (0).</p><p>Evaluation Metrics. We introduce the evaluation metrics in three tasks: token quality estimation, sentence quality estimation, and GEC.</p><p>To evaluate the model performance of tokenlevel quality estimation, we employ the same evaluation metrics from previous GED models <ref type="bibr" target="#b32">(Rei, 2017;</ref><ref type="bibr" target="#b33">Rei and S?gaard, 2019;</ref><ref type="bibr" target="#b41">Yannakoudakis et al., 2017)</ref>, including Precision, Recall, and F 0.5 . F 0.5 is our primary evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Training <ref type="table" target="#tab_0">Development  Test  FCE  28,350  2,191  2,695  BEA19  34,308  4,384  4,477  NUCLE  57,151  --CoNLL-2014  --1,312  JFLEG  --747  Total</ref> 119,809 6,575 9,231   <ref type="formula">(0)</ref> and others are labeled as correct (1). The "[SEP]" token denotes the end of the sentence.</p><p>For the evaluation of sentence-level quality estimation, we employ the same evaluation metrics from the previous quality estimation model <ref type="bibr" target="#b4">(Chollampatt and Ng, 2018b)</ref>, including two evaluation scenarios: (1) GEC evaluation metrics for the hypothesis that reranked top-1 and (2) Pearson Correlation Coefficient (PCC) between reranking scores and golden scores (F 0.5 ) for all hypotheses.</p><p>To evaluate GEC performance, we adopt GLEU <ref type="bibr" target="#b27">(Napoles et al., 2015)</ref> to evaluate model performance on the JFLEG dataset. The official tool ERRANT of the BEA19 shared task <ref type="bibr">(Bryant et al., 2019</ref>) is used to calculate Precision, Recall, and F 0.5 scores for other datasets. For the CoNLL-2014 dataset, the M 2 evaluation <ref type="bibr" target="#b8">(Dahlmeier and Ng, 2012)</ref> is also adopted as our main evaluation.</p><p>Baselines. BERT-fuse (GED) <ref type="bibr" target="#b20">(Kaneko et al., 2020</ref>) is compared in our experiments, which trains BERT with the GED task and fuses BERT representations into the Transformer. For quality estimation, we consider two groups of baseline models in our experiments, and more details of these models can be found in Appendices A.1.</p><p>(1) BERT based language models. We employ three BERT based language models to estimate the quality of hypotheses. BERT-LM (Chollampatt et al., 2019) measures hypothesis quality with the perplexity of the language model. BERT-GQE  is trained with annotated GEC data and estimates if the hypothesis has grammatical errors. We also conduct BERT-GED (SRC) that predicts token level grammar indicator labels, which is inspired by GED models <ref type="bibr" target="#b41">(Yannakoudakis et al., 2017)</ref>. BERT shows significant improvement compared to LSTM based models for the GED task (Appendices A.2). Hence the LSTM based models are neglected in our experiments.</p><p>(2) GEC accuracy estimation models. These models further consider the source-hypothesis interactions to evaluate GEC accuracy. We take a strong baseline NQE <ref type="bibr" target="#b4">(Chollampatt and Ng, 2018b)</ref> in experiments. NQE employs the encoder-decoder (predictor) architecture to encode source-hypothesis pairs and predicts F 0.5 score with the estimator architecture. All their proposed architectures, NQE (CC), NQE (RC), NQE (CR), and NQE (RR) are compared. For NQE (XY), X indicates the predictor architecture, and Y indicates the estimator architecture. X and Y can be recurrent (R) or convolutional (C) neural networks. In addition, we also employ BERT to encode source-hypothesis pairs and then predict the F 0.5 score to implement the BERT-QE model. We also come up with two baselines, BERT-GED (HYP) and BERT-GED (JOINT). They leverage BERT to encode source-hypothesis pairs and are supervised with the token-level quality estimation label. BERT-GED (HYP) is trained with the supervision of hypotheses, and BERT-GED (JOINT) is supervised with labels from both source and hypothesis sentences.</p><p>Implementation Details. In all experiments, we use the base version of BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and ELECTRA <ref type="bibr" target="#b6">(Clark et al., 2020)</ref>. BERT is a widely used pretrained language model and trained with the mask language model task. ELEC-TRA is trained with the replaced token detection task and aims to predict if the token is original or replaced by a BERT based generator during pretraining. ELECTRA is a discriminator based pretrained language model and is more like the GED task. We regard BERT as our main model for text encoding and leverage ELECTRA to evaluate the generalization ability of our model. Both BERT and ELECTRA inherit huggingface's PyTorch implementation <ref type="bibr">(Wolf et al., 2020)</ref>. <ref type="bibr">Adam (Kingma and Ba, 2015)</ref> is utilized for parameter optimization. We set the max sentence length to 120 for source and hypothesis sentences, learning rate to 5e-5, batch size to 8, and accumulate step to 4 during training.</p><p>For hypothesis reranking, we leverage the learning-to-rank method, Coordinate Ascent (CA) <ref type="bibr" target="#b25">(Metzler and Croft, 2007)</ref>, to aggregate the ranking features and basic GEC score to conduct the ranking score. We assign the hypotheses with the highest F 0.5 score as positive instances and the others as negative ones. The Coordinate Ascent method is implemented by RankLib 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Results</head><p>We conduct experiments to study the performance of VERNet from three aspects: token-level quality estimation, sentence-level quality estimation, and the VERNet's effectiveness in GEC models. Then we present the case study to qualitatively analyze the effectiveness of the proposed two types of attention in VERNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance of Token Level Quality Estimation</head><p>We first evaluate VERNet's effectiveness on tokenlevel quality estimation. BERT-GED (SRC) is the previous state-of-the-art GED model . Additional two variants, HYP and JOINT, of BERT-GED are conducted as baselines by considering the first-ranked GEC hypothesis in beam search decoding. As shown in <ref type="table">Table 3</ref>, there are two scenarios, source and hypothesis, are conducted to evaluate model performance. The source scenario evaluates the ability of grammaticality quality estimation, which is the same as GED models <ref type="bibr" target="#b33">(Rei and S?gaard, 2019)</ref>. The hypothesis scenario tests the quality estimation ability on GEC accuracy.</p><p>For the source scenario, BERT-GED (JOINT) outperforms BERT-GED (SRC) and illustrates that the GEC result can help estimate the grammaticality quality of source sentences. For the hypothesis scenario, BERT-GED (JOINT) shows better performance than BERT-GED (HYP), which thrives from the supervisions from source sentences. For both scenarios, BERT-VERNet shows further improvement compared with BERT-GED (JOINT). Such improvements demonstrate that various GEC evidence from multiple hypotheses benefits the tokenlevel quality estimation.</p><p>Moreover, the detection style pre-trained model ELECTRA <ref type="bibr" target="#b6">(Clark et al., 2020)</ref> is also used as our sentence encoder. VERNet is boosted a lot on all scenarios and datasets, which illustrates the strong ability of ELECTRA in token-level quality estimation and the generalization ability of VERNet.  <ref type="table">Table 3</ref>: Performance of Token Level Quality Estimation. Both source and hypothesis scenarios are conducted to evaluate grammatical quality estimation ability on source sentences and GEC quality estimation ability on hypotheses, respectively. BERT-GED (SRC) only encodes source sentences while others encode source, hypothesis pairs. BERT-GED (JOINT) is supervised with golden labels from source and hypothesis sentences.   <ref type="bibr" target="#b4">(Chollampatt and Ng, 2018b)</ref> uses RNN or CNN models for GEC quality estimation. BERT-LM <ref type="bibr" target="#b5">(Chollampatt et al., 2019)</ref> measures perplexity without fine-tuning. BERT-GQE  and BERT-GED (SRC) are supervised with sentence-level and token-level labels from source sentences to estimate grammatical quality, respectively. NQE and BERT-QE encode source, hypothesis pairs and directly predict F 0.5 score. BERT-GED (HYP) and BERT-GED (JOINT) encode the source, hypothesis pairs to estimate the quality of generated tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance of Sentence Level Quality Estimation</head><p>In this part, we evaluate VERNet's performance on sentence-level quality estimation by reranking hypotheses from beam search decoding. Baselines can be divided into two groups: language model based and GEC accuracy based quality estimation models. The former focuses on grammaticality and fluency, including BERT-LM, BERT-GQE and BERT-GED (SRC). The others focus on estimating the GEC accuracy, including NQE, BERT-QE, BERT-GED (HYP)/(JOINT).</p><p>As shown in <ref type="table" target="#tab_4">Table 4</ref>, we find that language model based quality estimation prefers higher recall but lower precision, which leads to more redundant corrections. Only considering grammaticality is insufficient since such unnecessary correction suggestions may mislead users. By contrast, GEC accuracy based quality estimation models get much better Precision and F 0.5 , and provide more precise feedback for users. Furthermore, BERT-GED (HYP) outperforms BERT-QE, manifesting that token-level supervisions provide finer-granularity signals to help the model better distinguish subtle differences among hypotheses. VERNet outperforms all baselines, which supports our claim that multi-hypotheses from beam search provide valuable GEC evidence and help conduct more effective quality estimation for generated GEC hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">VERNet's Effectiveness in GEC Models</head><p>This part explores the effectiveness of VERNet on improving GEC models. We conduct VERNet ? by aggregating scores from the basic GEC model and VERNet for hypothesis reranking.</p><p>As shown in <ref type="table" target="#tab_6">Table 5</ref>, two baseline models are compared in our experiments, Basic GEC <ref type="bibr" target="#b22">(Kiyono et al., 2019)</ref> and BERT-fuse (GED) <ref type="bibr" target="#b20">(Kaneko et al., 2020)</ref>. Compared to BERT-fuse (GED), BERT-VERNet ? achieves comparable performance on CoNLL-2014 and more improvement on BEA19. It demonstrates that reranking hypotheses with VER-   <ref type="bibr" target="#b22">(Kiyono et al., 2019)</ref> and VERNet for hypothesis reranking with Coordinate Ascent. BERT-fuse (GED) <ref type="bibr" target="#b20">(Kaneko et al., 2020</ref>) is the Transformer model that fuses BERT representations. * Note that R2L models incorporate four right-to-left Transformer models that are trained with unpublished data and these models are not supplied in their open source codes, thus these results are hard to reimplement. Net provides an effective way to improve basic GEC model performance without changing the Transformer architecture. R2L models incorporate four right-to-left Transformer models to improve GEC performance. However, these R2L models are not available. ELECTRA-VERNet ? incorporates only one model and achieves comparable performance on BEA19 and JFLEG. <ref type="figure" target="#fig_2">Figure 4</ref> presents VERNet ? 's performance on different grammatical error types. We plot the F 0.5 scores of both basic GEC model and VERNet ? on BEA19. VERNet ? achieves improvement on most types and performs significantly better for word morphology and word usage errors, such as Noun Inflection (NOUN:INFL) and Pronoun (PRON). Such results illustrate that VERNet ? is able to leverage clues learned from multi-hypotheses to verify the GEC quality. However, we also find that VERNet ? discounts GEC performance on a few error types, e.g., Contraction (CONTR). The annotation biases may cause such a decrease in CONTR errors. For example, for both "n't" and "not", they are both right according to grammaticality, but annotators usually come up with different corrections with different GEC standards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Study</head><p>We select one case from CoNLL-2014 and visualize node interaction and node selection attention weights to study what VERNet learns from multihypotheses of beam search, as shown in <ref type="figure" target="#fig_3">Figure 5</ref>.</p><p>Given a source sentence, "Do one who suffered from this disease keep it a secret of infrom their relatives ?", and its five hypotheses from the Basic GEC Model, we plot the node interaction attention weights towards the word "suffers" in the hypothesis of node 2, which is assigned more higher score by BERT-VERNet. The word usage "suffers" is ). The selected node by BERT-VERNet is annotated (Node2). The node selection attention assigned to each hypothesis is annotated with dark orange. The node interaction attention towards the edited token "suffers" in the second node is also plotted. Darker red indicates higher attention weights. more appropriate than "suffered" according to the context.</p><p>The node interaction attention accurately picks up the associated tokens "Does" from nodes 1, 3, and 4, and "suffers" from node 5. "Does" and "suffers" indicate the present tense and provide sufficient evidence to verify the quality of "suffers" in node 2. For node selection attention, the hypothesis (node 2) shares more attention than other nodes, which is more appropriate than other hypotheses. It demonstrates that the node attention is effective to select high-quality corrections with the source-hypothesis interactions.</p><p>The attention patterns are intuitive and effective, which further demonstrates VERNet's ability to well model the interactions of multi-hypotheses for better quality estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>This paper presents VERNet for GEC quality estimation with multi-hypotheses. VERNet models the interactions of multiple hypotheses by building a reasoning graph, and then extracts clues with two kinds of attention: node selection attention and node interaction attention. They summarize and aggregate GEC evidence from multi-hypotheses to verify the quality of tokens. Experiments on four datasets show that VERNet achieves the state-ofthe-art GED and quality estimation performance, and improves one published state-of-the-art GEC system. In the future, we will explore the impact of different kinds of hypotheses used in VERNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) CoNLL2014 (ann. 1). (b) CoNLL2014 (ann. 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Model Performance of Different Grammatical Error Types on BEA19. VERNet ? reranks hypotheses with the aggregated score of basic GEC model and VERNet. All types are from ERRANT<ref type="bibr" target="#b2">(Bryant et al., 2017)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of Attention Weight. Each node is the concatenation of the source sentence (with [SEP]) and a corresponding hypothesis sentence (with [SEP]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Data Statistics.</figDesc><table><row><cell>Sentence</cell><cell cols="3">The 1 a 2 Mobile phone is a marvelous invention to 9 charge 10 the world 12 [SEP]</cell></row><row><cell></cell><cell>Operation</cell><cell>Span</cell><cell>Edit</cell></row><row><cell>Correction</cell><cell>Delete Replace</cell><cell>1,2 9,10</cell><cell>-change</cell></row><row><cell></cell><cell>Insert</cell><cell>12,12</cell><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: An Example of Token Label Annotation. All</cell></row><row><cell>sentences are annotated with ERRANT according to</cell></row><row><cell>the golden correction. The words in red color are la-</cell></row><row><cell>beled as incorrect</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>43.34 64.97 59.84 27.11 48.20 77.94 25.02 54.77 BERT-GED (JOINT) 75.62 44.44 66.32 60.79 27.33 48.83 77.42 25.23 54.77 BERT-VERNet 81.53 45.71 70.48 62.64 30.62 51.80 82.25 28.49 59.71 ELECTRA-VERNet 80.94 50.51 72.24 62.50 35.61 54.30 81.69 32.97 63.06 Hypothesis BERT-GED (HYP) 80.27 40.58 67.14 74.28 34.20 60.17 66.49 27.68 51.93 BERT-GED (JOINT) 76.71 46.94 68.07 71.15 38.30 60.73 64.79 31.52 53.50 BERT-VERNet 81.85 44.27 69.97 76.03 34.02 60.97 71.79 29.04 55.46 ELECTRA-VERNet 80.62 49.16 71.48 74.80 39.26 63.33 72.55 34.42 59.39</figDesc><table><row><cell>Model</cell><cell>P</cell><cell>FCE test set R</cell><cell>F0.5</cell><cell>CoNLL-2014 ann. 1 P R F0.5</cell><cell>CoNLL-2014 ann. 2 P R F0.5</cell></row><row><cell>BERT-GED (SRC)</cell><cell>74.22</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Source</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of Sentence Level Quality Estimation. The ranked top-1 hypothesis is used to calculate GEC metrics. NQE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>44.87 62.03 64.26 43.59 58.69 55.11 41.61 51.75 66.20 61.48 65.20 61.00 Basic GEC w. R2L BERT-VERNet ? (Top2) 69.98 43.69 62.47 65.62 41.98 58.98 58.57 41.53 54.13 68.42 60.32 66.63 61.17 BERT-VERNet ? (Top3) 70.49 43.16 62.57 65.92 41.22 58.86 59.20 41.53 54.55 69.03 60.20 67.06 61.24 BERT-VERNet ? (Top4) 70.79 42.72 62.56 66.65 40.94 59.21 59.55 41.55 54.80 69.43 60.17 67.36 61.16 BERT-VERNet ? (Top5) 70.60 42.50 62.36 66.41 40.74 58.98 59.68 41.48 54.86 69.39 60.12 67.32 61.10 ELECTRA-VERNet ? (Top2) 71.21 44.24 63.47 66.95 42.97 60.22 58.31 41.97 54.09 69.27 61.22 67.50 61.60 ELECTRA-VERNet ? (Top3) 71.87 44.13 63.84 67.51 42.38 60.35 59.02 41.99 54.59 70.64 61.78 68.67 61.80 ELECTRA-VERNet ? (Top4) 71.85 43.81 63.69 67.48 42.19 60.25 59.65 42.12 55.07 70.96 62.03 68.98 62.05 ELECTRA-VERNet ? (Top5) 71.58 43.57 63.43 67.15 42.10 60.01 59.95 42.19 55.29 70.79 61.74 68.77 62.07</figDesc><table><row><cell>Model</cell><cell>CoNLL-2014 (M 2 ) P R F0.5</cell><cell>P</cell><cell cols="2">CoNLL-2014 R F0.5</cell><cell>P</cell><cell>FCE R</cell><cell>F0.5</cell><cell>P</cell><cell>BEA19 R</cell><cell>JFLEG F0.5 GLEU</cell></row><row><cell>Basic GEC</cell><cell>68.59 72.4 46.1 65.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">74.7 56.7 70.2</cell><cell>61.4</cell></row><row><cell>BERT-fuse (GED)</cell><cell>69.2 45.6 62.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">67.1 60.1 65.6</cell><cell>61.3</cell></row><row><cell>BERT-fuse (GED) w. R2L  *</cell><cell>72.6 46.4 65.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">72.3 61.4 69.8</cell><cell>62.0</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance of Hypothesis Reranking. BERT/ELECTRA-VERNet ? aggregates the scores of Basic GEC Model</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sourceforge.net/p/lemur/wiki/ RankLib/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers and Shuo Wang for their valuable comments and advice. This research is mainly supported by Science &amp; Tech Innovation 2030 Major Project "New Generation AI" (Grant no. 2020AAA0106500) as well as supported in part by a project from Shanghai-Tsinghua International Innovation Center and the funds of Beijing Advanced Innovation Center for Language Resources under Grant TYZ19005.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Model Details of Sentence Quality Estimation Score Calculation</head><p>This part describes the details of sentence score calculation of BERT based quality estimation models. Given a source sentence s with m tokens and k-th hypothesis c k with n tokens, we can get the representation H k of the k-th source, hypothesis sentence pair through BERT:</p><p>or only the representation H k of the k-th hypothesis through BERT:</p><p>The "[CLS]" representations are H k 0 and H k 0 . BERT-LM. We mask tokens in the k-th hypothesis sentence c k and calculate the Perplexity of the k-th hypothesis sentence:</p><p>BERT-GQE. BERT-GQE uses the "[CLS]" representation H k 0 of k-th hypothesis to estimate the sentence quality with the probability P (y s |c k ):</p><p>where W is the parameter and the label y s is categorized into two groups: correct (y s = 1) and incorrect (y s = 0). Then the sentence-level quality estimation score of hypothesis c k is calculated:</p><p>BERT-QE. BERT-QE uses the "[CLS]" representation H k 0 of k-th source, hypothesis sentence pair to estimate the quality of GEC hypothesis:</p><p>where W is the parameter. The quality estimation score f QE (s, c k ) of BERT-QE is trained to approximate the F 0.5 score of the k-th hypothesis c k . BERT-GED. Take BERT-GED (HYP) as an example, it uses the hypothesis representation H k m+2:m+n+2 of the k-th source, hypothesis sentence pair to estimate the quality of GEC hypothesis. Note that the "[SEP]" token is also used in BERT-GED to denote the end of the sentence.  We calculate the probability of token quality estimation label y for the i-th token w k i in the k-th source, hypothesis sentence pair:</p><p>where W is the parameter. The label y is categorized into two groups: correct (y = 1) and incorrect (y = 0).</p><p>To estimate the quality of hypotheses, we average all token quality estimation probability P (y = 1|w k i ) as the sentence quality estimation score f (s, c k ) for the k-th hypothesis c k :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Grammatical Error Detection Performance with LSTM</head><p>In this experiment, we evaluate the effectiveness of BERT and LSTM on the grammatical error detection (GED) task. We keep the same setting as previous work <ref type="bibr" target="#b33">(Rei and S?gaard, 2019)</ref>. The FCE dataset is used for evaluation. Precision, Recall, and F 0.5 are used as our evaluation metrics. As shown in <ref type="table">Table 6</ref>, three models, LSTM, LSTM-ATTN, and LSTM-JOINT from <ref type="bibr" target="#b33">Rei and S?gaard (2019)</ref> are compared with the BERT model. The LSTM model leverages the LSTM encoder and adds language modeling objectives in the training process <ref type="bibr" target="#b32">(Rei, 2017)</ref>. LSTM-ATTN and LSTM-JOINT further add attention constraints and sentence level supervision to achieve better performance <ref type="bibr" target="#b33">(Rei and S?gaard, 2019)</ref>. The BERT model is the same as our <ref type="bibr">BERT-GED (SRC)</ref>.</p><p>The BERT based model shows significant improvement than LSTM based models. Thus we do not consider LSTM based GED models in the experiments of GEC quality estimation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parallel iterative edit models for local sequence transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasna</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabyasachi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4260" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Andersen, and Ted Briscoe. 2019. The BEA-2019 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>?istein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<biblScope unit="page" from="52" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic annotation and evaluation of error types for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="793" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multilayer convolutional encoder-decoder neural network for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5755" to="5762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural quality estimation of grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2528" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-sentence grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="435" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ELECTRA: pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner English: The NUS corpus of learner English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic extraction of learner errors in ESL sentences using linguistically enhanced alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-hypothesis machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Fomicheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1218" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fluency boost learning and inference for neural grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1055" to="1065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural grammatical error correction systems with unsupervised pre-training on synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GitHub typo corpus: A large-scale multilingual dataset of misspellings and grammatical errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Hagiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6761" to="6768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting n-best hypotheses to improve an SMT approach to grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2803" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Approaching neural grammatical error correction as a low-resource machine translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubha</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="595" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TMU transformer system using BERT for re-ranking at BEA 2019 grammatical error correction on restricted track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kengo</forename><surname>Hotate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoru</forename><surname>Katsumata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multihead multi-layer attention to deep language representations for grammatical error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computaci?n y Sistemas</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Encoder-decoder models can benefit from pre-trained masked language models in grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4248" to="4254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An empirical study of incorporating pseudo data into grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Corpora generation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Lichtarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3291" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Encode, tag, realize: High-precision text editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Mirylenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5054" to="5065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Linear feature-based models for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/content/pdf/10.1007/s10791-006-9019-z.pdf</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>formation Retrieval</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mining revision log of language learning SNS for automated Japanese error correction of second language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ground truth for grammatical error correction metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="588" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">JFLEG: A fluency corpus and benchmark for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="229" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GECToR -grammatical error correction: Tag, not rewrite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostiantyn</forename><surname>Omelianchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Atrasevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Chernodub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Skurzhanskyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Analyzing uncertainty in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3953" to="3962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2121" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Jointly learning to label sentences and tokens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016916</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6916" to="6923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Denoising based sequenceto-sequence pre-training for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4003" to="4015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving back-translation with uncertainty-based confidence estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="791" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gugger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<editor>Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020</editor>
		<meeting>EMNLP</meeting>
		<imprint>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Noising and denoising natural language: Diverse backtranslation for grammar correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Genthial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="619" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading ESOL texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural sequence-labelling models for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?istein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2795" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grammatical error correction using neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="380" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
