<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interpretable Visual Reasoning via Induced Symbolic Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">U of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MIT-IBM Watson AI Lab &amp; IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MIT-IBM Watson AI Lab &amp; IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">U of Oregon</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Picsart AI Research (PAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interpretable Visual Reasoning via Induced Symbolic Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of concept induction in visual reasoning, i.e., identifying concepts and their hierarchical relationships from question-answer pairs associated with images; and achieve an interpretable model via working on the induced symbolic concept space. To this end, we first design a new framework named object-centric compositional attention model (OCCAM) to perform the visual reasoning task with object-level visual features. Then, we come up with a method to induce concepts of objects and relations using clues from the attention patterns between objects' visual features and question words. Finally, we achieve a higher level of interpretability by imposing OCCAM on the objects represented in the induced symbolic concept space. Experiments on the CLEVR and GQA datasets demonstrate: 1) our OCCAM achieves a new state of the art without human-annotated functional programs; 2) our induced concepts are both accurate and sufficient as OCCAM achieves an on-par performance on objects represented either in visual features or in the induced symbolic concept space.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in Visual Question Answering (VQA) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b32">33]</ref> usually rely on carefully designed neural attention models over images, and rely on pre-defined lists of concepts to enhance the compositional reasoning ability of the attention modules. Human prior knowledge plays an essential role in the success of the model design.</p><p>We focus on a less-studied problem in this field -given only question-answer pairs and images, induce the visual concepts that are sufficient for completing the visual reasoning tasks. By sufficiency, we hope to maintain the predictive accuracy for VQA when using the induced concepts in place of the original visual features. We consider concepts that are important for visual reasoning, including properties of objects (e.g., red, cube) and relations between objects (e.g., <ref type="bibr">*</ref> Correspondence to H. <ref type="bibr">Shi</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and M. Yu.</head><p>There is a big sphere behind the brown cylinder; does it have the same color as the small rubber sphere on the left side of the small red rubber ball? What is the big thing that is in front of the block that is behind the block that is in front of the large shiny block made of? left, front). The aforementioned scope and sufficiency criterion require accurately associating the induced symbols of concepts to both visual features and words, so that each new instance of question-image pair can be transformed into the induced concept space for further computations. Additionally, it is necessary to identify super concepts, i.e., hypernyms of concept subsets (e.g., shape). The concepts inside a super concept are exclusive, so that the system knows each object can only possess one value in each subset. This introduces structural information to the concept space (multiple one-hot vectors for each visual object) and further guarantees the accuracy of the aforementioned transformation.</p><p>The value of the study has two folds. First, our proposed problem aims to identify visual concepts, their argument patterns (properties or relations) and their hierarchy (super concepts) without using any concept-level su-pervision. Solving the problem frees both the efforts of human annotations and human designs of concept schema required in previous visual reasoning works. At the same time, the problem is technically more challenging compared to the related existing problem like unsupervised or weaklysupervised visual grounding <ref type="bibr" target="#b45">[46]</ref>. Second, by constraining the visual reasoning models to work over the induced concepts, the ability of concept induction improves the interpretability of visual reasoning models. Unlike previous interpretable visual reasoning models that rely on humanwritten rules to associate neural modules with given concept definitions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>, our method resolves the concept definitions and associations interpretability automatically in the learning process, without the need of trading off for hand-crafted model designs. We achieve the proposed task in three steps. First, we propose a new model architecture, object-centric compositional attention model (OCCAM), that performs objectlevel visual reasoning instead of pixel-level by extracting object-level visual features with ResNet <ref type="bibr" target="#b13">[14]</ref> and pooling the features according to each object's bounding box. The object-level reasoning not only improves over the state-ofthe-arts, but also provides a higher-level interpretability for concept association and identification. Second, we benefit from the trained OCCAM's attention values over objects to create classifiers mapping visual objects to words; then derive the concepts and super concepts from the object-word cooccurrence matrices as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. Finally, our concept-based visual reasoning framework predicts the concepts of objects and object relations; then performs compositional reasoning using the predicted symbolic concept embeddings instead of the original visual features.</p><p>Experiments on the CLEVR and GQA datasets confirm that our overall approach improves the interpretability of neural visual reasoning, and maintains the predictive accuracy: (1) our OCCAM improves over the previous state-ofthe-art models that do not use external training data; (2) our induced concepts and concept hierarchy are accurate in human study; and (3) our induced concepts are sufficient for visual reasoning -replacing visual features with concepts leads to only ?1% performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Visual Question Answering (VQA) requires models to reason a question about an image to infer an answer. Recent VQA approaches can be partitioned into two groups: holistic models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19]</ref> and modular models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b32">33]</ref>, according to whether the approach has explicit sub-task structures. A typical holistic model, MAC <ref type="bibr" target="#b18">[19]</ref>, perform iterative reasoning steps with an attention mechanism on the image. A modular framework, NS-CL <ref type="bibr" target="#b32">[33]</ref>, designs multiple principle functions over the extracted features to explain the reasoning process.</p><p>Scene graph grounding requires to construct the rela-tionship among objects in an image. <ref type="bibr" target="#b43">[44]</ref> designs a graph R-CNN model to detect objects and classify relations among them simultaneously. <ref type="bibr" target="#b4">[5]</ref> uses graphs to ground words and phrases to image regions. <ref type="bibr" target="#b45">[46]</ref> proposes to link words to image concepts in an unsupervised setting. However, all these works have predefined object and relation concepts. We focus on inducing the concepts from the language compositionality to better interpret the reasoning framework.</p><p>Model interpretability aims to explain the neural model predictions. <ref type="bibr" target="#b5">[6]</ref> proposed network dissection to quantify interpretability of CNNs. <ref type="bibr" target="#b48">[49]</ref> explains a CNN at the semantic level with decision trees. <ref type="bibr" target="#b39">[40]</ref> generates scene graphs from images to explicitly trace the reasoning-flow. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b15">16]</ref> focused on visual attentions to provide enhanced interpretability. Our work is closely related to the self-explaining systems via rationalization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b47">48]</ref>. These works usually extract subsets of inputs as explanations, while our work moves one-step further by learning parts of the structural explanation definitions (i.e., our concept hierarchy) together with explanations (i.e., the concept-level reasoning flow).</p><p>Visual concept learning contributes to broad visuallinguistic applications, such as cross-modal retrieval <ref type="bibr" target="#b28">[29]</ref>, visual captioning <ref type="bibr" target="#b24">[25]</ref>, and visual-question answering <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4]</ref>. <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b32">33]</ref> attempt to disentangle visual concept learning and reasoning. Based on the visual concepts learned from VQA, <ref type="bibr" target="#b11">[12]</ref> learns metaconcepts, i.e., relational concepts about concepts, with augmented QA-pairs about metaconcepts. Our work differs from the previous ones in learning concepts and super concepts without external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OCCAM: Object-Centric Visual Reasoning</head><p>This section introduces a new neural architecture, objectcentric compositional attention model (OCCAM), that performs visual reasoning over the object-level visual features. This model not only achieves state-of-the-art performance, but also plays a key role in inducing object-wise or relational concepts as will be described in section 4. <ref type="figure" target="#fig_2">Figure 2</ref> shows our general framework with two training phases, each consists to the process of attaining the answers from the input images and questions. Phase 1 (blackcolored paths) corresponds to the training of our OCCAM, in which we train the object-level feature extractor, the compositional reasoning module and the question embedding LSTM. Phase 2 (red-colored paths) corresponds to the induction of symbolic concepts based on the aforementioned trained neural modules, as well as the training of a concept projection module so that the induced concepts can be accommodated in the OCCAM pipeline. The figure shows the central role that the OCCAM plays in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background on compositional reasoning</head><p>Notations. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, we name the visual vectors as vs, the output memory vector from the compositional reasoning module as m, the embedded word vectors for questions as ws, and the question embedding as q.</p><p>What is the material of the small cylinder that is the same color as the matte cube?  The compositional reasoning framework follows a VQA setting: given a question and an image as inputs, the model is required to return the correct answer choice. The target function can thus be written as: L(ws, vs, q) = ? k?K y k log F(q k , G(ws k , vs k , q k )) q k = Q(ws k ), vs k = I(im k ).</p><p>(1) K is the total number of image-question pairs, y is the onehot ground truth vector, F is the classifier, G is the reasoning module, Q is the question embedding LSTM, I is the visual feature extractor and im is the image input.</p><p>The MAC reasoning module <ref type="bibr" target="#b18">[19]</ref> processes visual and language inputs in a sequential way. Shown in <ref type="figure" target="#fig_2">Figure 2</ref> (right), each MAC cell contains a control unit that uses word embedding to control what object features should be read and written into memory and an R/W (Read/Write) unit that performs reading and writing object features; the blue diagrams labeled with w stand for fully connected layers and the symbol ? stands for Hadamard product. More details can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Object-centric compositional attention model</head><p>Our OCCAM network is shown in <ref type="figure" target="#fig_2">Figure 2</ref> with phase 1 path. It performs MAC-style reasoning, but over the objectlevel visual features generated by our proposed object-level feature extractor ( <ref type="figure" target="#fig_3">Figure 3</ref>). Fed with an image, the extractor produces a set of vectors vs, each encodes a single object's unary visual features and its interactions with other objects. The module works as the following steps:</p><p>(1) Following <ref type="bibr" target="#b32">[33]</ref>, we use Mask-RCNN <ref type="bibr" target="#b12">[13]</ref> to detect all objects in an image and output the bounding boxes for them. The image is fed to a ResNet34 network <ref type="bibr" target="#b13">[14]</ref> pretrained on ImageNet <ref type="bibr" target="#b9">[10]</ref> to generate the feature maps.</p><p>(2) On top of the ResNet34 feature maps, we apply a global average pooling to get a single global feature vector (the gray vector in the figure). We concatenate this global vector with the feature map at each location, followed by three convolution layers. This global vector is crucial since it allows the visual features to encode the interaction among objects; and the three convolution layers fuse the local and global features into a single visual vector at each position.</p><p>(3) Finally, to get object-level features from the above pixel-level fused features, we use RoI align <ref type="bibr" target="#b12">[13]</ref> to project the objects' bounding boxes onto the fused feature vectors to generate the RoI feature maps; and average pool these RoI maps for each object to produce the object-level vs.</p><p>Our object feature extractor is jointly optimized with the reasoning module with Eqn (1) in the phase 1 training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Concept Induction and Reasoning</head><p>This section describes how we achieve our goal of inducing symbolic concepts for objects and performing compositional reasoning on the induced concepts. First, we formalize the problem of concept induction (section 4.1). Second, built on the learned OCCAM network introduced in the previous section, we propose to induce concepts of both unary object properties or the binary relations between objects (section 4.2). Finally, we propose compositional reasoning over symbolic concepts by substituting the objectlevel features with the induced concepts (section 4.3).  and v2 are the object-level visual vectors representing two objects respectively, and cw is the word vector. m0 is a fixed vector and mw equals to m0 for the unary concept classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem definition</head><p>We consider identifying three types of concepts: (1) the unary concepts C u that are properties of objects (e.g., red, cube, etc.); (2) the binary concepts C b that are relation descriptions between any two objects (e.g., left, front etc.); and (3) the super concepts C sup that are hypernyms of certain subsets of concepts (e.g., color, shape, etc.), subject to that each object can only possess one concept under each super concept, e.g., cube and sphere.</p><p>As questions refer to objects and describe object relations in images and, more importantly, include all the semantic information to reach an answer, it is natural to induce the concepts from question words. Therefore we assume that all the unary and binary concepts have their corresponding words; and these words are a subset of the nouns or adjectives from all the training questions. We denote the sets of words that describe unary concepts and binary concepts as M u and M b respectively. Therefore, the goal of concept induction consists of the following tasks: ? Visual mapping: for each concept c ? C u or C b , learning a mapping from the visual feature v to c. In other words, a prediction function f c (v) ? {0, 1} is learned to predict the existence of concept c from the visual feature v of an object. ? Word mapping: for each concept c ? C u or C b , identifying a subset of words S c ? M u or M b that are synonyms representing the same concept, e.g., the concept of 'cube' corresponds to set of words {cube, cubes, block, blocks, ...}.</p><p>? Super concept induction: clustering of concepts to form super concepts. Each super concept c contains a set of con-</p><formula xml:id="formula_0">cepts {c 1 , ? ? ? , c k } ? C u or C b .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Concept induction</head><p>This section describes how we achieve the aforementioned tasks of concept induction. The key idea of our approach includes: (1) benefiting from the R/W unit from the trained MAC cells to achieve the visual mapping to textual words; (2) utilizing the inclusiveness of words' visual mapping to induce each concept's multiple word descriptions;</p><p>(3) clustering super concepts from the mutual exclusiveness between concepts. To achieve the above, we first train two binary classifiers that can determine if a word correctly describes an object's unique feature and if a word correctly describes a relation between two objects respectively. Then, Algorithm 1: Classifier training data generation. ST(?) splits a vector ??R ? to a set of ? values. GMM(?) uses Gaussian Mixture Model to cluster a set of data points. FB(?) finds the decision boundary for the 2 Gaussian components. 1 is the indicator function.</p><formula xml:id="formula_1">Result: P u , P b P u = {}, P b = {} for x ? M u ? M b do Sx = {}, bdx = 0 for vs, ws ? DATASET do for cw ? ws ? M u do Sc w = Sc w ? ST(R(vs, cw, m0)) for cw ? ws ? M b do for v ? vs do Sc w = Sc w ? ST(R(vs, cw, W(m0, v))) for x ? M u ? M b do bdx = FB(GMM(Sx)) for vs, ws ? DATASET do for v1 ? vs do for cw ? ws ? M u do y = 1(R(v1, cw, m0) &gt; bdc w ) P u = P u ? {(v1, cw, y)} for cw ? ws ? M b do for v2 ? {vs ? v1} do y = 1(R(v1, cw, W(m0, v2)) &gt; bdc w ) P b = P b ? {(v1, v2, cw, y)}</formula><p>with the help of these classifiers, we produce zero-one vectors for words that properly describe the unique features for each object and the relations between any pair of objects in single images across the dataset. Finally, we perform a clustering method on the word vectors to generalize unary and binary concepts, and the super concept sets. Visual mapping via regression from MAC cells The concept regression module is shown in <ref type="figure" target="#fig_5">Figure 4</ref>. It is composed of a classifier for the unary concept word regression,</p><formula xml:id="formula_2">B u (v 1 , c w ) ? [0, 1]</formula><p>, and a classifier for the binary concept word regression,</p><formula xml:id="formula_3">B b (v 1 , v 2 , c w ) ? [0, 1]. B u is expected to produce 1 if v 1 can be described by the word vector c w . Likewise, B b is expected to produce 1 if the relation of v 1</formula><p>to v 2 can be described by the word vector c w . We generate training data points</p><formula xml:id="formula_4">P u = {(v 1 u i , c w u i , y u i )} and P b = {(v 1 b i , v 2 b i , c w b i , y b i )} for B u and B b</formula><p>by utilizing the Read/Write unit <ref type="figure" target="#fig_2">(Figure 2</ref>(right)) in the reasoning module after phase 1 training. The whole generation process is described in Algorithm 1. We denote R(vs, c i , m i?1 ) ? R |O| for the sequence of functions before the softmax operation in the Read unit and W(m i?1 , r i ) ? R D for the function of the Write unit, where O is the set of objects in an image and D is the vector dimension.</p><p>Specifically, our algorithm first uses R(?, ?, ?) and W(?, ?) to find the attention logits on the objects corresponding to words describing the unary and binary concepts in a question as shown in <ref type="figure" target="#fig_6">Figure 5</ref>(a&amp;b). We then use the values of logits to determine if the object possesses the concept of the word (positive) or not (negative). Noticing the attention logit distribution of the sampled objects for each word is a two-peak distribution ( <ref type="figure" target="#fig_6">Figure 5</ref>(c)), we use a GMM <ref type="bibr" target="#b42">[43]</ref> with two Gaussian components to model the distribution and find the decision boundary for each word's attention logit distribution. Observe that the distribution of a binary concept word has two interfering waves, because in some cases it is hard to tell if two objects have that relation ('front' is inappropriate if two objects are on the same horizon). P u and P b are generated by classifying the data points to positives and negatives with the decision boundaries. Finally, we can train B u and B b with data P u and P b by minimizing the binary cross entropy loss. </p><formula xml:id="formula_5">? R |M u | and ? b ? R |M b | respectively: ? u = 1i&gt;0.5(B u (v1, C u )) ? b = 1i&gt;0.5(B b (v1, v2, C b )),<label>(2)</label></formula><p>where v 1 and v 2 are the object-level visual vectors of o 1 and o 2 ,</p><formula xml:id="formula_6">C u ? R |M u |?D and C b ? R |M b |?D are the stacks of word embeddings in vocabulary M u and M b . 1 ? (?)</formula><p>performs elementwise on ?: return 1 if the element satisfies condition ? or 0 otherwise.</p><p>By applying ? u and ? b to all the objects and relations in the dataset, we can attain a matrix</p><formula xml:id="formula_7">? u ? {0, 1} M u ,N u and a matrix ? b ? {0, 1} M b ,N b</formula><p>as shown in <ref type="figure" target="#fig_7">Figure 6</ref>, where N u small tiny big cube ball and N b are the total numbers of objects and co-occurred object pairs. The two matrices summarize each word's corresponding visual objects in the whole dataset. Concept/super-concept induction Finally, we group synonym words to unary and binary concepts and generate the super concepts. These two tasks are achieved via exploring the word inclusiveness and the concept exclusiveness captured by ? u and ? b : (1) words describing the same concept correspond to similar column vectors, e.g., ? u small and ? u tiny ; (2) words describing exclusive concepts have column vectors that usually do not have 1 values on same objects simultaneously, e.g., ? u cube and ? u ball . Based on the aforementioned ideas, we define the correlation metric between two words c w1 and c w2 as below:</p><formula xml:id="formula_8">1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 ? ? ? ? ? ? ? ? ? ? ? ? left right front behind 0 1 1 0 1 0 0 1 1 0 1 0 ? ? ? ? ? (a) (b)</formula><formula xml:id="formula_9">? cw 1 ,cw 2 = P (? cw 1 = 1 | ? cw 2 = 1)+ P (? cw 2 = 1 | ? cw 1 = 1) = |? cw 1 ? ? cw 2 | 1 1 |? cw 2 | 1 1 + |? cw 1 ? ? cw 2 | 1 1 |? cw 1 | 1 1 .<label>(3)</label></formula><p>Algorithm 2: Concept vector generalization. MAX(?) and HARDMAX(?) return the largest value in vector ? and its position as a one-hot vector, respectively. <ref type="figure">Figure 7</ref>: The structure of the concept projection module. We label the dimensions of matrices near them in the graph.</p><formula xml:id="formula_10">Result: K u , K b K u = 0 |O|?|E u | , K b = 0 |O|?|O|?|E b | for i ? O do for e u ? E u do K u [i][e u ] =MAX(B u (vi, ?eu )) for l u ? L u do K u [i][l u ] =HARDMAX(K u [i][l u ]) for j ? O ? {i} do for e b ? E b do K b [i][j][e b ] =MAX(B b (vi, vj, ? e b )) for l b ? L b do K b [i][j][l b ] =HARDMAX(K u [i][j][l b ]) w w conv 1D ? |"|?|"|?|$ ! | ? |"|?(|"||$ ! |) ? |"|?' ? |"|?((') ? |"|?' ? |"|? $ " ? |"|?' binary concepts ( ! ) unary concepts ( " )</formula><p>This guarantees that ? ? 0 + for two synonym words, ? ? 2 ? for two words corresponding to exclusive concepts and ? ? (0, 2) for words corresponding to different nonexclusive concepts. We can produce the correlation sets for the words describing the unary concepts and the binary concepts respectively with Eqn <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_11">? x = {?c w 1 ,cw 2 }; cw 1 , cw 2 ? M x ; x ? {u, b}<label>(4)</label></formula><p>Our final step fits two GMM on ? u and ? b respectively. Each GMM has three components N 0 , N 1 and N 2 , with their mean values initialized with 0,1 and 2. We then induce the unary and binary concepts, where each concept consists of synonym words whose mutual correlation is clustered to the Gaussian component N 0 . Similarly, we induce the super concepts, where each super concept contains multiple concepts and any two words from different concepts have correlation clustered to the Gaussian component of N 2 .</p><p>We denote the set of words corresponding to a concept e as ? e , the set of the super concept sets as L, the set of all concepts as E. Then, we can represent all the objects in an image with a unary concept matrix K u and represent all the relations between any two objects in an image with a binary concept matrix K b with Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Concept compositional reasoning</head><p>Our ultimate goal is to perform compositional reasoning to answer a question with the generated concept representa-tions K u and K b for an image; so as to confirm that our induced concepts are accurate and sufficient. We achieve this with the phase 2 training process in <ref type="figure" target="#fig_2">Figure 2</ref>. The key idea is to transplant the learned compositional reasoning module from manipulating the visual features to manipulating K u and K b , for attaining the answer to a question.</p><p>To this end, first, we project K u and K b to the same vector space with vs with the concept projection module shown in <ref type="figure">Figure 7</ref>, so that the compositional module can perform the reasoning steps on the projected concept vectors. Specifically, we first reduce the dimension of K b from</p><formula xml:id="formula_12">R |O|?|O|?|E b | to R |O|?|O||E b | , resulted inK b , because K b</formula><p>can be understood as the relations to other objects for each object in an image. Then, we use two separate fully connected networks to project K u andK b respectively, concatenate and use a sequence of 1D convolution layers to project the results to the same dimension of vs's.</p><p>Second, to minimize the discrepancy between the distribution of our projected vectors and that of the original visual vectors vs, we fix the weights of other modules in the framework and only train the concept project module by optimizing the target function Eqn. <ref type="bibr" target="#b0">(1)</ref>. Then, we train the concept projection module and the compositional reasoning module with other modules' weights fixed to better optimize Eqn. <ref type="bibr" target="#b0">(1)</ref>. The result is a compositional reasoning model that works on the induced concepts only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Settings</head><p>Datasets <ref type="formula">(1)</ref> We first evaluate our model on the CLEVR [23] dataset. The dataset comprises images of synthetic objects of various shapes, colors, sizes and materials and question/answer pairs about these images. The questions require multi-hop reasoning, such as finding the transitive relations, counting numbers, comparing properties, to attain correct answers. Each question corresponds to a ground truth human-written programs. Because the programs rely on pre-defined concepts thus do not fit our problem, we let our framework learn from scratch without using the program annotations. There are 70k/15k images and ?700k/?150k questions in the training/validation sets. We follow the previous works <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33</ref>] to train our model on the whole training set and test on the validation set.</p><p>(2) To demonstrate the generalizability of our approach, we further evaluate on the GQA dataset. GQA is a realworld visual reasoning benchmark. It consists of 113K images collected from the Visual Genome dataset <ref type="bibr" target="#b29">[30]</ref> and 22M questions. It has a train split for model training and three test splits (val, test, test-dev) <ref type="bibr" target="#b19">[20]</ref>. The dataset provides the detected object features extracted from a Faster RCNN detector <ref type="bibr" target="#b37">[38]</ref>, so each object is represented as a 2048-dimensional vector. Implementation details We include the checklist of our implementation details in Appendix B.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Object-level reasoning</head><p>We first perform the end-to-end phase 1 training shown in <ref type="figure" target="#fig_2">Figure 2</ref>, i.e., our OCCAM model. The performance comparison of our model to the state-of-the-art models is shown in <ref type="table" target="#tab_1">Table 1</ref>. Under the setting that no external human-labeled programs and no pretraining are used, our model achieves state-of-the-arts compared with published results on both CLEVR and GQA datasets. For comparison with models on GQA leaderboard, we also train our OCCAM model on train-all split and achieves an accuracy of 58.5% on the test-standard split of GQA dataset, which outperforms other popular models (e.g. MCAN, BAN and LCGN) trained with no additional data (accuracies are 57%?58%). While transformer-based methods with pretraining phase boost the performance, however, they undermine the model's explainability and make it difficult to induce concepts. On CLEVR, our model also has an onpar performance with the best model <ref type="bibr" target="#b46">[47]</ref> that uses external human-labeled programs.</p><p>Compared to the original MAC <ref type="bibr" target="#b18">[19]</ref> framework which uses image-level attentions, our model proves that the constraint of attentions on the objects are useful for improving the performance on both datasets, with significant improvement on the validation sets. We do not use the position embedding to explicitly encode the positions of objects for relational reasoning; however, we use the global features to enhance the model's understanding of inter-object relations. This shows that the relations among objects are learnable concepts without external knowledge for the deep network. <ref type="table" target="#tab_2">Table 2</ref> further gives an ablation study on the numbers of reasoning steps, i.e., the number of MAC modules, for our model. The reasoning model with 4 steps has a performance gap to the models with 8, 12 or 16 steps, while the latter three models have on-par performances. We conjecture that the model with low reasoning steps may not be able to capture multiple hops of a question and the model performance converges with an increasing number of reasoning steps. We also did ablation study on the contribution of object-level feature extractor on CLEVR dataset. With pretrained ResNet101 features, learnable ResNet34 features, learnable ResNet34 features plus global features respectively, the model achieves 97.9%, 99.0% and 99.4% on the validation set. It shows the importance of enhancing global context understanding at object level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Concept induction and reasoning</head><p>Next we evaluate the performance of our concept induction method, i.e., the phase 2 training in <ref type="figure" target="#fig_2">Figure 2</ref>. To qualitatively show that our induced concepts capture sufficient and accurate information for visual reasoning, we replace the visual inputs to the objects' induced concepts according to Section 4.3. The resulted model, denoted as OCCAM concept , is expected to perform closely to the original OCCAM with high-quality induced concepts. <ref type="table" target="#tab_3">Table 3</ref> gives the results. To achieve the balance of the performance and the interpretability, we make the OCCAM model run 8 reasoning steps for both concept induction and reasoning. It is observed that our concept-based OCCAM (with induced concept features only) achieves on-par performance with the original OCCAM model (with full input visual features). We also visualize the reasoning steps for the OCCAM concept model in Appendix F.</p><p>Human study of concept induction We present the unary concept correlations ? u in <ref type="figure" target="#fig_8">Figure 8</ref> and 9 for both CLEVR and GQA. Since GQA consists of a huge vocabulary with highly-correlated concepts, we demonstrated a sub-set of concepts associated to general words/phrases.</p><p>On CLEVR, the concept definition from the data generator can be perfectly recovered by our approach: from Fig- ure 8, the correlation between any pair of synonyms is close to 2, the correlation between words belonging to the same super concept set is close to 0, and the correlation between words belonging to two different super concept sets is in the middle of the range[0, 2]. Appendix C provides the full generated concept hierarchy, which perfectly matches the definition in CLEVR generator and human prior knowledge, i.e., 100% accuracy according to our human investigation. On GQA, the correlations between words are complicated. We present a subset of word correlations in <ref type="figure" target="#fig_9">Figure  9</ref>. Instead of using Eqn. (3), here we show the conditional probability that a column attribute exists given that the row attribute exists. We observe that words describing a similar property have a high positive correlation, such as 'yellow' and 'orange', 'concrete' and 'stone'. They can be grouped into a single concept. Words of exclusive meanings negatively correlates with each other, such as 'flying' and 'standing', 'pointy' and 'sandy'. They can be grouped into a super concept. However, the real-world data makes it difficult to induce some commonsense super concepts. For example, the same object can have multiple colors (e.g., sky can be both gray and blue). Also, object concept can have degrees (light or dark blue), so we have to use soft values to represent the concepts. We additionally conduct human studies on the pairwise accuracy of detected concept and super concept clusters, which can be found in Appendix G.</p><p>We provide more visualization results in Appendix D and E, including the extension of word analogy <ref type="bibr" target="#b33">[34]</ref> (e.g., "Madrid" -"Spain" + "France" ? "Paris") to multimodality and the quantification of distance between two visual objects with the super concept space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Our proposed OCCAM framework performs pure object-level reasoning and achieves a new state-of-theart without human-annotated functional programs on the CLEVR dataset. Our framework makes the object-word cooccurrence information available, which enables induction of the concepts and super concepts based on the inclusiveness and the mutual exclusiveness of words' visual mappings. When working on concepts instead of visual features, OCCAM achieves comparable performance, proving the accuracy and sufficiency of the induced concepts. For future works, our method can be extended to more sophisticated induction tasks, such as inducing concepts from phrases, with more complicated hierarchy, with degrees of features (e.g., dark blue, light blue) and inducing compli-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of compositional reasoning frameworks</head><p>Baseline visual reasoning framework The original compositional reasoning framework <ref type="bibr" target="#b18">[19]</ref> is similar to the phase 1 of our framework in <ref type="figure" target="#fig_2">Figure 2</ref> of the main paper, except that it works on pixel-level instead of object-level features.</p><p>To generate vs, it feeds the image to a ResNet101 <ref type="bibr" target="#b13">[14]</ref> pretrained on ImageNet <ref type="bibr" target="#b9">[10]</ref> and flatten the last feature maps across the width and height as vs. For the question inputs, we first convert each question word to its word embedding vector (ws), then input ws to a bidirectional LSTM <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11]</ref> to extract the question embedding vector q. The compositional reasoning module takes vs, ws and q as inputs and performs multi-step reasoning to attain m, the final step memory output. Finally, the classifier outputs the probability for each answer choice with a linear classifier over the concatenation of m and q.</p><p>The MAC reasoning module At each step, the i-th MAC cell receives the control signal c i?1 and the memory output from the previous step, m i?1 , and outputs the new memory vector m i . The control unit computes the single c i to control reading of vs in the R/W unit. Specifically, it computes the interactions among c i?1 , q i , and each vector in ws to produce the attention weights, and weighted averages ws to produce c i . The control unit of each MAC cell has a unique question embedding projection layer, while all other layers are shared. The R/W unit aims to read the useful vs and store the read information into m i . It first computes the interactions among m i?1 , c i?1 and each vector in vs to attain the attention weights, weighted averages vs to produce a read vector r i , and finally computes the interaction of r i and m i?1 to produce m i . The weights of the R/W units are shared across all MAC cells. The initial control signal and memory c 0 and m 0 are learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>CLEVR We set the hidden dimension D to 512 in all modules. We follow <ref type="bibr" target="#b18">[19]</ref> to design the question embedding module, the compositional module and the classifier. For the object-level feature extracter, we make the backbone ResNet34 learnable and zero-pad the output vs to 12 vectors in total for any image. Notice that the maximum number of objects in an image is 11, so that the reasoning module is able to read nothing into the memory for some steps. For the concept projection module, to cover the full view of vs, the conv1D consists of five 1D convolution layers with kernel sizes <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b4">5)</ref>, each followed by a Batch Norm layer <ref type="bibr" target="#b21">[22]</ref> and an ELU activation layer <ref type="bibr" target="#b8">[9]</ref>. We use Adam optimizer <ref type="bibr" target="#b27">[28]</ref> with momentum 0.9 and 0.999. Phase 1 and phase 2 share a same training schedule: the learning rate is initiated with 10 ?4 for the first 20 epochs  and is halved every 5 epochs afterwards until stopped at the 40th epoch. We train the concept regression module separately with learning rate of 10 ?4 for 6 epochs. All the training process is conducted with a batch size of 256.</p><p>GQA The implementation details in the GQA setting basically follows the details on CLEVR. To better handle the complexity in GQA, we concatenate the object features with their corresponding bounding box coordinates to enhance the objects' location representations similar to <ref type="bibr" target="#b17">[18]</ref>. We use GloVe <ref type="bibr" target="#b35">[36]</ref> to initialize question word embeddings and maintain an exponential moving average with a decay rate of 0.999 to update the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization of the induced concept hierarchy</head><p>After visual mapping, binary coding and concept/superconcept induction, the unary concepts and super concepts are induced as shown in <ref type="figure" target="#fig_1">Figure 10</ref>; the binary concepts are 'left', 'right', 'front' and 'behind', and {'left', 'right'} and {'front', 'behind'} form two super concept sets. The generated concept hierarchy perfectly recovers the definition in CLEVR data generator and matches human prior knowledge, showing the success of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-modal concept analogy</head><p>Our concept induction results bridge the visual and symbolic spaces. The results enable to extend word anal-ogy <ref type="bibr" target="#b33">[34]</ref> (e.g., "Madrid" -"Spain" + "France" ? "Paris") into the multi-modality setting. <ref type="figure" target="#fig_1">Figure 11</ref> gives an example, starting with the initial object v 0 and its predicted concepts K 0 , subtracting concepts K 1 and adding new concepts K 2 result in a new concept set K 3 <ref type="figure" target="#fig_1">(Figure 11 (bottom)</ref>). Then if we retrieve visual object v i with each concept set K i along the path <ref type="figure" target="#fig_1">(Figure 11 (top)</ref> With the induced concepts and super concept sets, each object can be represented with a zero-one vector, k, where the entry is 1 if that object possesses the corresponding concept or 0 otherwise. Notice that the super concept sets split the whole concept set; we thereby name the entries of k corresponding to one super concept set as a super concept. The super concept is thus a zero-one vector with exactly one entry to be 1. We name this pattern as the super concept constraint. Therefore, we can define the semantic distance between two visual objects by the number of different super concepts or by Eqn. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_13">), we have v 0 ? v 1 + v 2 ? v 3 in</formula><formula xml:id="formula_14">? k1,k2 = |k 1 ? k 2 | 1 1 2 ,<label>(5)</label></formula><p>where k 1 and k 2 are the concept vectors representing two objects and ? is the operation XOR. Studying the concepts and super concept sets induced, we acknowledge that the super concept sets correspond to color, shape, size and material in semantics. Thereby, we give an example of the se- mantic distances of multiple objects to one object as shown in <ref type="figure" target="#fig_1">Figure 12</ref>. The circle radii indicate the semantic distances to the object at the centers of these circles. The inner three circles are segmented so that each segment represents what super concepts are different. The outer circle represents all the 4 super concepts are different between the object on that circle and the object at the center. We can further interpret the semantic analogy in the visual feature space with the induced concept vectors. Shown in <ref type="figure" target="#fig_1">Figure 13</ref>, we first generate four images of different objects; then, we use our trained OCCAM structure to extract the object-level features corresponding to the objects bounded by red rectangles. Shown in <ref type="figure" target="#fig_1">Figure 14</ref>(a), we can move the visual feature vector of the leftmost object closer to that of the rightmost object by subtracting and adding visual feature vectors of two other objects. The proximity between pairs of visual feature vectors is measured with cosine similarity as shown in <ref type="figure" target="#fig_1">Figure 14</ref>(b). In the concept vector space, we can define a 'minus' operation, k 1 \k 2 , as eliminate the shared super concepts between k 1 and k 2 from k 1 . We can also define a 'plus' operation, k ? 1 ? k 2 , between a concept vector template k ? 1 and a concept vector k 2 as add the super concepts of o 2 that o ? 1 misses to o ? 1 . Therefore, The operations in the visual feature space can be explained with the operations we defined in the concept vector space shown in <ref type="figure" target="#fig_1">Figure 15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visualization of reasoning steps</head><p>We give an example of the compositional reasoning steps on the induced concept space of OCCAM as shown in <ref type="figure" target="#fig_1">Figure 16</ref>. While the attention is directly imposed on the projected concept vectors in the read unit of the compositional reasoning module, the attention can be equally mapped to the concept vectors and the visual objects as the projected concept vector to the concept vector or the projected concept vector to the visual object is a one-to-one mapping relationship. We also give an example of the compositional reasoning steps on the GQA dataset shown in <ref type="figure" target="#fig_1">Figure 17</ref>. As the dimension of the induced concept vectors is too high, here we only present the attention on objects in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Human study</head><p>We assess the concept and super concept induction by studying how the word correlation conforms with our human knowledge. We present an extended subset of GQA concept correlations shown in <ref type="figure" target="#fig_1">Figure 18</ref>. It consists of the 98 most common single words for describing objects. Each entry in the matrix represents the conditional probability that the column attribute exists given the row attribute ex-ists. A pair of mutual high correlation values between two words indicates that these words belong to the same concept, while the opposite means that the concepts represented by those words belong to a super concept. Therefore, we can evaluate the concept induction by assessing the conditional probabilities of synonyms or uncorrelated words for each word, because from us human understanding, a synonym is used to describe the same concept while an uncorrelated word describes a concept belonging to the same super concept.</p><p>For each word in the extended subset words, we first let annotators choose 2 synonyms and 2 uncorrelated words from the rest 97 words. Then, rank the four chosen words in a descending order of similarity between them and the original word. Based on these annotations, we conduct two experiments: 1) measure the accuracy of classifying the chosen words to synonyms and uncorrelated words; 2) measure the Kendall tau distance <ref type="bibr" target="#b25">[26]</ref> between the word similarity ranking based on the conditional probability and that ranking based on human knowledge.</p><p>For the first experiment, we use a binary classifier with threshold 0.5 to classify the chosen words by humans. If a word's conditional probability given the original word is greater than the threshold, this word is classified as a synonym; if smaller, this word is classified as an uncorrelated word. The accuracy can be calculated with Eqn. <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_15">A = 1 |S| i?S 1 |Wi| ( j?W pos i 1(Ri,j &gt; t) + j?W neg i 1(Ri,j &lt; t)),<label>(6)</label></formula><p>where A represents accuracy, S is the subset of words, W i represents the set of synonyms and uncorrelated words chosen for word i, R i,j represents the conditional probability of word j given word i exists and t is the threshold. For comparison, we also calculate the cosine similarity of word GloVe <ref type="bibr" target="#b35">[36]</ref> embeddings to substitute the conditional probability and serve as R in Eqn. <ref type="bibr" target="#b5">(6)</ref>. For this setting, we tune the threshold t to be 0.21 to reach the best accuracy. The result in <ref type="table" target="#tab_4">Table 4</ref> shows that our induction highly conforms with our human sense in grouping words into concepts but does not agree much with humans in grouping super concepts. By further studying specific cases, we realize that a word and its uncorrelated words defined by humans can simultaneously describe one object. For example, 'white' and 'black' can be used together to describe a zebra; 'leafy' and 'leafless' both describes a status of a plant. Such words have high correlations, which defects with our human understanding.</p><p>The second experiment measures how the induced word proximity conforms with our human knowledge. For a word w i , our annotators rank the chosen synonyms and uncorrelated words a i = (a i1 , a i2 , a i3 , a i4 ) with a descending order of word similarity to w i and assign a sequence   <ref type="figure" target="#fig_1">1, 2, 3)</ref> to a i . Then, we rank (a i1 , a i2 , a i3 , a i4 ) with a descending order of their conditional probabilities and assign a sequence of order indices O induce i to a i . For comparison, we further rank (a i1 , a i2 , a i3 , a i4 ) in a descending order of cosine similarities between the GLoVe embeddings of (a i1 , a i2 , a i3 , a i4 ) and w i and assign a sequence of order indices O word2vec i to a i . The average ranking distance can be calculated with Eqn. <ref type="bibr" target="#b6">(7)</ref>.</p><formula xml:id="formula_16">D(O x ) = 1 |S| i?S K(O human i , O x i ),<label>(7)</label></formula><p>where D represents the average ranking distance, x ? {induce, word2vec}, K represents the operation for calculating the normalized Kendall tau distance between two  rankings. The result in Table <ref type="bibr" target="#b4">(5)</ref> proves that our induction from visual language relations encodes word proximity that is more aligned with human knowledge than the one encoded by GloVe embeddings from language-only data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Error analysis</head><p>The reasoning process may reach a false answer if 1) a concept is mentioned in the question and 2) that concept There is a big brown metal cylinder; how many large matte cubes are behind it? 0 1</p><p>What is the color of the rubber cube? red red <ref type="figure" target="#fig_1">Figure 19</ref>: Error analysis. The predicted unary and binary concepts corresponding to each object in the image above are shown in the tables at the middle; the digits colored in red are wrong predicted concepts. The questions, the predicted answers and the ground truth answers are shown in the table at the bottom.</p><p>is wrongly classified for the objects ought to be attended. However, the reasoning process may still reach a correct answer if either of these two conditions is not sufficed. We present two examples in <ref type="figure" target="#fig_1">Figure 19</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>matteFigure 1 :</head><label>1</label><figDesc>Illustration of our framework. Our model induces the concepts and super concepts with the attention correlation between the objects and question words in image-question pairs as the paths shown in blue arrows. Then, it answers a question about an image via compositional reasoning on the induced symbolic representations of objects and object relations, shown as the orange paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The framework and the compositional reasoning module. The left graph shows the general framework; The phase 1 training path is drawn in purple and the phase 2 training paths are drawn in red. The black paths are shared for both training phases. The structures of our proposed object-level feature extractor, concept regression module and concept projection module are shown in Figures 3, 4 and 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of our object-level feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>The structure of the concept regression module. v1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Attention visualization and attention logit distributions. (a) The attention visualization corresponding to the words describing the unary concepts by performing R(vs, cw, m0). Each of the words above the latter 4 images corresponds to a unique cw and the value on each object is the attention logit (the same applies to (b)). (b) The attention visualization corresponding to the words describing the binary concepts by performing R(vs, cw, W(m0, v2)). v2 represents the object bounded by a red rectangle in the first image. (c) the attention logit distribution corresponding to each word describing a concept.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>The zero-one matrices indicating word descriptions of objects and object relations across the dataset. (a) The matrix ? u indicates what words can describe objects. (b) The matrix ? b indicates what words can describe the relations object v1's (bounded by green rectangles) are to object v2's (bounded by red rectangles).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>The CLEVR unary concept correlations ? u .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>The subset of GQA concept correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>cated relations between objects (e.g. a little bigger).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>unary concepts/super-concepts (b) binary concepts/super-concepts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Concepts and super concept sets. Each circle represents a concept described by the words in that circle. A super concept set comprises the concepts represented by circles of the same color. + ? ! = {brown, sphere, large, matte} " = {brown, sphere, small, metal} # = {purple, cylinder, small, metal} $ = {purple, cylinder, large, matte} An multi-modal analogy example enabled by our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Illustration of the semantic distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>The original images for extracting visual features. The object-level features corresponding to the objects bounded by red rectangles are used for the illustration of semantic operations in the visual feature space. Illustration of the semantic analogy in the visual feature space. (a) The operations on the visual features. (b) The cosine similarities between pairs of visual feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Operations on the concept vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Visualization of reasoning steps on CLEVR dataset. (a) The question, image, prediction and ground truth answer. The index of each object is shown on the upper left of the object. (b) The induced concepts of objects and relations. (c) The stepwise attentions on question words. (d) The stepwise attentions on objects. (e) The concept vector read into the memory of the reasoning module in each step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Visualization of reasoning steps on GQA dataset. of order indices O human i = (0,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>The extended subset of GQA concept correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Binary coding of objects With trained B u and B b , we represent an object o 1 with a binary code vector. Each dimension corresponds to a word. A dimension has value 1 if the corresponding word can describe o 1 and 0 otherwise. The binary vectors of object properties and of the relations between two objects, o 1 and o 2 can be computed with the functions ? u</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The comparison of our OCCAM framework to the state-of-the-art methods on CLEVR (left) and GQA (right) datasets. ? means training with additional program supervision. ? means pretraining on larger visual+language corpora. ? means pretraining a scene-graph extraction model with additional rich annotated data.</figDesc><table><row><cell></cell><cell></cell><cell>(a) CLEVR</cell><cell></cell><cell></cell><cell></cell><cell>(b) GQA</cell><cell></cell></row><row><cell>method</cell><cell cols="2">overall count exist comp</cell><cell>query</cell><cell>comp</cell><cell>method</cell><cell>val</cell><cell>test-dev</cell><cell>test</cell></row><row><cell></cell><cell></cell><cell>num</cell><cell>attr</cell><cell>attr</cell><cell>MAC [3]</cell><cell>57.5</cell><cell>-</cell><cell>54.1</cell></row><row><cell>RN [39]</cell><cell>95.5</cell><cell cols="3">90.1 93.6 97.8 97.1 97.9</cell><cell>LXMERT [41]</cell><cell>-</cell><cell>50.0</cell><cell>-</cell></row><row><cell>FiLM [37]</cell><cell>97.6</cell><cell cols="3">94.5 93.8 99.2 99.2 99.0</cell><cell>LCGN [18]</cell><cell>63.9</cell><cell>55.8</cell><cell>56.1</cell></row><row><cell>MAC [19]</cell><cell>98.9</cell><cell cols="3">97.2 99.4 99.5 99.3 99.5</cell><cell cols="2">OCCAM (ours) 64.5</cell><cell>56.2</cell><cell>56.2</cell></row><row><cell>NS-CL [33] OCCAM (ours) NS-VQA  ? [47]</cell><cell>98.9 99.4 99.8</cell><cell cols="3">98.2 99.0 98.8 99.3 99.1 98.1 99.8 99.0 99.9 99.9 99.7 99.9 99.9 99.8 99.8</cell><cell>MMN [8]  ? NSM [21]  ? LXMERT [41]  ?</cell><cell>---</cell><cell>60.4 63.0 60.0</cell><cell>60.8 63.2 60.3</cell></row><row><cell>Human [24]</cell><cell>92.6</cell><cell cols="3">86.7 96.6 86.5 95.0 96.0</cell><cell>ViLT [27]  ? ?</cell><cell>-</cell><cell>65.1</cell><cell>64.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effect of the choice of reasoning steps for our model.</figDesc><table><row><cell>steps</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16</cell></row><row><cell>accuracy (CLEVR)</cell><cell cols="4">94.3 98.6 99.4 99.1</cell></row><row><cell cols="5">accuracy (GQA test-dev) 55.1 55.6 55.2 56.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of our visual feature-based OCCAM and our concept-only OCCAM. The number of reasoning steps is 8.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>(a) CLEVR</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) GQA</cell><cell></cell></row><row><cell>method</cell><cell cols="6">overall count exist comp num query attr comp attr</cell><cell>method</cell><cell>val</cell><cell>test-dev</cell></row><row><cell>OCCAMvisual</cell><cell>98.6</cell><cell>95.9</cell><cell>99.8</cell><cell>96.2</cell><cell>99.8</cell><cell>99.7</cell><cell>OCCAMvisual</cell><cell>63.8</cell><cell>55.6</cell></row><row><cell>OCCAMconcept</cell><cell>97.9</cell><cell>95.6</cell><cell>98.7</cell><cell>97.3</cell><cell>98.4</cell><cell>99.3</cell><cell cols="2">OCCAMconcept 63.1</cell><cell>54.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The accuracy of classifying synonyms and uncorrelated words. A pos represents the accuracy of classifying only synonyms. A neg represents the accuracy of classifying only antonyms. ? For word2vec, we tune the threshold on ground truth, while our method is used out of the box without threshold tuning (i.e., threshold set to 0.5).</figDesc><table><row><cell>Method</cell><cell>A pos</cell><cell>A neg</cell><cell>A</cell></row><row><cell cols="4">word2vec  ? 76.02% 60.71% 68.37%</cell></row><row><cell>induction</cell><cell cols="3">92.35% 63.78% 78.06%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The average ranking distance to human rankings.</figDesc><table><row><cell cols="2">D(O word2vec ) D(O induce )</cell></row><row><cell>0.3418</cell><cell>0.2585</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work is in part supported by IBM-Illinois Center for Cognitive Computing Systems Research (C3SR) -a research collaboration as part of the IBM AI Horizons Network.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NACACL)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NACACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">G3raphground: Graph-based language grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4281" to="4290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6541" to="6549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to explain: An information-theoretic perspective on model interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="883" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meta module network for compositional visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual concept-metaconcept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5001" to="5012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explainable neural computation via stack neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language-conditioned graph networks for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10294" to="10303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning by abstraction: The neural state machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03950</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2989" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maurice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Vilt: Visionand-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03334</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attentive explanations: Justifying decisions and pointing to the evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Dong Huk Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04757</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Explainable and explicit visual reasoning over scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8376" to="8384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5103" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Em algorithms of gaussian mixture model and hidden markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqi</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="145" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised textual grounding: Linking words to image concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6125" to="6134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural-symbolic vqa: Disentangling reasoning from vision and language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1031" to="1042" />
		</imprint>
	</monogr>
	<note>Pushmeet Kohli, and Josh Tenenbaum</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking cooperative rationalization: Introspective extraction and complement control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4085" to="4094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Interpreting cnns via decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6261" to="6270" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
