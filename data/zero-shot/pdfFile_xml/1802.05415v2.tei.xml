<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Teaching Machines to Code: Neural Markup Generation with Interpretable Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumeet</forename><forename type="middle">S</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Independent Researcher Saratoga</orgName>
								<address>
									<postCode>95070</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Teaching Machines to Code: Neural Markup Generation with Interpretable Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a neural transducer model with visual attention that learns to generate L A T E X markup of a real-world math formula given its image. Applying sequence modeling and transduction techniques that have been very successful across modalities such as natural language, image, handwriting, speech and audio; we construct an image-to-markup model that learns to produce syntactically and semantically correct L A T E X markup code over 150 words long and achieves a BLEU score of 89%; improving upon the previous state-of-art for the Im2Latex problem. We also demonstrate with heat-map visualization how attention helps in interpreting the model and can pinpoint (localize) symbols on the image accurately despite having been trained without any bounding box data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past decade, deep neural network models based on RNNs 1 , CNNs 2 and 'attention' <ref type="bibr" target="#b29">[29]</ref> have been shown to be very powerful sequence modelers and transducers. Their ability to model joint distributions of real-world data has been demonstrated through remarkable achievements in a broad spectrum of generative tasks such as; image synthesis <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b25">25]</ref>, image description <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b30">30]</ref>, video description <ref type="bibr" target="#b7">[7]</ref>, speech and audio synthesis <ref type="bibr" target="#b26">[26]</ref>, handwriting recognition <ref type="bibr">[12,</ref><ref type="bibr" target="#b1">2]</ref>, handwriting synthesis <ref type="bibr" target="#b9">[9]</ref>, machine translation <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b24">24]</ref>, speech recognition <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b11">11]</ref>, etc. <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b29">29]</ref> One class of sequence models employ the so-called encoder-decoder <ref type="bibr" target="#b5">[5]</ref> or sequence-to-sequence <ref type="bibr" target="#b24">[24]</ref> architecture, wherein an encoder encodes a source sequence into feature vectors, which a decoder employs to produce the target sequence. The source and target sequences may either belong to the same modality (e.g. in machine translation use-cases) or different modalities (e.g. in image-to-text, text-to-image, speech-to-text); the encoder / decoder sub-models being constructed accordingly. The entire model is trained end-to-end using supervised-learning techniques. In recent years, this architecture has been augmented with an attention and alignment model which selects a subset of the feature vectors for decoding. It has been shown to help with longer sequences <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">19]</ref>. Among other things, this architecture has been used for image-captioning <ref type="bibr" target="#b31">[31]</ref>. In our work we employ a encoder-decoder architecture with attention, to map images of math formulas into corresponding L A T E X markup code. The contributions of this paper are: 1) Solves the Im2Latex problem 100 and improves over the previous best reported BLEU score by 1.27% BLEU, 2) Pushes the boundaries of the neural encoder-decoder architecture with visual attention, 3) Analyses variations of the model and cost function. Specifically we note the changes to the base model <ref type="bibr" target="#b31">[31]</ref> and what impact those had on performance, 4) Demonstrates the use of attention visualization for model interpretation and <ref type="bibr" target="#b0">1</ref> Recurrent Neural Network. <ref type="bibr" target="#b1">2</ref> Convolutional Neural Networks and variants such as dilated CNNs <ref type="bibr" target="#b32">[32]</ref>. <ref type="bibr" target="#b5">5</ref>) Demonstrates how attention can be used to localize objects (symbols) in an image despite having been trained without bounding box data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The IM2LATEX problem</head><p>The IM2LATEX Problem is a request for research proposed by OpenAI. The challenge is to build a Neural Markup Generation model that can be trained end-to-end to generate the L A T E X markup of a math formula given its image. Data for this problem was produced by rendering single-line real-world L A T E X formulas obtained from the KDD Cup 2003 dataset. The resulting grayscale images were used as the input samples while the original markup was used as the label/target sequence. Each training/test sample <ref type="figure">(Figure 1)</ref> is comprised of an input image x and a cor-</p><formula xml:id="formula_0">S 0 = l 1 2? 2 l Tr ? a l ? a ?l + l 1 2 2 l</formula><p>Tr f a l f a ?l + r 1 g r Tr? a r ? a r . 2 Image to markup model Our model <ref type="figure">(Figure 2a</ref>) has the same basic architecture as <ref type="bibr" target="#b31">[31]</ref> (which we call our baseline model) in the way the encoder, decoder and a visual attention interact. However there are significant differences in the sub-models which we notate in the remainder of this paper and in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder</head><p>All images are standardized to a fixed size by centering and padding with white pixels. Then they are linearly transformed (whitened) to lie in the range [-0.5,0.5].</p><p>A Each pooled feature vector can be viewed as a rectangular window into the image, bounded by its receptive field. <ref type="bibr" target="#b3">3</ref> The idea behind this is to partition the image into spatially localized regional encodings and setup a decoder architecture (Section 2.2) that selects/emphasizes only the relevant  regions at each time-step t, while filtering-out/de-emphasizing the rest. Bahdanau et al. 1 showed that such piecewise encoding enables modeling longer sequences as opposed to models that encode the entire input into a single feature vector <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b5">5]</ref>. <ref type="bibr" target="#b4">4</ref> Pooling allows us to construct encoders with different receptive field sizes. We share results of two such models: I2L-NOPOOL with no feature pooling and pooled feature grid shape <ref type="bibr" target="#b4">[4,</ref><ref type="bibr">34]</ref> and I2L-STRIPS having stride <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b0">1]</ref> and pooled feature grid shape <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">34]</ref>. Finally, for convenience we represent A as a flattened sequence a (Equation 2). See the appendix for more details.</p><formula xml:id="formula_1">Encoder Decoder RNN C t y t-1 , C t-</formula><formula xml:id="formula_2">LSTM 1 LSTM Q c t Q c t 1 h t 1 h t Q h t-1 Q , c t-1 Q h t-1 1 , c t-1 1 ? t h t-1 Q z t z t C 0 ? t a E DRNN (b)</formula><p>a := (a 1 , . . . , a L ) ; a l ? R D ; l = H(h ? 1) + w; L = HW (2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoder</head><formula xml:id="formula_3">p t : {1, . . . , K} ? [0, 1] y t ? p t p t (y t ) := P r (y t |y &lt;t , a)<label>(3)</label></formula><formula xml:id="formula_4">P r (y|a) = ? t=1 p t (y t )<label>(4)</label></formula><p>The decoder is a language modeler and generator. It is a Recurrent Neural Network (DRNN in <ref type="figure">Figure 2b</ref>) that models the discrete probably distribution p t , of the output word y t , conditioned on the sequence of previous words y &lt;t and relevant regions of the encoded image a 5 (Equations 3). Probability of the entire output sequence y given image a is therefore given by Equation 4.</p><formula xml:id="formula_5">DRNN : {a; y t?1 ; C t?1 } ? {p t ; C t }<label>(5)</label></formula><p>The DRNN receives the previous word y t?1 and encoded image a as inputs. In addition, it maintains an internal state C t that propagates information (features) extracted from an initial state, the output sequence unrolled thus far and image regions attended to thus far (Equation 5). It is as complex model, comprised of the following sub-models ( <ref type="figure">Figure 2b</ref>): 1) A LSTM-Stack <ref type="bibr" target="#b13">[13]</ref> responsible for memorizing C t and producing a recurrent activation H t , 2) A Visual attention and alignment model responsible for selecting relevant regions of the encoded image for input to the LSTM-Stack, <ref type="bibr" target="#b6">6</ref> 3) A Deep Output Layer <ref type="bibr" target="#b20">[20]</ref> that produces the output probabilities p t , 4) Init Model: A model that generates the initial state C 0 and 5) An embedding matrix E (learned by training) that transforms y t into a dense representation ? R m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Inferencing</head><p>After the model is trained, the output sequence is generated by starting with the word 'bos' and then repeatedly sampling from p t until &lt;eos&gt; is produced. The sequence of words thus sampled is the predicted sequence:? := (? 1 , . . . ,?? ) ;? t ? R K . For this procedure we use beam search decoding <ref type="bibr" target="#b8">[8]</ref> with a beam width of 10. <ref type="figure">Figure 1</ref> shows an example predicted sequence and <ref type="figure">Figures 5 and 6</ref> show examples of predictions rendered into images by a L A T E X 2 ? compiler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Visual attention and alignment model</head><formula xml:id="formula_6">? t := (? t,1 , . . . , ? t,L ) 0?? t,l ?1 L l ? t,l =1 (6) ? t = f att (a; H t?1 ) (7) z t = ? t a<label>(8)</label></formula><p>As previously alluded, the decoder soft selects/filters relevant (encoded) image regions at each step. This is implemented via. a 'soft attention' mechanism 7 which computes a weighted sum z t of the pooled feature vectors a l . The visual attention model f att , computes the weight distribution ? t (Equations 6, 7 and 8). f att is modeled by an MLP (details in the appendix). While it is a possible for ? t to end up uniformly distributed over (a 1 . . . a L ), in practice we see a unimodal shape with most of the weight concentrated on 1-4 neighborhood (see <ref type="figure">Figure 3</ref>) around the mode. We call this neighborhood the focal-region -i.e. the focus of attention. In other words we empirically observe that the attention model's focus is 'sharp'; converging towards the 'hard attention' formulation described by Xu et al. <ref type="bibr" target="#b31">[31]</ref>. Also note that <ref type="figure">(Figure 3</ref>), the attention model is able to utilize the extra granularity available to it in the I2L-NOPOOL case and consequently generates much sharper focal-regions than I2L-STRIPS. Furthermore, the model aligns the focal-region with the output word and thus scans text on the image left-to-right (I2L-STRIPS) or left-right and up-down (I2L-NOPOOL) just like a person would read it ( <ref type="figure">Figure 3</ref>). We also observe that it doesn't focus on empty margins of the image except at the first and last (&lt;eos&gt;) steps which is quite intuitive for determining the beginning or end of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">LSTM stack</head><formula xml:id="formula_7">LST M q : {x q t ; h q t?1 ; c q t?1 } ? {h q t ; c q t } 1 ? q ? Q ; h q t , c q t ? R n x q t = h q?1 t ; q = 1 (9) x 1 t = {z t ; Ey t?1 }</formula><p>The core sequence generator of the DRNN is a multilayer LSTM <ref type="bibr" target="#b9">[9]</ref>  <ref type="figure">(Figure 2b</ref>). Our LSTM cell implementation follows Graves et al. <ref type="bibr" target="#b11">[11]</ref>. The LSTM cells are stacked in a multi-layer configuration <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b20">20]</ref> as in <ref type="bibr">Equation 9</ref>. LST M q is the LSTM cell at position q with x q t , h q t and c q t being its input, hidden activation and cell state respectively. LST M 1 receives the stack's input: soft attention context z t and previous output word Ey t?1 . LST M Q produces the stack's output H t = h Q t , which is sent up to the Deep Output Layer. Accordingly, the stack's activation (H t ) and state (C t ) are defined as:</p><formula xml:id="formula_8">H t = h Q t and C t := (c 1 t , . . . , c Q t , h 1 t , . . . , h Q t )</formula><p>. We <ref type="bibr" target="#b6">6</ref> The LSTM-Stack and Visual Attention and Alignment model jointly form a Conditioned Attentive LSTM (CALSTM); Ht and Ct being its activation and internal state respectively. Our source-code implements the CALSTM as a RNN cell which may be used as a drop-in replacement for a RNN cell. <ref type="bibr" target="#b7">7</ref> 'Soft' attention as defined by Xu et al. <ref type="bibr" target="#b31">[31]</ref> and originally proposed by Bahdanau et al. <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure">Figure 3</ref>: Focal-regions learnt by the attention model: to the left by I2L-STRIPS and to the right by I2L-NOPOOL. Image darkness is proportional to ? t . Notice how ? t concentrates on the image region corresponding to the output word (shown above the image). The \frac command starts a fraction, \mathrm sets a font and \eos is the &lt;eos&gt; token.</p><p>do not use skip or residual connections between the cells. Both of our models have two LSTM layers with n = 1500. Further discussion and details of this model can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Deep output layer</head><p>We use a Deep Output Layer <ref type="bibr" target="#b20">[20]</ref> to produce the final output probabilities: p t = f out (H t ; z t ; Ey t?1 ). f out is modeled by an MLP. Note that the output layer receives skip connections from the LSTM-Stack input (Equation <ref type="formula">9</ref>). Details of this model can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">Init model</head><p>Hidden MLP The Init Model f init , produces the initial state C 0 of the LSTM-Stack. f init is intended to 'look' at the entire image (a) and setup the decoder appropriately before it starts generating the output.</p><formula xml:id="formula_9">f init : a ? (c 1 0 , . . . , c Q 0 , h 1 0 , . . . , h Q 0 ) (10) h q 0 , c q 0 ? R n<label>That</label></formula><p>said, since it only provides a very small improvement in performance in exchange for over 7 million parameters, its need could be questioned. f init is modeled as an MLP with common hidden layers and 2Q distinct output layers, one for each element of C 0 , connected as in <ref type="figure" target="#fig_0">Figure 4</ref>. See the appendix for more detail and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><formula xml:id="formula_10">J = ? 1 ? log (P r (y|a)) + ? R R (11) R = 1 2 ? ? 2 (11a)</formula><p>The entire model was trained end-to-end by minimizing the objective function J (Equation 11) using back propagation through time. The first term in Equation <ref type="bibr" target="#b11">11</ref> is the average (per-word) log perplexity of the predicted sequence <ref type="bibr" target="#b8">8</ref> and is the main objective. R is the L2-regularization term, equal to L2-norm of the model's parameters ? (weights and biases) and ? R is a hyperparameter requiring tuning. Following Xu et al. <ref type="bibr" target="#b31">[31]</ref> at first, we had included a penalty term intended to bias the distribution of the cumulative attention placed on an imagelocation ? l := ? t=1 ? t,l . However we removed it for various reasons which are discussed in the appendix along with other details and analyses.</p><p>We split the dataset into two fixed parts: 1) training dataset = 90-95% of the data and 2) test dataset 5-10%. At the beginning of each run, 5% of the training dataset was randomly held out as the validation-set and the remainder was used for training. Therefore, each such run had a different training/validation data-split, thus naturally cross-validating our learnings across the duration of the project. We trained the model in minibatches of 56 using the ADAM optimizer <ref type="bibr" target="#b17">[17]</ref>; periodically evaluating it over the validation set <ref type="bibr" target="#b9">9</ref> . For efficiency we batched the data such that each minibatch had similar length samples. For the final evaluation however, we fixed the training and validation dataset split and retrained our models for about 100 epochs (? 2 1 2 days). We then picked the model-snapshots with the best validation BLEU score and evaluated the model over the test-dataset for publication. <ref type="table" target="#tab_4">Table 1</ref> lists the training parameters and metrics of various configurations. Training sequence predictions (?) were obtained by CTC-decoding [10] p t . Training BLEU score was then calculated over 100 consecutive mini-batches. We used two Nvidia GeForce 1080Ti graphics cards in a parallel towers configuration. Our implementation uses the Tensorflow toolkit and is distributed under AGPL license. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Given that there are multiple possible L A T E Xsequences that will render the same math image, ideally we should perform a visual evaluation. However, since there is no widely accepted visual evaluation metric, we report corpus BLEU (1,2,3 &amp; 4 grams) and per-word Levenstein Edit Distance 10 scores (see <ref type="table" target="#tab_6">Table 2</ref>). We also report a (non-standard) exact visual match score 103 which reports the percentage of exact visual matches, discarding all partial matches. While the predicted and targeted images match in at least 70% 103 of the cases, the model generates different but correct sequences (i.e. y =?) in about 40% of the cases ( <ref type="figure">Figure 5</ref>). For the cases where the images do not exactly match, the differences in most cases are minor ( <ref type="figure">Figure 6</ref>). Overall, our models produce syntactically correct sequences <ref type="bibr" target="#b11">11</ref> for at least 99.85% of the test samples ( <ref type="table" target="#tab_6">Table 2)</ref>. Please visit our website to see hundreds of sample visualizations, analyses and discussions, data-set and source-code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Interpretability via attention</head><p>Since the LSTM stack only sees a filtered view (i.e. focal-region) of the input, it can only base its predictions on the focal-regions seen thus far and initial-state C 0 . Further since the init-model has a negligible impact on performance we can drop it from the model <ref type="table" target="#tab_4">(Table 1</ref>) and thereby the <ref type="bibr" target="#b9">9</ref> Evaluation cycle was run once or twice per epoch and/or when a training BLEU score calculated on sequences decoded using CTC-Decoding <ref type="bibr" target="#b10">[10]</ref> jumped significantly. <ref type="bibr" target="#b10">10</ref> i.e. Edit distance divided by number of words in the target sequence. <ref type="bibr" target="#b11">11</ref> i.e. Those that were successfully rendered by L A T E X 2?. <ref type="bibr">103</ref> We use the 'match without whitespace' algorithm provided by Deng et al. <ref type="bibr" target="#b6">[6]</ref> wherein two images count as matched if they match pixel-wise discarding white columns and allowing for upto 5 pixel image translation (a pdflatex quirk). It outputs a binary match/no-match verdict for each sample -i.e. partial matches however close, are considered a non-match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image / Rendered Sequence</head><formula xml:id="formula_11">y len?len 0 T +q +2?2 = 1 2 ? i q? (? +2i +2 ? 1? ?2? ?? +2i ?2 ? 1? +2? ), T +q +p?2 = 1 2 ? i q? ? +2i +p ? 1? ?2? , 147 155 1 ?ij (x ? ,y ? ;x + )= dP ? 4? dp + 4? dk + 4? e ? i 2 P ? x + e ? i 2 (p + x ?? k + y ? ) ?ij (p + ,k + ;P ? ), 150 151 2 G(f ) (n) ? =? n m=0 {? (n?m) ? ,f (m) } (q) +? (n?2) m=0 {? (n?m) ? ,f (m+2) } (?) +{? (n+1) ? ,f (1) } (?) 150 150 3 S0= l 1 2? 2 l Tr ? a l ? a ?l + l 1 2 2 l</formula><p>Tr f a l f a ?l + r 1 gr Tr? a r ? a r . 145 148</p><formula xml:id="formula_12">4 ds 2 =? t 2 (t 2 +r 2 ? )(t 2 ?r 2 + ) dt 2 +t 2 (d?+ r + r ? t 2 dr) 2 + (t 2 +r 2 ? )(t 2 ?r 2 + ) t 2 dr 2 . 147 147 5 H= 1 2E U ? ? ? ? 0 0 0 0 ?m 2 21 0 0 0 ?m 2 31 ? ? ? ? U ? + 1 2E ? ? ? ? a ?b 0 ? * b ? b 0 0 0 0 ? ? ? ? , 147 147 6 D ab ?? (p,p3)= ? ab ? a3 p 2 ?p 2 3 +i ?g?? +p?p? (1??p 3 ,0) 1 p 2 3 +?p 3 ,0(1??) 1 p 2 +i 139 145 7 V (H1,H2)= 1 8 (g 2 2 +g 2 1 )(|H1| 2 ?|H2| 2 ) 2 +m 2 1 |H1| 2 +m 2 2 |H2| 2 ?m 2 3 (H1H2+h.c.) 144 145 8 A 3 0 (? ?0)=2g d ? (1) ? ? (2) ? ? (3) ? {? ?? (p ? 1 ?p ? 2 )+? ?? (p ? 3 ?p ? 1 )+? ?? (p ? 2 ?p ? 3 )}. 146 145 9 U ? L M l U l R =M * l , U ? L M L U * L =M * L , U ? L M D U ? R =M * D , U ? R T M R U ? R =M * R . 149 145 10 ? ?gg ? 1 ? 1 g ? 2 ? 2 ???g ? d?p ? d?pF ? 1 ? 2 ...? d?p = 1 p! ? 1 ? 2 ...? d?p ? 1 ? 2 ...?p F? 1 ? 2 ...?p , 147 145 11 dE dz = dE el dz + dE rad dz ? C 2 ?s ? ? 2 ln 3ET 2? 2 ln 9E ? 3 T + 3? 2 ?s 2? 2 T 2 . 130 144 12 L0=(2n+1) |h| 2 +|h|d ? 0 d0? |h| 2 ?|h| ? k=1 (d ? k d k ?d ? kd k +a ? k a k ?b ? k b k )+L f ree 0 . 149 144 13 Q7? = e 8? 2 m bq? ? ?? (1+?5)b?F?? , Q 8G = g 8? 2 m bq? ? ?? t a ?? b ? G a ?? , (q=d or s). 141 143 14 ds 2 = ? u 2 h(u) R 2 e ?A dx 2 0 + u 2 R 2 e ?C dx 2 i + R 2 u 2 h(u) e ?B du 2 +R 2 e ?D d? 2 5 , 143 143 15 sin(p 1 ?k 2 ) sin(p 2 ?k 2 ) sin(p 3 ?k 2 )=? 1 4 (sinp1?k+sinp2?k+sinp3?k) 133 143 16 [P0,X0]=i, [Pi,Xj ]=?i ?ij 1? P 2 ? 2 e P 0 /? , [P0,Xi]=? 2i ? Pi e P 0 /? 139 143 17 J a 1 ? 1 (P1)...J an ?n (Pn) T ?(?i) n N T 2 12 ?? a 1 ...an ? 1 ...?n (P1,...,Pn)+O( 1 f 2 ? ) , 143 143 18??( k 1 ) ?? ? ( k1)=N 2 ? cos 2 ?( k)? | p| 2 4 (cos?( k)c2( k)+ 1 3 k 2 b1( k) 2 ) . 142 142 19 ?0(y)= 1 ? 1 =0 .. 1 ? 4 =0 Y (0) ? 1 ..? 4 (t1Z1) ? 1 (t2Z2) ? 2 (t5Z5) ? 3 (t8Z8) ? 4 143 142 20 sin(2?) 32 ? 2 I(?)? sin(2?) 32 ? 2 (I(?)+c 2 2 c 2 3 ?1I(M G ,M G1 )+c 2 2 s 2 3 ?2I(M G ,M G2 )) 142 142 21 W=YeL j E c H i 1 ij +Y d Q ja D c a H i 1 ij +YuQ ja U c a H i 2 ij +?H i 1 H j 2 ij 141 141 22 ?i? (?) ? ? ? (?) ??=?iC?? ?s(T I ) ? ? ? Adj (u ?1 ) a I ea=?iC?? ?s(T I ) ? ? k (I) ,</formula><p>145 141 <ref type="figure">Figure 5</ref>: A sample of correct predictions by I2L-STRIPS. We've shown the long predictions hence lengths are touching 150. Note that at times the target length is greater than the predicted length and at times the reverse is true (though the original and predicted images were identical). All such cases would evaluate to a less than perfect BLEU score or edit-distance. This happens in about 40% of the cases. For more examples visit our website. </p><formula xml:id="formula_13">y? 0 ?:S 2 ?{M1,M2} ?:S 2 ?{M1,M2} 1 ln E+ ? E 2 ?m 2 l ?m? E? ? E 2 ?m 2 l ?m? . ln E+ ? E 2 ?m 2 t ?m? E? ? E 2 ?m 2 l ?m? . 2 ( r 0 s )d?g 2?k s . ( r 0 s )d?g 2?k s . 3?a+ b a p b ???I?=0?a+ b a p b ??? ??=0 5T =V +t?V ,T =V +f?V , 7 ? A =(? ? , ??), ? A =(??, ??), {?A,? B }=? B A ? A =(? ? , v?), ? A =(??, ??), {?A,? B }=? B A 8?? (M Z )| DR = m pole ? ? e?? (m pole ? )| DR v(M Z )| DR cos?(M Z )? ? (M Z )| DG = m vot r ?kez? (m pele r )| DH ?(M Z )|DEcs?(M Z )<label>9 3</label></formula><formula xml:id="formula_14">10 D ? ? =(? 2 +2eBS3) ? ? D ? ? =(? 2 +2eBS3) ? ? 12 dzdzdbdb n V (f )=0 dzdzdbd n V (f )=0 13 L (p 4 ) ?S=1 =G8F 2 37 i=1 NiWi L (s 0 ) ?S 2 =1 =G8F 2 37 i=1 NiWi 14 det(M (0) (N0))=? Q(0) r,s [?(h?hr,s)] P (N 0 ?rs/K) , det(M (0) (N0))=? Q (0) r,s [?(h?hr,s)] P f (N 0 ?rs/K) , 17 ? ? e 2 N 2T</formula><p>. ? ? e 2 N 2T . <ref type="figure">Figure 6</ref>: A random sample of mistakes made by I2L-STRIPS. Observe that usually the model gets most of the formula right and the mistake is only in a small portion of the overall formula (e.g. sample # 1; generating one subscript t instead of an l ). In some cases the mistake is in the font and in some cases the images are identical but were incorrectly flagged by the image-match evaluation software (e.g. sample # 0 &amp; #17). In some cases the predicted formula appears more correct than the original! (sample # 10 where position of the subscript ? has been 'corrected' by I2L-STRIPS). 87.73% -79.88% dependency on C 0 (now randomly initialized). Therefore if I t is the focal-region at step t defined by the predicate ? t,l &gt; 0, then p t (? t ) = f L (I t , I t?1 . . . I 0 ) where f L represents the LSTM-stack and Deep Output Layer. This fact aids considerably in interpreting the predictions of the model. We found heat-map type visuals of the focal-regions ( <ref type="figure">Figure 3</ref>) very useful in interpreting the model even as we were developing it.</p><p>Object detection via attention: Additionally, we observe that the model settles on a step-by-step alignment of I t with the output-word's location on the image: i.e. p t (? t ) ? f L (I t ). In other words I t marks the bounding-box of? t even though we trained without any bounding-box data. Therefore our model -whose encoder has a narrow receptive field-can be applied to the object detection task without requiring bounding box training data, bottom-up region proposals or pretrained classifiers. Note that this is not possible with encoder architectures having wide receptive fields, e.g. those that employ a RNN <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b0">1]</ref> because their receptive fields encompass the entire input. A future work will quantify the accuracy of object detection <ref type="bibr" target="#b18">[18]</ref> using more granular receptive fields. Pedersoli et al. <ref type="bibr" target="#b21">[21]</ref> have also used attention for object detection but their model is more complex in that it specifically models bounding-boxes although it doesn't require them for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset</head><p>Datasets were created from single-line L A T E X math formulas extracted from scientific papers and subsequently processed as follows: 1) Normalize the formulas to minimize spurious ambiguity. <ref type="bibr">12</ref> 2) Render the normalized formulas using pdflatex and discard ones that didn't compile or render successfully. 3) Remove duplicates. 4) Remove formulas with low-frequencey words (frequency-threshold = 24 for Im2latex-90k and 50 for I2l-140K). 5) Remove images bigger than 1086 ? 126 and formulas longer than 150. Processing the Im2latex-100k dataset 104 (103559 samples) as above resulted in the Im2latex-90k dataset which has 93741 samples. Of these, 4648 were set aside as the test dataset and the remaining 89093 were split into training (95%) and validation (5%) sets before each run (section 2.3). We found the Im2latex-90k dataset too small for good generalization and therefore augmented it with additional samples from KDD Cup 2003. This resulted in the I2L-140K dataset with 114406 (training), 14280 (validation) and 14280 (test) samples. Since the normalized formulas are already space separated token sequences, no additional tokenization step was necessary.</p><p>The vocabulary was therefore produced by simply identifying the set of unique space-separated words in the dataset.</p><p>Ancillary material All ancillary material: Both datasets, our model and data-processing source code, visualizations, result samples etc. is available at our website. Appendix is provided alongside this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Qualitative analyses and details</head><p>This section is an appendix to the paper. We present here further details, analyses and discussion of our experiments and comparison with related work. <ref type="table" target="#tab_7">Table 3</ref> shows the configuration of the Encoder CNN. All convolution kernels have shape (3,3), stride (1,1) and tanh non-linearity, whereas all maxpooling windows have shape (2,2) and stride <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b1">2)</ref>. We initially experimented with the output of the VGG16 model <ref type="bibr" target="#b23">[23]</ref> -per Xu et al. <ref type="bibr" target="#b31">[31]</ref>. However (presumably since VGG16 was trained on a different dataset and a different problem) the BLEU score didn't improve beyond 40%. Then we started training VGG16 along with our model but the end-to-end model didn't even start learning (the log-loss curve was flat) -possibly due to the large overall depth of the end-to-end model. Reducing the number of convolution layers to 6 and changing y? 0 ?A 0?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Encoder</head><formula xml:id="formula_15">?t =?i[A0?,H F 0 ], ?A 0? ?t =?i[A0?,H F 0 ], 1 {? i (x),? j (y)}= ij ? 2 (x?y). {? i (x),? j (y)}= ij ? 2 (x?y). 2 V total = i ?W ?z i 2 +V D +V sof t V total = i ?W ?z i 2 +V D +V sof t 3 ? ?a ? (p)= d 3 x e ?ip?x [e ? ?(?A a ?iE a )+ ? (f1? a +f2? a )] ? ?? ? (p)= d 3 x e ?ip?x [e ? ?(?A a ?iE a )+ ? (f1? a +f2? a )] 4 Hstat(k)=P+ i vk+i , Hstat(k)=P+ i vk+i , 5 (? * Ps+?P * s ) 2? 2 M 2 , (? * Ps+?P * s ) 2? 2 M 2 , 6 H G/H = 1 2 (??? i 2 ??)g ?? (??+ i 2 ? ? )= 1 2 ??g ?? ? ? +V G/H , H G/H = 1 2 (??? i 2 ??)g ?? (??+ i 2 ? ? )= 1 2 ??g ?? ? ? +V G/H , 7 S[?]=S[?]+S[?]+Sint[?,?] S[?]=S[?]+S[?]+Sint[?,?] 8 ?1= ? 4? ?2= ? 4 ?1= ? 4? 9 ?= 1?r 2 8?M B (|M S | 2 +|M P | 2 ) . ?= 1?r 2 8?M B (|M S | 2 +|M P | 2 ) . 10 E(r)=?( 2GE? m 2 r ? q 1 q 2 r + m? S 1 S 2 r ) E(r)=?( 2GE? m 2 r ? q 1 q 2 r + m? S 1 S 2 r ) 11 ?(x1,x2)= 0|T ?(x1)?(x2)|P . ?(x1,x2)= 0|T ?(x1)?(x2)|P . 12 L (2) ?=h (2) 1 v 2 ? j ( ? i ?)(?? ij ?)+h (2) 1 v i v j ? k ( ? i ?)(?? jk ?), L (2) ?=h (2) 1 v 2 ? j ( ? i ?)(?? ij ?)+h (2) 1 v i v j ? k ( ? i ?)(?? jk ?), 13 sin 2 2?=sin 2 2?sun= 4 |U e1 | 2 |U e2 | 2 (|U e1 | 2 +|U e2 | 2 ) 2 . sin 2 2?=sin 2 2?sun= 4|U e1 | 2 |U e2 | 2 (|U e1 | 2 +|U e2 | 2 ) 2 . 14 F1 = g 2 192? 5/2 M P l 1.5?10 15 GeV. F1 = g 2 192? 5/2 M P l 1.5?10 15 GeV. 15 b&lt;i&gt;= 0?p&lt;q?p i X ? p (i)? q (i) (z,z) . b&lt;i&gt;= 0?p&lt;q?p X r(i) i? (i) (z,z) . 16 F W + D 1 (x)=[d p (x)+? p (x)+d n (x)+? n (x)+2s(x)+2c(x)]/2. F W + D 1 (x)=[d p (x)+? p (x)+d n (x)+? n (x)+2s(x)+2c(x)]/2. 17 u=q 2 b 2 /2 or Q=Q0u, Q0= 1 Am N b 2 u=q 2 b 2 /2 or Q=Q0u, Q0= 1 Am N b 2 18hv ? hv= 1 2 Tr ? Pv h v hv? 1 2 Tr ???5 Pv ? Pv h v ? ? ?5 hv ,hv ? hv= 1 2 Tr(? Pv)hv hv? 1 2 Tr(???5 Pv ? Pv)hv ? ? ?5 hv , 19Pg(z)=?ns 1 z + 1 1?z .Pg(z)=?n 1 z + 1 1?z . 20</formula><p>S?SH, T ?TH, Ec?EBH, for HR?1 S?SH, T ?TH, Ec?EBH, for HR?1</p><formula xml:id="formula_16">21 ??N ?1/2 10 23 s ?1 exp ? 8 ? 2 3?137 ( ?E me ) 3/2 B 0 N B A 1/2 ( mp me ) 1/2 , ??N ?1/2 10 23 e ?1 exp ? 8 ? 2 3?137 ( ?E me ) 3/2 B 0 N B A 1/2 ( mp me ) 1/2 , 22 2|J| 2 m 02 ? = m b 2 m d 2 ?2.5?10 5 . 2 | 2 m 0 2 ? = m 2 b m 2 d ?2.5?10 5 .</formula><p>23</p><formula xml:id="formula_17">u= z U ?1/2 ? ?t , u= z U ?1/2 ? ?t , 24</formula><p>?= </p><formula xml:id="formula_18">ds 2 =(k+f0 R 2 0 R 2 ) ?1 dR 2 +R 2 d? 2 k ?(k+f0 R 2 0 R 2 )[dx 5 +A R (R)dR] 2 ds 2 =(k+f0 R 2 0 R 2 ) ?1 dR 2 +R 2 d? 2 k ?(k+f0 R 2 0 R 2 )[dx 5 +A R (R)dR] 2 28 [?0,?0]=0, [? A 0 , [?0,?0]=0, [? A 0 , 29 L4=(F 1 ??5A 2 ) dW dA 1 +??? . L4=(F 1 ??5A 2 ) dW dA 1 +??? . 30 ?j ?i=?ij ?q ?1R ikjl ? l ? k ?j ?i=?ij ?q ?1R iklj ? l ? k 31</formula><p>?i=0 , 33</p><formula xml:id="formula_19">? k =? k ?0+cm 2 h k =0 , ?i=0 , ? k =? k ?0+cm 2 h k =0 ,</formula><formula xml:id="formula_20">? A =?2H s AA , ? A =?2H s AA , 34D ?1 ?? =D ?1 ?? ? ?? ? ?? + ? n=1 An(D ?1 ) n ?? ,D ?1 ?? =D ?1 ?? ? ?? ? ?? + ? n=1 An(D ?1 ) n ?? , 35 H G (x 2 )=? 1 8?Ge 2 ([B(x 2 )] ?2 ?1), H G (x 2 )=? 1 8?Ge 2 ([B(x 2 )] ?2 ?1), 36 T?? =T + ?? + a 2 ? a 2 + T ? ?? . T?? =T + ?? + a 2 ? a 2 + T ? ?? .</formula><p>37 <ref type="figure">Figure 7</ref>: A random sample of predictions of I2L-STRIPS containing both good and bad predictions. Note that though this is a random sample, prediction mistakes are not obvious and it takes some effort to point them out! For more examples visit our website. 105 the non-linearity to tanh (to keep the activations in check) got us good results. Further reducing number of layers to 5 yielded the same performance, therefore we stuck with that configuration <ref type="table" target="#tab_7">(Table  3</ref>). In additon, we experimented with I2L-STRIPS because it reduces the rectangular image-map to a linear map, thereby presumably making the alignment model's task easier because now it would only need to scan in one-dimension. However, it performed around the same as I2L-NOPOOL and therefore that hypothesis was debunked. In fact we prefer I2L-NOPOOL since it has fewer parameters and its attention model has sharper focal-regions which helps with model interpretation.</p><formula xml:id="formula_21">G H d+1 = l 1?d ? d +? ? l dx sinh d x . G H d+1 = l l?d ? d +? q dx sinh d x . 38 ? (n) {?} = ? (n) ?(A ) ?A ? 1 (x 1 )????A ? j (x j )????A ?n (xn ) , ? (n) {?} = ? n(n) ?(A ) ?A ? 1 (x 1 )????A ?? (x j )????A o ?n (xn) , 39? ab =? ab = 40 ?? 0 a =0, ?? i a =f b ac ? i b ? c 2 , ??0i=0, ??ij =0, ?? 0 a =0, ?? i a =f b ac ? i b ? c 2 , ??0i=0, ??ij =0, 41 |Z1| 2 =|Z2| 2 = 1 (4G) 2 e ?? 0 [(Q R 1 ) 2 +(Q R 2 ) 2 ] , |Z1| 2 =|Z2| 2 = 1 (4G) 2 e ?? 0 [(Q R 1 ) 2 +(Q R 2 ) 2 ] ,</formula><p>A.2 Attention model  ( ? t=1 ? t,l ? 1) 2 which they call 'doubly stochastic optimization'. Our formulation uses the true mean of ? l , ? /L instead of 1, normalizes it to a fixed range so that it can be compared across models and more importantly, includes a target-ASE term ASE T . Without this term, i.e. with ASE T = 0, A would bias the attention model towards uniformly scanning all the L image locations. This is undesirable since there are many empty regions of the images where it makes no sense for the attention model to spend much time. Conversely, there are some densely populated regions (e.g. a symbol with complex superscript and subscripts) where the model would reasonably spend more time because it would have to produce a longer output sequence. In other words, the optimal scanning pattern would have to be non-uniform -ASE T = 0. Also, the scanning pattern would vary from sample to sample, but ASE T is set to a single value (even if zero) for all samples. Therefore we preferred to remove the attention-model bias altogether from the objective function by setting ? A = 0 in all situations except when the attention model needed a 'nudge' in order to 'get off the ground'. In such cases we set ASE T based on observed values of ASE N <ref type="table" target="#tab_13">(Table 8)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 LSTM stack</head><formula xml:id="formula_22">i t = ? (W xi x t + W hi h t?1 + W ci c t?1 + b i ) f t = ? (W xf x t + W hf h t?1 + W cf c t?1 + b f ) c t = f t c t?1 + i t tanh (W xc x t + W hc h t?1 + b c ) o t = ? (W xo x t + W ho h t?1 + W co c t + b o ) h t = o t tanh(c t ) i t , f t , o t , c t , h t ? R n<label>(12)</label></formula><p>Our LSTM cell implementation ( <ref type="figure" target="#fig_3">Figure. 8 and equation 12)</ref> follows Graves et al. <ref type="bibr" target="#b11">[11]</ref>, Zaremba et al. <ref type="bibr" target="#b33">[33]</ref>. In equation 12 ? is the logistic sigmoid function and i t , f t , o t , c t and h t are respectively the input gate, forget gate, output gate, cell and hidden activation vectors of size n.</p><p>During experimentation our penultimate LSTM-stack which had 3 LSTM layers with 1000 units each, gave us a validation score of 87.45%. At that point experimental observations suggested that the LSTM stack was the accuracy 'bottleneck' because other sub-models were performing very well. Increasing the number of LSTM units to 1500 got us better validation score -but a worse overfit. Reducing the number of layers down to 2 got us the best overall validation score. In comparison, Xu et al. <ref type="bibr" target="#b31">[31]</ref> have used a single LSTM layer with 1000 cells. Note that the output layer receives skip connections from the LSTM-Stack input (p t = f out (H t ; z t ; Ey t?1 )). We observed a 2% impact on the BLEU score with the addition of input-to-output skip-connections. This leads us to believe that adding skip-connections within the LSTM-stack may help further improve model accuracy. Overall accuracy also improved by increasing the number of layers from 2 to 3. Lastly, observe that this sub-model is different from Xu et al. <ref type="bibr" target="#b31">[31]</ref> wherein the three inputs are affine-transformed into D dimensions, summed and then passed through one fully-connected layer. After experimenting with their model we ultimately chose to instead feed the inputs (concatenated) to a fully-connected layer thereby allowing the MLP to naturally learn the input-to-output function. We also increased the number of layers to 3, changed activation function of hidden units from relu to tanh 101 and ensured that each layer had at least as many units as the softmax layer (K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Deep output layer</head><p>A.5 Init model The init model MLP is specified in <ref type="table" target="#tab_11">Table 6</ref>. We questioned the need for the Init Model and experimented just using zero values for the initial state. That caused a slight but consistent decline (&lt; 1%) in the validation score, indicating that the initial state learnt by our Initial State Model did contribute in some way towards learning and generalization. Note however that our Init Model is different than 31, in that our version uses all L feature vectors of a while theirs takes the average. We also added a hidden layer and used tanh activation function instead of relu. We did start off with their version but that did not provide an appreciable impact to the bottom line (validation). This made us hypothesize that perhaps taking an average of the feature vectors was causing a loss of information; and we mitigated that by taking in all the L feature vectors without summing them. After making all these changes, the Init Model yields a consistent albiet small performance improvement <ref type="table" target="#tab_12">(Table. 7</ref>). But given that it consumes ?7.5 million parameters, its usefulness remains in question.  <ref type="bibr">101</ref> We changed from relu to tanh partly in order to remedy 'activation-explosions' which were causing floating-point overflow errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Training and dataset</head><p>A.6.1 Alpha penalty Please see equations 13 through 13e. The loss function equation stated in the paper is Equation 13 but with ? A set to 0. That was the case when training models who's results we have published, however at other times we had included a penalty term ? A A which we discuss next. Observe that while L l ? t,l = 1, there is no constraint on how the attention is distributed across the L locations of the image. The term ? A A serves to steer the variance of ? l by penalizing any deviation from a desired value. ASE (Alpha Squared Error) is the sum of squared-difference between ? l and its mean ? /L; and ASE N is its normalized value 13 ? [0,100] <ref type="bibr" target="#b14">14</ref> . Therefore ASE N ? ASE ? ? 2 ? l . ASE T which is the desired value of ASE N , is a hyperparameter that needs to be discovered through experimentation <ref type="bibr" target="#b15">15</ref> . <ref type="table" target="#tab_13">Table 8</ref> shows training results with alpha-penalty details.  </p><p>Default values of ? 1 and? 2 of the ADAM optimizer -0.9 and 0.99 -yielded very choppy validation score curves with frequent down-spikes where the validation score would fall to very low levels, ultimately resulting in lower peak scores. Reducing the first and second moments (i.e. ? 1 and? 2 ) fixed the problem suggesting that the default momentum was too high for our 'terrain'. We did not use dropout for regularization, however increasing the data-set size (I2L-140K) and raising the minimum-word-frequency threshold from 24 (Im2latex-90k) to 50 ((I2L-140K)) did yield better generalization and overall test scores <ref type="table" target="#tab_13">(Table 8</ref>). Finally, normalizing the data <ref type="bibr" target="#b16">16</ref> yielded about 25% more accuracy than without. <ref type="bibr" target="#b13">13</ref> It can be shown that ? 2 L?1 L is the maximum possible value of ASE. <ref type="bibr" target="#b14">14</ref> We normalize ASE so that it may be compared across batches, runs and models. <ref type="bibr" target="#b15">15</ref> Start with ASET = 0, observe where ASEN settles after training, then set ASET to that value and repeat until approximate convergence. <ref type="bibr" target="#b16">16</ref> Normalization was performed using the method and software used by <ref type="bibr" target="#b6">[6]</ref> which parses the formulas into an AST and then converts them back to normalized sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Init Model. FC = Fully Connected Layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>? ?c ?= ? ?c 25 e</head><label>25</label><figDesc>(2r+1)?iL(0) Y1(v,x)e ?(2r+1)?iL(0) =Y1((?1) L(0) v,?x), e (2r+1)?iL(0) Y1(v,x)e ?(2r+1)?iL(0) =Y1((?1) L(0) v,?x),26A2= d 2 x A2(x) * ??(x) , A2= d 2 x A2(x) * ??(x) ,27</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>32 sin ?= s 23 c 23 s 2 c 2 sin ?13 sin ?= s 23 c 23 s 2 c 2</head><label>322</label><figDesc>sin ?13</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>LSTM Cell</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>r (y|a)) + ? R R + ? A A (13)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>deep CNN then encodes the whitened image into a visual feature grid A, havingH ?? (i.e.</figDesc><table><row><cell>The visual feature vectors are then concatenated</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(pooled) together in strides of shape [S</cell><cell>?</cell><cell cols="3">a (1,1) . . . a (1,W )</cell><cell>?</cell></row><row><cell>A :=</cell><cell>? ?</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>? ? (1)</cell></row><row><cell></cell><cell cols="4">a (H,1) . . . a (H,W )</cell><cell></cell></row></table><note>height ? width) visual feature vectors a (h,?) ? RD.H , S W ]; beget- ting pooled feature vectors a (h,w) ? R D , where D =D ? S H ? S W . The resulting feature map A, has a correspondingly shrunken shape [H, W ]; where H = H/S H and W = S W /? .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Training metrics. ? R = 0.00005 and ? 2 = 0.9 for all runs. The number after @ sign is the training epoch of the selected model-snapshot. * denotes that the row corresponds toTable 2.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Init</cell><cell>? 1 Train</cell><cell>Train</cell><cell>Validation</cell><cell>Valid'n</cell></row><row><cell></cell><cell></cell><cell>Model?</cell><cell cols="3">Epochs BLEU BLEU</cell><cell>ED</cell></row><row><cell>I2L-140K</cell><cell>I2L-STRIPS</cell><cell>Yes</cell><cell>0.5 104</cell><cell cols="3">0.9361 0.8900@72  *  0.0677</cell></row><row><cell></cell><cell>I2L-STRIPS</cell><cell>No</cell><cell>0.5 75</cell><cell cols="2">0.9300 0.8874@62</cell><cell>0.0691</cell></row><row><cell></cell><cell cols="2">I2L-NOPOOL Yes</cell><cell>0.5 104</cell><cell cols="3">0.9333 0.8909@72  *  0.0684</cell></row><row><cell></cell><cell cols="2">I2L-NOPOOL No</cell><cell>0.1 119</cell><cell cols="2">0.9348 0.8820@92</cell><cell>0.0738</cell></row><row><cell cols="2">Im2latex-90k I2L-STRIPS</cell><cell>Yes</cell><cell>0.5 110</cell><cell cols="3">0.9366 0.8886@77  *  0.0688</cell></row><row><cell></cell><cell>I2L-STRIPS</cell><cell>No</cell><cell>0.5 161</cell><cell cols="3">0.9386 0.8810@118 0.0750</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Test results. Im2latex-100k results are from Deng et al.<ref type="bibr" target="#b6">[6]</ref>. The last column is the percentage of successfully rendering predictions.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>BLEU</cell><cell>Edit</cell><cell>Visual</cell><cell>Compiling</cell></row><row><cell></cell><cell></cell><cell>Score</cell><cell cols="3">Distance Match 103 Predictions</cell></row><row><cell>I2L-140K</cell><cell>I2L-NOPOOL</cell><cell>89.0%</cell><cell>0.0676</cell><cell>70.37%</cell><cell>99.94%</cell></row><row><cell></cell><cell>I2L-STRIPS</cell><cell>89.0%</cell><cell>0.0671</cell><cell>69.24%</cell><cell>99.85%</cell></row><row><cell>Im2latex-90k</cell><cell>I2L-STRIPS</cell><cell cols="2">88.19% 0.0725</cell><cell>68.03%</cell><cell>99.81%</cell></row><row><cell cols="2">Im2latex-100k IM2TEX</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Specification of the Encoder CNN.</figDesc><table><row><cell>Layer</cell><cell>Output Shape</cell><cell>Channels</cell></row><row><cell>Input (Image)</cell><cell>128 ? 1088</cell><cell>1</cell></row><row><cell>Convolution</cell><cell>128 ? 1088</cell><cell>64</cell></row><row><cell>Maxpool</cell><cell>64 ? 544</cell><cell>64</cell></row><row><cell>Convolution)</cell><cell>64 ? 544</cell><cell>128</cell></row><row><cell>Maxpool</cell><cell>32 ? 272</cell><cell>128</cell></row><row><cell>Convolution</cell><cell>32 ? 272</cell><cell>256</cell></row><row><cell>Maxpool</cell><cell>16 ? 136</cell><cell>256</cell></row><row><cell>Convolution</cell><cell>16 ? 136</cell><cell>512</cell></row><row><cell>Maxpool</cell><cell>8 ? 68</cell><cell>512</cell></row><row><cell>Convolution</cell><cell>8 ? 68</cell><cell>512</cell></row><row><cell>Maxpool</cell><cell cols="2">4 ? 34 = (H ?? ) 512 = (D)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">: Specification of the Visual Attention</cell></row><row><cell cols="3">Model MLP. L = 34 for I2L-STRIPS and and</cell></row><row><cell cols="2">136 for I2L-NOPOOL.</cell><cell></cell></row><row><cell>Layer</cell><cell>Num Units</cell><cell>Activation</cell></row><row><cell cols="2">3 (output) L</cell><cell>softmax</cell></row><row><cell>2</cell><cell cols="2">max(128, L) tanh</cell></row><row><cell>1</cell><cell cols="2">max(256, L) tanh</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc>Xu et al. 31's formulation of z t = ? t ? ? t ? a includes a scalar ? t = M LP (H t?1 ) which informs the LSTM how much emphasis to place on the image v/s the language model. Experimentally we found that it had no impact on end-to-end performance, therefore we dropped it from our model.Xu et al. 31 also use a simpler formula for A =</figDesc><table><row><cell>specifies the configuration of the attention</cell></row><row><cell>model MLP. Xu et al. 31's formulation of attention</cell></row><row><cell>model (? t,l = M LP (a l ; H t?1 )) receives inputs</cell></row><row><cell>from only a single image location. In comparison, our</cell></row><row><cell>formulation (? t = f att (a; H t?1 )) receives the full</cell></row><row><cell>encoded image a in its input. This change was needed</cell></row><row><cell>because the previous formulation did not progress be-</cell></row><row><cell>yond a point, presumably because this problem war-</cell></row><row><cell>ranted a wider receptive field. The new formulation</cell></row><row><cell>works equally well with different pooling strides (and</cell></row><row><cell>correspondingly different values of L).</cell></row><row><cell>Also,</cell></row></table><note>L l=1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Configuration of the Deep Output Layer MLP. K = 339 and 358 for I2L-140K and Im2latex-90k datasets respectively.</figDesc><table><row><cell>Layer</cell><cell>Num Units</cell><cell>Activation</cell></row><row><cell cols="2">3 (output) K</cell><cell>softmax</cell></row><row><cell>2</cell><cell cols="2">max(358, K) tanh</cell></row><row><cell>1</cell><cell cols="2">max(358, K) tanh</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Init Model layers.</figDesc><table><row><cell>Layer</cell><cell cols="3">Num Units Activation</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Function</cell></row><row><cell cols="2">Output 2Q</cell><cell>n</cell><cell>tanh</cell></row><row><cell cols="2">Hidden 1</cell><cell>100</cell><cell>tanh</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Impact of the Init Model on overall performance. Since it comprises 10-12% of the total params, it may as well be omitted in exchange for a small performance hit.</figDesc><table><row><cell>Model</cell><cell cols="3">Init Model Validation Num</cell></row><row><cell></cell><cell>Present?</cell><cell>BLEU</cell><cell>Params</cell></row><row><cell>I2L-NOPOOL</cell><cell>Yes</cell><cell>89.09%</cell><cell>7,569,300</cell></row><row><cell>I2L-NOPOOL</cell><cell>No</cell><cell>88.20%</cell><cell>0</cell></row><row><cell>I2L-STRIPS</cell><cell>Yes</cell><cell>89.00%</cell><cell>7,569,300</cell></row><row><cell>I2L-STRIPS</cell><cell>No</cell><cell>88.74%</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Training metrics. ? R = 0.00005 and ? 2 = 0.9 for all runs.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Init</cell><cell>? A</cell><cell cols="4">? 1 Training Training Validation ASE N</cell></row><row><cell></cell><cell></cell><cell>Model?</cell><cell></cell><cell>Epochs</cell><cell>BLEU</cell><cell>ED</cell><cell></cell></row><row><cell>I2L-140K</cell><cell>I2L-STRIPS</cell><cell>Yes</cell><cell>0.0</cell><cell>0.5 104</cell><cell>0.9361</cell><cell>0.0677</cell><cell>5.3827</cell></row><row><cell></cell><cell>I2L-STRIPS</cell><cell>No</cell><cell>0.0</cell><cell>0.5 75</cell><cell>0.9300</cell><cell>0.0691</cell><cell>4.9899</cell></row><row><cell></cell><cell cols="2">I2L-NOPOOL Yes</cell><cell>0.0</cell><cell>0.5 104</cell><cell>0.9333</cell><cell>0.0684</cell><cell>4.5801</cell></row><row><cell></cell><cell cols="2">I2L-NOPOOL No</cell><cell>0.0</cell><cell>0.1 119</cell><cell>0.9348</cell><cell>0.0738</cell><cell>4.7099</cell></row><row><cell cols="2">Im2latex-90k I2L-STRIPS</cell><cell>Yes</cell><cell>0.0</cell><cell>0.5 110</cell><cell>0.9366</cell><cell>0.0688</cell><cell>5.1237</cell></row><row><cell></cell><cell>I2L-STRIPS</cell><cell>No</cell><cell cols="2">0.0005 0.5 161</cell><cell>0.9386</cell><cell>0.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Neighboring regions overlap but each region is distinct overall.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">That said, Bahdanau et al. 1 employ a bidirectional-LSTM<ref type="bibr" target="#b8">[8]</ref> encoder whose receptive field does encompass the entire input anyway! (Although that does not necessarily mean that the bi-LSTM will encode the entire image). Likewise Deng et al. 6 who also solve the IM2LATEX problem also employ a bi-directional LSTM stacked on top of a CNN-encoder in order to get full view of the image. In contrast, our visual feature vectors hold only spatially local information which we found are sufficient to achieve good accuracy. This is probably owing to the nature of the problem; i.e. transcribing a one-line math formula into L A T E Xsequence requires only local information at each step.<ref type="bibr" target="#b5">5</ref> This is now a very standard way to model sequence (sentence) probabilities in neural sequence-generators. See<ref type="bibr" target="#b24">[24]</ref> for example.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">i.e. Average cross-entropy, negative log-likelihood or negative log-probability.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint line segmentation and transcription for end-to-end handwritten paragraph recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Normalization was performed using the method and software used by [6] which parses the formulas into an AST and then converts them back to normalized sequences</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comparison of sequence-trained deep neural networks and recurrent neural networks optical modeling for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Listen, attend and spell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1508.01211</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>?aglar G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image-to-markup generation with coarse-to-fine attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanervisto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno>abs/1411.4389</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Computational Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1303.5778</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>abs/1511.07571</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural machine translation in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1610.10099</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention correctness in neural image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>abs/1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>?aglar G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1312.6026</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Areas of attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno>abs/1612.01033</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno>abs/1701.05517</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1409.3215</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative image modeling using spatial lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1606.05328</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Recurrent neural network regularization. CoRR, abs/1409.2329</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
