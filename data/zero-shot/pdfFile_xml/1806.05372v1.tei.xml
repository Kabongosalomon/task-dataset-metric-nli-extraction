<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
							<email>sunming05@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
							<email>yuanyuchen02@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
							<email>zhoufeng09@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fine-grained Classification</term>
					<term>Metric Learning</term>
					<term>Visual Atten- tion</term>
					<term>Multi-Attention Multi-Class Constraint</term>
					<term>One-Squeeze Multi-Excitation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention-based learning for fine-grained image recognition remains a challenging task, where most of the existing methods treat each object part in isolation, while neglecting the correlations among them. In addition, the multi-stage or multi-scale mechanisms involved make the existing methods less efficient and hard to be trained end-to-end. In this paper, we propose a novel attention-based convolutional neural network (CNN) which regulates multiple object parts among different input images. Our method first learns multiple attention region features of each input image through the one-squeeze multi-excitation (OSME) module, and then apply the multi-attention multi-class constraint (MAMC) in a metric learning framework. For each anchor feature, the MAMC functions by pulling same-attention same-class features closer, while pushing different-attention or different-class features away. Our method can be easily trained end-to-end, and is highly efficient which requires only one training stage. Moreover, we introduce Dogs-in-the-Wild, a comprehensive dog species dataset that surpasses similar existing datasets by category coverage, data volume and annotation quality. This dataset will be released upon acceptance to facilitate the research of fine-grained image recognition. Extensive experiments are conducted to show the substantial improvements of our method on four benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past few years, the performances of generic image recognition on largescale datasets (e.g., ImageNet <ref type="bibr" target="#b0">[1]</ref>, Places <ref type="bibr" target="#b1">[2]</ref>) have undergone unprecedented improvements, thanks to the breakthroughs in the design and training of deep neural networks (DNNs). Such fast-pacing progresses in research have also drawn attention of the related industries to build software like Google Lens on smartphones to recognize everything snapshotted by the user. Yet, recognizing the fine-grained category of daily objects such as car models, animal species or food dishes is still a challenging task for existing methods. The reason is that the global geometry and appearances of fine-grained classes can be very similar, and how to identify their subtle differences on the key parts is of vital importance. For instance, to differentiate the two dog species in <ref type="figure" target="#fig_0">Figure 1</ref> Our method is capable of capturing the subtle differences on the head and tail without manual part annotations.</p><p>consider their discriminative features on the ear, tail and body length, which is extremely difficult to notice even for human without domain expertise. Thus the majority of efforts in the fine-grained community focus on how to effectively integrate part localization into the classification pipeline. In the pre-DNN era, various parametric <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> and non-parametric <ref type="bibr" target="#b5">[6]</ref> part models have been employed to extract discriminative part-specific features. Recently, with the popularity of DNNs, the tasks of object part localization and feature representation can be both learned in a more effective way <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. The major drawback of these strongly-supervised methods, however, is that they heavily rely on manual object part annotations, which is too expensive to be prevalently applied in practice. Therefore, weakly-supervised frameworks have received increasing attention in recent researches. For instance, the attention mechanism can be implemented as sequential decision processes <ref type="bibr" target="#b11">[12]</ref> or multi-stream part selections <ref type="bibr" target="#b12">[13]</ref> without the need of part annotations. Despite the great progresses, these methods still suffer several limitations. First, their additional steps, such as the part localization and feature extraction of the attended regions, can incur expensive computational cost. Second, their training procedures are sophisticated, requiring multiple alternations or cascaded stages due to the complex architecture designs. More importantly, most works tend to detect the object parts in isolation, while neglect their inherent correlations. As a consequence, the learned attention modules are likely to focus on the same region and lack the capability to localize multiple parts with discriminative features that can differentiate between similar fine-grained classes.</p><p>From extensive experimental studies, we observe that an effective visual attention mechanism for fine-grained classification should follow three criteria: 1) The detected parts should be well spread over the object body to extract noncorrelated features; 2) Each part feature alone should be discriminative for separating objects of different classes; 3) The part extractors should be lightweight in order to be scaled up for practical applications. To meet these demands, this paper presents a novel framework that contains two major improvements. First, we propose one-squeeze multi-excitation module (OSME) to localize different parts inspired by the latest ImageNet winner SENet <ref type="bibr" target="#b13">[14]</ref>. OSME is a fully differentiable unit and is capable of directly extracting part features with budgeted computational cost, unlike existing methods that explicitly cropping the object part first and then feedforward again for the feature. Second, inspired by metric learning loss, we propose the multi-attention multi-class constraint (MAMC) to coherently enforce the correlations among different parts in the training of finegrained object classifiers. MAMC encourages same-attention same-class features to be closer than different-attention or different-class ones. In addition, we have collected a new dataset of dog species called Dogs-in-the-Wild, which exhibits higher category coverage, data volume and annotation quality than similar public datasets. Experimental results show that our method achieves substantial improvements on four benchmark datasets. Moreover, our method can be easily trained end-to-end, and unlike most existing methods that require multiple feedforward processes for feature extraction <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> or multiple alternative training stages <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>, only one stage and one feedforward are required for each training step of our network, which offers significantly improved efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fine-Grained Image Recognition</head><p>In the task of fine-grained image recognition, since the inter-class differences are subtle, more specialized techniques, including discriminative feature learning and object parts localization, need to be applied. A straightforward way is supervised learning with manual object part annotations, which has shown promising results in classifying birds <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, dogs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>, and cars <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20]</ref>. However, it is usually laborious and expensive to obtain object part annotations, which severely restricts the effectiveness of such methods.</p><p>Consequently, more recently proposed methods tend to localize object parts with weakly-supervised mechanisms, such as the combination of pose alignment and co-segmentation <ref type="bibr" target="#b7">[8]</ref>, dynamic spatial transformation of the input image for better alignment <ref type="bibr" target="#b20">[21]</ref>, and parallel CNNs for bilinear feature extraction <ref type="bibr" target="#b21">[22]</ref>. Compared with previous works, our method also takes a weakly-supervised mechanism, but can directly extract the part features without cropping them out, and is highly efficient to be scaled up with multiple parts.</p><p>In recent years, more advanced methods emerge with improved results. For instance, the bipartite-graph labeling <ref type="bibr" target="#b22">[23]</ref> leverages the label hierarchy on the fine-grained classes, which is less expensive to obtain. The work in <ref type="bibr" target="#b23">[24]</ref> exploit unified CNN framework with spatially weighted representation by the Fisher vector <ref type="bibr" target="#b24">[25]</ref>. <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref> incorporate human knowledge and various types of computer vision algorithms into a human-in-the-loop framework for the complementary strengths of both ends. And in <ref type="bibr" target="#b27">[28]</ref>, the average and bilinear pooling are combined to learn the pooling strategy during training. These techniques can also be potentially combined with our method for further works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Attention</head><p>The aforementioned part-based methods have shown strong performances in fine-grained image recognition. Nevertheless, one of their major drawbacks is that they need meaningful definitions of the object parts, which are hard to obtain for non-structured objects such as flowers <ref type="bibr" target="#b28">[29]</ref> and food dishes <ref type="bibr" target="#b29">[30]</ref>. Therefore, the methods enabling CNN to attend loosely defined regions for general objects have emerged as a promising direction. For instance, the soft proposal network <ref type="bibr" target="#b30">[31]</ref> combines random walk and CNN for object proposals. The works in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b31">[32]</ref> introduce long short-term memory <ref type="bibr" target="#b32">[33]</ref> and reinforcement learning to attention-based classification, respectively. And the class activation mapping <ref type="bibr" target="#b33">[34]</ref> generates the heatmap of the input image, which provides a better way for attention visualization. On the other hand, the idea of multi-scale feature fusion or recurrent learning has become increasingly popular in recent works. For instance, the work in <ref type="bibr" target="#b16">[17]</ref> extends <ref type="bibr" target="#b33">[34]</ref> and establishes a cascaded multi-stage framework, which refines the attention region by iteration. The residual attention network <ref type="bibr" target="#b14">[15]</ref> obtains the attention mask of input image by up-sampling and down-sampling, and a series of such attention modules are stacked for feature map refinement. And the recurrent attention CNN <ref type="bibr" target="#b12">[13]</ref> alternates between the optimization of softmax and pairwise ranking losses, which jointly contribute to the final feature fusion. Even an acceleration method <ref type="bibr" target="#b34">[35]</ref> with reinforcement learning is proposed particularly for the recurrent attention models above.</p><p>In parallel to these efforts, our method not only automatically localizes the attention regions, but also directly captures the corresponding features without explicitly cropping the ROI and feedforwarding again for the feature, which makes our method highly efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Metric Learning</head><p>Apart from the techniques above, deep metric learning aims at the learning of appropriate similarity measurements between sample pairs, which provides another promising direction to fine-grained image recognition. Classical metric learning may be considered as learning of the Mahalanobis distance between pairs of points <ref type="bibr" target="#b35">[36]</ref>. The pioneer work of Siamese network <ref type="bibr" target="#b36">[37]</ref> formulates the deep metric learning with a contrastive loss that minimizes distance between positive pairs while keeps negative pairs apart. Despite its great success on face verification <ref type="bibr" target="#b37">[38]</ref>, contrastive embedding requires that training data contains realvalued precise pair-wise similarities or distances. The triplet loss <ref type="bibr" target="#b38">[39]</ref> addresses this issue by optimizing the relative distance of the positive pair and one negative pair from three samples. It has been proven that triplet loss is extremely effective for fine-grained product search <ref type="bibr" target="#b39">[40]</ref>. Later, triplet loss is improved to automatically search for discriminative patches <ref type="bibr" target="#b40">[41]</ref>. Nevertheless, compared with softmax loss, triplet loss is difficult to train due to its slow convergence. To alleviate this issue, the N-pair loss <ref type="bibr" target="#b41">[42]</ref> is introduced to consider multiple negative samples in training, and exhibits higher efficiency and performance. More recently, the angular loss <ref type="bibr" target="#b42">[43]</ref> enhances N-pair loss by integrating high-order constraint that captures additional local structure of triplet triangles. Conv</p><formula xml:id="formula_0">m 1 m 2 S 1 S 2 z z W 1 1 W 2 1 W 1 2 W 2 2 W 1 3 W 2 3 f 1 f 2 ? Fig. 2:</formula><p>Overview of our network architecture. Here we visualize the case of learning two attention branches given a training batch with four images of two classes. The MAMC and softmax losses would be replaced by a softmax layer in testing. Unlike hard-attention methods like <ref type="bibr" target="#b12">[13]</ref>, we do not explicitly crop the parts out. Instead, the feature maps (S 1 and S 2 ) generated by the two branches provide soft response for attention regions such as the birds' head or torso, respectively.</p><p>Our method differs previous metric learning works in two aspects: First, we take object parts instead of the whole images as instances in the feature learning process; Second, our formulation simultaneously considers the part and class labels of each instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we present our proposed method which can efficiently and accurately attend discriminative regions despite being trained only on imagelevel labels. As shown in <ref type="figure">Figure 2</ref>, the framework of our method is composed by two parts: 1) A differentiable one-squeeze multi-excitation (OSME) module that extracts features from multiple attention regions with a slight increase in computational burden. 2) A multi-attention multi-class (MAMC) constraint that enforces the correlation of the attention features in favor of the fine-grained classification task. In contrast to many prior works, the entire network of our method can be effectively trained end-to-end in one stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">One-Squeeze Multi-Excitation Attention Module</head><p>There have been a number of visual attention models exploring weakly supervised part localization, and the previous works can be roughly categorized in two groups. The first type of attention is also known as part detection, i.e., each attention is equivalent to a bounding box covering a certain area. Well-known examples include the early work of recurrent visual attention <ref type="bibr" target="#b11">[12]</ref>, the spatial transformer networks <ref type="bibr" target="#b20">[21]</ref>, and the recent method of recurrent attention CNN <ref type="bibr" target="#b12">[13]</ref>. This hard-attention setup can benefit a lot from the object detection community in the formulation and training. However, its architectural design is often cumbersome as the part detection and feature extraction are separated in different modules. For instance, the authors of <ref type="bibr" target="#b20">[21]</ref> apply three GoogLeNets <ref type="bibr" target="#b43">[44]</ref> for detecting and representing two parts of birds. As the base network goes deeper, the memory and computational cost would become too high to afford for more than three object parts even using the latest GPUs. The second type of attention can be considered as imposing a soft mask on the feature map, which origins from activation visualization <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. Later, people find it can be extended for localizing parts <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b16">17]</ref> and improving the overall recognition performance <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>. Our approach also falls into this category. We adopt the idea of SENet <ref type="bibr" target="#b13">[14]</ref>, the latest ImageNet winner, to capture and describe multiple discriminative regions in the input image. Compared to other soft-attention works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b14">15]</ref>, we build on SENet because of its superiority in performance and scalability in practice.</p><p>As shown in <ref type="figure">Figure 2</ref>, our framework is a feedforward neural network where each image is first processed by a base network, e.g., ResNet-50 <ref type="bibr" target="#b46">[47]</ref>. Let x ? R W ?H ?C denote the input fed into the last residual block ? . The goal of SENet is to re-calibrate the output feature map,</p><formula xml:id="formula_1">U = ? (x) = [u1, ? ? ? , uC ] ? R W ?H?C ,<label>(1)</label></formula><p>through a pair of squeeze-and-excitation operations. In order to generate P attention-specific feature maps, we extend the idea of SENet by performing one-squeeze but multi-excitation operations.</p><p>In the first one-squeeze step, we aggregate the feature maps U across spatial</p><formula xml:id="formula_2">dimensions W ? H to produce a channel-wise descriptor z = [z 1 , ? ? ? , z C ] ? R C .</formula><p>The global average pooling is adopted as a simple but effective way to describe each channel statistic:</p><formula xml:id="formula_3">zc = 1 W H W w=1 H h=1 uc(w, h).<label>(2)</label></formula><p>In the second multi-excitation step, a gating mechanism is independently employed on z for each attention p = 1, ? ? ? , P :</p><formula xml:id="formula_4">m p = ? W p 2 ?(W p 1 z) = [m p 1 , ? ? ? , m p C ] ? R C ,<label>(3)</label></formula><p>where ? and ? refer to the Sigmod and ReLU functions respectively. We adopt the same design of SENet by forming a pair of dimensionality reduction and increasing layers parameterized with W p</p><formula xml:id="formula_5">1 ? R C r ?C and W p 2 ? R C? C r .</formula><p>Because of the property of the Sigmod function, each m p encodes a non-mutually-exclusive relationship among channels. We therefore use it to re-weight the channels of the original feature map U,</p><formula xml:id="formula_6">S p = [m p 1 u1, ? ? ? , m p C uC ] ? R W ?H?C .<label>(4)</label></formula><p>To extract attention-specific features, we feed each attention map S p to a fully connected layer W p 3 ? R D?W HC :</p><formula xml:id="formula_7">f p = W p 3 vec(S p ) ? R D ,<label>(5)</label></formula><p>where the operator vec(?) flattens a matrix into a vector.</p><p>In a nutshell, the proposed OSME module seeks to extract P feature vectors {f p } P p=1 for each image x by adding a few layers on top of the last residual block. Its simplicity enables the use of relatively deep base networks and an efficient one-stage training pipeline.</p><p>It is worth to clarify that the SENet is originally not designed for learning visual attentions. By adopting the key idea of SENet, our proposed OSME module implements a lightweight yet effective attention mechanism that enables an end-to-end one-stage training on large-scale fine-grained datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Attention Multi-Class Constraint</head><p>Apart from the attention mechanism introduced in Section 3.1, the other crucial problem is how to guide the extracted attention features to the correct class label. A straightforward way is to directly evaluate the softmax loss on the concatenated attention features <ref type="bibr" target="#b20">[21]</ref>. However, the softmax loss is unable to regulate the correlations between attention features. As an alternative, another line of research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref> tends to mimic human perception with a recurrent search mechanism. These approaches iteratively generate the attention region from coarse to fine by taking previous predictions as references. The limitation of them, however, is that the current prediction is highly dependent on the previous one, thereby the initial error could be amplified by iteration. In addition, they require advanced techniques such as reinforcement learning or careful initialization in a multi-stage training. In contrast, we take a more practical approach by directly enforcing the correlations between parts in training. There has been some prior works like <ref type="bibr" target="#b40">[41]</ref> that introduce geometrical constraints on local patches. Our method, on the other hand, explores much richer correlations of object parts by the proposed multi-attention multi-class constraint (MAMC).</p><p>Suppose that we are given a set of training images {(x, y), ? ? ?} of K finegrained classes, where y = 1, ? ? ? , K denotes the label associated with the image x. To model both the within-image and inter-class attention relations, we construct each training batch,</p><formula xml:id="formula_8">B = {(x i , x + i , y i )} N i=1</formula><p>, by sampling N pairs of images 1 similar to <ref type="bibr" target="#b41">[42]</ref>. For each pair (x i , x + i ) of class y i , the OSME module extracts P attention features {f p i , f p+ i } P p=1 from multiple branches according to Eq. 5. Given 2N samples in each batch <ref type="figure">(Figure 3a)</ref>, our intuition comes from the natural clustering of the 2N P features <ref type="figure">(Figure 3b</ref>) extracted by the OSME modules. By picking f p i , which corresponds to the i th class and p th attention region as the anchor, we divide the rest features into four groups:</p><formula xml:id="formula_9">-same-attention same-class features, S sasc (f p i ) = {f p+ i }; -same-attention different-class features, S sadc (f p i ) = {f p j , f p+ j } j =i ; -different-attention same-class features, S dasc (f p i ) = {f q i , f q+ i } q =p ; -different-attention different-class features S dadc (f p i ) = {f q j , f q+ j } j =i,q =p .</formula><p>1 N stands for the number of sample pairs as well as the number of classes in a mini-batch. Limited by GPU memory, N is usually much smaller than K, the total number of classes in the entire training set.</p><p>Our goal is to excavate the rich correlations among the four groups in a metric learning framework. As summarized in <ref type="figure">Figure 3c</ref>, we compose three types of triplets according to the choice of the positive set for the anchor f p i . To keep notation concise, we omit f p i in the following equations. Same-attention same-class positives. The most similar feature to the anchor f p i is f p+ i , while all the other features should have larger distance to the anchor. The positive and negative sets are then defined as:</p><formula xml:id="formula_10">Psasc = Ssasc, Nsasc = S sadc ? S dasc ? S dadc .<label>(6)</label></formula><p>Same-attention different-class positives. For the features from different classes but extracted from the same attention region, they should be more similar to the anchor than the ones also from different attentions:</p><formula xml:id="formula_11">P sadc = S sadc , N sadc = S dadc .<label>(7)</label></formula><p>Different-attention same-class positives. Similarly, for the features from same class but extracted from different attention regions, we have:</p><formula xml:id="formula_12">P dasc = S dasc , N dasc = S dadc .<label>(8)</label></formula><p>For any positive set P ? {P sasc , P sadc , P dasc } and negative set N ? {N sasc , N sadc , N dasc } combinations, we expect the anchor to be closer to the positive than to any negative by a distance margin m &gt; 0, i.e.,</p><formula xml:id="formula_13">f p i ? f + 2 +m ? f p i ? f ? 2 , ?f + ? P, f ? ? N .<label>(9)</label></formula><p>To better understand the three constraints, let's consider the synthetic example of six feature points shown in <ref type="figure">Figure 4</ref>. In the initial state <ref type="figure">(Figure 4a</ref>), the S sasc feature point (green hexagon) stays further away from the anchor f p i at the center than the others. After applying the first constraint (Eq. 6), the underlying feature space is transformed to <ref type="figure">Figure 4b</ref>, where the S sasc positive point (green ) has been pulled towards the anchor. However, the four negative features (cyan rectangles and triangles) are still in disordered positions. In fact, S sadc and S dasc should be considered as the positives compared to S dadc given the anchor. By further enforcing the second (Eq. 7) and third (Eq. 8) constraints, a better embedding can be achieved in <ref type="figure">Figure 4c</ref>, where S sadc and S dasc are regularized to be closer to the anchor than the ones of S dadc .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Loss</head><p>To enforce the triplet constraint in Eq. 9, a common approach is to minimize the following hinge loss:</p><formula xml:id="formula_14">f p i ? f + 2 ? f p i ? f ? 2 +m + .<label>(10)</label></formula><p>Despite being broadly used, optimizing Eq. 10 using standard triplet sampling leads to slow convergence and unstable performance in practice. Inspired by the recent advance in metric learning, we enforce each of the three constraints by minimizing the N-pair loss 2 <ref type="bibr" target="#b41">[42]</ref>,</p><formula xml:id="formula_15">L np = 1 N f p i ?B f + ?P log 1 + f ? ?N exp(f pT i f ? ? f pT i f + ) . (11)</formula><p>In general, for each training batch B, MAMC jointly minimizes the softmax loss and the N-pair loss with a weight parameter ?:</p><formula xml:id="formula_16">L mamc = L sof tmax + ? L np sasc + L np sadc + L np dasc .<label>(12)</label></formula><p>Given a batch of N images and P parts, MAMC is able to generate 2(P N ? 1) + 4(N ? 1) 2 (P ? 1) + 4(N ? 1)(P ? 1) 2 constraints of three types (Eq. 6 to Eq. 8), while the N-pair loss can only produce N ? 1. To put it in perspective, we are able to generate 130? more constraints than N-pair loss with the same data under the normal setting where P = 2 and N = 32. This implies that MAMC leverages much richer correlations among the samples, and is able to obtain better convergence than either triplet or N-pair loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Dogs-in-the-Wild Dataset</head><p>Large image datasets (such as ImageNet <ref type="bibr" target="#b0">[1]</ref>) with high-quality annotations enables the dramatic development in visual recognition. However, most datasets for fine-grained recognition are out-dated, non-natural and relatively small (as shown in <ref type="table" target="#tab_0">Table 1</ref>). Recently, there are several attempts such as Goldfinch <ref type="bibr" target="#b47">[48]</ref> and the iNaturalist Challenge <ref type="bibr" target="#b48">[49]</ref> in building large-scale fine-grained benchmarks. However, there still lacks a comprehensive dataset with large enough data volume, highly accurate data annotation, and full tag coverage of common dog species. We hence introduce the Dogs-in-the-Wild dataset with 299,458 images of 362 dog categories, which is 15? larger than Stanford Dogs <ref type="bibr" target="#b17">[18]</ref>. We generate the list of dog species by combining multiple sources (e.g., Wikipedia), and then crawl the images with search engines (e.g., Google, Baidu). The label of each image is then checked with crowd sourcing. We further prune small classes with less than 100 images, and merge extremely similar classes by applying confusion matrix and manual validation. The whole annotation process is conducted three times to guarantee the annotation quality. Last but not least, since most of the experimental baselines are pre-trained on ImageNet, which has substantial category overlap with our dataset, we exclude any image of ImageNet from our dataset for fair evaluation. This dataset will be released upon acceptance. <ref type="figure" target="#fig_3">Figure 5a</ref> and <ref type="figure" target="#fig_3">Figure 5b</ref> qualitatively compare our dataset with the two most relevant benchmarks, Stanford Dogs <ref type="bibr" target="#b17">[18]</ref> and the dog section of Goldfinch <ref type="bibr" target="#b47">[48]</ref>. It can be seen that our dataset is more challenging in two aspects: (1) The intra-class variation of each category is larger. For instance, almost all common patterns and hair colors of Staffordshire Bull Terriers are covered in our dataset, as illustrated in <ref type="figure" target="#fig_3">Figure 5a.</ref> (2) More surrounding environment types are covered, which includes but is not limited to, natural scenes, indoor scenes and even artificial scenes; and the dog itself could either be in its natural appearance or dressed up, such as the first Boston Terrier in <ref type="figure" target="#fig_3">Figure 5a</ref>. Another feature of our dataset is that all of our images are manually examined to minimize annotation errors. Although Goldfinch has comparable class number and data volume, it is common to find noisy images inside, as shown in <ref type="figure" target="#fig_3">Figure 5b</ref>.</p><p>We then demonstrate the statistics of the three datasets in <ref type="figure" target="#fig_3">Figure 5c</ref> and <ref type="table" target="#tab_0">Table 1</ref>. It is observed that our dataset is significantly more imbalanced in term of images per category, which is more consistent with real-life situations, and notably increases the classification difficulty. Note that the curves in <ref type="figure" target="#fig_3">Figure 5c</ref> are smoothed for better visualization. On the other hand, the average images per category of our dataset is higher than the other two datasets, which contributes to its high intra-class variation, and makes it less vulnerable to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We conduct our experiments on four fine-grained image recognition datasets, including three publicly available datasets CUB-200-2011 <ref type="bibr" target="#b49">[50]</ref>, Stanford Dogs <ref type="bibr" target="#b17">[18]</ref> and Stanford Cars <ref type="bibr" target="#b19">[20]</ref>, and the proposed Dogs-in-the-Wild dataset. The detailed  statistics including class numbers and train/test distributions are summarized in <ref type="table" target="#tab_0">Table 1</ref>. We adopt top-1 accuracy as the evaluation metric.</p><p>In our experiments, the input images are resized to 448?448 for both training and testing. We train on each dataset for 60 epochs; the batch size is set to 10 (N=5), and the base learning rate is set to 0.001, which decays by 0.96 for every 0.6 epoch. The reduction ratio r of W p 1 and W p 2 in Eq. 3 is set to 16 in reference to <ref type="bibr" target="#b13">[14]</ref>. The weight parameter ? is empirically set to 0.5 as it achieves consistently good performances. And for the FC layers, we set the channels C = 2048 and D = 1024. Our method is implemented with Caffe <ref type="bibr" target="#b50">[51]</ref> and one Tesla P40 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Analysis</head><p>To fully investigate our method, <ref type="table" target="#tab_1">Table 2a</ref> provides a detailed ablation analysis on different configurations of the key components.</p><p>Base networks. To extract convolutional feature before the OSME module, we choose VGG-19 <ref type="bibr" target="#b51">[52]</ref>, ResNet-50 and ResNet-101 <ref type="bibr" target="#b46">[47]</ref> as our candidate baselines. Based on <ref type="table" target="#tab_1">Table 2a</ref>, ResNet-50 and ResNet-101 are selected given their good balance between performance and efficiency. We also note that although a better ResNet-50 baseline on CUB is reported in <ref type="bibr" target="#b34">[35]</ref> (84.5%), it is implemented in Torch <ref type="bibr" target="#b52">[53]</ref> and tuned with more advanced data augmentation (e.g., color jittering, scaling). Our baselines, on the other hand, are trained with simple augmentation (e.g., mirror and random cropping) and meet the Caffe baselines of other works, such as 82.0% in <ref type="bibr" target="#b31">[32]</ref> and 78.4% in <ref type="bibr" target="#b53">[54]</ref>.</p><p>Importance of OSME. OSME is important in attending discriminative regions. For ResNet-50 without MAMC, using OSME solely with P = 2 can offer 3.2% performance improvement compared to the baseline (84.9% vs. 81.7%). With MAMC, using OSME boosts the accuracy by 0.5% than without OSME (using two independent FC layers instead, 86.2% vs. 85.7%). We also notice that two attention regions (P = 2) lead to promising results, while more attention regions (P = 3) provide slightly better performance.</p><p>MAMC constraints. Applying the first MAMC constraint (Eq. 6) achieves 0.5% better performance than the baseline with ResNet-50 and OSME. Using all of the three MAMC constraints (Eq. 6 to Eq. 8) leads to another 0.8% improvement. This indicates the effectiveness of each of the three MAMC constraints.</p><p>Complexity. Compared with the ResNet-50 baseline, our method provides significantly better result (+4.5%) with only 30% more time, while a similar method <ref type="bibr" target="#b12">[13]</ref> offers less optimal result but takes 3.6? more time than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with State-of-the-Art</head><p>In reference to <ref type="bibr" target="#b12">[13]</ref>, we select 18 baselines as shown in <ref type="table" target="#tab_1">Table 2b</ref>. Quantitative experimental results on the four datasets are shown in <ref type="table" target="#tab_1">Table 2b</ref>-2e.</p><p>We first analyze the results on the CUB-200-2011 dataset in <ref type="table" target="#tab_1">Table 2b</ref>. It is observed that with ResNet-101, our method achieves the best overall performance (tied with MACNN) against state-of-the-art. Even with ResNet-50, our method exceeds the second best method using extra annotation (PN-CNN) by 0.8%, and exceeds the second best method without extra annotation (RAM) by 0.2%. The fact that our method outperforms all of the methods with extra annotation demonstrates that good results are not necessarily linked with high costs.</p><p>For the weakly supervised methods without extra annotation, PDFR and MG-CNN conduct feature combination from multiple scales, and RACNN is trained with multiple alternative stages, while our method is trained with only one stage to obtain all the required features. Yet our method outperforms all of the the three methods by 2.0%, 4.8% and 1.2%, respectively. The methods B-CNN and RAN share similar multi-branch ideas with the OSME in our method, where B-CNN connects two CNN features with outer product, and RAN combines the trunk CNN feature with an additional attention mask. Our method, on the other hand, applies the OSME for multi-attention feature extraction in one step, which surpasses B-CNN and RAN by 2.4% and 3.7%, respectively.</p><p>Our method exhibits similar performances on the Stanford Dogs and Stanford Cars datasets, as shown in <ref type="table" target="#tab_1">Table 2c and Table 2d</ref>. On Stanford Dogs, our method exceeds all of the comparison methods except RACNN, which requires multiple stages for feature extraction and is hard to be trained end-to-end. On Stanford Cars, our method obtains 93.0% accuracy, outperforming all of the comparison methods. It is worth noting that compared with the methods exploiting multi-scale or multi-stage information like DVAN and RAN, our method achieves significant improvements with only one feedforward stage for multiattention multi-class feature extraction, which further validates the effectiveness and efficiency of our method.  <ref type="figure">Fig. 6</ref>: Visualization of the attention regions detected by the OSME. For each dataset, the first column shows the input image, the second column shows the heatmap from the last conv layer of the baseline ResNet-101; the third and fourth columns show the heatmaps of the two detected attention regions via OSME.</p><p>Finally, on the Dogs-in-the-Wild dataset, our method still achieves the best result with remarkable margins. Since this dataset is newly proposed, the results in <ref type="table" target="#tab_1">Table 2e</ref> can be used as baselines for future explorations. Moreover, by comparing the overall performances in <ref type="table" target="#tab_1">Table 2c</ref> and <ref type="table" target="#tab_1">Table 2e</ref>, we find that the accuracies on Dogs-in-the-wild are significantly lower than those on Stanford Dogs, which witness the relatively higher classification difficulty of this dataset.</p><p>By adopting our network with ResNet-101, we visualize the S p in Eq. 4 of each OSME branch (which corresponds to an attention region) as its channelwise average heatmap, as shown in the third and fourth columns of <ref type="figure">Figure 6</ref>, . In comparison, we also show the outputs of the last conv layer of the baseline network (ResNet-101) as heatmaps in the second column. It is seen that the highlighted regions of OSME outputs reveal more meaningful parts than those of the baseline, that we humans also rely on to recognize the fine-grained label, e.g., the head and wing for birds, the head and tail for dogs, and the headlight/grill and frame for cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel CNN with the multi-attention multi-class constraint (MAMC) for fine-grained image recognition. Our network extracts attention-aware features through the one-squeeze multi-excitation (OSME) module, supervised by the MAMC loss that pulls positive features closer to the anchor, while pushing negative features away. Our method does not require bounding box or part annotation, and can be trained end-to-end in one stage. Extensive experiments against state-of-the-art methods exhibit the superior performances of our method on various fine-grained recognition tasks on birds, dogs and cars. In addition, we have collected and will release the Dogs-in-the-Wild, a comprehensive dog species dataset with the largest data volume, full category coverage, and accurate annotation compared with existing similar datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>, it is important to arXiv:1806.05372v1 [cs.CV] 14 Jun 2018 Two distinct dog species from the proposed Dogs-in-the-Wild dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Data hierarchy in training. (a) Each batch is composed by 2N input images in N-pair style. (b) OSME extracts P features for each image according to Eq. 5. (c) The group of features for three MAMC constraints by picking one feature f p i as the anchor. Feature embedding of a synthetic batch. (a) Initial embedding before learning. (b) The result embedding by applying Eq. 6. (c) The final embedding by enforcing Eq. 7 and Eq. 8. See text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative and quantitative comparison of dog datasets. (a) Example images from Stanford Dogs and Dogs-in-the-Wild; (b) Common bad cases from Goldfinch that are completely non-dog. (c) Images per category distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the related datasets.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell></cell><cell cols="6">#Class #Train #Test #Avg. Train/Class</cell></row><row><cell></cell><cell cols="2">CUB-200-2011</cell><cell>200</cell><cell>5,994</cell><cell cols="3">5,794</cell><cell>30</cell></row><row><cell></cell><cell>Stanford Dogs</cell><cell></cell><cell>120</cell><cell cols="4">12,000 8,580</cell><cell>100</cell></row><row><cell></cell><cell>Stanford Cars</cell><cell></cell><cell>196</cell><cell>8,144</cell><cell cols="3">8,041</cell><cell>42</cell></row><row><cell></cell><cell>Goldfinch</cell><cell></cell><cell cols="3">515 342,632</cell><cell>-</cell><cell></cell><cell>665</cell></row><row><cell></cell><cell cols="7">Dogs-in-the-Wild 362 258,474 40,984</cell><cell>714</cell></row><row><cell>Stanford Dogs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60 70 80</cell><cell></cell><cell></cell><cell>Stanford Dogs Goldfinch Dogs-in-the-Wild</cell></row><row><cell>Ours</cell><cell>Boston Terrier</cell><cell>(a)</cell><cell cols="2">Staffordshire Bull Terrier</cell><cell>30 40 50 # Categories</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell>500</cell><cell>1000</cell><cell>1500 # Images</cell><cell>2000</cell><cell>2500</cell><cell>3000</cell></row><row><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experimental results. "Anno." stands for using extra annotation (bounding box or part) in training. "1-Stage" indicates whether the training can be done in one stage. "Acc." denotes the top-1 accuracy in percentage.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell cols="4">#Attention(P ) 1-Stage Acc. Time(ms)</cell></row><row><cell>VGG-19</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>79.0</cell><cell>79.8</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>81.7</cell><cell>48.6</cell></row><row><cell>ResNet-101</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>82.5</cell><cell>82.7</cell></row><row><cell>ResNet-50 + OSME</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell>84.9</cell><cell>63.3</cell></row><row><cell>RACNN [13]</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>?</cell><cell>85.3</cell><cell>229</cell></row><row><cell cols="4">ResNet-50 + OSME + MAMC (Eq. 6)</cell><cell>2</cell><cell></cell><cell>85.4</cell><cell>63.3</cell></row><row><cell cols="4">ResNet-50 + FC + MAMC (Eq. 6?8)</cell><cell>2</cell><cell></cell><cell>85.7</cell><cell>60.3</cell></row><row><cell cols="4">ResNet-50 + OSME + MAMC (Eq. 6?8)</cell><cell>2</cell><cell></cell><cell>86.2</cell><cell>63.3</cell></row><row><cell cols="4">ResNet-50 + OSME + MAMC (Eq. 6?8)</cell><cell>3</cell><cell></cell><cell>86.3</cell><cell>68.1</cell></row><row><cell cols="4">ResNet-101 + OSME + MAMC (Eq. 6?8)</cell><cell>2</cell><cell></cell><cell cols="2">86.5 102.1</cell></row><row><cell cols="7">(a) Ablation analysis of our method on CUB-200-2011.</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Anno. 1-Stage Acc.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DVAN [16]</cell><cell>?</cell><cell>?</cell><cell>79.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepLAC [7]</cell><cell></cell><cell></cell><cell>80.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NAC [55]</cell><cell>?</cell><cell></cell><cell>81.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Part-RCNN [10]</cell><cell></cell><cell>?</cell><cell>81.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MG-CNN [56]</cell><cell>?</cell><cell>?</cell><cell>81.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50 [47]</cell><cell>?</cell><cell></cell><cell>81.7</cell><cell>Method</cell><cell cols="3">Anno. 1-Stage Acc.</cell></row><row><cell>PA-CNN [8] RAN [15] MG-CNN [56] B-CNN [22] ST-CNN [21] FCAN [32] PDFR [24] ResNet-101 [47] FCAN [32] SPDA-CNN [57] RACNN [13]</cell><cell>? ? ? ? ? ? ?</cell><cell>? ? ? ? ? ?</cell><cell>82.8 82.8 83.0 84.1 84.1 84.3 84.5 84.5 84.7 85.1 85.3</cell><cell>DVAN [16] FCAN [32] ResNet-50 [47] RAN [15] B-CNN [22] FCAN [32] ResNet-101 [47] RACNN [13] PA-CNN [8] MACNN [58]</cell><cell>? ? ? ? ? ? ? ?</cell><cell>? ? ? ?</cell><cell>87.1 89.1 89.8 91.0 91.3 91.3 91.9 92.5 92.8 92.8</cell></row><row><cell>PN-CNN [9]</cell><cell></cell><cell>?</cell><cell>85.4</cell><cell>Ours (ResNet-50)</cell><cell>?</cell><cell></cell><cell>92.8</cell></row><row><cell>RAM [35]</cell><cell>?</cell><cell>?</cell><cell>86.0</cell><cell cols="2">Ours (ResNet-101) ?</cell><cell></cell><cell>93.0</cell></row><row><cell>MACNN [58]</cell><cell>?</cell><cell></cell><cell>86.5</cell><cell cols="3">(d) Stanford Cars.</cell><cell></cell></row><row><cell>Ours (ResNet-50)</cell><cell>?</cell><cell></cell><cell>86.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ours (ResNet-101) ?</cell><cell></cell><cell>86.5</cell><cell>Method</cell><cell cols="3">Anno. 1-Stage Acc.</cell></row><row><cell cols="3">(b) CUB-200-2011.</cell><cell></cell><cell>ResNet-50 [47]</cell><cell>?</cell><cell></cell><cell>74.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ResNet-101 [47]</cell><cell>?</cell><cell></cell><cell>75.6</cell></row><row><cell>Method</cell><cell cols="3">Anno. 1-Stage Acc.</cell><cell>RAN [15]</cell><cell>?</cell><cell>?</cell><cell>75.7</cell></row><row><cell>PDFR [24]</cell><cell>?</cell><cell>?</cell><cell>72.0</cell><cell>RACNN [13]</cell><cell>?</cell><cell>?</cell><cell>76.5</cell></row><row><cell>ResNet-50 [47]</cell><cell>?</cell><cell></cell><cell>81.1</cell><cell>Ours (ResNet-50)</cell><cell>?</cell><cell></cell><cell>77.9</cell></row><row><cell>DVAN [16]</cell><cell>?</cell><cell>?</cell><cell>81.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RAN [15]</cell><cell>?</cell><cell>?</cell><cell>83.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCAN [32]</cell><cell>?</cell><cell></cell><cell>84.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-101 [47]</cell><cell>?</cell><cell></cell><cell>84.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RACNN [13]</cell><cell>?</cell><cell>?</cell><cell>87.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (ResNet-50)</cell><cell>?</cell><cell></cell><cell>84.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ours (ResNet-101) ?</cell><cell></cell><cell>85.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(c) Stanford Dogs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">It is worth to point out that the implementation of MAMC is independent to the use of N-pair loss, as MAMC is a general framework that can be combined with other triplet-based metric learning loss as well. The N-pair loss is taken as a reference because of its robustness and good convergence in practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Ours (ResNet-101) ? 78.5 (e) Dogs-in-the-Wild.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Birdlets: Subordinate categorization using volumetric primitives and pose-normalized appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Jointly optimizing 3D model fitting and fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The truth about cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dog breed classification using part localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep LAC: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<editor>BMVC.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Part-based R-CNNs for finegrained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Look closer to see better: recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<title level="m">Squeeze-and-excitation networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Visual concept recognition and localization via iterative introspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops on Fine-Grained Visual Categorization</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning features and parts for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<editor>ICPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. In: NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fine-grained image classification by exploring bipartite-graph labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Picking deep filter responses for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fisher vectors meet neural networks: A hybrid classification architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The ignorant led by the blind: A hybrid human-machine vision system for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="29" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning concept embeddings with combined human-machine expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00487</idno>
		<title level="m">Generalized orderless pooling performs implicit salient matching</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICVGIP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Soft proposal networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fully convolutional attention networks for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10332</idno>
		<title level="m">Dynamic computational time for visual attention</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Metric learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="287" to="364" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;Siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Mining discriminative triplets of patches for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<editor>ICLR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The iNaturalist challenge 2017 dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">Kernel pooling for convolutional neural networks. In: CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Multiple granularity descriptors for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">SPDA-CNN: Unifying semantic part detection and abstraction for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
