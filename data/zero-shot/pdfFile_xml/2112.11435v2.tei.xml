<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learned Queries for Efficient Local Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moab</forename><surname>Arar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Reichman University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><forename type="middle">H</forename><surname>Bermano</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learned Queries for Efficient Local Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformers (ViT) serve as powerful vision models. Unlike convolutional neural networks, which dominated vision research in previous years, vision transformers enjoy the ability to capture long-range dependencies in the data. Nonetheless, an integral part of any transformer architecture, the self-attention mechanism, suffers from high latency and inefficient memory utilization, making it less suitable for high-resolution input images. To alleviate these shortcomings, hierarchical vision models locally employ self-attention on non-interleaving windows. This relaxation reduces the complexity to be linear in the input size; however, it limits the cross-window interaction, hurting the model performance. In this paper, we propose a new shift-invariant local attention layer, called query and attend (QnA), that aggregates the input locally in an overlapping manner, much like convolutions. The key idea behind QnA is to introduce learned queries, which allow fast and efficient implementation. We verify the effectiveness of our layer by incorporating it into a hierarchical vision transformer model. We show improvements in speed and memory complexity while achieving comparable accuracy with state-of-the-art models. Finally, our layer scales especially well with window size, requiring up-to x10 less memory while being up-to x5 faster than existing methods. The code is publicly available at https://github.com/ moabarar/qna.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Two key players take the stage when considering data aggregation mechanisms for image processing. Convolutions were the immediate option of choice. They provide locality, which is an established prior for image processing, and efficiency while doing so. Nevertheless, convolutions capture local patterns, and extending them to global context is difficult if not impractical. Attention-based models <ref type="bibr" target="#b71">[70]</ref>, on the other hand, offer an adaptive aggregation mechanism, where the aggregation scheme itself is input-dependent, or spatially dynamic. These models <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b18">17]</ref> are the de-facto <ref type="bibr" target="#b26">25</ref>   <ref type="figure">Figure 1</ref>. Performance-Efficiency Comparisons On 224 2 Input Size. QnA-ViT (our method) demonstrates better accuracyefficiency trade-off compared to state-of-the-art baselines. As suggested by Dehghani et al. <ref type="bibr" target="#b16">[15]</ref>, we report the ImageNet-1k <ref type="bibr" target="#b59">[58]</ref> Top-1 accuracy (y-axis) trade-off with respect to parameter count (left), floating point operations (middle) and inference throughput (right). The throughput is measured using the timm <ref type="bibr" target="#b74">[73]</ref> library, as tested on NVIDIA V100 with 16GB memory. Other metrics, are from the original publications <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b75">74,</ref><ref type="bibr" target="#b84">82,</ref><ref type="bibr" target="#b91">89]</ref> choice in the natural-language processing field and have recently blossomed for vision tasks as well. Earlier variants of the Vision Transformers (ViT) <ref type="bibr" target="#b19">[18]</ref> provide global context by processing non-interleaving image patches as word tokens. For these models to be effective, they usually require a vast amount of data <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b62">61]</ref>, heavy regularization <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b66">65]</ref> or modified optimization objectives <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b22">21]</ref>. Even more so, it was observed that large scale-training drives the models to attend locally <ref type="bibr" target="#b57">[56]</ref>, especially for early layers, encouraging the notion that locality is a strong prior.</p><p>Local attention mechanisms are the current method of choice for better vision backbones. These backbones follow a pyramid structure similar to convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b73">72,</ref><ref type="bibr" target="#b91">89]</ref>, and process high-resolution inputs by restricting the self-attention to smaller windows, preferably with some overlap <ref type="bibr" target="#b70">[69]</ref> or other forms of intercommunication <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b84">82]</ref>. The latter approaches naturally induce locality while benefiting from spatially dynamic ag-gregation. On the other hand, these architectures come at the cost of computational overhead and, more importantly, are not shift-equivariant.</p><p>In this paper, we revisit the design of local attention and introduce a new aggregation layer called Query and Attend (QnA). The key idea is to leverage the locality and shiftinvariance of convolutions and the expressive power of attention mechanisms.</p><p>In local self-attention, attention scores are computed between all elements comprising the window. This is a costly operation of quadratic complexity in the window size. We propose using learned queries to compute the aggregation weights, allowing linear memory complexity, regardless of the chosen window size. Our layer is also flexible, showing that it can serve as an effective up-or down-sampling operation. We Further observe that combining different queries allows capturing richer feature subspaces with minimal computational overhead. We conclude that QnA layers interleaved with vanilla transformer blocks form a family of hierarchical ViTs that achieve comparable or better accuracy compared to SOTA models while benefiting from upto x2 higher throughput and fewer parameters and floatingpoint operations (see <ref type="figure">Figure 1</ref>).</p><p>Through rigorous experiments, we demonstrate that our novel aggregation layer holds the following benefits:</p><p>? QnA imposes locality, granting efficiency without compromising accuracy.</p><p>? QnA can serve as a general-purpose layer. For example, strided QnA allows effective down-sampling, and multiple-queries can be used for effective upsampling, demonstrating improvements over alternative baselines.</p><p>? QnA naturally incorporates locality into existing transformer-based frameworks. For example, we demonstrate how replacing self-attention layers with QnA ones in an attention-based object-detection framework <ref type="bibr" target="#b8">[7]</ref> is beneficial for precision, and in particular for small-scale objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Convolutional Networks: CNN-based networks have dominated the computer vision world. For several years now, the computer vision community is making substantial improvements by designing powerful architectures <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b55">54,</ref><ref type="bibr" target="#b60">59,</ref><ref type="bibr" target="#b63">62,</ref><ref type="bibr" target="#b65">64,</ref><ref type="bibr" target="#b81">79]</ref>. A particularly related CNN-based work is RedNet <ref type="bibr" target="#b43">[42]</ref>, which introduces an involution operation. This operation extracts convolution kernels for every pixel through linear projection, enabling adaptive convolution operations. Despite its adaptive property, RedNet uses linear projections that lack the expressiveness of the selfattention mechanism.</p><p>Vision-Transformers: The adaptation of self-attention showed promising results in various vision tasks including image recognition <ref type="bibr">[3,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b92">90]</ref>, image generation <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b90">88]</ref>, object-detection <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b93">91]</ref> and semantic-segmentation <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b72">71]</ref>. These models, however, did not place pure selfattention as a dominant tool for vision models. In contrast, vision transformers <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b66">65]</ref>, brought upon a conceptual shift. Initially designed for image classification, these models use global self-attention on tokenized imagepatches, where each token attends all others. T2T-ViT <ref type="bibr" target="#b85">[83]</ref> further improves the tokenization process via light-weight self-attention at early layers. Similarly, carefully designing a Conv-based STEM-block <ref type="bibr" target="#b80">[78]</ref> improves convergence rate and accuracy. CrossViT <ref type="bibr" target="#b9">[8]</ref> propose processing at both a coarse-and fine-grained patch levels. TNT-ViT <ref type="bibr" target="#b28">[27]</ref> on the other hand, splits coarse-patches into locally attending parts. This information is then fused into global attention between patches. ConViT <ref type="bibr" target="#b15">[14]</ref> improves performance by carefully initializing the self-attention block to encourage locality. LeViT <ref type="bibr" target="#b26">[25]</ref> offers an efficient vision transformer through careful design, that combines convolutions and extreme down-sampling. Common to all these models is that, due to memory considerations, expressive feature maps are extracted on very low resolutions, which is not favorable in down-stream tasks such as object-detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Self-Attention:</head><p>Dense prediction tasks involve processing high-resolution images. Global attention is not tractable in this setting, due to quadratic memory and computational requirements. Instead, pyramid architectures employing local attention are used <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b70">69,</ref><ref type="bibr" target="#b77">75,</ref><ref type="bibr" target="#b84">82,</ref><ref type="bibr" target="#b91">89]</ref>. Typically for such approaches, self-attention is performed within each window, with down-sampling usually applied for global context. Liu et al. <ref type="bibr" target="#b46">[45]</ref> propose shifted windows, showing that communication between windows is preferable to independent ones <ref type="bibr" target="#b73">[72]</ref>. Halo-Net <ref type="bibr" target="#b70">[69]</ref> expands the neighborhood of each window to increase context and inter-window communication. Chu et al. <ref type="bibr" target="#b12">[11]</ref> use two-stage self-attention. In the first stage, local attention is employed, while in the second stage a global-self attention is applied on sub-sampled windows. These models however, are not shift-invariant, which is a property we maintain. Closest to our work, is the stand-alone self-attention layer (SASA) <ref type="bibr" target="#b50">[49]</ref>. As detailed in later sections, this layer imposes restrictive memory overhead, and is significantly slower, with similar accuracy compared to ours (see <ref type="figure" target="#fig_10">Fig. 3</ref>).</p><p>Learned Queries: The concept of learned queries has been explored in the literature in other settings <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b42">41]</ref>. In Set Transformers <ref type="bibr" target="#b42">[41]</ref>, learned queries are used to project the input dimension to a smaller output dimension, either for computation consideration or decoding the output prediction. Similarly, the Perceiver networks fam-ily <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b40">39]</ref> use small latent arrays to encode information from the input array. Goyal et al. <ref type="bibr" target="#b24">[23]</ref> propose a modification for transformer architectures where learned queries (shared workspace) serve as communication-channel between tokens, avoiding quadratic, pair-wise communication. Unlike QnA, the methods above use cross-attention on the whole input sequence. In QnA, the learned queries are shared across overlapping windows. The information is aggregated locally, leveraging the powerful locality priors that have been so well established through the vast usage of convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Query and Attend is a context-aware local feature processing layer. The key design choice of QnA is a convolution-like operation in which aggregation kernels vary according to the context of the processed local region. The heart of QnA is the attention mechanism, where overlapping windows are efficiently processed to maintain shiftinvariance. Recall that three primary entities are deduced from the input features in self-attention: queries, keys, and values. The query-key dot product, which defines the attention weights, can be computationally pricey. To overcome this limitation, we detour from extracting the queries from the window itself but learn them instead (see <ref type="figure" target="#fig_1">Figure 2c</ref>). This process is conceptually similar to convolution kernels, as the learned queries determine how to aggregate token values, focusing on feature subspaces pre-defined by the network. We show that learning the queries maintains the expressive power of the self-attention mechanism and facilitates a novel efficient QnA implementation that uses only simple and fast operations. Finally, our layer can be extended to perform other functionalities (e.g., downsampling and upsampling), which are non-trivial in existing methods <ref type="bibr" target="#b50">[49,</ref><ref type="bibr" target="#b70">69]</ref>.</p><p>Before the detailed explanation of QnA, we will briefly discuss the benefits and limitations of convolutions and selfattention. We let H and W be the height and width of the input feature maps, and denote D as the embedding dimension. Otherwise, throughout this section, we use upper-case notation to denote a matrix or tensor entities, and lower-case notation to denote scalars or vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolution</head><p>The convolution layer aggregates information by considering a local neighborhood of each element (e.g., a pixel) of the input feature X ? R H?W ?D . Specifically, given a kernel W ? R k?k?D?D , the convolution output at location (i, j) is:</p><formula xml:id="formula_0">z i,j = (n,m)? N k (i,j) x n,m ? W k/2 +i?n, k/2 +j?m ,<label>(1)</label></formula><p>where the k ? k-spatial neighborhood of location (i, j) is</p><formula xml:id="formula_1">N k (i, j) = {(n, m)| ? k/2 &lt; (i ? n), (j ? m) ? k/2}</formula><p>(see <ref type="figure" target="#fig_1">Figure 2a</ref>). To simplify the notation, we omit k from Equation <ref type="formula" target="#formula_0">(1)</ref> and re-write it in matrix notation as:</p><formula xml:id="formula_2">z i,j = X Ni,j ? W,<label>(2)</label></formula><p>For brevity, we assume a stride 1 for all strided operations, and padding is applied to maintain spatial consistency. The number of convolutional parameters is quadratic in kernel size, inhibiting usage of large kernels, therefore limiting the ability to capture global interactions. In addition, reusing convolutional filters across different locations does not allow adaptive content-based filtering. Nevertheless, the locality and shift-invariance properties of convolutions benefit vision tasks. For this reason, convolutions are widely adopted in computer vision networks, and deep learning frameworks support hardware-accelerated implementation of Equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-Attention</head><p>A vision transformer network processes a sequence of D-dimensional vectors, X ? R N ?D , by mixing the sequence of size N through the self-attention mechanism. These vectors usually encode some form of image patches where N = H ? W and H, W are the number of patches in each spatial dimension. Specifically, the input vectors are first projected into keys K = XW K , values V = XW V and queries Q = XW Q via three linear projection matrices W K , W V , W Q ? R D?D . Then, the output of the selfattention operation is defined by:</p><formula xml:id="formula_3">SA(X) = Attention (Q, K) ? V = Softmax QK T / ? D ? V,<label>(3)</label></formula><p>where Attention (Q, K) is an attention score matrix of size N ? N which is calculated using Softmax that is applied over each row. Unlike convolutions, self-attention layers have a global receptive field and can process the whole input sequence, without affecting the number of learned parameters. Furthermore, every output of the self-attention layer is an inputdependent linear combination of the V values, whereas in convolutions the aggregation is the same across the spatial dimension. However, the self-attention layer suffers from quadratic run-time complexity and inefficient memory usage, which makes it less favorable for processing high-resolution inputs. Furthermore, it has been shown that vanilla transformers don't attend locally very well <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b66">65]</ref>, which is a desired prior for downstream tasks. These models tend to become more local in nature only after a long and data-hungry training process <ref type="bibr" target="#b57">[56]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Query-and-Attend</head><p>To devise a high-powered layer, we will adapt the selfattention mechanism into a convolution-like aggregation operation. The motivation behind this is that, as it has already been shown <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b19">18]</ref>, self-attention layer has better capacity than the convolution layer, yet, the inductive bias of convolutions allows better transferability and generalization capability <ref type="bibr" target="#b14">[13]</ref>. Specifically, the locality and shiftinvariance priors (for early stages) impose powerful guidance in the image domain.</p><p>We begin by revisiting the Stand-Alone-Self-Attention approach (SASA) <ref type="bibr" target="#b50">[49]</ref>, where attention is computed in small overlapping k ? k-windows, much like a convolution. The output z i,j of SASA is defined as:</p><formula xml:id="formula_4">z i,j = Attention q i,j , K Ni,j ? V Ni,j ,<label>(4)</label></formula><p>where q i,j = X i,j W Q . In other words, in order to aggregate tokens locally, self-attention is applied between the tokens of each local window, and a single query is extracted from the window center (see <ref type="figure" target="#fig_1">Figure 2b</ref>). While SASA <ref type="bibr" target="#b50">[49]</ref> enjoy expressiveness and locality, through an input-adaptive convolution-like operation, it demands heavy memory usage. Specifically, to the best of our knowledge, all publicly available implementations use an unfolding operation that extracts patches from the input tensor. This operation expands the memory requirement by k 2 if implemented naively. Vaswani et al. <ref type="bibr" target="#b70">[69]</ref> improved the memory-requirement of SASA <ref type="bibr" target="#b50">[49]</ref> using local attention with halo expansion. Nevertheless, this implementation requires x3-x10 more memory than QnA while being x5-x8 slower, depending on k (see <ref type="figure" target="#fig_10">Figure 3</ref>). This limitation makes the SASA layer infeasible for processing highresolution images, employing larger kernels, or using sizable batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">QnA -Single Query</head><p>To alleviate the compute limitation of SASA <ref type="bibr" target="#b50">[49]</ref>, we redefine the key-query dot product in Equation <ref type="formula" target="#formula_4">(4)</ref> by introducing learned queries. As we will later see, this modification leverages the weight-sharing principle (just like convolutions) and enables the efficient implementation of the QnA layer (see Section 3.4).</p><p>We begin by first replacing the queries q i,j from Equation (4) with a single D-dimensional vectorq, that is learned during training. More particularly, we define the output of the QnA layer at location (i, j) to be:</p><formula xml:id="formula_5">z i,j = Attention q, K Ni,j ? V Ni,j .<label>(5)</label></formula><p>Through the above modification, we interpret the querykey dot product as the scalar-projection of the keys onto Ddimensional query directions. Therefore, the token values are aggregated according to their relative orientation with the query vectors. Intuitively, the keys can now be extracted such that relevant features' keys will be closely aligned with q. This means that the network can optimize the query direction to detect contextually related features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">QnA -Multiple Queries</head><p>As it turns out, performance can be further pushed forward under our paradigm, with minimal computational overhead and negligible additional memory. The naive approach is to add channels or attention heads when considering multihead attention. While this enhances expressiveness, additional heads induce a larger memory footprint and computational overhead. To improve the layer expressiveness, we can use L-different queriesQ ? R L?D instead of one. Nevertheless, simply plugging inQ in Equation <ref type="formula" target="#formula_5">(5)</ref> leads to L ? D output, which expands the memory usage by L (also known as cross-attention). Instead, we weight-sum the attention maps learned by the queries into a single attention   map (for each attention head) and use it to aggregate the values. Therefore our QnA output becomes:</p><formula xml:id="formula_6">z i,j = ? ? i?[L] W i * Attention Q i , K Ni,j ? ? ? V Ni,j ,<label>(6)</label></formula><p>where W ? R L?k 2 is a learned weight matrix, and * is the element-wise multiplication operation. The overall extra space used in this case is O(L ? k 2 ), which is relatively small, as opposed to the naive solution, which requires O(L ? D) extra space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">QnA Variants</head><p>Our layer naturally accommodates the improvements made for the vanilla self-attention layer <ref type="bibr" target="#b71">[70]</ref>. Specifically, we use relative-positional embedding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b56">55,</ref><ref type="bibr" target="#b78">76]</ref> and multihead attention in all our models (further details can be found in Appendix F).</p><p>Upsampling &amp; Downsampling Using QnA downsampling can be trivially attained using strided windows. To up-scale tokens by a factor s, we can use a QnA layer with L = s 2 learned queries. Assigning the result of each query as an entry in the upsampled output, we effectively construct a spatially dynamic upsampling kernel of size s ? s. We define the upsampling operation more formally in Appendix F. We show that QnA could be used to efficiently perform the upsampling function (Section 4.4) with improved performance, suggesting it can be incorporated into other vision tasks such as image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation &amp; Complexity Analysis</head><p>The shared-learned queries across windows allow us to implement QnA using efficient operations that are available in existing deep-learning frameworks (e.g., Jax <ref type="bibr" target="#b3">[4]</ref>). In particular, the query-key dot product can be calculated once on the whole input sequence, avoiding extra space allocation. Then, we can use window-based operations to effectively calculate the softmax operation over the overlapping windows, leading to a linear time-and-space complexity (see <ref type="figure" target="#fig_10">Figure 3</ref>). Full-implementation details of our method are in Appendix D, along with a code snippet in Jax/Flax <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">The QnA-ViT Architecture</head><p>The QnA-ViT architecture is composed of vision transformer blocks <ref type="bibr" target="#b19">[18]</ref> (for global context) and QnA blocks (for local context). The QnA block shares a similar structure with the ViT block, except we replace the multi-head selfattention layer with the QnA layer. We present a family of architectures that follow the design of ResNet <ref type="bibr" target="#b29">[28]</ref>. Specifically, we use a 4-stage hierarchical architecture. The base dimension D varies according to the model size. Below we indicate how many layers we use in each stage (T stands for ViT-blocks and Q stands for QnA-blocks): </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Recognition &amp; ImageNet-1K Results</head><p>Setting: we evaluate our method using the ImageNet-1K <ref type="bibr" target="#b59">[58]</ref> benchmark, and follow the training recipe of DEiT <ref type="bibr" target="#b66">[65]</ref>, except we omit EMA <ref type="bibr" target="#b54">[53]</ref> and repeated augmentations <ref type="bibr" target="#b32">[31]</ref>. For full-training details please refer to Appendix B.</p><p>Results: A summary comparison between different models appears in <ref type="table">Table 1</ref>. As shown from the table, most transformer-based vision models outperform CNN-based ones in terms of the top-1 accuracy, even when the CNN models are trained using a strong training procedure. For example, ResNet50 <ref type="bibr" target="#b29">[28]</ref> with standard ImageNet training achieves 76.6% top-1 accuracy. However, as argued in <ref type="bibr" target="#b75">[74]</ref>, with better training, its accuracy sky-rockets to 80.4%. Indeed, this is a very impressive improvement, yet it falls short behind transformer models. In particular, our model (the tiny version) improves upon ResNet by 1.3% with 40% fewer parameters and FLOPs. In terms of speed, CNNs are very fast and have a smaller memory footprint (see <ref type="figure" target="#fig_10">Figure 3</ref>). The throughput gap can be evident by investigating the vision transformers reported in <ref type="table">Table 1</ref>. A particular strong ViT is the Focal-ViT <ref type="bibr" target="#b84">[82]</ref>; in its tiny version, it improves upon ResNet101 by 0.7% while the latter enjoys x1.4-times better throughput. Nonetheless, our model stands out in terms of the speed-accuracy trade-off. Comparing QnA-Tiny with Focal-Tiny, we achieve only 0.5% less accuracy while having x2-times better throughput, parameter-count, and flops. We can even reduce this gap by training the QnA with a larger receptive field. For example, setting the receptive field of the QnA to be 7x7, instead of 3x3, achieve 82.0% accuracy, with negligible effect on the model speed and size.</p><p>Finally, we notice that most Vision Transformers achieve similar Top-1 accuracy. More specifically, tiny models (in terms of parameters and number of FLOPs) achieve roughly the same Top-1 accuracy of 81.2-82.0%. The accuracy difference is even less significant in larger models (e.g., base variants accuracy differs by only 0.1%), and this accuracy difference can be easily tipped to either side by many factors, even by choosing a different seed <ref type="bibr" target="#b53">[52]</ref>. Nonetheless, our model is faster, all while using fewer resources.  <ref type="table">Table 2</ref>. Multiple queries effect. We compare the performance of SASA <ref type="bibr" target="#b50">[49]</ref> to QnA with a varying amount of queries. As can be seen, using multiple queries improves QnA, reaching comparable performance, using an order of magnitude less memory.</p><formula xml:id="formula_7">QnA SASA L = 1 L = 2 L = 3 L = 4</formula><p>The reason behind better accuracy-efficiency trade-off:</p><p>QnA-ViT achieves a better accuracy-efficiency trade-off for several reasons. First, QnA is fast, which is crucial for better throughput. Further, most of the vision transformer's parameter count is due to the linear projection matrices. Our method reduces the number of linear projections by omitting the query projections (i.e., the W q matrix is replaced with 2-learned queries). Furthermore, the feed-forward network requires ?2 more parameters than the self-attention. Our model uses smaller embedding dimensions than existing models without sacrificing accuracy. Namely, NesT-Tiny <ref type="bibr" target="#b91">[89]</ref> uses an embedding dimension of 192, while Swin-Tiny <ref type="bibr" target="#b46">[45]</ref> and Focal-Tiny <ref type="bibr" target="#b84">[82]</ref> use 96 embedding dimensions. On the other hand, our method achieves a similar feature representation capacity, with a lower dimension of 64.</p><p>Finally, other parameter efficient methods achieve low parameter count by training on larger input images <ref type="bibr" target="#b65">[64,</ref><ref type="bibr" target="#b70">69]</ref>. This is shown to improve image-classification accuracy <ref type="bibr" target="#b68">[67]</ref>. However, it comes at the cost of lower-throughput and more FLOPs. For example, EfficientNet-B5 <ref type="bibr" target="#b65">[64]</ref>, which was trained and tested on images of 456 ? 456 resolution, achieves 83.6% accuracy while using only 30M parameters. Nonetheless, the network's throughput is 170 images/sec, and it uses 9.9 GFLOPs. Compared to our base model, QnA achieves similar accuracy with twice the throughput. Also, it is important to note that these models were optimized via Neural Architecture search, an automated method for better architecture design. We believe employing methods with similar purpose <ref type="bibr" target="#b83">[81]</ref> would even further optimize our models' parameter count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation &amp; Design Choices</head><p>Number of Queries: Using multiple queries allows us to capture different feature subspaces.</p><p>We consider SASA <ref type="bibr" target="#b50">[49]</ref> as our baseline, which extracts the self-attention queries from the window elements. Due to its heavy memory footprint, we cannot consider SASA variants similar to QnA-ViT. Instead, we consider a lightweight variant that combines local self-attention with SASA. All SASA layers use a 3x3 window size. Downsampling is performed similar to QnA-ViT, except that we replace QnA with SASA.  <ref type="table" target="#tab_4">Table 3</ref>. Ablation studies and design choices. In the first two columns we specify the number of global-attention and QnA layers used in each stage. See Section 4.2 for further details, and the supp. materials for more configurations.</p><p>Finally, the local-self attention layers use a 7x7 window size without overlapping (see Appendix E.1). The results are summarized in <ref type="table">Table 2</ref>. As can be seen, we achieved comparable results to SASA. In addition, two queries outperform one, but this improvement saturates quickly. We hence recommend using two queries, as it enjoys efficiency and expressiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of heads:</head><p>Most vision transformers use large head dimension (e.g., ? 32) <ref type="bibr" target="#b67">[66]</ref>. However, we found that the QnA layer enjoys more heads. We trained various models based on QnA and self-attention layers with different training setups to verify this. Our experiments found that a head dimension d = 8 works best for QnA layers. Similar to previous work <ref type="bibr" target="#b67">[66]</ref>, in hybrid models, where both self-attention and QnA layers are used, we found that selfattention layers still require a large head dimension (i.e., d = 32). Moreover, we found that using more heads for QnA is considerably better (up to 1% improvement) for small networks. Moreover, this performance gap is more apparent when training the models for fewer epochs without strong augmentations (see Appendix E.2 for further details). Intuitively, since the QnA layer is local, it benefits more from local pattern identifications, unlike global context, which requires expressive representation.</p><p>How many QnA layers do you need? In order to verify the expressive power of QnA, we consider a dozen different models. Each model consists of four stages. In each stage, we consider using self-attention and QnA layers. A summary report can be found in formance. QnA is fast and improves the model's efficiency. Finally, the QnA layer is a very effective down-sampling layer. For example, we considered two baseline architectures which are mostly composed of transformer blocks, (1) one model uses simple 2x2 strided-convolution to reduce the feature maps (adopted in <ref type="bibr" target="#b46">[45]</ref>), and the <ref type="formula" target="#formula_2">(2)</ref> other is based on the down-sampling used in NesT <ref type="bibr" target="#b91">[89]</ref>, which is a 3x3 convolution, followed by a layer-norm and maxpooling layer. These two models achieve similar accuracy, which is 81.2%. On the other hand, when merely replacing the downsampling layers with the QnA layer, we witness a 0.7% improvement without increasing the parameter count and FLOPs. Note, global self-attention is still needed to achieve good performance. However, it can be diminished by local operations, e.g., QnA.</p><p>Deep models: To scale-up our model, we chose to increase the number of layers in the network's third stage (as typical in previous works <ref type="bibr" target="#b29">[28]</ref>). This design choice is adapted mainly for efficiency reasons, where the spatial and feature dimension are manageable in the third stage.</p><p>In particular, we increase the total number of layers in the third stage from 7 to 19 and consider four configurations where each configuration varies by the number of QnA layers used. The models' accuracies are reported in <ref type="table" target="#tab_4">Table 3</ref>. As seen from the table, the model's accuracy can be maintained by reducing the number of global attention. This indicates that while self-attention can capture global information, it is beneficial to a certain degree, and local attention could be imposed by the architecture design for efficiency consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object Detection</head><p>Setting: To evaluate the representation quality of our pretrained networks, we use the DETR <ref type="bibr" target="#b8">[7]</ref> framework, which is a transformer-based end-to-end object detection framework. We use three backbones for our evaluations; ResNet50 <ref type="bibr" target="#b29">[28]</ref>, and two variants of QnA-ViT, namely, QnA-ViT-Tiny, and QnA-ViT-Tiny-7x7, which uses a 7x7 receptive for all QnA  Revisiting DETR transformer design: DETR achieves comparable results to CNN-based frameworks <ref type="bibr" target="#b58">[57]</ref>. However, it achieves less favorable average precision when tested on smaller objects. The DETR model uses a vanilla transformer encoder to process the input features extracted from the backbone network. As argued earlier, global attention suffers from locality issues. To showcase the potential of incorporating QnA in existing transformer-based networks, we propose DETR-QnA architecture, in which two transformer blocks are replaced with four QnA blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>We report the results in <ref type="table">Table 4</ref>. As can be seen, DETR trained with QnA-Tiny achieves +2.2 better AP compared to the ResNet50 backbone. Using a larger receptive field (7?7) further improves the AP by 0.4. However, much improvement is due to better performance on large objects (+0.7). Finally, when incorporating QnA into the DETR encoder, we gain an additional +0.6AP (and +1.0AP relative to using the DETR model). More particularly, incorporating QnA with DETR achieves an impressive +2.2 AP improvement on small objects, indicating the benefits of QnA's locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">QnA as an upsampling layer</head><p>We suggest that QnA can be adapted to other tasks besides classification and detection. To demonstrate this, we train an autoencoder network on the CelebA <ref type="bibr" target="#b47">[46]</ref> dataset, using the L 1 reconstruction loss. We consider two simple baselines that are convolution-based. In particular, one baseline uses bilinear up-sampling to upscale the feature maps, and another baseline uses the transposed convolution layer <ref type="bibr" target="#b88">[86]</ref>. Qualitative and quantitative results appear in <ref type="figure" target="#fig_6">Figure 4</ref> and <ref type="figure">Figure 5,</ref>   <ref type="figure">Figure 5</ref>. Quantitative Auto-Encoder Results are reported, compared to the same convolutional baselines as in <ref type="figure" target="#fig_6">Figure 4</ref>. We compare our method (gray) to bilinear upsampling (green) and transposed covolution-based upsampling (pink). We show consistent improvement across epochs (horizontal axes) in the L1 loss (top left), the pSNR (top right), the SSIM (bottom left), and MSE (bottom right) metrics.</p><p>the QnA-based auto-encoder achieves better qualitative and quantitative results and introduces fewer artifacts (see Appendix G for further details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations &amp; Conclusion</head><p>We have presented QnA, a novel local-attention layer with linear complexity that is also shift-invariant. Through rigorous experiments, we showed that QnA could serve as a general-purpose layer and improve the efficiency of vision transformers without compromising on the accuracy part. Furthermore, we evaluated our method in the object-detection setting and improved upon the existing self-attention-based method. Our layer could also be used as an up-sampling layer, which we believe is essential for incorporating transformers in other tasks, such as image generation. Finally, since QnA is attention-based, it requires additional intermediate memory, whereas convolutions operate seamlessly, requiring no additional allocation. Nonetheless, QnA has more expressive power than convolution. In addition, global self-attention blocks are more powerful in capturing global context. Therefore, we believe that our layer mitigates the gap between self-attention and convolutions and that future works should incorporate all three layers to achieve the best performance networks. <ref type="figure">Figure 6</ref>. QnA attention visualization of different heads. To visualize a specific location's attention score, we sum the attention scores obtained for that location in all windows. Attention maps are up-sampled and overlaid on the image for better visualization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Attention Visualization</head><p>In QnA, the aggregation kernel of each window is derived from the attention scores between the learned queries and the window keys. To visualize the attention of the whole image, we choose to sum the scores of each spatiallocation as obtained in all relevant windows. You can find the visualization in <ref type="figure">Figure 6</ref>. As shown in <ref type="figure">Figure 6</ref>, the attentions are content-aware, suggesting the window aggregation kernels are spatially adaptive. For the learned kernels in each window, please refer to <ref type="figure" target="#fig_7">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Full training details B.1. Image Classification</head><p>We evaluate our method using the ImageNet-1K <ref type="bibr" target="#b59">[58]</ref> benchmark, which contains 1.28M training images and 50,000 validation images from 1,000 classes. We follow the training recipe of DEiT <ref type="bibr" target="#b66">[65]</ref>, except we omit EMA <ref type="bibr" target="#b54">[53]</ref> and repeated augmentations <ref type="bibr" target="#b32">[31]</ref>. Particularly, we train all models for 300-epochs, using the AdmaW <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b49">48]</ref> optimizer.</p><p>We employ a linearly scaled learning rate lr = 5e-4 ? Batch size 256 <ref type="bibr" target="#b25">[24]</ref>, with warmup epochs <ref type="bibr" target="#b48">[47]</ref> varying according to model size and weight decay wd = 5e-2. For augmentations, we apply RandAugment <ref type="bibr" target="#b13">[12]</ref>, mixup <ref type="bibr" target="#b89">[87]</ref> and CutMix <ref type="bibr" target="#b87">[85]</ref> with label-smoothing <ref type="bibr" target="#b86">[84]</ref>, and color-jitter <ref type="bibr" target="#b10">[9]</ref>. Finally, an increasing stochastic depth is applied <ref type="bibr" target="#b37">[36]</ref>. Note this training recipe (with minor discrepancy between previous papers), is becoming the standard when training a Vision Transformer on the ImageNet benchmark. Finally, we normalize the queries in all QnA layers to be unit-vectors for better training stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Object Detection</head><p>We train the DETR model on COCO 2017 detection dataset <ref type="bibr" target="#b45">[44]</ref>, containing 118K training images and a 5k validation set size. We utilize the training setting of DETR, in which the input is resized such that the shorter side is between 480 and 800 while the longer side is at most 1333 <ref type="bibr" target="#b79">[77]</ref>. An initial learning rate of 1e ? 4 is set for the detection transformer, and 1e ? 5 learning rate for the backbone network. Due to computational limitations, we use a short training scheduler of 75 epochs with a batch size of 32. The learning rates are scaled by 0.1 after 50 epochs. We trained DETR using the implementation in <ref type="bibr" target="#b17">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. QnA-ViT architecture</head><p>The QnA-ViT architecture is composed of transformer blocks <ref type="bibr" target="#b19">[18]</ref> and QnA blocks. First we split the image into 4?4 patches, and project them to form the input tokens. The vision transformer block is composed of a multihead attention layer (MSA) and an inverted-bottleneck feed forward network (FFN), with expansion 4. The output of block-l is:</p><formula xml:id="formula_8">z l = MSA (LayerNorm(z l?1 )) + z l?1 z l = FFN (LayerNorm(z l )) + z l .</formula><p>The QnA block shares a similar structure, except we replace the MSA layer with QnA layer. Downampling is performed using QnA blocks with stride set to 2 (to enable skip-connections we use 1?1-convolution with similar stride). We employ pre-normalization <ref type="bibr" target="#b82">[80]</ref> to stabilize training. Finally, we use global average pooling <ref type="bibr" target="#b44">[43]</ref> right before the classification head, with LayerNorm [1] employed prior to the pooling operation. Full architecture details can be found in <ref type="table" target="#tab_7">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation &amp; complexity -extended</head><p>In this section we provide full details on the efficient implementation of QnA. To simplify the discussion, we only consider a single-query without positional embedding.</p><p>Let us first examine the output of a QnA layer, by ex-panding the softmax operation inside the attention layer:</p><formula xml:id="formula_9">z i,j = Attention q, K Ni,j ? V Ni,j = Ni,j eq Kn,m v n,m</formula><p>Ni,j eq Kn,m . Recall, N i,j is the k ?k-window at location (i, j). While it may seem that we need to calculate the query-key dot product for each window, notice that since we use the same query over each window, then we can calculateqK T once for the entire input. Also, we can leverage the matrix multiplication associativity and improve the computation complexity by calculatingqW T k first (this fused implementation reduces the memory by avoiding the allocation of the key entities). Once we calculate the query-key dot product, we can efficiently aggregate the dot products using the sumreduce operation supported in many deep learning frameworks (e.g., Jax <ref type="bibr" target="#b3">[4]</ref>). More specifically, let Sum k (. . . ) be a function that sums-up values in each k ? k-window, then:</p><formula xml:id="formula_10">z i,j = Ni,j eq Kn,m v n,m</formula><p>Ni,j eq Kn,m =</p><formula xml:id="formula_11">Sum k eq K T * V Sum k eq K T ,</formula><p>where * is the element wise multiplication. A pseudo-code of our method can be found in Algorithm 1. We further provide a code-snippet of the QnA module, implemented in Jax/Flax <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b31">30]</ref> (see <ref type="figure" target="#fig_9">Figure 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity Analysis:</head><p>in the single query variant, extracting the values and computing the key-query dot product require 2HW D 2 computation and HW + HW D extra space. Additionally, computing the softmax using the above method requires additional O(k 2 HW D) computation (for summation and division), and O(HW D) space (which is independent of k, i.e., the window size). For multiplequeries variant (L = 2), see empirical comparison in <ref type="figure" target="#fig_10">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Design choices -full report E.1. Number of queries</head><p>To verify the effectiveness of using multiple queries, we trained a lightweight QnA-ViT network composed of local self-attention blocks and QnA blocks. We set the window size of all local self-attention layers to be 7x7, and we use a 3x3 receptive field for QnA layers. All the downsampling performed are QnA-Based. A similar architecture was used for the SASA <ref type="bibr" target="#b50">[49]</ref>      <ref type="bibr" target="#b5">[5]</ref> and Flax <ref type="bibr" target="#b31">[30]</ref>. Full implementation, with pre-trained networks weights, will be made publicly available. use only QnA blocks and the second where we use both QnA blocks and (global) self-attention blocks. In the first experiment, we use the standard ImageNet training preprocessing <ref type="bibr" target="#b64">[63]</ref>, meaning we employ random crop with resize and random horizontal flip. In the second experiment, we used DeiT training preprocessing. We show the full report in <ref type="table">Table 6 and Table 7</ref>.</p><p>First, from <ref type="table">Table 6</ref>, we notice that training shallow QnAnetworks for fewer epochs requires many attention heads. Furthermore, it is better to maintain a fixed dimension head across stages -this is done by doubling the number of heads between two consecutive stages. For deeper networks, the Algorithm 1 Efficient implementation of QnA. All operations can be implemented efficiently with little memoryoverhead. Further, Sum k applies sum-reduction to all elements in each k ? k-window.</p><p>Input: X ? R H?W ?D Parameters: W K , W V ? R D?D ,q ? R D // Compute values:</p><p>1:</p><p>V ? XW V // Compute the query-key dot product ( S ?qK T ): Let C ? B * V * is the element-wise product <ref type="bibr">8:</ref> return Sum k (C)/Sum k (B)</p><p>QnA Blocks Heads AugReg Epochs Top-1 Acc. <ref type="table">Table 6</ref>. The affect of number of heads on QnA. We train two networks using the Inception preprocessing <ref type="bibr" target="#b64">[63]</ref>, i.e., random crop and horizontal flip. We set the number of QnA blocks according to the ResNet-18 and ResNet-50 networks (the number of blocks for each stage is stated in the first column). As can be seen, using fixed head-dimension is better than increasing the head-dimension as we propagate through the network. Shallow networks benefit from having many heads, while deeper networks gain less from more heads. Therefore we suggest increasing the head-dimension for deeper networks for better memory utilization.</p><p>advantage of using more heads becomes less significant. This is because the network can capture more feature subspaces by leveraging its additional layers. Finally, when using both QnA and ViT blocks, it is still best to use more heads for QnA layers, while for ViT blocks, it is best to use a high dimension representation by having fewer heads (see <ref type="table">Table 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. How many QnA layers do you need?</head><p>To understand the benefit of using QnA layers, we consider a dozen network architectures that combine vanilla  <ref type="table">Table 7</ref>. The number of heads affect on QnA and ViT Blocks. QnA still benefits from more heads, while ViT blocks need higherdimension representation, specially for longer training.</p><p>ViT and QnA blocks. For ViT blocks, we tried to use global attention in the early stages but found it better to use local self-attention and restrict the window size to be at most 14x14. We group the architecture choices into three groups:</p><p>1. We consider varying the number of QnA blocks in the early stages.</p><p>2. We change the number of QnA blocks in the third stage.</p><p>3. We use lower window size for the local-self attention blocks. Namely, 7x7 window in all ViT blocks at all stages.</p><p>Finally, all networks were trained for 300 epochs following DeiT preprocessing. The full report can be found in <ref type="table">Table 8</ref>. From <ref type="table">Table 8</ref>, local-self attention is not very beneficial in the early stages and can be omitted by using QnA blocks only. Furthermore, using more global-attention blocks in deeper stages is better, but the network's latency can be reduced by having a considerable amount of QnA blocks. Finally, local self-attention becomes less effective when using a lower window size. In particular, since QnA is shiftinvariant, it can mitigate the lack of cross-window interactions, reflected in the improvement gain we achieve when using more QnA blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. QnA Variants -extended version</head><p>As discussed in the paper, we incorporate multi-head attention and positional embedding in our layer.</p><p>Positional Embedding Self-attention is a permutation invariant operation, meaning it does not assume any spatial relations between the input tokens. This property is not desirable in image processing, where relative context is essential. Position encoding can be injected into the self-attention mechanism to solve this. Following recent literature, we use relative-positional embedding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b56">55,</ref><ref type="bibr" target="#b78">76]</ref>. This introduces a spatial bias into the attention scheme, rendering Eq. (3) (from the main text) now to be:</p><p>QnA-based auto-encoders, we use QnA layers for both down-sampling and up-sampling. All networks are trained on the CelebA dataset <ref type="bibr" target="#b47">[46]</ref>, where all images are centeraligned and resized to resolution of size 256 2 . All networks were trained for 10-epochs, using the Adam <ref type="bibr" target="#b41">[40]</ref> optimizer (learning rates were chosen according to the best test-loss).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>QnA Overview. Local layers operate on images by considering overlapping windows (left), where the output is computed by aggregating information within each window: (a) Convolutions apply aggregation by learning weighted filters that are applied on each window. (b) Stand-Alone-Self-Attention (SASA) combines the window tokens via self-attention [49] -a time and memory consuming operation. (c) Instead of attending all window elements with each other, we employ learned queries that are shared across windows. This allows linear space complexity, while maintaining the expressive power of the attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><label></label><figDesc>Figure 3. Single Layer Computational Complexity During Forward Pass. QnA outperforms SASA [49], HaloNet [69], and local self-attention baselines in terms of speed and memory consumption. In particular, during forward pass, HaloNet [69] requires at least x3 additional memory allocation while being x5 slower. For larger kernels, the computation overhead becomes significant where up-to x10 additional memory allocation is needed. Convolutional layers are the most memory efficient, however they are x1.8 slower compared to QnA for large kernels. All experiments tested with PyTorch [51], on input size 256 ? 256 ? 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>Tiny: D, T, Q = {64, [0, 0, 4, 2] , [3, 4, 3, 0]} ? Small: D, T, Q = {64, [0, 0, 12, 2] , [3, 4, 7, 0]} ? Base: D, T, Q = {96, [0, 0, 12, 2] , [3, 4, 7, 0]} For further details, please refer to Appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative Auto-Encoder Results. We train a simple Autoencoder using convolution layers (a-b), and (c) QnA layers. We show reconstructed images from the CelebA test set<ref type="bibr" target="#b47">[46]</ref>. QnA shows preferable reconstructions. See Section 4.4 for more details. layers (instead of 3x3). Complete training details are provided in the supplemental material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>QnA aggregation kernels visualization. The attention kernels are tiled in the visualization instead of overlapped, causing the uniform grid effect. Brighter areas indicate higher attention scores. (best viewed when zoomed-in).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>29 )</head><label>29</label><figDesc>.reshape([B, H_out, W_out, L, heads, 1]) 30 out_heads = jnp.sum(A / B, axis=-2).reshape([B, H_out, W_out, D]) 31 final_out = nn.Dense(self.D)(out_heads) 32 return final_out</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Code snippet of the QnA module implemented in Jax</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>3 :</head><label>3</label><figDesc>for l ? [L] , i, j ? [H] ? [W ] do 4: S l,i,j ? A l,i,j ? X T Compute the final output: 6:Let B ? e S be the element-wise exponent of S 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3. Single Layer Computational Complexity During Forward Pass. QnA outperforms SASA<ref type="bibr" target="#b50">[49]</ref>, HaloNet<ref type="bibr" target="#b70">[69]</ref>, and local self-attention baselines in terms of speed and memory consumption. In particular, during forward pass, HaloNet<ref type="bibr" target="#b70">[69]</ref> requires at least x3 additional memory allocation while being x5 slower. For larger kernels, the computation overhead becomes significant where up-to x10 additional memory allocation is needed. Convolutional layers are the most memory efficient, however they are x1.8 slower compared to QnA for large kernels. All experiments tested with PyTorch<ref type="bibr" target="#b52">[51]</ref>, on input size 256 ? 256 ? 64.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Method Params GFLOPS ThroughputTop-1 Acc.</figDesc><table><row><cell>ResNet50 [28, 74]</cell><cell>26M</cell><cell>4.1</cell><cell>1287</cell><cell>80.4</cell></row><row><cell>ResNet101 [28, 74]</cell><cell>45M</cell><cell>7.9</cell><cell>770</cell><cell>81.5</cell></row><row><cell>ResNet152 [28, 74]</cell><cell>60M</cell><cell>11.6</cell><cell>539</cell><cell>82.0</cell></row><row><cell>DeiT-S [65]</cell><cell>22M</cell><cell>4.6</cell><cell>940</cell><cell>79.8</cell></row><row><cell>DeiT-B [65]</cell><cell>86M</cell><cell>17.5</cell><cell>292</cell><cell>81.8</cell></row><row><cell>Swin-Tiny [45]</cell><cell>29M</cell><cell>4.5</cell><cell>723</cell><cell>81.3</cell></row><row><cell>Swin-Small [45]</cell><cell>50M</cell><cell>8.7</cell><cell>425</cell><cell>83.0</cell></row><row><cell>Swin-Base [45]</cell><cell>88M</cell><cell>15.4</cell><cell>277</cell><cell>83.5</cell></row><row><cell>Swin-Base [45]?384</cell><cell>88M</cell><cell>47.0</cell><cell>85</cell><cell>84.5</cell></row><row><cell>NestT-Tiny [89]</cell><cell>17M</cell><cell>5.8</cell><cell>568</cell><cell>81.5</cell></row><row><cell>NestT-Small [89]</cell><cell>38M</cell><cell>10.4</cell><cell>352</cell><cell>83.3</cell></row><row><cell>NestT-Base [89]</cell><cell>68M</cell><cell>17.9</cell><cell>233</cell><cell>83.8</cell></row><row><cell>Focal-Tiny [45]</cell><cell>29M</cell><cell>4.9</cell><cell>546</cell><cell>82.2</cell></row><row><cell>Focal-Small [45]</cell><cell>51M</cell><cell>9.1</cell><cell>282</cell><cell>83.5</cell></row><row><cell>Focal-Base [45]</cell><cell>90M</cell><cell>16.0</cell><cell>207</cell><cell>83.8</cell></row><row><cell>QnA-Tiny</cell><cell>16M</cell><cell>2.5</cell><cell>1060</cell><cell>81.7</cell></row><row><cell>QnA-Tiny7?7</cell><cell>16M</cell><cell>2.6</cell><cell>895</cell><cell>82.0</cell></row><row><cell>QnA-Small</cell><cell>25M</cell><cell>4.4</cell><cell>596</cell><cell>83.2</cell></row><row><cell>QnA-Base</cell><cell>56M</cell><cell>9.7</cell><cell>372</cell><cell>83.7</cell></row><row><cell>QnA-Base?384</cell><cell>56M</cell><cell>30.6</cell><cell>177</cell><cell>84.</cell></row></table><note>8 Table 1. ImageNet-1K [58] pre-training results. All models were pre-trained and tested on input size 224 ? 224. Models marked with ? 384 are later also fine-tuned and tested on 384 2 resolution, following [67]. The Accuracy, parameter count, and floating point operations are as reported in the corresponding pub- lication. Throughput was calculated using the timm [73] library, on a single NVIDIA V100 GPU with 16GB memory. For QnA7?7, a 7?7 window size was used instead of 3?3. Our model achieves comparable results to state-of-the-art models, with fewer parame- ters and better computation complexity.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Params (M). 16.440 16.182 16.188 16.192 16.200.</figDesc><table><row><cell>Top-1 Acc.</cell><cell>80.86</cell><cell>80.3</cell><cell>80.7</cell><cell>80.76</cell><cell>80.81</cell></row><row><cell>FLOPS (G)</cell><cell>2.620</cell><cell>2.378</cell><cell>2.400</cell><cell>2.420</cell><cell>2.442</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>ModelBackbone AP<ref type="bibr" target="#b51">50</ref> AP 75 AP L AP M AP S AP R50 55.4 36.6 53.2 38.0 15.1 35.3 DETR QnA-Ti 58.9 38.6 56.8 40.6 16.0 37.5 QnA-Ti7 59.6 39.3 57.6 41.2 16.0 37.9</figDesc><table><row><cell>DETR-QnA QnA-Ti</cell><cell>59.6 39.7 57.4 41.8 18.2 38.5</cell></row><row><cell cols="2">Table 4. DETR [7] Based Object detection on the COCO</cell></row><row><cell cols="2">Dataset [44]. Incorporating QnA-ViT-Tiny with DETR substan-</cell></row><row><cell cols="2">tially improves upon the ResNet50 backbone (by up to 3.2). QnA</cell></row><row><cell cols="2">with receptive field 7x7 improves the average precision on large</cell></row><row><cell cols="2">objects (APL), and incorporating QnA into the DETR network im-</cell></row><row><cell cols="2">proves performance on smaller objects, indicating locality.</cell></row></table><note>(for the full report, please see Appendix E.3). In our experiments, we conclude that the QnA layer is effective in the early stages and can replace global attention without affecting the model's per-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>respectively. The figures show that</figDesc><table><row><cell>L1 Loss -Test</cell><cell>pSNR -Test</cell></row><row><cell>Bilinear</cell><cell></cell></row><row><cell>Conv Trans.</cell><cell></cell></row><row><cell>QnA (Ours)</cell><cell></cell></row><row><cell>Epoch SSIM -Test</cell><cell>Epoch L2 Loss -Test</cell></row><row><cell>Epoch</cell><cell>Epoch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>baseline, where we replaced the QnA layers with SASA layers. The number of QnA/SASA blocks used for each stage are [2, 2, 2, 0] and the number of local self-attention blocks are [1, 1, 5, 2].E.2. Number of headsUsing more attention heads is beneficial for QnA. More specifically, we conducted two experiments, one where we</figDesc><table><row><cell cols="2">Stage Output</cell><cell></cell><cell></cell><cell cols="2">QnA-T</cell><cell></cell><cell></cell><cell></cell><cell cols="2">QnA-S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>QnA-B</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">QnA Blocks</cell><cell cols="2">SA Blocks</cell><cell></cell><cell cols="2">QnA Blocks</cell><cell></cell><cell cols="3">SA Blocks</cell><cell></cell><cell>QnA Blocks</cell><cell>SA Blocks</cell></row><row><cell>1</cell><cell>56x56</cell><cell cols="4">4x4 Conv, stride 4, dim 64 3x3 QnA-Block, ? ?</cell><cell></cell><cell cols="5">4x4 Conv, stride 4, dim 64 3x3 QnA-Block, ? ?</cell><cell></cell><cell></cell><cell cols="2">4x4 Conv, stride 4, dim 96 3x3 QnA-Block, ? ?</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>stride 1,</cell><cell>? ? 2</cell><cell>None</cell><cell></cell><cell>?</cell><cell>stride 1,</cell><cell>? ? 2</cell><cell></cell><cell>None</cell><cell></cell><cell></cell><cell>?</cell><cell>stride 1,</cell><cell>? ? 2</cell><cell>None</cell></row><row><cell></cell><cell></cell><cell></cell><cell>head 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>head 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>head 6</cell></row><row><cell>2</cell><cell>28x28</cell><cell cols="4">3x3 QnA, stride 2, head 16, dim 128 3x3 QnA-Block, ? ?</cell><cell></cell><cell cols="5">3x3 QnA, stride 2, head 16, dim 128 3x3 QnA-Block, ? ?</cell><cell></cell><cell></cell><cell cols="2">3x3 QnA, stride 2, head 16, dim 192 3x3 QnA-Block, ? ?</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>stride 1,</cell><cell>? ? 3</cell><cell>None</cell><cell></cell><cell>?</cell><cell>stride 1,</cell><cell>? ? 3</cell><cell></cell><cell>None</cell><cell></cell><cell></cell><cell>?</cell><cell>stride 1,</cell><cell>? ? 3</cell><cell>None</cell></row><row><cell></cell><cell></cell><cell></cell><cell>head 16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>head 16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>head 12</cell></row><row><cell>3</cell><cell>14x14</cell><cell cols="4">3x3 QnA, stride 2, head 32, dim 256 3x3 QnA-Block, ? ? ? SA-Block,</cell><cell>?</cell><cell cols="5">3x3 QnA, stride 2, head 32, dim 256 3x3 QnA-Block, ? ? ? SA-Block,</cell><cell></cell><cell>?</cell><cell cols="2">3x3 QnA, stride 2, head 32, dim 384 3x3 QnA-Block, ? ? ? SA-Block,</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>stride 1,</cell><cell>? ? 2</cell><cell cols="2">? win sz. 14 x 14, ? ? 4</cell><cell>?</cell><cell>stride 1,</cell><cell>? ? 6</cell><cell cols="3">? win sz. 14 x 14,</cell><cell>? ? 12</cell><cell>?</cell><cell>stride 1,</cell><cell>? ? 6</cell><cell>?</cell><cell>win sz. 14 x 14,</cell><cell>? ? 12</cell></row><row><cell></cell><cell></cell><cell></cell><cell>head 32</cell><cell></cell><cell>head 8</cell><cell></cell><cell></cell><cell>head 32</cell><cell></cell><cell></cell><cell>head 8</cell><cell></cell><cell></cell><cell></cell><cell>head 24</cell><cell>head 12</cell></row><row><cell>4</cell><cell>7x7</cell><cell></cell><cell cols="3">3x3 QnA, stride 2, head 64, dim 512 ? SA-Block,</cell><cell>?</cell><cell></cell><cell cols="4">3x3 QnA, stride 2, head 64, dim 512 ? SA-Block,</cell><cell cols="2">?</cell><cell></cell><cell>3x3 QnA, stride 2, head 48, dim 768 ? SA-Block,</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>None</cell><cell></cell><cell cols="2">? win sz. 7 x 7, ? ? 2</cell><cell></cell><cell>None</cell><cell></cell><cell>?</cell><cell cols="3">win sz. 7 x 7, ? ? 2</cell><cell></cell><cell>None</cell><cell>? win sz. 7 x 7,</cell><cell>? ? 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>head 16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>head 16</cell><cell></cell><cell></cell><cell></cell><cell>head 24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>QnA-ViT architecture details. QnA is used to down-sample the feature maps between two consecutive stages. In stage 3 we first employ global self-attention blocks.</figDesc><table><row><cell>2</cell><cell>@nn.compact</cell></row><row><cell>3</cell><cell>def __call__(self, X):</cell></row><row><cell>4</cell><cell># Initialize Parameters:</cell></row><row><cell>5</cell><cell>Q = # Query vectors [L, h, D//h]</cell></row><row><cell>6</cell><cell>Wk = # Linear Projection for keys [D, D]</cell></row><row><cell>7</cell><cell>Ws = # Attention weight scale [k, k, L * h]</cell></row><row><cell>8</cell><cell>B_rpe = #Relative PE [k, k, L * h]</cell></row><row><cell>13</cell><cell># Compute V</cell></row><row><cell>14</cell><cell>V = nn.Dense(D)(X).reshape([B, H, W, heads, D // heads])</cell></row><row><cell>15</cell><cell># Compute Attention:</cell></row><row><cell>16</cell><cell>exp_similarity = jnp.exp(QK_similariy) # [B, H, W, L, h]</cell></row><row><cell>17</cell><cell>exp_similarity_v = exp_similarity[..., jnp.newaxis] * V[:,:,: jnp.newaxis,:] # [B, H, W, L, D]</cell></row><row><cell>18</cell><cell>aux_kernel = (jnp.exp(B_rpe) * Ws).repeat(repeats=D // heads, axis=-1) # [k, k, d, L * h]</cell></row><row><cell>19</cell><cell>A = jax.lax.conv_general_dilated(exp_similarity_v.reshape([B, H, W, LD]),</cell></row><row><cell>20</cell><cell>aux_kernel, window_strides=[s, s], padding='SAME',</cell></row><row><cell>21</cell><cell>feature_group_count=L * D,).reshape([B, H_out, W_out, L, heads, D // heads</cell></row><row><cell></cell><cell>])</cell></row><row><cell>22</cell><cell>A = jnp.reshape(A, [B, H_out, W_out, L, heads, D // heads])</cell></row><row><cell>23</cell><cell>aux_kernel = jnp.exp(B_rpe) # [k, k, 1, L * h]</cell></row><row><cell>24</cell><cell>B = jax.lax.conv_general_dilated(exp_similarity.reshape([B, H, W, -1]),</cell></row><row><cell>25</cell><cell>aux_kernel,</cell></row><row><cell>26</cell><cell>window_strides=[s, s],</cell></row><row><cell>27</cell><cell>padding='SAME',</cell></row><row><cell>28</cell><cell>dimension_numbers=_conv_dimension_numbers(I.shape)</cell></row></table><note>1 class QnA(nn.Module):9 # Fused implementation of Q * (X * W_K).Transpose10 Wk = Wk.reshape([-1, heads, D // heads])11 QWk = jnp.einsum('lhd,Dhd-&gt;Dlh', Q, Wk)12 QK_similariy = jnp.einsum('BHWD,BDqh-&gt;BHWqh', X, QWk)</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>Research supported with Cloud TPUs from Google's TPU Research Cloud (TRC).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention (Q, K) = Softmax QK T / ? D + B , <ref type="bibr" target="#b8">(7)</ref> where B ? R k?k is a learned relative positional encoding. Note, different biases are learned for each query in the QnA layer, which adds O(L ? k 2 ) additional space.</p><p>Multi-head attention: As in the original self-attention layer <ref type="bibr" target="#b71">[70]</ref>, we use multiple heads in order to allow the QnA layer to capture different features simultaneously. In fact, as we will show in Appendix E.2, using more attention heads is beneficial to QnA. In mutli-head attention, all queries Q, keys K, and values V entities are split into h sub-tensors, which will correspond to vectors in the lower dimensional space R D/h . More specifically, let Q (i) , K (i) , V (i) be the i-th sub vector of each query, key and value, respectively. The selfattention of head-i becomes:</p><p>, where d h = D/h, and the output of the Multi-Head Attention (MHA) is:</p><p>Up-sampling using QnA Up-sampling by factor s can be defined using s 2 -learned queries, i.e.,Q ? R s 2 ?D . To be precise, the output of each window N i,j is expressed via:</p><p>Note, the window output given by Eq. <ref type="formula">(10)</ref> is now a matrix of size s 2 ? D, and the total output Z is a tensor of size H ? W ? s 2 ? D. To form the up-sampled output Z s , we need to reshape the tensor and permute its axes:</p><p>G. QnA as an upsampling layer</p><p>In the paper, we showed how QnA could be used as an upsampling layer. In particular, we trained a simple autoencoder network composed of five downsampling layers and five upsampling layers. We use the L 1 reconstruction loss as an objective function to train the auto-encoder. We considered three different encoder layers:</p><p>? ConvS2-IN: 3x3 convolution with stride 2 followed by an Instance Normalization layer <ref type="bibr" target="#b69">[68]</ref>.</p><p>? Conv-IN-Max: 3x3 convolution with stride 1 followed by an Instance Normalization layer and maxpooling with stride 2.</p><p>? LN-QnA: Layer Normalization <ref type="bibr" target="#b0">[1]</ref> followed by a 3x3 single-query QnA layer (without skip-connections).</p><p>and three different decoder layers:</p><p>? Bilinear-Conv-IN: x2 bilinear up-sampling followed by 3x3 convolution and Instance Normalization.</p><p>? ConvTransposed-IN: A 2d transposed convolution followed by Instance Normalization.</p><p>? LN-UQnA: Layer Normalization followed by a 3x3 up-sampling QnA layer (without skip-connections)</p><p>For our baseline networks, we found it best to use Conv-In-Max in the encoder path, and chose either Bilinear-Conv-In or ConvTranspose-IN for the decoder path. For</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="3285" to="3294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<imprint>
			<publisher>Peter Hawkins</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<imprint>
			<publisher>Peter Hawkins</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford; NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Dario Amodei</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu ;</forename><surname>Richard) Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">When vision transformers outperform resnets without pretraining or strong data augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno>abs/2106.01548, 2021. 1</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The efficiency misnomer. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<idno>abs/2110.12894, 2021. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11403,2021.14</idno>
		<title level="m">Scenic: A JAX library for computer vision research and beyond</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>ICLR 2021, Virtual Event. OpenReview.net, 2021. 1, 2, 3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Xcit: Cross-covariance image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>abs/2106.09681, 2021. 1</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno>abs/2104.11227, 2021. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coordination among neural modules through a shared global workspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Rajiv Didolkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikeya</forename><surname>Badola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Nan Rosemary Ke, Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Curtis Mozer, and Yoshua Bengio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Levit: A vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="12259" to="12269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6307" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Flax: A neural network library and ecosystem for JAX</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Rondepierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Zee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Flax: A neural network library and ecosystem for JAX</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Rondepierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Zee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<idno>abs/2107.14795</idno>
		<title level="m">Perceiver IO: A general architecture for structured inputs &amp; outputs. CoRR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Involution: Inverting the inherence of convolution for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12321" to="12330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Piotr Doll&apos;a r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer G. Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4052" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Torch. manual seed (3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10425" to="10433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>21:140:1-140:67</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Do vision transformers see like convolutional neural networks? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno>abs/2108.08810</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<editor>Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno>abs/2106.10270, 2021. 1</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>PMLR, 2021. 1, 2, 3, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>abs/2103.17239, 2021. 7</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="8250" to="8260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE, 2021. 1, 2, 3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12349</biblScope>
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno>abs/2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/2110.00476</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Rethinking and improving relative position encoding for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<idno>abs/2107.14222</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<idno>2019. 14</idno>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/2106.14881</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR, 2020. 14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Nvit: Vision transformer compression and parameter redistribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanrui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno>abs/2110.04869</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
		<idno>2020. 14</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3902" to="3910" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2010</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for abnormal event detection in videos based on self-attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoping</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124847" to="124860" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Aggregating nested transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12723</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10073" to="10082" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">An empirical study of spatial attention mechanisms in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="6687" to="6696" />
		</imprint>
	</monogr>
	<note>2,2,2,2</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<title level="m">Blocks Params GFLOPS Top-Acc QnA SA Changes in stages 1</title>
		<imprint>
			<biblScope unit="page">74</biblScope>
		</imprint>
	</monogr>
	<note>2 [1,1,4,0. 3,3,3,2] 16.62M 3.200 81.70 [2,2,4,0. 2,2,3,2] 16.51M 2.909 81</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">The number of QnA and local self-attention (SA) blocks in each stage are indicated in the first row. In the first two sub-tables, a window size of 14 ? 14 except in the last stage, where a 7 ? 7 window size was set</title>
	</analytic>
	<monogr>
		<title level="m">Table 8. How much QnA do you really need? -full report</title>
		<imprint/>
	</monogr>
	<note>In the last sub-table, we reduce the window size to</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
