<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MANUSCRIPT UNDER REVIEW 1 Rethinking Graph Auto-Encoder Models for Attributed Graph Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nairouz</forename><surname>Mrabah</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Bouguessa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Fawzi</forename><surname>Touati</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riadh</forename><surname>Ksantini</surname></persName>
						</author>
						<title level="a" type="main">MANUSCRIPT UNDER REVIEW 1 Rethinking Graph Auto-Encoder Models for Attributed Graph Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Unsupervised Learning</term>
					<term>Graph Clustering</term>
					<term>Graph Auto-Encoders !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most recent graph clustering methods have resorted to Graph Auto-Encoders (GAEs) to perform joint clustering and embedding learning. However, two critical issues have been overlooked. First, the accumulative error, inflicted by learning with noisy clustering assignments, degrades the effectiveness and robustness of the clustering model. This problem is called Feature Randomness. Second, reconstructing the adjacency matrix sets the model to learn irrelevant similarities for the clustering task. This problem is called Feature Drift. Furthermore, the theoretical relation between the aforementioned problems has not yet been investigated. We study these issues from two aspects: (1) there is a trade-off between Feature Randomness and Feature Drift when clustering and reconstruction are performed at the same level, and (2) the problem of Feature Drift is more pronounced for GAE models, compared with vanilla auto-encoder models, due to the graph convolutional operation and the graph decoding design. Motivated by these findings, we reformulate the GAE-based clustering methodology. Our solution is two-fold. First, we propose a sampling operator ? that triggers a protection mechanism against the noisy clustering assignments. Second, we propose an operator ? that triggers a correction mechanism against Feature Drift by gradually transforming the reconstructed graph into a clustering-oriented one. As principal advantages, our solution grants a considerable improvement in clustering effectiveness and can be easily tailored to existing GAE models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Most recent attributed graph clustering methods leverage graph embedding <ref type="bibr" target="#b0">[1]</ref>. This strategy consists of projecting the graph structure and the node content in a low-dimensional compact space to harness the complementary modalities of attributed graphs. Graph embedding usually achieves exploitable representations for the clustering task. A significant part of the graph embedding literature revolves around edge modeling <ref type="bibr" target="#b1">[2]</ref>, matrix factorization <ref type="bibr" target="#b2">[3]</ref>, and random walks <ref type="bibr" target="#b3">[4]</ref>. Yet, these methods fall short of the expressive power of deep learning.</p><p>The last years witnessed the emergence of a promising graph embedding strategy, referred to as Graph Neural Networks (GNNs) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. GNNs extend the deep learning framework to graphstructured data. Among the prominent categories of GNNs, we find Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b6">[7]</ref>, which generalize the convolution operation to graph data. Specifically, the intuition of the graph convolutional operation is to exploit the graph structure in smoothing the content features of each node over its neighborhood. Motivated by GCNs, Graph Auto-Encoders (GAEs) <ref type="bibr" target="#b7">[8]</ref> and Variational Graph Auto-Encoders (VGAEs) <ref type="bibr" target="#b7">[8]</ref> have shown notable achievements in several attributed graph clustering applications <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Typical GAE-based clustering methods project the input data in a low dimensional space using graph convolutional layers and then reconstruct the adjacency matrix. Minimizing the reconstruction objective for the clustering task rules out the situation where the encoder is only trained based on noisy clustering assignments. The accumulated error makes the trained model capture non-representative features <ref type="bibr" target="#b11">[12]</ref>, which in turn corrupt the latent structure of the data. In our analysis, we adopt the terminology of Feature Randomness (FR) from our previous work <ref type="bibr" target="#b12">[13]</ref> for investigating this problem in the context of GAEs.</p><p>As mentioned before, adding the decoder component is key to optimizing the reconstruction objective, which is a handy way to lower FR's effect. However, the nature of the reconstructed graph is generally problematic to the clustering task. First, realworld graphs carry noisy and clustering-irrelevant links that can mislead the model into grouping together nodes from different clusters. This aspect can cause an under-segmentation problem. Second, it is also common for real-world graphs to come in a highly sparse structure. As a result, poor connectivity within the same cluster gives rise to an over-segmentation problem. Besides, the controversial relationship between clustering and reconstruction makes it hard to identify a static balance between them, during the training process. This problem, which is referred to as Feature Drift (FD) in our previous work <ref type="bibr" target="#b12">[13]</ref>, remains unexplored for GNNs.</p><p>To address the aforementioned issues, we reformulate the GAEbased clustering methodology from an FR and FD perspective. We start by organizing the existing approaches into two groups, and we provide abstract formulations for each one. Next, we leverage the abstract description to examine the limitations of existing methods. Then, we provide formal characterizations to problems associated with the analyzed formulations on the authority of recent insights. After that, we propose a new conceptual design, which can mitigate the impact of FR and FD.</p><p>To put our conceptual design into action, we propose two operators, which can be easily integrated into GAE-based clustering methods. Possible options for addressing FR are: (1) operationalizing a correction mechanism that can reverse the randomness effect, <ref type="bibr" target="#b1">(2)</ref> supplying the model with a protection mechanism that can exclude the sources of randomness as much as possible. Recently, the authors of <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> have observed that pretraining a network arXiv:2107.08562v3 <ref type="bibr">[cs.</ref>LG] 13 Dec 2021 with random labels then fine-tuning with clean ones leads to considerably lower test accuracy, compared with a network trained with clean labels from scratch. From this standpoint, we advocate accounting for FR using a protection strategy. Specifically, we design a sampling operator, prioritizing correctness by considering the difference between the first high-confidence and second highconfidence clustering assignment scores.</p><p>Additionally, we conceive a second operator that can control the effect of FD. Our design capitalizes on converting a generalpurpose objective function into a task-specific one. Unlike previous GAE-based approaches, which optimize static objective functions during the whole clustering process, we gradually eliminate the graph reconstruction cost in favor of a clusteringoriented graph construction objective. Furthermore, our second operator contributes to preventing the over-segmentation and undersegmentation problems. More specifically, we gradually update the self-supervision graph by adding clustering-friendly edges and dropping clustering-irrelevant links.</p><p>The algorithmic intuitions behind our conceptual design and operators are supported by theoretical and empirical results. Theoretically, we demonstrate the existence of a trade-off between FR and FD for GAE-based clustering. Under mild assumptions, we prove that the graph convolutional operation and performing clustering and reconstruction at the same level aggravate the FD problem. Experimentally, we show that our operators can significantly improve the clustering effectiveness of existing GAE models without causing run-time overheads. Moreover, we show that our operators can mitigate the impact of FR and FD, and we provide empirical evidence that the improvement is imputed to the capacity of our operators in handling the trade-off between FR and FD. The significance of this work can be summarized as follows:</p><p>? Analysis: We organize GAE-based clustering approaches into two groups, and we provide abstract formulations for each one. Accordingly, we analyze and formalize the problems associated with the examined formulations. Then, we present a new conceptual design that can favorably control the trade-off between FR and FD. From a theoretical standpoint, we prove the existence of this trade-off, and we study two important aspects that differentiate GAE models from vanilla autoencoder methods. Specifically, we investigate the impact of performing clustering and reconstruction at different layers on FR and FD. Moreover, we inspect the influence of the graph convolutional operation on FD. ? Methods: First, we propose a sampling operator ? that triggers a protection mechanism against FR. Second, we propose an operator ? that triggers a correction mechanism against FD by gradually transforming the reconstructed graph into a clustering-oriented one. ? Experiments: We conduct extensive experiments to investigate the behavior and profit from using our operators. Our empirical results provide strong evidence that the proposed operators improve the clustering performance of GAE models in effectiveness, by mitigating the effect of FR and FD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A NEW VISION FOR GAE-BASED CLUSTERING</head><p>This section advocates a new vision for building GAE-based clustering models beyond the classical perception of designing better clustering objectives. We begin by describing existing GAE methods, which we organize into two groups. While the first group contains models that separate clustering from embedding learning, the second group only considers the methods that perform joint clustering and embedding learning. For each group, we devise abstract formulations, and we study their associated limitations. Finally, we propose a new conceptual design to mitigate the examined problems. We consider our work to be the first initiative to analyze GAE-based clustering models from FR and FD perspectives. We consider a non-directed attributed graph G = (V, E, X), where V = {v 1 , v 2 , ..., v N } is a set of nodes with |V| = N , and N is the number of nodes. E = {e ij } represents the set of edges. The topological structure of the graph G is denoted by the adjacency matrix A = (a ij ) ? R N ?N , where a ij = 1 if (v i , v j ) ? E and a ij = 0 otherwise. X = {x 1 , ..., x N } represents the matrix of features, where x i ? R J is the feature vector associated with the node v i , and J is the dimensionality of the input space. We consider that the graph G can be clustered into K clusters C clus k K k=1 . Our study investigates the auto-encoding architecture for attributed graph clustering. Consequently, two functions should be specified. The first one is a non-linear encoder, which takes as inputs X and A, and outputs low-dimensional latent representations denoted by the matrix Z ? R N ?d . d denotes the dimension of the latent space. ? stands for the set of learnable weights. The second function is a decoder, which outputs a matrix? = sigmoid(ZZ T ); A measures the pairwise similarities between the latent codes.</p><p>The idea of self-supervision involves solving a pretext task that requires a high-level understanding of the data. Specifically, the reconstruction loss is among the standard self-supervision methods for pretraining GAE models. It is generally expressed as a binary cross-entropy L bce (?(Z(?)), A self ), where A self is a self-supervision graph, and it is set equal to A, and the order in?(Z(?)) describes dependencies between?, Z, and ?. Let A C be a clustering algorithm and P ? R N ?K be the clustering assignment matrix obtained by applying A C to the embedded representations. L clus (P (Z(?))) is the clustering loss associated with algorithm A C . Without a pretraining stage, the clustering algorithm would be applied to random latent representations.</p><p>As mentioned at the beginning of this section, we organize existing approaches into two groups. For the first group, clustering is performed separately from embedding learning. Thus, we express the formulation associated with models from the first group as:</p><formula xml:id="formula_0">P * = arg min P L clus (P (Z(?))),<label>(1)</label></formula><p>where ? is initialized by the pretraining task, and P * is a solution to Equation <ref type="bibr" target="#b0">(1)</ref>. Examples from the first group include MGAE (Marginalized Graph Auto-Encoder) <ref type="bibr" target="#b15">[16]</ref>, which improves the clustering performance by increasing robustness to small input disturbances. From the same group, ARGAE (Adversarially Regularized Graph Auto-Encoder) <ref type="bibr" target="#b8">[9]</ref> leverages an adversarial regularization technique that enforces the embeddings to match a prior distribution using a discriminator network. Nevertheless, methods from the first group, such as MGAE and ARGAE, lack the capacity to learn clustering-oriented features. In another perspective, Ansuini et al. <ref type="bibr" target="#b16">[17]</ref> have shown that the embedded representations of a deep network lie on highly curved manifolds. This aspect implies that the Euclidean geometry is not suitable to assess the embedded similarities after the pretraining phase. To alleviate this problem, the second group of GAE-based clustering methods achieves joint clustering and embedded learning. In this regard, we reformulate Equation (1) in a way that enforces the embedded representations to follow the clustering assumptions based on euclidean geometry. To ensure this quality, the formulation of the second group is articulated as: ? * , P * = arg min ?,P L clus (P (Z(?))),</p><p>where ? * and P * are solutions to Equation <ref type="bibr" target="#b1">(2)</ref>. Typical clustering losses aim at decreasing the intra-cluster variances and increasing the inter-cluster variances. By optimizing ?, the embedded points move in a way that establishes a clustering-oriented distribution. Therefore, the choice of the clustering cost becomes less important. However, the formulation of Equation <ref type="formula" target="#formula_1">(2)</ref> is still problematic because the embedded points can move in a way that violates their semantic categories, while still decreasing the embedded clustering penalty. Let Q be the matrix of true hard-clustering assignments. A supervised deep clustering problem can be described by Equation <ref type="bibr" target="#b2">(3)</ref>. Compared with Equation <ref type="formula" target="#formula_1">(2)</ref>, the supervised objective pushes the latent codes to be clustering-friendly according to the true clustering assignment matrix Q. Let A H be the Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref>, which finds the best linear mapping from a true clustering assignment matrix Q to a predicted clustering assignment matrix P . The algorithm A H outputs a matrix Q = A H (Q, P ). By analogy with pseudo-supervision, y(P ) = arg max j?{0,...,K} (P :,j ) can be considered as pseudo-labels for solving Equation <ref type="formula" target="#formula_1">(2)</ref>, and y(Q) = arg max j?{0,...,K} (Q :,j ) can be considered as groundtruth labels for solving Equation <ref type="formula" target="#formula_2">(3)</ref>. The ultimate goal of deep clustering is to formulate an optimization problem, where a solution for the clustering assignment matrix is P * , such that y(P * ) = y(Q ).</p><formula xml:id="formula_2">? * = arg min ? L clus (Q(Z(?))).<label>(3)</label></formula><p>Under the extreme condition of entirely random labels, Zhang et al. <ref type="bibr" target="#b18">[19]</ref> have shown that an over-parameterized neural network can perfectly fit the training set. This finding inspired the scientific community to investigate the difference between "training with true labels" and "training with random labels". Keskar et al. <ref type="bibr" target="#b19">[20]</ref> have proposed a metric for measuring the sharpness of a minimizer to assess generalization. In <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, the authors have investigated the intrinsic dimensionality of the embedded representations to understand the impact of random labels. In our previous work <ref type="bibr" target="#b12">[13]</ref>, we have proposed to measure the effect of randomness by computing the cosine similarity between the gradient of the supervised loss and the gradient of the pseudo-supervised loss. However, the impact of random labels on Graph Neural Networks for graph datasets remains unexplored. As previously explained, embedded graph clustering can be considered a pseudo-supervised task. Thus, we can exploit our previously proposed metric <ref type="bibr" target="#b12">[13]</ref> to assess the impact of FR as described by: <ref type="bibr" target="#b0">1]</ref>. Higher values are associated with less FR. Possible strategies for countering random projections are: (1) performing pseudo-supervision based on self-paced training, and (2) pretraining by self-supervision (pretext task), and finetuning by combining pseudo-supervision (main task) and self-supervision (pretext task). An example of the first strategy is AGE <ref type="bibr" target="#b21">[22]</ref>, which constructs a pseudo-supervised graph by linking high similarity pairs and disconnecting the low similarity ones. However, two limitations are associated with the first strategy. First, it does not involve pretraining using a pretext task, and the pseudo-labels are initially constructed from the input data. Hence, the first strategy is limited to datasets, where the node features have low-semantic similarities that can be extracted without neural networks. Second, the first strategy does not combine pseudo-supervision and selfsupervision during the clustering phase. You et al. <ref type="bibr" target="#b22">[23]</ref> have shown that combining the main task with a self-supervised pretext brings more generalizability and robustness to GCNs. For the second strategy, adjacency reconstruction constitutes the standard selfsupervised technique for GAE models. In this work, we focus on the relation between pseudo-supervision (i.e., main task: clustering) and self-supervision (i.e., pretext task: reconstruction), which is governed by FR and FD. Accordingly, we reformulate Equation <ref type="formula" target="#formula_2">(3)</ref> to take into consideration the reconstruction loss: ? * , P * = arg min ?,P L clus (P (Z(?))) + ?L bce (?(Z(?)), A), <ref type="bibr" target="#b4">(5)</ref> where ? * and P * are solutions to Equation <ref type="bibr" target="#b4">(5)</ref>. ? is a balancing hyper-parameter that controls the trade-off between clustering and reconstruction. Examples from this category include DAEGC (Deep Attentional Embedded Graph Clustering) <ref type="bibr" target="#b10">[11]</ref>, which employs an attention mechanism to adjust the influence of the neighboring nodes. Another example is GMM-VGAE (Variational Graph Auto-Encoder with Gaussian Mixture Models) <ref type="bibr" target="#b9">[10]</ref>, which harnesses Gaussian Mixture Models to capture variances between the different clusters. However, the strong competition between embedded clustering and reconstruction causes FD. On the one hand, clustering aims at decreasing intra-cluster variances and increasing intercluster variances. On the other hand, the reconstruction objective pushes the latent representations to maintain all variances (i.e., intra-cluster and inter-cluster variances). The features learned by embedded clustering can be destroyed by the reconstruction cost, which captures clustering-irrelevant similarities.</p><formula xml:id="formula_3">? F R = cos ?L clus (P (Z(?))) ?? , ?L clus (Q (Z(?))) ?? . (4) ? F R lies within the range [?1,</formula><p>FD is an artificially-created problem to counter random projections. Thus, it can be completely solved by excluding the selfsupervised loss from the optimized objective. However, abrupt elimination of the reconstruction loss aggravates FR. Among two existing methods for alleviating FD is ADEC (Adversarial Deep Embedded Clustering) <ref type="bibr" target="#b12">[13]</ref>. Instead of minimizing vanilla reconstruction, ADEC optimizes an adversarially constrained reconstruction. More specifically, the vanilla reconstruction is circumvented by blocking the direct connection between the encoder and the decoder, using an additional discriminator network. Nevertheless, ADEC inherits the stability limitations of adversarial training such as mode collapse <ref type="bibr" target="#b23">[24]</ref>, failure to converge <ref type="bibr" target="#b24">[25]</ref>, and memorization. In another work, DynAE (Dynamic Auto-Encoder) <ref type="bibr" target="#b25">[26]</ref> leverages the decoder to gradually construct images of the latent centers, instead of reconstructing the input images. However, DynAE was designed to generate euclidean representations (i.e., images corresponding to the embedded centroids) and can not generate graph-structured data. Added to that, DynAE is considered an improved version of DeepCluster <ref type="bibr" target="#b26">[27]</ref>. Both models (i.e., DynAE and DeepCluster) perform hard clustering using K-means and do not consider covariances of the embedded clusters. Enforcing pseudo-labels obtained by a hard clustering algorithm may destroy relevant similarities and hence give rise to FR. Last but not least, none of these methods can take both topological structure and content information as input signals.</p><p>To overcome the limitations of previous methods, we propose a new conceptual design. Our solution fixes the deficiency of existing GAE models from the perspective of FR and FD. In <ref type="figure" target="#fig_0">Figure  1</ref>, we illustrate the generic framework of this conceptual design. More precisely, our formulation depends on two operators and does not require adversarial training. First, we develop a sampling operator ? to gradually spot the nodes with reliable clustering assignments, denoted by the set ?. We exploit the reliable nodes for optimizing the embedded clustering objective. Second, we propose a graph-specific operator ? to gradually transform the general-purpose self-supervisory signal A into a clustering-oriented self-supervisory signal A self clus . The formulation of our conceptual design is expressed by: ? * , P * = arg min ?, P L clus (P (?(Z(?)))) + ?L bce (?(Z(?)), ?(A, P (?(Z(?))), ?)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(6)</head><p>To estimate the impact of FD, <ref type="bibr" target="#b12">[13]</ref> measures the cosine similarity between the gradient of the self-supervised loss and the gradient of the clustering loss. This metric was only tested when clustering and reconstruction losses are computed based on Euclidean distances. However, in the graph case, we found that this metric does not reveal any interpretive pattern, probably because the structure of the output signal is non-euclidean. Thus, we propose a new metric that measures the cosine similarity between two gradients of the same loss but with different configurations, namely, the gradient of the self-supervised loss L bce (?(Z(?)), ?(A, P (?(Z(?))), ?)), and the gradient of its supervised version L bce (?(Z(?)), ?(A, Q (Z(?)), V)). Our new metric is described by:</p><formula xml:id="formula_4">? F D = cos ?L bce (?(Z(?)), ?(A, P (?(Z(?))), ?)) ?? , ?L bce (?(Z(?)), ?(A, Q (Z(?)), V)) ?? .<label>(7)</label></formula><p>? F D lies in the range [?1 <ref type="bibr">, 1]</ref>. Higher values are associated with less FD. ?(A, P (?(Z(?))), ?) makes a single-step modification to the input graph A. It only affects the sub-graph defined by the reliable nodes ?. As opposed to that, ?(A, Q (Z(?)), V) outputs the clustering-oriented graph that we want to obtain at the end of the training process. It is generated by transforming the whole graph using the supervisory signal. A small discrepancy between both graphs, in terms of gradient direction implies that the two signals have the same impact at the level of gradient computation. Thus, ? F D assesses to what extent the generated self-supervision graph is clustering-oriented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORETICAL ANALYSIS</head><p>Our conceptual design aims at reducing the impact of FD without causing excessive FR. An intuitive explanation of the trade-off between FR and FD is provided in the previous section. In this section, we discuss the problems of FR and FD for GAE models from a theoretical standpoint. Our formal analysis includes three points: (1) proving the existence of a trade-off between FR and FD for GAE models, (2) understanding the impact of performing clustering and reconstruction at different layers on FR and FD, and (3) understanding the impact of the graph convolutional operation, which is performed by all encoding layers, on FD. All mathematical proofs and derivations are provided in the Appendices.</p><p>We start our theoretical analysis by showing that it is possible to write the loss function of a typical GAE model in a way that explicitly demonstrates the trade-off between FR and FD. More specifically, we found that solving a typical GAE-based clustering problem is equivalent to smoothing the embedded representations over a linear combination between the input graph and a clustering graph, using a graph Laplacian regularization loss <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. The two aforementioned graphs are different in nature and suffer from different problems. While the clustering graph has several random edges, the input graph is sparse and comes with clusteringirrelevant links.</p><p>Virtually, the problem of clustering Euclidean data using an auto-encoder model appears to be similar to clustering graphs using a GAE model. However, there are two important differences between the two approaches. First, vanilla auto-encoders are not designed to deal with graph-structured data, whereas GAE models can capitalize the structural information thanks to the graph convolutional operation. Therefore, it is important to investigate the impact of this operation on FR and FD. Second, vanilla autoencoder models have symmetric decoders. As opposed to that, the decoder of a GAE model is nothing more than the sigmoid of an inner product. We analyze the impact of these differences (i.e., graph decoding design and graph convolutional operation) on FR and FD from a theoretical standpoint. Under mild assumptions, our formal analysis demonstrates that the problem of FD is more pronounced for GAE models compared with the FD problem for vanilla auto-encoder models due to the graph convolutional operation and the graph decoding design. Furthermore, we provide sufficient conditions for comparing a typical GAE model against a GAE with a multi-layer decoder, in terms of FR and FD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Trade-off between FR and FD</head><p>Given a GAE-based clustering model and an attributed graph G, we consider that the nodes of G are associated with K ground-truth labels defining K real clusters {C sup k } K k=1 , and that the embedded representations Z can be clustered into K clusters C clus k K k=1 according to an algorithm A C . Let L C be a generic loss, which takes as input an adjacency matrix A = (a ij ) ? R N ?N and a feature matrix Z ? R N ?d , and can be written in the form:</p><formula xml:id="formula_5">L C (Z , A ) = 1 2 1 i,j N a ij z i ? z j 2 2 .</formula><p>We define three graphs based on their adjacency matrices: a selfsupervision graph A self = (a self ij ) ? R N ?N , a clustering graph A clus = (a clus ij ) ? R N ?N , and a supervision graph A sup = (a sup ij ) ? R N ?N . For the reconstruction loss, the self-supervision signal A self is equal to the input graph A. The clustering and supervision graphs are expressed as follows:</p><formula xml:id="formula_6">a clus ij = 1 |C clus k | if ? k such that i, j ? C clus k 0 otherwise, a sup ij = 1 |C sup k | if ? k such that i, j ? C sup k 0 otherwise.</formula><p>Proposition 1. The reconstruction loss for a GAE model can be expressed as:</p><formula xml:id="formula_7">L bce (?(Z(?)), A self ) = L C (Z(?), A self ) + L R (Z(?), A self ), L R (Z(?), A self ) = i,j log(1 + exp(z T i z j )) ? 1 2 a self ij ( z i 2 2 + z j 2 2 ) .</formula><p>In Proposition 1, we write the reconstruction loss of a GAE model in the form of a linear combination between a graph Laplacian regularization term L C (Z(?), A self ) and another loss L R (Z(?), A self ). A trivial solution to minimize L C (Z(?), A self ) consists of mapping the features of all nodes to the same latent code. State-of-the-art self-supervised methods rely on negative pairs <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> or a cross-model supplementary loss function <ref type="bibr" target="#b32">[33]</ref> to avoid degenerate solutions. In our case, the trivial solutions are ruled out by the function L R (Z(?), A self ). More precisely, minimizing log(1 + exp(z T i z j )) implies an increase in the angle between the two vectors z i and z j and/or a decrease in their norms if their angle is lower than 90 ? or an increase in their norms if their angle is greater than 90 ? . However, minimizing the second part of L R (i.e., ? 1 2 a self ij z i 2 2 + z j 2 2 ) increases the norm of z i and z j when there is a link between the two nodes i and j. Hence, we can conclude that L R increases the angle between each couple of vectors z i and z j if there is a link between them. Otherwise, decreasing the norm of both vectors might be sufficient.</p><p>Proposition 2. The k-means clustering loss applied to the embedded representations can be expressed as:</p><formula xml:id="formula_8">L clus (Z(?)) = L C (Z(?), A clus ).</formula><p>In Proposition 2, we write the embedded k-means loss in the form of a graph Laplacian regularization loss L C (Z(?), A clus ). As we can see, the graph required for embedded k-means is different from the graph required for the reconstruction loss. Furthermore, training the encoder to minimize embedded k-means without a reconstruction loss can easily lead to degenerate solutions. Theorem 1. The linear combination between reconstruction and embedded k-means for a GAE model can be expressed as:</p><formula xml:id="formula_9">L clus (Z(?)) + ? L bce (?(Z(?)), A self ) = L C (Z(?), A clus + ?A self ) + ? L R (Z(?), A self ).</formula><p>In Theorem 1, we have a typical GAE-based clustering model that optimizes a linear combination between embedded k-means and reconstruction. Based on Proposition 1 and Proposition 2, we can write the loss function of this GAE model in the form of a linear combination between a graphweighted loss L C (Z(?), A clus + ?A self ) and a regularization term L R (Z(?), A self ). The regularization term L R enables the training process to avoid degenerate solutions. The graph associated with L C is a combination between the clustering graph and the self-supervision graph. Based on this result, we can clearly spot the trade-off between FR and FD, which is caused by combining two graphs of different nature. On the one hand, decreasing the balancing hyper-parameter ? reinforces the impact of the clustering graph on the optimization process, which in turn gives rise to FR. On the other hand, increasing ? leads to higher levels of FD due to the high-sparsity and clustering-irrelevant links within the self-supervision graph. It is important to highlight that our previous work <ref type="bibr" target="#b12">[13]</ref> has shown the trade-off between FR and FD only for the specific case, where the encoder and decoder are linear functions and the weight matrices are constrained to the Stiefel manifold. As opposed to that, Theorem 1 holds for all GAE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Impact of performing clustering and reconstruction at different layers on FR and FD</head><p>To understand the impact of a GAE model on FR and FD compared with a vanilla auto-encoder model, we analyze ? F R and ? F D in a variety of contexts. To this end, we start by computing the gradient of the clustering and reconstruction losses w.r.t. the embedded representations.</p><p>Proposition 3. The gradient of the reconstruction loss L bce (?(Z(?)), A self ) w.r.t. the embedded representation z i can be expressed as:</p><formula xml:id="formula_10">?L bce (?(Z(?)), A self ) ?z i = 1 j N (? ij ? a self ij )z j .</formula><p>Proposition 4. The gradient of the clustering loss L clus (Z(?)) w.r.t. the embedded representation z i can be expressed as:</p><formula xml:id="formula_11">?L clus (Z(?)) ?z i = 1 j N a clus ij (z i ? z j ).</formula><p>In Proposition 3, we compute the gradient of the reconstruction loss, and in Proposition 4, we compute the gradient of the embedded k-means loss. To facilitate the theoretical analysis of FR and FD, we perform three simplifications. Since the tradeoff between FR and FD is only related to the graph-weighted functions L C , we exclude the regularization term L R from the gradient computation. Restraining our analysis to the L C functions simplifies the analytical computation for evaluating FR and FD. In another simplification, we use the inner product for measuring the similarity between the gradient vectors instead of using the cosine function. Using the inner product overcomes the need to deal with the gradient norms. The final simplification consists of using normalized graphs. We denote the normalized self-supervised</p><formula xml:id="formula_12">adjacency matrix by? self = D ? 1 2 A self D ? 1 2 = (? self ij ) ij , where D = diag(d 1 , ..., d n ) is the degree matrix of A self such that d i = n j=1 A self ij .</formula><p>Furthermore, A clus and A sup are normalized matrices by definition. Based on the aforementioned simplifications, we can obtain elementary metrics for assessing FR and FD as explained by Definition 1 and Definition 2 respectively. Definition 1. For a GAE model Q, we define a metric ? F R (Q, z i ) to evaluate the impact of FR at the level of an embedded point z i as follows: Definition 2. For a GAE model Q, we define a metric ? F D (Q, z i ) to evaluate the impact of FD at the level of an embedded point z i as follows:</p><formula xml:id="formula_13">? F R (Q, z i ) = ?L C (Z(?), A clus ) ?z i , ?L C (Z(?), A sup ) ?z i .</formula><formula xml:id="formula_14">? F R (Q, z i ) = ?L C (Z(?),? self ) ?z i , ?L C (Z(?), A sup ) ?z i .</formula><p>Modern neural networks are Lipschitz functions. The Lipschitz constant of a function informs how much the output can change in proportion to an input change. Constraining the Lipschitz constant of a neural network is connected to several interesting aspects. For instance, reducing this constant enhances adversarial robustness <ref type="bibr" target="#b33">[34]</ref>. For classification, reducing the Lipschitz constant induces better generalization bounds as shown by several previous works <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. In this section, we show the impact of constraining the Lipchitz constant on FR and FD for two specific situations. In the subsequent analysis, we make use of the following definition:</p><formula xml:id="formula_15">Definition 3. Given two metric spaces (X , d X ) and (Y, d Y ),</formula><p>where d X is a metric on set X and d Y is a metric on set Y, a function f : X ? Y is called Lipschitz continuous if:</p><formula xml:id="formula_16">? ? 1 0, ? x 1 , x 2 ? X f (x 2 ) ? f (x 1 ) d Y ? 1 x 2 ? x 1 d X ,</formula><p>and the Lipschitz constant ? * 1 of f is defined as:</p><formula xml:id="formula_17">? * 1 = sup x1 =x2 f (x 2 ) ? f (x 1 ) d Y x 2 ? x 1 d X .</formula><p>If f is a Lipschitz function and there exists ? 2 0 such that for all</p><formula xml:id="formula_18">x 1 , x 2 ? X f (x 2 ) ? f (x 1 ) d Y 1 ?2 x 2 ? x 1 d X , then f is bi- Lipschitz. We denote the Lipschitz constant of f ?1 : f (X ) ? X as ? * 2 .</formula><p>Unlike typical auto-encoder models for euclidean data clustering, GAE models perform clustering and reconstruction at the same level (i.e., same layer). We study the impact of performing clustering and reconstruction at different layers on FR and FD. To this end, we consider two possible scenarios. Let N N (d, d , L) be a family of fully-connected layers denoted by f :</p><formula xml:id="formula_19">f : R d ? R d z ? ReLU (W l ...ReLU (W 1 z + b 1 )... + b l ),</formula><p>such that l = 1, ..., L indexes the different layers of the network <ref type="bibr" target="#b0">(1)</ref> , and d = d (l) . The first scenario consists of adding fully-connected encoding layers on top of the last graph convolutional layer, and performing clustering at the level of the last encoding layer. This scenario is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. The second scenario consists of adding fully-connected decoding layers on top of the last graph convolutional layer, and performing reconstruction at the level of the last decoding layer. This scenario is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Accordingly, we compare the behaviour of a typical GAE-based clustering model with the two versions described by <ref type="figure" target="#fig_1">Figure 2</ref> and <ref type="figure" target="#fig_2">Figure 3</ref>, in terms of FR and FD, at the level of the embedded representations.</p><formula xml:id="formula_20">f , W l ? R d (l) ?d (l?1) , b l ? R d (l) , d = d</formula><p>Theorem 2. Given two GAE models Q 1 and Q 2 , which have the same graph convolutional layers. Q 1 optimizes the objective function in Equation <ref type="formula" target="#formula_21">(8)</ref> and Q 2 minimizes the loss function in Equation <ref type="formula">(9)</ref></p><formula xml:id="formula_21">, where f ? N N (d, d , L) and d d. Let ? * 1 be the Lipschitz constant of f ,Z i = (z jj ? z ij ) j,j ? R N ?d , ? i = ( z j ? z i 2 ) j ? R N , and a i is the i th row of A. L Q1 = L clus (Z(?)) + ?L bce (?(Z(?)), A self ),<label>(8)</label></formula><formula xml:id="formula_22">L Q2 = L clus (f (Z(?))) + ?L bce (?(Z(?)), A self ). (9) ? ? F D (Q 2 , z i ) = ? F D (Q 1 , z i ). ? If ? * 1 (Z T i a sup i ) T (Z T i a clus i ) (? T i a sup i )(? T i a clus i ) , then ? F R (Q 2 , z i ) ? F R (Q 1 , z i ).</formula><p>In Theorem 2, we study the first scenario where a bunch of encoding layers is added on top of the last graph convolutional layer, and the clustering loss is applied at the level of the last encoding layer. We know that reducing the Lipchitz constant is linked to a better generalization capacity <ref type="bibr" target="#b36">[37]</ref>. Based on Theorem 2, we found that a constrained Lipchitz constant of the network f leads to more FR compared with the initial GAE-based clustering model. Furthermore, we found that FD is not affected by the added encoding layers. Hence, we conclude that adding encoding layers independently from the decoding operation increases FR without affecting FD. An intuitive interpretation of this result comes from the fact that the gradient of the reconstruction loss does not back-propagate through the added encoding layers. Therefore, the clustering loss becomes more prone to random projections. Theorem 3. Given two GAE models Q 1 and Q 2 , which have the same graph convolutional layers. Q 1 optimizes the objective function in Equation <ref type="formula" target="#formula_0">(10)</ref> and Q 2 minimizes the loss function in Equation <ref type="formula" target="#formula_0">(11)</ref></p><formula xml:id="formula_23">, where f ? N N (d, d , L) an injective function and d d. Let ? * 2 be the Lipschitz constant of f ?1 : f (R d ) ? R d ,Z i = ((f (z j )) j ? (f (z i )) j ) j,j ? R N ?d , ? i = ( f (z j ) ? f (z i ) 2 ) j ? R N , and a i is the i th row of A. L Q1 = L clus (Z(?)) + ?L bce (?(Z(?)), A self ),<label>(10)</label></formula><formula xml:id="formula_24">L Q2 = L clus (Z(?)) + ?L bce (?(f (Z(?))), A self ).<label>(11)</label></formula><formula xml:id="formula_25">? ? F R (Q 2 , z i ) = ? F R (Q 1 , z i ). ? If ? * 2 (Z T i a sup i ) T (Z T i? self i ) (? T i a sup i )(? T i? self i ) , then ? F D (Q 2 , z i ) ? F D (Q 1 , z i ).</formula><p>In Theorem 3, we study the second scenario where a bunch of decoding layers is added on top of the last graph convolutional layer, and the reconstruction loss is applied at the level of the last decoding layer. This case is similar to the typical auto-encoder, where the decoder has several layers. Based on Theorem 3, we found that a constrained Lipchitz constant of f ?1 leads to less FD compared with the initial GAE-based clustering model. Intuitively, it is expected that the decoding layers attenuate the effect of FD when the gradient of the reconstruction loss has to back-propagate through several layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Impact of the graph convolutional operation on FD</head><p>The graph convolutional operation constitutes a principal difference between a typical auto-encoder model and a GAE model. For this reason, we study the impact of this operation on the clustering task from the perspective of FD. Features propagation for a single GCN layer is expressed by the rule</p><formula xml:id="formula_26">X (k+1) = ?(? self X (k) W k ), where X (k)</formula><p>represents the node features of the k th layer, W k is the matrix of trainable weights associated with this layer, and ? is an activation function. The multiplication of the graph filter? self with the graph signal X (k) defines the graph convolutional operation. Let h be an aggregation function such that h sup (x i ) = j? sup ij x j is the center of the true cluster associated with x i (computed based on ground-truth assignments), and h self (x i ) = j? self ij x j is the center of the immediate neighbors of x i according to A self . In Equation <ref type="formula" target="#formula_0">(12)</ref>, we define a function P to locally assess the impact of the graph filtering operation on the clustering task.</p><formula xml:id="formula_27">P(x i ) = x i ? h sup (x i ) 2 ? h self (x i ) ? h sup (x i ) 2 . (12)</formula><p>If P(x i ) 0, we say that the graph filtering operation has a positive impact on clustering the node v i . To understand the impact of the filtering operation on FD, we consider two possible scenarios.</p><p>Assumption 1. The self-supervision adjacency matrix? self represents the immediate neighbors with a small error, that is,</p><formula xml:id="formula_28">?i, j ? [|1, N |] , such that? self ij = 0, x i = x j + ij ,</formula><p>where ij ? R J is a small error (i.e., ij almost equal to zero). Assumption 2. The immediate neighbors of a node v i are assumed to activate the same neurons for a well-trained ReLU-Affine layer with a training weight W , that is,</p><formula xml:id="formula_29">?i, j if? self ij = 0 then Sign(W T x i ) = Sign(W T x j ).</formula><p>Theorem 4. Given two models Q 1 and Q 2 , which optimize the same objective function as described by Equation <ref type="bibr" target="#b12">(13)</ref>. Q 1 has a single fully-connected encoding layer characterized by the function f 1 (X) = ReLU (XW ), where W represents the learning weights of this layer. Q 2 has a single graph convolutional layer characterized by the function f 2 (X) = ReLU (? self XW ).</p><formula xml:id="formula_30">L Q1 = L Q2 = L clus (Z(?)) + ? L bce (?(Z(?)), A self ). (13)</formula><p>Under Assumption 1 and Assumption 2, we have:</p><formula xml:id="formula_31">If P(f 1 (x i )) 0 then ? F D (Q 2 , x i ) ? F D (Q 1 , x i ).</formula><p>In Theorem 4, we study the first scenario, which consists of comparing a one-layer graph convolutional encoder against a one-layer fully-connected encoder. Our proof depends on two reasonable properties of? self . Specifically, we know by definition that? self connects each node with few immediate neighbors as opposed to A sup , which connects each node with all nodes from the same true cluster. Assumption 1 states that the immediate neighbors of a node v i are represented with small errors. The second Assumption 2 asserts that the immediate neighbors of a node v i activate the same neurons for a well-trained layer. Under these mild assumptions, Theorem 4 indicates that performing a graph convolutional operation before a fully-connected layer increases the effect of FD on a node v i , if the graph convolutional operation has a positive impact on clustering v i . Intuitively,? self only considers the immediate neighbors (due to the sparsity of? self ) and maintains some clustering-irrelevant links. For every layer, we know that the graph convolutional operation is equivalent to minimizing the loss function L C (X (k) ,? self ) <ref type="bibr" target="#b6">[7]</ref>, which implies an increase of FD at the level of the same layer.</p><p>Theorem 5. Given two models Q 1 and Q 2 , which optimize the same objective function as described by Equation <ref type="formula" target="#formula_0">(14)</ref>. Q 1 has a single graph convolutional layer characterized by the function f 1 (X) = ReLU (? self XW 1 ), where W 1 represents the learning weights of this layer. Q 2 has two graph convolutional layers characterized by the function f 2 (X) = ReLU (? self ReLU (? self XW 1 ) W 2 ), where W 2 represents the learning weights of the second layer. We suppose that the Lipschitz constant ? * 1 of the second graph convolutional layer is less or equal to 1.</p><formula xml:id="formula_32">L Q1 = L Q2 = L clus (Z(?)) + ?L bce (?(Z(?)), A self ). (14)</formula><p>Under Assumption 1 and Assumption 2, we have:</p><formula xml:id="formula_33">If P(f 1 (x i )) 0 then ? F D (Q 2 , x i ) ? F D (Q 1 , x i ).</formula><p>In Theorem 5, we study the second scenario, which consists of comparing a one-layer graph convolutional encoder against a two-layer graph convolutional encoder. Similar to Theorem 4, our proof relies on Assumption 1 and Assumption 2. As a result, we found that adding a graph convolutional layer increases the effect of FD on a node v i , if the graph convolutional operation has a positive impact on clustering v i . Intuitively, the smoothing effect of each layer propagates to the embedded representations Z, which in turn drift the clustering-oriented structures. For instance, an infinite-depth graph convolutional network produces the same embedded vector for each node <ref type="bibr" target="#b37">[38]</ref>. Mapping all nodes to the same embedded point renders the clustering irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED OPERATORS</head><p>Our theoretical analysis indicates the limitations of GAE models in tackling the FR and FD problems. Motivated by these limitations, we propose two operators that can be easily integrated into existing models. Most importantly, our operators gradually transform the general-purpose self-supervised graph into a clustering-oriented graph. Firstly, we design a sampling operator ? that triggers a protection mechanism against FR. More precisely, ? can delay FR from quickly taking place. Secondly, we propose an operator ? that triggers a correction mechanism against FD. ? revokes the impact of FD by gradually transforming the reconstructed graph into a clustering-oriented one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A protection mechanism against FR</head><p>Some supervised methods <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> handle the impact of corrupted labels by an iterative selection protocol. Specifically, samples with clean labels are selected to train the model. Then, this latter is progressively used to select more samples with clean labels. In most cases, consistently high-confidence predictions, during training, are generally associated with uncorrupted samples. This strategy can be considered a correction mechanism as identifying the noisy samples requires training with them in advance. However, it is not clear to what extent the predictions of a noisy classifier (i.e., trained with random labels) are sufficient to recognize samples with corrupted labels.</p><p>Compared with existing sampling techniques for supervised learning, our strategy is motivated by two additional insights. The first idea consists of using a protection mechanism against FR, instead of a correction one. In fact, it has been observed that fine-tuning a model by training it on ground-truth labels, once the pretraining phase is performed on random labels, can not reverse the impact of labels' randomness <ref type="bibr" target="#b13">[14]</ref>. Since a correction mechanism can not reverse the effect of labels' randomness, we opt for a protection mechanism that prioritizes the selection of samples with uncorrupted labels, before using them for training. Our sampling technique is initiated directly after the pretraining phase and exploits two strong criteria to collect a sufficient portion of nodes with reliable clustering assignments. Second, we argue that it is important to control the selection process according to the difference between the first high-confidence and second highconfidence clustering assignment scores. This aspect is quite useful when the labels can be flipped between two similar clusters.</p><p>We propose three guidelines to develop our sampling operator ?. The first guideline consists of transforming hard clustering assignments into soft assignments. To this end, we compute the matrix (p ij ) i,j ? R N ?K . If (p ij ) i,j is already a soft assignment matrix, then we set p ij = p ij . If the matrix (p ij ) i,j is a hard Algorithm 1 Operator ?. assignment matrix, then we measure the similarity between the embedded points and the clustering representatives according to:</p><formula xml:id="formula_34">p ij = exp(? 1 2 (z i ? ? j ) T ? ?1 j (z i ? ? j )) K j=1 exp(? 1 2 (z i ? ? j ) T ? ?1 j (z i ? ? j )) ,<label>(15)</label></formula><p>where ? j stands for the center of cluster C clus j , and ? j is a diagonal matrix representing the cluster variances. The second guideline consists of extracting the first and second high-confidence assignment scores from matrix (p ij ) i,j for each node. The first score associated with z i is denoted by ? 1 i :</p><formula xml:id="formula_35">? 1 i = max j?{1,...,K} (p ij ).<label>(16)</label></formula><p>The second high-confidence assignment score for the embedded representation z i is denoted by ? 2 i :</p><formula xml:id="formula_36">? 2 i = max j?{1,...,K} (p ij | p ij &lt; ? 1 i ).<label>(17)</label></formula><p>The third guideline consists of constructing a set ?(t) that contains nodes, whose clustering assignments at iteration t are reliable enough to decide to which cluster they belong. Points from ? are selected according to two criteria as described by Eqn. <ref type="bibr" target="#b17">(18)</ref>.</p><formula xml:id="formula_37">? = i ? V| ? 1 i ? ? 1 and (? 1 i ? ? 2 i ) ? ? 2 .<label>(18)</label></formula><p>First, a node from ? is situated close to its closest cluster representative. Consequently, its first high-confidence assignment score is greater than a threshold ? 1 , where ? 1 is a tunable hyperparameter within the range [0, 1]. Second, a point from ? is located far from the borderline between neighbor clusters as described by Equation <ref type="bibr" target="#b17">(18)</ref>. Consequently, the difference between the first and second high-confidence assignment scores is greater than a threshold ? 2 . We set ? 2 = ?1 2 . Our sampling operator ? is summarized in Algorithm 1. The computational complexity of Algorithm 1 is O(N K 2 d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A correction mechanism against FD</head><p>Real-world graphs carry edges that connect nodes from different clusters. Reconstructing the input graph structure is not suitable for learning clustering-oriented embeddings. To attenuate FD, we use the embeddings of reliable nodes ?(Z(?)) to gradually transform the reconstruction objective into a clustering-oriented cost. This can be done by gradually substituting the self-supervisory signal A self with a task-specific signal ?(A, P (?(Z(?))), ?).</p><p>We propose two guidelines for developing the graph transforming operator ?. The first guideline consists of identifying a centroid node for each cluster. To this end, we compute? j , which averages Algorithm 2 Operator ?. </p><formula xml:id="formula_38">k 1 ? arg max k (P [i, k]) 7: j ? ?[k 1 ] 8: k 2 ? arg max k (P [j, k]) 9: if (j / ? A[i].indices) and (k 1 = k 2 ) then A[i].</formula><p>indices indicates the list of nodes connected to node i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>A the embedded representations of reliable nodes from cluster C clus j . Then, for each? j , we search for its nearest node, in the embedded space, among the set ?. The list of obtained nodes is denoted by ? = [i ? V| i = 1-NN(? j , ?) and j ? {1, ..., K}], where 1-NN represents the nearest neighbor algorithm.</p><p>The second guideline consists of constructing a new selfsupervisory signal A self clus based on the original graph structure A. To this end, we start by connecting each node from ? with its associated centroid from ?. Then, we drop edges between nodes from ?, which are members of different clusters. As a result, the obtained graph A self clus contains K star-shaped sub-graphs representing the different clusters. Algorithm 2 summarizes our proposed operator ?. The worst-case complexity of Algorithm 2 is O(N (d + K) + |E|(N + K)).</p><p>A protection mechanism against FD can be established by transforming the self-supervisory signal A into a clusteringoriented signal ?(A, P (Z(?)), V), in a single step. This is done by applying ? to the whole set of nodes V, instead of ?. We argue that a correction mechanism, which allows FD to take place then gradually attenuates this problem, is a more advantageous solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In order to validate the suitability of our conceptual design and our proposed operators, we conduct an extensive experimental protocol * . We show that it is possible to substantially improve the clustering performance of several GAE-based clustering models by integrating operators that can control FR and FD. We obtain promising results, which calls for further research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental settings</head><p>Due to the limited number of second-group models, we propose a new approach entitled DGAE from this group. For the sake of reproducibility, we provide a technical description of this method * We bring to the attention of the reader that our code can be found at: https://github.com/nairouz/R-GAE in Appendix B. Our experimental protocol covers six models (GAE <ref type="bibr" target="#b7">[8]</ref>, VGAE <ref type="bibr" target="#b7">[8]</ref>, ARGAE <ref type="bibr" target="#b8">[9]</ref>, ARVGAE <ref type="bibr" target="#b8">[9]</ref>, GMM-VGAE <ref type="bibr" target="#b9">[10]</ref>, and DGAE). GAE, VGAE, ARGAE, and ARVGAE belong to the first GAE-based clustering group, which, as discussed in Section 2, establish clustering and embedding learning separately. DGAE and GMM-VGAE are members of the second group, which ensures joint clustering and embedding learning. For GAE, VGAE, ARGAE, and ARVGAE, we use the publicly available implementations. For GMM-VGAE, we reproduce their reported results by performing our implementation. We integrate our operators ? and ? into the aforementioned models. For the first group, we use ? and ? to gradually transform the reconstruction loss into a clustering-oriented objective, during the pretraining phase. We keep the original settings (optimizer, hyper-parameters, architecture) of each model for fairness of comparison. The obtained methods are abbreviated by (R-GAE, R-VGAE, R-ARGAE, R-ARVGAE, R-GMM-VGAE, R-DGAE). "R-D" stands for Rethinking the model D (i.e., GAE, VGAE, ARGAE, ARVGAE, GMM-VGAE, DGAE) from the perspective of FR and FD. To avoid training instability due to the consistent modification of the self-supervisory signal, we update ? and A self clus every M 1 and M 2 iterations, respectively. We train the obtained models until meeting the convergence criterion |?| ? 0.9 * |V|. Compared with the original approaches, that is, GAE, VGAE, ARGAE, ARVGAE, GMM-VGAE, and DGAE, three additional hyper-parameters, namely M 1 , M 2 and ? 1 , should be specified. The values of these parameters are provided in Appendix C. We assess the proposed operators on six benchmark datasets. Our evaluation includes three citation networks (Cora, Citeseer, and Pubmed <ref type="bibr" target="#b41">[42]</ref>) and three air-traffic networks (USA, Europe, and Brazil <ref type="bibr" target="#b42">[43]</ref>). Since the air-traffic networks go without node attributes, we leverage the one-hot encoding of node degrees to construct the feature matrix X similar to <ref type="bibr" target="#b43">[44]</ref>. For all datasets, X is (row-)normalized with the Euclidean norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We present the principal results of our experiments in this section. However, due to limited space, we provide further experiments and results in Appendix D.</p><p>Effectiveness: In <ref type="table" target="#tab_3">Tables 1, 2, 3</ref>, and 4, we report the best and average clustering results among three trials on six datasets. For all tables, we mark the best methods in bold and the clustering performances in %. For fairness of comparison, we ensure that each couple of methods D and R-D share the same pretraining weights before starting the clustering phase. <ref type="table" target="#tab_3">Table 1</ref> provides the best clustering performances on three citation networks. From this table, we observe that the second GAE group methods yield considerably better results than methods from the first group. These results confirm that performing joint clustering and embedding learning is advantageous to the clustering task. Among the first group, we can see that (R-GAE, R-VGAE, R-ARGAE, R-ARVGAE) generally have better ACC, NMI, and ARI compared with their counterparts (GAE, VGAE, ARGAE, ARVGAE). The embedded representations of (GAE, VGAE, ARGAE, ARVGAE) are optimized using the reconstruction objective. These methods do not suffer from FR and FD. By gradually transforming the graph reconstruction into a clustering-oriented loss, during the training process, (R-GAE, R-VGAE, R-ARGAE, R-ARVGAE) make the embedded representations more clustering-oriented. Among the second group, we observe that (R-GMM-VGAE, R-DGAE) outperform their counterparts (GMM-VGAE, DGAE) consistently by a significant margin. To confirm these results, we extend the performed experiments to three additional datasets as shown in <ref type="table" target="#tab_5">Table 3</ref>. Our results offer strong evidence that the proposed operators can improve the clustering effectiveness of GAE models in terms of ACC, NMI, and ARI. Since this manuscript aims at investigating the impact of FR and FD, we focus on (R-GMM-VGAE, R-DGAE) and their counterparts (GMM-VGAE, DGAE) in the subsequent experiments. Moreover, we provide a comprehensive comparison against several recent graph clustering methods in Appendix D.</p><p>Efficiency: In <ref type="table" target="#tab_7">Table 5</ref>, we compare (R-GMM-VGAE, R-DGAE) with their counterparts (GMM-VGAE, DGAE) in terms of run-time. We report the best, the mean, and the variance in execution time over ten trials. Although Pubmed has almost ten times more edges and features than Cora and Citeseer, we observe that the difference in execution time between (R-GMM-VGAE, R-DGAE) and their counterparts (GMM-VGAE, DGAE) remains considerably small on Pubmed. In accordance with the provided complexity analysis for Algorithm 1 and Algorithm 2, our results confirm that the designed operators do not cause any significant overhead in execution time, compared with the original models.</p><p>Visualisation of A self clus : In <ref type="figure">Figure 4</ref>, we visualize the selfsupervisory graph A self clus constructed by ?, during the training of R-GMM-VGAE on Cora. As the training progresses, more nodes are connected with their associated centroids. Furthermore, we observe that several clustering-unfriendly edges are dropped. At epoch 120, A self clus contains 7 star-shaped sub-graphs representing the different clusters. These results confirm the ability of our operator ? to gradually transform the reconstructed graph into a clustering-oriented graph.</p><p>Feature Randomness: In this part, we discuss the evolution of ? F R values for GMM-VGAE and R-GMM-VGAE on Cora. The cosine similarity between the gradient of L clus (Z(?), P ) and the gradient of L clus (Z(?), Q ) is denoted by ? F R (GMM-VGAE), whereas ? F R (R-GMM-VGAE) denotes the cosine similarity between the gradient of L clus (?(Z(?)), P ) and the gradient of L clus (Z(?), Q ). We illustrate both metrics, during training of R-GMM-VAGE and GMM-VGAE, in <ref type="figure">Figures 5 (a)</ref> and (b), respectively. To facilitate our analysis, we also provide the normalized cumulative difference between ? F R (R-GMM-VGAE) and ? F R (GMM-VGAE), during the training of R-GMM-VAGE and GMM-VGAE, in <ref type="figure">Figures 5 (d)</ref> and (e), respectively. As a general observation from <ref type="figure">Figures 5 (a)</ref>, (b), and (c), ? F R (GMM-VGAE) and ? F R (R-GMM-VGAE) start from very high values (close to one). This implies that the unsupervised gradient, at an early training stage, has the same direction as the supervised one. This result is congruent with recent findings, which suggest that training with ground-truth or random labels prioritizes learning simple patterns first at the level of the earlier layers <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. These simple patterns are not dependent on the target labels <ref type="bibr" target="#b13">[14]</ref>.</p><p>For the first experiment <ref type="figure">(Figures 5 (a) and (d)</ref>), we train R-GMM-VGAE and we report ? F R (GMM-VGAE), ? F R (R-GMM-VGAE), and the normalized cumulative difference between both of them. We can see that there are two stages. The first stage ranges from iteration 0 to 60, and the second stage ranges from iteration 60 to 140. For the first stage, we observe that ? F R (R-GMM-VGAE) is higher than ? F R (GMM-VGAE). This result is confirmed by observing the cumulative difference between ? F R (R-GMM-VGAE) and ? F R (GMM-VGAE) in <ref type="figure">Figure 5 (d)</ref>, which has a pronounced increasing tendency. These results demonstrate the ability of our operator ? to reduce FR, during the first stage. For the second stage (from iteration 60 to 140 of <ref type="figure">Figure 5 (a)</ref>), the blue and green curves become closer to each other. This observation is confirmed by a lower slope for the curve of <ref type="figure">Figure 5</ref> (d) compared with the slope of the same curve for the first stage (i.e., between iterations 0 and 60 of <ref type="figure">Figure 5 (d)</ref>). At this point, ? gradually approaches V. Therefore, ? F R (R-GMM-VGAE) becomes approximately equal to ? F R (GMM-VGAE).</p><p>For the second experiment ( <ref type="figure">Figures 5 (b)</ref> and (e)), we train GMM-VGAE and we report ? F R (GMM-VGAE), ? F R (R-GMM-VGAE), and the normalized cumulative difference between both of them. We observe that ? F R (R-GMM-VGAE) is consistently close to 1. From <ref type="figure">Figure 5</ref> (e), we can see that the cumulative difference between ? F R (R-GMM-VGAE) and ? F R (R-GMM-VGAE) has almost a constant slope. These results suggest that ? can consistently select a sufficient amount of reliable nodes even after learning based on unreliable nodes. Thus, ? is capable of playing the role of a protection mechanism against FR.</p><p>For the third experiment ( <ref type="figure">Figures 5 (c)</ref> and (f)), we train GMM-VGAE and report ? F R (GMM-VGAE), we train R-GMM-VGAE and report ? F R (R-GMM-VGAE), and we finally report the normalized cumulative difference between both of them. We can see that there are three stages. The first stage ranges from iteration 0 to 50, the second stage ranges from iteration 50 to 100, and the third stage ranges from iteration 100 to 140. For the first stage, we observe that R-GMM-VGAE outperforms GMM-VGAE in terms of ? F R thanks to our operator ?. For the second stage, we observe that GMM-VGAE yields better results than R-GMM-VGAE in terms of ? F R . To reduce FD, R-GMM-VGAE transforms the reconstruction loss into a clustering-oriented loss. However, eliminating the reconstruction gives rise to FR. Unlike R-GMM-VGAE, GMM-VGAE maintains the reconstruction loss, during the second stage, which is considered an implicit mechanism against FR. <ref type="figure">Figure 5</ref> (f) shows clearly the trade-off between FR and FD. Although both models have reduced the same amount of FR, delaying the effect of FR has a favorable impact on the clustering performance. For the third stage, both models tie together. This experiment shows that using a protection mechanism delays the effect of FR and does not prevent it from taking place. By delaying the effect of randomness using a protection mechanism, it is possible to improve the clustering performance considerably.</p><p>Feature Drift: In this part, we discuss the evolution of ? F D values for GMM-VGAE and R-GMM-VGAE on Cora. The cosine similarity between the gradient of L bce (?(Z(?)), A) and the gradient of L bce (?(Z(?)), ?(A, Q (Z(?)), V)) is denoted by ? F D (GMM-VGAE), whereas ? F D (R-GMM-VGAE) denotes the cosine similarity between the gradient of L bce (?(Z(?)), ?(A, P (?(Z(?))), ?)) and the gradient of L bce (?(Z(?)), ?(A, Q (Z(?)), V)). We illustrate both metrics, during training of R-GMM-VAGE and GMM-VGAE, in <ref type="figure" target="#fig_4">Figures 6 (a)</ref> and (b), respectively. To facilitate our analysis, we also provide the normalized cumulative difference between ? F D (R-GMM-VGAE) and ? F D (GMM-VGAE), during training of R-GMM-VAGE and GMM-VGAE, in <ref type="figure">Figures 5 (d)</ref> and (e), respectively. As a general observation from <ref type="figure" target="#fig_4">Figures 6 (a), (b)</ref>, and (c), ? F D (R-GMM-VGAE) and ? F D (GMM-VGAE) start from very high values (close to one) then gradually decrease. This implies that the unsupervised gradient, at an early training stage, has the same direction as the supervised one. A recent body of work <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> has shown that a neural network learns simple patterns first using the early layers. In another work, the authors of <ref type="bibr" target="#b46">[47]</ref>         have shown that these simple patterns can be learned through selfsupervision just as well as through real supervision (with groundtruth labels). Thus, optimizing a supervised objective function has the same effect (learning low-level patterns) as optimizing a self-supervised objective function for the first few iterations.</p><p>For the first experiment <ref type="figure" target="#fig_4">(Figures 6 (a) and (d)</ref>), we   train R-GMM-VGAE and we report ? F D (GMM-VGAE), ? F D (R-GMM-VGAE), and the normalized cumulative difference between both of them. We can see that there are two stages. The first stage ranges from iteration 0 to 40, and the second stage ranges from iteration 40 to 140. For the first stage, we observe that ? F D (R-GMM-VGAE) values are very close to ? F D (GMM-VGAE) values. A possible explanation is that ? can only affect a small part of the self-supervisory graph A self clus at the beginning, and most of the graph remains identical to A. Furthermore, we observe that ? F D (R-GMM-VGAE) is decreasing rapidly for this stage. This aspect is desirable. In fact, ? allows FD to occur at the beginning to counter random projections. From <ref type="figure" target="#fig_4">Figure 6 (d)</ref>, we can see that the cumulative difference between ? F D (R-GMM-VGAE) and ? F D (GMM-VGAE) has a low slope for the first stage. This result confirms that our operator ? allows FD to take place, during the first stage. For the second stage, we observe that ? F D (R-GMM-VGAE) is increasing slowly between iterations 40 and 60. After allowing FD to occur, during the first stage, ? gradually attenuates this problem during the second stage. From <ref type="figure" target="#fig_4">Figure 6 (d)</ref>, we can see that the cumulative difference between ? F D (R-GMM-VGAE) and ? F D GMM-VGAE) has a pronounced increasing tendency compared with the first phase. After allowing FD to occur, ? gradually attenuates this problem, during the second stage.</p><p>For the second experiment <ref type="figure" target="#fig_4">(Figures 6 (b)</ref> and (e)), we train GMM-VGAE and we report ? F D (GMM-VGAE), ? F D (R-GMM-VGAE), and the normalized cumulative difference between both of them. From <ref type="figure" target="#fig_4">Figure 6</ref> (e), we can see that the cumulative difference between ? F D (R-GMM-VGAE) and ? F D (GMM-VGAE) has a pronounced increasing tendency starting from iteration 40. This result suggests that ? can consistently construct a reliable self-supervisory signal even after learning based on unreliable nodes. Additionally, we observe a decreasing tendency of ? F D between iterations 0 and 100. After 100 iterations, the two curves of ? F D in <ref type="figure" target="#fig_4">Figure 6</ref> (b) oscillate around a horizontal line (indicating the stability of FD). The absence of a considerable time slot, where ? F D (GMM-VGAE) achieves a clear increasing tendency, suggests that GMM-VGAE does not have any implicit or explicit mechanism to reduce FD. Based on the same experiment, we can see that ? F D (GMM-VGAE) can reach very low values compared with ? F R (GMM-VGAE) (see <ref type="figure">Figure 5</ref> (b)). In addition to that, we observe that ? F D (GMM-VGAE) has more pronounced fluctuations than ? F R (GMM-VGAE). While GMM-VGAE does not have any explicit mechanism against FR or FD, the reconstruction loss is an implicit mechanism against FR.</p><p>For the third experiment <ref type="figure" target="#fig_4">(Figures 6 (c)</ref> and (f)), we train GMM-VGAE and report ? F D (GMM-VGAE), we train R-GMM-VGAE and report ? F D (R-GMM-VGAE), and we finally report the normalized cumulative difference between both of them. We observe that R-GMM-VGAE considerably outperforms GMM-VGAE in terms of ? F D . More interestingly, while R-GMM-VGAE can attenuate FD after the initial decrease of ? F D , GMM-VGAE falls short of this capacity.</p><p>Protection vs correction: In <ref type="table" target="#tab_8">Table 6</ref>, we compare between a protection mechanism and a correction mechanism against FR, during the training of R-GMM-VGAE and R-DGAE on Cora. A protection mechanism is established by initiating the sampling technique directly after the pretraining phase. For the correction case, we delay the sampling technique for different epochs (10, 30, 50, 100, and 150) to allow FR to occur. This experiment aims to test if a correction mechanism can reverse the effect of labels' randomness. As we can see from <ref type="table" target="#tab_8">Table 6</ref>, the protection strategy yields better results than the correction approaches for both models. Moreover, further delay of correction is generally associated with lower clustering performance. These results show that a correction mechanism can not reverse the effect of labels' randomness. In <ref type="table" target="#tab_9">Table 7</ref>, we compare between a protection mechanism and a correction mechanism against FD, during the training of R-GMM-VGAE and R-DGAE on Cora. A protection mechanism is established by transforming the self-supervisory signal A into a clustering-oriented signal ?(A, P (Z(?)), V), in a single step. This is done by applying ? to the whole set of nodes V, instead of ?, to eliminate the reconstruction. We observe that the correction strategy yields better results than the protection approach for both models. We conclude that a correction mechanism, which allows FD to take place then gradually attenuates this problem, is a more advantageous solution.</p><p>One confidence threshold vs two confidence thresholds: In this part, we perform an ablation study to investigate the performance overhead provided by ?. Our investigation includes four cases: ablation of the sampling criteria related to ? 1 , ablation of the sampling criteria related to ? 2 , ablation of both (i.e., eliminating the operator ?), and no ablation. As shown in <ref type="table" target="#tab_10">Table 8</ref>, the obtained results show the importance of using two criteria for selecting reliable nodes. Specifically, we observe that ablating the requirement related to ? 2 leads to a degradation in performance. In fact, ? 2 helps in excluding points, which are situated near the borderline of two similar clusters.</p><p>Adding edges vs dropping edges: In this part, we perform an ablation study to investigate the performance contribution of ?. Our investigation includes four cases: ablation of "drop_edge", ablation of "add_edge", ablation of both (i.e., eliminating the operator ?), no ablation. As shown in <ref type="table" target="#tab_11">Table 9</ref>, the obtained results show the importance of "add_edge" and "drop_edge" operations for building a reliable self-supervisory signal A self clus .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this manuscript, we advocate a new vision for building GAEbased clustering models from the perspective of Feature Randomness and Feature Drift. We start by introducing a new conceptual design that gradually reduces Feature Drift without causing an abrupt rise in random features. Our strategy depends on two operators. In this regard, we design a sampling function ? that triggers a protection mechanism against random projections. Moreover, we propose a function ? that triggers a correction mechanism against Feature Drift. As a key advantage, ? and ? can be easily tailored to existing GAE-based clustering models. Experiments on standard benchmarks demonstrate that our operators improve the clustering performance. Furthermore, our results show that: (1) ? effectively delays the impact of Feature Randomness, and (2) ? allows Feature Drift to occur then gradually reduces this problem. Our operators can be viewed as the first initiative to control Feature Randomness and Feature Drift for GAE-based clustering models. For future work, we plan to investigate the extensibility of our operators to multiplex graphs, in which each couple of nodes can be connected by multiple edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A HARDWARE AND SOFTWARE CONFIGURATIONS</head><p>All experiments are conducted on a server under the same environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hardware:</head><p>? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B DGAE TECHNICAL DETAILS</head><p>Due to the limited number of second group models, we propose a new approach abbreviated by DGAE (Discriminative Graph Auto-Encoder) from the second group. DGAE has a simple graph autoencoder architecture with two GCN layers. We pretrain this model using vanilla reconstruction for 200 epochs. For the clustering phase, DGAE minimizes a linear combination of clustering and reconstruction. The clustering loss of DGAE is the Kullback Leibler divergence between a soft clustering assignment distribution P = (p ij ) i,j and its associated hard clustering assignment distribution Q = (q ij ) i,j as described by Equation <ref type="formula" target="#formula_0">(19)</ref>:</p><formula xml:id="formula_39">L clus (P (Z(?))) = KL(Q||P ) = i j q ij log( q ij p ij ). (19)</formula><p>The soft clustering assignment P is computed based on the Student's t-distribution as follows:</p><formula xml:id="formula_40">p ij = 1 + z i ? ? j 2 j (1 + z i ? ? j 2 ) ,<label>(20)</label></formula><p>where ? j represent the clustering centers of the embedded representations Z. At the beginning of the training process, the embedded centers ? j are initialized based on K-means. Then, DGAE is trained to jointly optimize the embedded representations and the clustering centers by minimizing L clus (P (Z(?))) + ?L bce (?(Z(?)), A). All settings of DGAE are described in <ref type="table" target="#tab_3">Table  10</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C HYPER-PARAMETER SETTINGS</head><p>We report the hyper-parameter settings for R-GAE, R-VGAE, R-ARGAE, R-ARVGAE, R-GMM-VGAE, and R-DGAE on Cora in <ref type="table" target="#tab_3">Table 11</ref>, on Citeseer in <ref type="table" target="#tab_3">Table 12</ref>, on Pubmed in <ref type="table" target="#tab_3">Table 13</ref>, on Brazil Air traffic in <ref type="table" target="#tab_3">Table 14</ref>, on Europe Air traffic in <ref type="table" target="#tab_3">Table 15</ref>, and on USA Air traffic in <ref type="table" target="#tab_3">Table 16</ref>.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D FURTHER RESULTS</head><p>Comparison with graph clustering methods: In this part, we compare R-GMM-VGAE and R-DGAE with several recent graph clustering approaches. We report the paper results if the code is not publicly available. Otherwise, we run each experiment 10 times and we report the best results among these trials. As we can see from <ref type="table" target="#tab_3">Table 17</ref>, R-DGAE and R-GMM-VGAE yield generally better results than the other methods. Although being competitive on Cora and Citeseer, our methods outperform AGE on Pubmed. Robustness: In this part, we compare R-DGAE to DGAE on Cora after including or dropping edges or features. To establish a fair comparison, we ensure that the randomly included or dropped edges and features are the same for the two compared models. Moreover, we ensure that the evaluated models (i.e., DGAE and R-DGAE) share the same pretraining weights for each experiment. In <ref type="figure" target="#fig_5">Figure 7</ref>, we present two experiments. For the first one, we randomly connect pairs of unlinked nodes, we run both models, and we report their ACCs and ARIs. We observe that R-DGAE consistently outperforms DGAE with a various number of noisy edges. As the training progresses, we can see that the gap between both models increases. These results can be explained by the ability of our operator ? to drop random edges from the output graph. For the second experiment, we randomly add Gaussian noise with a mean value equal to zero, and a variance ranging in [0, 0.2]. We observe that R-DGAE yields better results than DGAE with various amounts of noise. In fact, ? only selects the most reliable samples. Therefore, the nodes, which are highly affected by noise, are probably ruled out by ?. In <ref type="figure">Figure 8</ref>, we present two additional experiments. In the first experiment, we randomly drop pairs of linked nodes. We observe that R-DGAE consistently outperforms DGAE with a various number of dropped edges. Interestingly, dropping few edges (less than 400) does not harm the clustering performance of R-DGAE (it even induces a small improvement). However, this is not the case for DGAE. While DGAE reconstructs the corrupted input graph, R-DGAE is endowed with a correction mechanism ? that can construct new clustering-friendly edges. For the second experiment, we randomly drop features columns from the matrix X. Similar to previous experiments, we observe that R-DGAE surpasses DGAE with various amounts of dropped features. A possible explanation suggests that ? can exclude the nodes, which are highly affected by the randomly dropped features.</p><p>Learning dynamics: In this part, we discuss the learning dynamics of R-GMM-VGAE on Cora. As we can see from <ref type="figure">Figure  9</ref> (a), the number of decidable nodes (i.e., nodes in ?) increases gradually. The gradual increase of ? demonstrates that performing embedded clustering with reliable nodes allows to gradually capture more challenging nodes. In <ref type="figure">Figure 9</ref> (b), we illustrate the evolution of ACC for the whole set of nodes V, and in <ref type="figure">Figure 9</ref> (c), we illustrate the evolution of ACC for ? and V ? ? (i.e., undecidable nodes whose clustering assignments are not sufficient to decide to which clusters they belong). In the beginning, the number of decidable nodes is equal to 586, and its ACC is around 0.88. At the end of the training, the accuracy of ? remains higher than 0.8, and the size of ? constitutes more than 90% of V. These results provide evidence that ? can collect a sufficient portion of nodes with reliable clustering assignments.</p><p>To investigate the role of our graph-transforming operator ?, we conduct a series of experiments to understand the evolution of the constructed graph A self clus . The obtained results are illustrated in <ref type="figure">Figures 9 (d)</ref>, (e), and (f). As we can see from <ref type="figure">Figure 9</ref> (d), the number of links for A self clus increases gradually. At the end of the training process, the number of links exceeds 10, 000. Most importantly, the number of false links (i.e., links between nodes with different labels) remains small compared to the number of true links (i.e., links between nodes with the same labels). From <ref type="figure">Figure  9</ref> (e), we can see that most of the added links are true links, and the number of false links among the added links is considerably inferior to the number of added true links. From <ref type="figure">Figure 9</ref> (f), we observe that the number of deleted links is one order of magnitude smaller than the number of added links. Thus, we expect that the impact of adding edges on clustering effectiveness is much stronger than the impact of dropping edges. We have investigated this aspect in our ablation study. Starting from epoch 60 of <ref type="figure">Figure  9</ref> (f), we observe that the number of false links among the deleted links is not always inferior to the number of deleted true links. This result indicates the possibility of improving our results by early stopping the operation "dropping edges". In the absence of a clear explanation to this observation, and to keep our solution as simple as possible, we refrain from adjusting the "dropping edges" operation according to the obtained results. Globally, our analysis suggests that ? gradually constructs a more clustering-oriented graph A self clus compared with the initial graph A. Visualisation of Z: In <ref type="figure" target="#fig_0">Figure 10</ref>, we visualize the latent representations of GMM-VGAE and R-GMM-VGAE, during training on Cora. It is noteworthy that both models share the same pretraining weights. At epoch 40, we observe that R-GMM-VGAE makes minor modifications to the embedded representations compared with GMM-VGAE. At this level, GMM-VGAE has already formed some well-separated clusters. Unlike GMM-VGAE, R-GMM-VGAE only uses the decidable nodes for performing embedded clustering. Therefore, it takes more iterations to obtain clusteringfriendly representations. Finally, at epoch 120, we observe that R-GMM-VGAE has better separability between the different clusters than GMM-VGAE. Mainly, R-MM-VGAE is more able to separate between the red and purple groups. Furthermore, unlike R-GMM-VGAE, GMM-VGAE can not separate between the blue and pink clusters. These results confirm the importance of our operator ? in building high-quality clusters.</p><p>Sensitivity to the confidence thresholds: In <ref type="figure" target="#fig_0">Figures 11 and  12</ref>, we illustrate the sensitivity of R-GMM-VGAE and R-DGAE, respectively, to the confidence thresholds ? 1 and ? 2 on Cora. For ? 1 , we try several values from the set {0.1, 0.2, 0.3, 0.4}. We find that setting ? 1 higher than 0.4 leads to an empty set ?. Therefore, 0.4 is the highest value we can try for ? 1 . Our strategy for setting ? 1 consists of choosing the highest value that can give birth to a nonempty set ?. For ? 2 , we evaluate several values from the set {0.05, 0.1, 0.15, 0.20, 0.25}. We find that setting ? 2 higher than 0.25 leads to an empty set ?. As we can see from both figures (i.e., <ref type="figure" target="#fig_0">Figure 11</ref> and <ref type="figure" target="#fig_0">Figure 12</ref>), R-GMM-VGAE and R-DGAE give reasonable results in a wide range of parameters.</p><p>Sensitivity to the balancing hyper-parameter: In <ref type="figure" target="#fig_0">Figure  13</ref>, we assess the sensitivity of R-GMM-VGAE and GMM-VGAE to the balancing hyper-parameter ? on Cora. As we can see from this figure, R-GMM-VGAE is less sensitive to ? than GMM-VGAE. By transforming the reconstruction loss into a clustering-oriented loss, the competition between the optimized functions of R-GMM-VGAE (i.e., L clus (P (?(Z(?)))) and L bce (?(Z(?)), ?(A, P (?(Z(?))), ?))) is less pronounced than the competition between the optimized functions of GMM-VGAE (i.e., L clus (P (Z(?))) and L bce (?(Z(?)), A)).     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E PROOF OF PROPOSITION 1</head><p>Proposition 1. The reconstruction loss for a GAE model can be expressed as:</p><formula xml:id="formula_41">L bce (?(Z(?)), A self ) = L C (Z(?), A self ) + L R (Z(?), A self ), L R (Z(?), A self ) = i,j log(1 + exp(z T i z j )) ? 1 2 a self ij ( z i 2 2 + z j 2 2 ) .</formula><p>Proof.</p><formula xml:id="formula_42">L bce (?(Z(?)), A self ) = ? 1 i,j N a self ij log( 1 1 + e ?z T i zj ) + (1 ? a self ij ) log( e ?z T i zj 1 + e ?z T i zj ) , = 1 i,j N a self ij log(e ?z T i zj ) + log(1 + e ?z T i zj ) ? log(e ?z T i zj ) , = 1 i,j N (1 ? a self ij ) z T i z j + log(1 + e ?z T i zj ) , = 1 2 1 i,j N (a self ij ? 1)(z i ? z j ) T (z i ? z j ) ? 1 2 1 i,j N (a self ij ? 1)(z T i z i + z T j z j ) + 1 i,j N log(1 + e ?z T i zj ), = 1 2 1 i,j N (a self ij ? 1) z i ? z j 2 2 ? 1 2 1 i,j N (a self ij ? 1)( z i 2 2 + z j 2 2 ) + 1 i,j N log(1 + e ?z T i zj ),</formula><p>And since</p><formula xml:id="formula_43">1 i,j N log(1 + e ?z T i zj ) = 1 i,j N log(1 + e 1 2 zi?zj 2 2 ? 1 2 ( zi 2 2 + zj 2 2 ) ), = 1 i,j N log e ? 1 2 zi?zj 2 2 + e ? 1 2 ( zi 2 2 + zj 2 2 ) e ? 1 2 zi?zj 2 2 , = 1 i,j N log(e ? 1 2 zi?zj 2 2 + e ? 1 2 ( zi 2 2 + zj 2 2 ) ) + 1 2 1 i,j N z i ? z j 2 2 , = 1 i,j N log(e ? 1 2 ( zi 2 2 + zj 2 2 ?2z T i zj ) + e ? 1 2 ( zi 2 2 + zj 2 2 ) ) + 1 2 1 i,j N z i ? z j 2 2 , = 1 i,j N log(e ? 1 2 ( zi 2 2 + zj 2 2 ) (1 + e z T i zj )) + 1 2 1 i,j N z i ? z j 2 2 , = 1 i,j N log(e ? 1 2 ( zi 2 2 + zj 2 2 ) ) + log(1 + e z T i zj ) + 1 2 1 i,j N z i ? z j 2 2 , = 1 i,j N log(1 + e z T i zj ) + 1 2 1 i,j N z i ? z j 2 2 ? 1 2 1 i,j N ( z i 2 2 + z j 2 2 ). Thus L bce (?(Z(?)), A self ) = 1 2 1 i,j N a self ij z i ? z j 2 2 ? 1 2 1 i,j N a self ij ( z i 2 2 + z j 2 2 ) + 1 i,j N log(1 + e z T i zj ), = L C (Z(?), A self ) + L R (Z(?), A self ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX F PROOF OF PROPOSITION 2</head><p>Proposition 2. The k-means clustering loss applied to the embedded representations can be expressed as:</p><formula xml:id="formula_44">L clus (Z(?)) = L C (Z(?), A clus ).</formula><p>Proof.  </p><formula xml:id="formula_45">L clus (Z(?)) = K k=1 i?C clus k z i ? ? k 2 2 , = K k=1 i?C clus k 1 C clus k j?C clus k z i ? 1 C clus k j?C clus k z j 2 2 , = K k=1 1 C clus k 2 i?C clus k j?C clus k (z i ? z j ) 2 2 , = K k=1 1 C clus k 2 i?C clus k j?C clus k (z i ? z j ) T j?C clus k (z i ? z j ) , = K k=1 1 C clus k 2 i?C clus k j?C clus k (z i ? z j ) T j ?C clus k (z i ? z j ) , = K k=1 1 C clus k 2 ? ? ? ? ? i,j?C clus k (z i ? z j ) T (z i ? z j ) + i,j,j ?C clus k j =j ,i =j,i =j (z i ? z j ) T (z i ? z j ) ? ? ? ? ? , = K k=1 1 C clus k 2 ? ? ? ? ? i,j?C clus k z i ? z j 2 2 + i,j,j ?C clus k j =j ,i =j,i =j (z T i z i ? z T i z j ? z T i z j + z T j z j ) ? ? ? ? ? , = K k=1 1 C clus k 2 ? ? ? ? ? i,j?C clus k z i ? z j 2 2 + 1 2 i,j,j ?C clus k j =j ,i =j,i =j z i ? z j 2 2 + z i ? z j 2 2 ? z j ? z j 2 2 ? ? ? ? ? , = K k=1 1 C clus k 2 ? ? ? ? ? i,j?C clus k z i ? z j</formula><formula xml:id="formula_46">z i ? z j 2 2 ? ? ? ? ? , = K k=1 1 C clus k 2 ? ? ? ? ? i,j?C clus k z i ? z j 2 2 + 1 2 i,j?C clus k j ?C clus k j =i,j =j z i ? z j 2 2 ? ? ? ? ? , = K k=1 1 C clus k 2 ? ? i,j?C clus k z i ? z j 2 2 + C clus k ? 2 2 i,j?C clus k z i ? z j 2 2 ? ? , = K k=1 1 2 C clus k i,j?C clus k z i ? z j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX G PROOF OF THEOREM 1</head><p>Theorem 1. The linear combination between reconstruction and embedded k-means for a GAE model can be expressed as:</p><formula xml:id="formula_47">L clus (Z(?)) + ? L bce (?(Z(?)), A self )) = L C (Z(?), A clus + ?A self ) + ? L R (Z(?), A self ).</formula><p>Proof. Based on Proposition 1 and Proposition 2, we can conclude that L clus (Z(?)) + ?L bce (?(Z(?)), A self )) = 1</p><formula xml:id="formula_48">2 1 i,j N a clus ij z i ? z j 2 2 + ? 2 1 i,j N a self ij z i ? z j 2 2 ? ? 2 1 i,j N a self ij ( z i 2 2 + z j 2 2 ) + ? 1 i,j N log(1 + exp(z T i z j )), = 1 2 1 i,j N (a clus ij + ?a self ij ) z i ? z j 2 2 ? ? 2 1 i,j N a self ij ( z i 2 2 + z j 2 2 ) + ? 1 i,j N log(1 + exp(z T i z j )),</formula><p>= L C (Z(?), A clus + ?A self ) + ? L R (Z(?), A self ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX H PROOF OF PROPOSITION 3</head><p>Proposition 3. The gradient of the reconstruction loss L bce (?(Z(?)), A) w.r.t. the embedded representation z i can be expressed as:</p><p>?L bce (?(Z(?)), A self ) ?z i = 1 j N (? ij ? a self ij )z j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof.</head><p>?L bce (?(Z(?)), A self ) ?z i =  ? F R (Q 2 , z i ) ? ? F R (Q 1 , z i ) (? * 1 ) 2 j,j a clus ij a sup ij z i ? z j 2 z i ? z j 2 ? j,j a clus ij a sup ij (z i ? z j ) T (z i ? z j ), j,j a clus ij a sup ij (? * 1 ) 2 z i ? z j 2 z i ? z j 2 ? (z i ? z j ) T (z i ? z j ) .</p><formula xml:id="formula_49">= 1 2 1 i,j N a self ij ?(z i ? z j ) T (z i ? z j ) ?z i ? 1 2 1 i,j N a self ij ?z T i z i ?z i + ?z T j z j ?z i + 1 i,j N ?log(1 + e z T i zj ) ?z i , = 1 2 1 i,j N a self ij ?(z T i z i ? 2z T i z j + z T j z j ) ?z i ? 1 2 1 i,j N a self ij ?z T i z i ?z i + ?z T j z j ?z i + 1 i,j N ?log(1 + e z T i zj ) ?z i , = ? 1 i,j N a self ij ?z T i z j ?z i + 1 i,j N ?log(1 + e z T i zj ) ?z i , = ? 1 i,j N</formula><p>We have Thus</p><formula xml:id="formula_50">? * 1 (Z T i a sup i ) T (Z T i a clus i ) (? T i a sup i )(? T i a clus i ) =? (? * 1 ) 2 (Z T i a sup i ) T (Z T i a clus i ) (? T i a sup i )(? T i a clus i ) ,</formula><formula xml:id="formula_51">(? * 1 ) 2 j,j a sup ij a clus ij z i ? z j 2 z i ? z j 2 j,j a sup ij a clus ij (z i ? z j ) T (z i ? z j ), =? j,j a clus ij a sup ij (? * 1 ) 2 z i ? z j 2 z i ? z j 2 ? (z i ? z j ) T (z i ? z j ) 0,</formula><p>=? ? F R (Q 2 , z i ) ? F R (Q 1 , z i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX K PROOF OF THEOREM 3</head><p>Theorem 3. Given two GAE models Q 1 and Q 2 , which have the same GCN architecture and weights. Q 1 optimizes the objective function in Equation <ref type="formula" target="#formula_1">(23)</ref> and Q 2 minimizes the loss function in Equation <ref type="formula" target="#formula_1">(24)</ref>, where f ? N N (d, d , L) an injective function and d d. Let ? * 2 be the Lipschitz constant of f ?1 : f (R d ) ? R d ,Z i = ((f (z j )) j ? (f (z i )) j ) j,j ? R N ?d , ? i = ( f (z j ) ? f (z i ) 2 ) j ? R N , and a i is the i th row of A.</p><p>L Q1 = L clus (Z(?)) + ?L bce (?(Z(?)), A self ),</p><p>L Q2 = L clus (Z(?)) + ?L bce (?(f (Z(?))), A self ).</p><p>? ? F R (Q 2 , z i ) = ? F R (Q 1 , z i ). </p><formula xml:id="formula_54">? F D (Q 1 , z i ) ? ? F D (Q 2 , z i ) = ? j? self ij z i ? z j 2 2 ?z i , ? j a sup ij z i ? z j 2 2 ?z i ? ? j? self ij f (z i ) ? f (z j ) 2 2 ?z i , ? j a sup ij f (z i ) ? f (z j ) 2 2 ?z i , = j,j ? self ij a sup ij (z i ? z j ) T (z i ? z j ) ? j,j ? self ij a sup ij (f (z i ) ? f (z j )) T (f (z i ) ? f (z j )), j,j ? self ij a sup ij z i ? z j 2 z i ? z j 2 ? j,j ? self ij a sup ij (f (z i ) ? f (z j )) T (f (z i ) ? f (z j )), (? * 2 ) 2 j,j ? self ij a sup ij f (z i ) ? f (z j ) 2 f (z i ) ? f (z j ) 2 ? j,j ? self ij a sup ij (f (z i ) ? f (z j )) T (f (z i ) ? f (z j )), j,j ? self ij a sup ij (? * 2 ) 2 f (z i ) ? f (z j ) 2 f (z i ) ? f (z j ) 2 ? (f (z i ) ? f (z j )) T (f (z i ) ? f (z j )) ,</formula><p>We have </p><formula xml:id="formula_55">? * 2 (Z T i a sup i ) T (Z T i? self i ) (? T i a sup i )(? T i? self i ) =? (? * ) 2 (Z T i a sup i ) T (Z T i? self i ) (? T i a sup i )(? T i? self i ) , (a sup i ) TZ iZ T i (? self i ) (a sup i ) T ? i ? T i (? self i ) , tr (a sup i ) TZ iZ T i (? self i ) tr (a sup i ) T ? i ? T i (? self i ) , tr (Z iZ T i ) T a sup i (? self i ) T tr (? i ? T i ) T a sup i (? self i ) T , j,j</formula><formula xml:id="formula_56">f (z i ) ? f (z j ) 2 f (z i ) ? f (z j ) 2 j,j a sup ij? self ij (f (z i ) ? f (z j )) T (f (z i ) ? f (z j )), =? j,j ? self ij a sup ij (? * 2 ) 2 f (z i ) ? f (z j ) 2 f (z i ) ? f (z j ) 2 ? (f (z i ) ? f (z j )) T (f (z i ) ? f (z j )) 0, =? ? F D (Q 1 , z i ) ? F D (Q 2 , z i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX L PROOF OF THEOREM 4</head><p>Theorem 4. Given two models Q 1 and Q 2 , which optimize the same objective function as described by Equation <ref type="bibr" target="#b24">25</ref>. Q 1 has a single fully-connected encoding layer characterized by the function f 1 (X) = ReLU (XW ), where W ? R d?d represents the learning weights of this layer. Q 2 has a single graph convolutional layer characterized by the function f 2 (X) = ReLU (? self XW ).</p><p>L Q1 = L Q2 = L clus (Z(?)) + ?L bce (?(Z(?)), A self ).</p><p>Under Assumption 1 and Assumption 2, we have:</p><p>If P(f 1 (x i )) 0 then ? F D (Q 2 , x i ) ? F D (Q 1 , x i ).</p><p>Proof. Let h be an aggregation function such that h sup (x i ) = j a sup ij x j , and h self (x i ) = j? self ij x j . Let the functions D 1 and D 2 be distance metrics such that D sup 1 (x i ) = 1 2 j a sup ij x i ? x j We conclude that f 1 (h self (x i )) = h self (f 1 (x i )). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The proposed conceptual design for GAE-based clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Adding fully-connected encoding layers on top of the last graph convolutional layer, and performing clustering at the level of the last encoding layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Adding fully-connected decoding layers on top of the last graph convolutional layer, and performing reconstruction at the level of the last decoding layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Visualizing the self-supervisory graph A self clus , on Cora using R-GMM-VGAE. (a) R-GMM-VGAE training (b) GMM-VGAE training (c) Independent training (d) R-GMM-VGAE training (e) GMM-VGAE training (f) Independent training Performance of R-GMM-VGAE and GMM-VGAE in terms of ? F R on Cora. Blue line: ? F R values of R-GMM-VGAE, during training of R-GMM-VGAE. Green line: ? F R values of GMM-VGAE, during training of R-GMM-VGAE. Gold line: ? F R values of R-GMM-VGAE, during training of GMM-VGAE. Red line: ? F R values of GMM-VGAE, during training of GMM-VGAE. Purple line: normalized cumulative difference between ? F R values of R-GMM-VGAE and ? F R values of GMM-VGAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>(a) R-GMM-VGAE training (b) GMM-VGAE training (c) Independent training (d) R-GMM-VGAE training (e) GMM-VGAE training (f) Independent training Performance of R-GMM-VGAE and GMM-VGAE in terms of ? F D on Cora. Blue line: ? F D values of R-GMM-VGAE, during training of R-GMM-VGAE. Green line: ? F D values of GMM-VGAE, during training of R-GMM-VGAE. Gold line: ? F D values of R-GMM-VGAE during training of GMM-VGAE. Red line: ? F D values of GMM-VGAE, during training of GMM-VGAE. Purple line: normalized cumulative difference between ? F D values of R-GMM-VGAE and ? F D values of GMM-VGAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Performance of R-DGAE and DGAE on Cora, in terms of ACC and ARI, after adding noisy edges and features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :Fig. 9 :Fig. 10 :</head><label>8910</label><figDesc>Performance of R-DGAE and DGAE on Cora, in terms of ACC and ARI, after dropping edges and features. (a) % of decidable and undecidable nodes (b) ACC: all nodes (c) ACC: decidable and undecidable nodes (d) # links A self clus (e) # added links A self clus (f) # deleted links A self clus Learning dynamics of R-GMM-VGAE on Cora 2D visualizations of the latent representations of GMM-VGAE and R-GMM-VGAE, on Cora using T-SNE. Top row: latent representations of GMM-VGAE; bottom row: latent representations of R-GMM-VGAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Influence of ? 1 and ? 2 values on ACC, NMI, and ARI for R-GMM-VGAE on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 :Fig. 13 :</head><label>1213</label><figDesc>Influence of ? 1 and ? 2 values on ACC, NMI, and ARI for R-DGAE on Cora. (a) R-GMM-VGAE (b) GMM-VGAE Sensitivity of R-GMM-VGAE and GMM-VGAE to the balancing hyper-parameter on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>j</head><label></label><figDesc>=j ,i =j,i =j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 2 , 1 |C 2 1 i,j N a clus ij z i ? z j 2 2</head><label>22122</label><figDesc>Let the matrix A clus = (a clus ij ) 1 i,j N ? R N ?N be defined as a clus ij = clus k | if ? k s.th i, j ? C clus k 0 otherwise.Hence, L clus (Z(?)) = 1 = L C (Z(?), A clus ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>?z T i zj z j , = ? 1 i,j N a self ij z j + 1 i,j N Sigmoid (z T i z j )z j , = ? 1 i,j N a self ij z j + 1 i,j N? ij z j , = 1 i,j N (? ij ? a self ij ) z j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>)</head><label></label><figDesc>T ) ? (? i ? T i ) jj , j,j a sup ij a clus ij (z i ? z j ) T (z i ? z j ) j,j a sup ij a clus ij z i ? z j 2 z i ? z j 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>2 ?z i , ? j a sup ij z i ? z j 2 2 ?z i ? ? j a clus ij z i ? z j 2 2</head><label>222</label><figDesc>? F D (Q 2 , z i ) ? F D (Q 1 , z i ).Proof.? F R (Q 2 , z i ) ? ? F R (Q 1 , z i ) = ? j a clus ij z i ? z j 2 ?z i , ? j a sup ij z i ? z j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>z i ) ? f (z j )) T (f (z i ) ? f (z j )) i ) ? f (z j ) 2 f (z i ) ? f (z j ) 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>2 2 , D self 1 (x i ) = 1 2 j? self ij x i ? x j 2 2 2 j,j ? self ij a sup ij x j ? x j 2 2. 1 .</head><label>212221</label><figDesc>, and D 2 (x i ) =<ref type="bibr" target="#b0">1</ref> We give three lemmas before proving Theorem 4.Lemma 1. ?a, b ? R d if Sign(a) = Sign(b) then Sign(a) = Sign(a + b).Proof.?m ? [|1, d|] Sign(a m + b m ) = a m + b m |a m + b m | , = a m |a m | |a m | |a m + b m | + b m |b m | |b m | |a m + b m | , = Sign(a m ) |a m | |a m + b m | + Sign(b m ) |b m | |a m + b m | , = Sign(a m ) |a m | |a m + b m | + Sign(a m ) |b m | |a m + b m | , (Sign(a) = Sign(b) =? Sign(a m ) = Sign(b m )), = Sign(a m ) |a m | + |b m | |a m + b m | . If a m 0 then |a m | + |b m | |a m + b m | = a m + b m a m + b m = Else if a m 0 then |a m | + |b m | |a m + b m | = ?a m ? b m ?(a m + b m ) = 1. =? |a m | + |b m | |a m + b m | = 1, =? Sign(a m + b m ) = Sign(a m ), =? Sign(a + b) = Sign(a). Lemma 2. ?i ? [|1, N |] f 1 (h self (x i )) = h self (f 1 (x i )).Proof. Based on Assumption 2, we have?j ? [|1, N |] such that? self ij = 0, Sign(W T x i ) = Sign(W T x j ).Applying Lemma 1, we obtainSign(W T x i ) = Sign(W T x j ), = Sign( j? self ij W T x j ), (? self ij? 0 thus it will not affect the sign)= Sign(W T j? self ij x j ).On one hand, we havef 1 (h self (x i )) = f 1 ( j? self ij x j ), = Diag(Sign(W T j? self ij x j )) W T j? self ij x j , = Diag(Sign(W T x i )) W T j? self ij x j .On the other hand, we haveh self (f 1 (x i )) = (W T x j )) W T x j , = j? self ijDiag(Sign(W T x i )) W T x j , (based on Assumption 2) = Diag(Sign(W T x i )) W T j? self ij x j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Lemma 3 .</head><label>3</label><figDesc>?i ? [|1, N |] x i ? h self (x i ).Proof. Based on Assumption 1,?j ? [|1, N |] , such that? self ij = 0, x i = x j + ij =? j? self ij x i = j? self ij x j + j? self ij ij , =? x i = h self (x i ) + j? self ij ij .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Input: Embedded data: Z, Number of clusters: K, First confidence threshold: ? 1 , Second confidence threshold: ? 2 . 2: Output: Embedded representations of decidable nodes: Z[?]. 3: Compute the matrix (p ij ) i,j ? R N ?K according to Eqn.<ref type="bibr" target="#b14">(15)</ref>.4:  for i = 0 to |X| do Construct ? according to Equation<ref type="bibr" target="#b17">(18)</ref>.</figDesc><table><row><cell>5: 6:</cell><cell>Compute ? 1 i according to Equation (16). Compute ? 2 i according to Equation (17).</cell></row><row><cell cols="2">7: end for</cell></row><row><cell>8:</cell><cell></cell></row></table><note>1:9: Return Z[?].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Input: Original sparse graph: A, Clustering assignment: P , Set of decidable nodes: ?. 2: Output: Clustering-oriented self-supervision graph: A self clus . 3: ? ? [i ? V| i = 1-NN(? j , ?) and j ? {1, ..., K}]. 4: A self clus ? A 5: for i in ? do</figDesc><table /><note>1:6:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>Best clustering performance for the original and proposed GAE models on Cora, Citeseer and Pubmed.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Cora</cell><cell></cell><cell>Citeseer</cell><cell></cell><cell>Pubmed</cell></row><row><cell></cell><cell cols="6">ACC NMI ARI ACC NMI ARI ACC NMI ARI</cell></row><row><cell>GAE</cell><cell>61.3</cell><cell>44.4 38.1</cell><cell>48.2</cell><cell>22.7 19.2</cell><cell>64.2</cell><cell>22.5 22.1</cell></row><row><cell>R-GAE</cell><cell>65.8</cell><cell>51.6 44.1</cell><cell>50.1</cell><cell>24.6 20.0</cell><cell>69.6</cell><cell>31.4 31.6</cell></row><row><cell>VGAE</cell><cell>64.7</cell><cell>43.4 37.5</cell><cell>51.9</cell><cell>24.9 23.8</cell><cell>69.6</cell><cell>28.6 31.7</cell></row><row><cell>R-VGAE</cell><cell>71.3</cell><cell>49.8 48.0</cell><cell>44.9</cell><cell>19.9 12.5</cell><cell>69.2</cell><cell>30.3 30.9</cell></row><row><cell>ARGAE</cell><cell>64.0</cell><cell>44.9 35.2</cell><cell>57.3</cell><cell>35.0 34.1</cell><cell>68.1</cell><cell>27.6 29.1</cell></row><row><cell>R-ARGAE</cell><cell>72.0</cell><cell>51.5 49.5</cell><cell>49.3</cell><cell>28.4 17.4</cell><cell>70.2</cell><cell>31.4 32.6</cell></row><row><cell>ARVGAE</cell><cell>63.8</cell><cell>45.4 40.1</cell><cell>54.4</cell><cell>26.1 24.5</cell><cell>63.5</cell><cell>23.2 22.5</cell></row><row><cell>R-ARVGAE</cell><cell>67.2</cell><cell>47.4 44.0</cell><cell>59.4</cell><cell>32.5 31.4</cell><cell>65.9</cell><cell>24.3 25.2</cell></row><row><cell>DGAE</cell><cell>70.2</cell><cell>50.7 47.2</cell><cell>67.7</cell><cell>40.9 42.5</cell><cell>68.4</cell><cell>29.0 29.1</cell></row><row><cell>R-DGAE</cell><cell>73.7</cell><cell>56.0 54.1</cell><cell>70.5</cell><cell>45.0 47.1</cell><cell>71.4</cell><cell>34.4 34.6</cell></row><row><cell>GMM-VGAE</cell><cell>71.9</cell><cell>53.3 48.2</cell><cell>67.5</cell><cell>40.7 42.4</cell><cell>71.1</cell><cell>29.9 33.0</cell></row><row><cell>R-GMM-VGAE</cell><cell>76.7</cell><cell>57.3 57.9</cell><cell>68.9</cell><cell>42.0 43.9</cell><cell>74.0</cell><cell>33.4 37.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>Mean and standard deviation of evaluation metrics for the original and proposed GAE models on Cora, Citeseer and Pubmed. ? 0.3 31.6 ? 0.8 30.8 ? 0.6 65.73 ? 0.2 23.7 ? 0.5 24.9 ? 0.3</figDesc><table><row><cell>Method</cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>Pubmed</cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>GAE</cell><cell>55.6 ? 4.9</cell><cell>41.2 ? 2.8</cell><cell>33.2 ? 4.5</cell><cell cols="3">42.5 ? 5.2 19.9 ? 2.6 13.7 ? 5.6</cell><cell>63.7 ? 0.5</cell><cell cols="2">23.3 ? 1.4 22.7 ? 1.6</cell></row><row><cell>R-GAE</cell><cell>65.0 ? 1.0</cell><cell>50.2 ? 1.3</cell><cell>43.3 ? 0.7</cell><cell cols="3">49.8 ? 0.5 24.3 ? 0.3 19.6 ? 0.6</cell><cell>68.0 ? 1.4</cell><cell cols="2">28.7 ? 2.4 29.3 ? 2.1</cell></row><row><cell>VGAE</cell><cell>58.6 ? 5.3</cell><cell>40.1 ? 2.9</cell><cell>34.2 ? 2.9</cell><cell cols="3">50.3 ? 1.6 23.6 ? 1.7 22.1 ? 2.4</cell><cell>68.9 ? 0.8</cell><cell cols="2">28.3 ? 1.1 30.6 ? 1.1</cell></row><row><cell>R-VGAE</cell><cell>70.3 ? 1.2</cell><cell>48.8 ? 0.9</cell><cell>46.7 ? 1.2</cell><cell cols="3">42.6 ? 2.1 14.9 ? 4.3 12.3 ? 0.3</cell><cell>68.9 ? 0.3</cell><cell cols="2">29.9 ? 0.4 30.6 ? 0.3</cell></row><row><cell>ARGAE</cell><cell>59.3 ? 4.0</cell><cell>42.2 ? 2.5</cell><cell>31.6 ? 5.0</cell><cell cols="3">36.6 ? 8.4 28.4 ? 4.0 16.1 ? 7.5</cell><cell>68 ? 0.1</cell><cell cols="2">29.4 ? 1.6 29.3 ? 0.3</cell></row><row><cell>R-ARGAE</cell><cell cols="2">71.2 ? 0.7 50.73 ? 0.8</cell><cell>47.1 ? 2.3</cell><cell cols="3">48.6 ? 0.7 28.5 ? 0.3 18.9 ? 1.3</cell><cell>69.2 ? 0.9</cell><cell cols="2">30.0 ? 1.2 30.9 ? 1.4</cell></row><row><cell>ARVGAE</cell><cell>63.4 ? 0.7</cell><cell>45.3 ? 0.3</cell><cell cols="4">39.17 ? 1.5 51.5 ? 2.9 26.3 ? 1.4 22.7 ? 1.8</cell><cell>63.4 ? 0.1</cell><cell cols="2">23.1 ? 0.1 22.4 ? 0.2</cell></row><row><cell cols="7">R-ARVGAE 59.2 DGAE 67.0 ? 0.2 47.2 ? 0.1 43.8 ? 0.5 69.8 ? 0.5 49.9 ? 0.7 46.3 ? 0.9 66.5 ? 1.1 39.2 ? 1.5 40.3 ? 1.9</cell><cell>67.8 ? 0.6</cell><cell cols="2">28.0 ? 1.0 28.0 ? 1.0</cell></row><row><cell>R-DGAE</cell><cell>73.1 ? 0.7</cell><cell>55.3 ? 0.7</cell><cell>53.0 ? 1.1</cell><cell cols="3">69.5 ? 0.8 43.7 ? 1.1 45.7 ? 1.2</cell><cell>71.0 ? 0.4</cell><cell cols="2">33.6 ? 0.9 33.9 ? 0.8</cell></row><row><cell>GMM-VGAE</cell><cell>71.7 ? 0.2</cell><cell>53.0 ? 0.3</cell><cell>47.9 ? 0.4</cell><cell cols="3">66.3 ? 0.5 39.5 ? 0.5 41.1 ? 0.6</cell><cell>70.6 ? 0.5</cell><cell cols="2">28.7 ? 1.1 32.0 ? 1.0</cell></row><row><cell>R-GMM-VGAE</cell><cell>75.7 ? 0.9</cell><cell>55.8 ? 1.3</cell><cell>56.2 ? 1.5</cell><cell cols="3">68.4 ? 0.4 41.5 ? 0.4 43.6 ? 0.3</cell><cell>72.8 ? 1.5</cell><cell cols="2">32.2 ? 1.9 35.7 ? 2.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 :</head><label>3</label><figDesc>Best clustering performance for the original and proposed GAE models on Air-Traffic datasets.</figDesc><table><row><cell>Method</cell><cell cols="2">USA Air-Traffic</cell><cell cols="2">Europe Air-Traffic</cell><cell cols="2">Brazil Air-Traffic</cell></row><row><cell></cell><cell cols="6">ACC NMI ARI ACC NMI ARI ACC NMI ARI</cell></row><row><cell>GMM-VGAE</cell><cell>48.1</cell><cell>21.9 13.2</cell><cell>53.1</cell><cell>31.1 24.4</cell><cell>70.2</cell><cell>46.0 41.9</cell></row><row><cell>R-GMM-VGAE</cell><cell>50.8</cell><cell>23.1 15.3</cell><cell>57.4</cell><cell>31.4 25.8</cell><cell>73.3</cell><cell>45.6 42.5</cell></row><row><cell>DGAE</cell><cell>46.4</cell><cell>28.0 18.4</cell><cell>53.6</cell><cell>33.3 23.3</cell><cell>71.0</cell><cell>48.0 41.2</cell></row><row><cell>R-DGAE</cell><cell>51.7</cell><cell>24.7 16.5</cell><cell>57.1</cell><cell>34.5 25.2</cell><cell>74.0</cell><cell>51.3 45.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>Mean and standard deviation of the evaluation metrics for the original and proposed GAE models on Air-Traffic datasets. ? 0.5 52.3 ? 1.0 29.2 ? 1.80 22.6 ? 1.5 69.0 ? 1.6 43.7 ? 2.6 38.8 ? 3.2 R-GMM-VGAE 50.4 ? 0.59 22.6 ? 0.5 15.2 ? 0.6 56.4 ? 1.3 31.2 ? 0.78 25.3 ? 0.8 71.8 ? 1.6 45.0 ? 2.7 41.6 ? 3.4 DGAE 45.8 ? 0.6 28.1 ? 0.2 18.2 ? 0.3 53.2 ? 0.5 33.1 ? 0.2 23.1 ? 0.2 70.7 ? 0.4 48.1 ? 1.0 39.9 ? 1.3 R-DGAE 51.3 ? 0.4 24.4 ? 0.4 16.2 ? 0.4 56.7 ? 0.7 33.2 ? 1.1 24.3 ? 0.8 74.1 ? 0.3 52.4 ? 1.3 45.7 ? 0.6</figDesc><table><row><cell>Method</cell><cell></cell><cell>USA Air-Traffic</cell><cell></cell><cell></cell><cell>Europe Air-Traffic</cell><cell></cell><cell></cell><cell>Brazil Air-Traffic</cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>GMM-VGAE</cell><cell>47.2 ? 0.9</cell><cell>21 ? 0.8</cell><cell>12.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>Execution time (in seconds) of the couples (GMM-VGAE, R-GMM-VGAE) and (DGAE, R-DGAE).</figDesc><table><row><cell>Method</cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>Pubmed</cell></row><row><cell></cell><cell>Best</cell><cell>Mean</cell><cell>Variance</cell><cell>Best</cell><cell>Mean</cell><cell>Variance</cell><cell>Best</cell><cell>Mean</cell><cell>Variance</cell></row><row><cell>GMM-VGAE</cell><cell cols="2">17.135 17.703</cell><cell>0.530</cell><cell cols="2">36.269 36.442</cell><cell>1.436</cell><cell cols="2">1341.190 1348.960</cell><cell>16.056</cell></row><row><cell cols="3">R-GMM-VGAE 21.928 24.509</cell><cell>2.589</cell><cell cols="2">40.084 41.910</cell><cell>2.884</cell><cell cols="3">1457.188 1477.405 155.492</cell></row><row><cell>DGAE</cell><cell cols="2">19.298 20.179</cell><cell>0.644</cell><cell cols="2">38.074 38.226</cell><cell>0.012</cell><cell cols="2">1067.301 1076.431</cell><cell>33.446</cell></row><row><cell>R-DGAE</cell><cell cols="2">28.981 31.053</cell><cell>1.464</cell><cell cols="2">51.363 52.976</cell><cell>1.850</cell><cell cols="3">1192.913 1215.241 361.036</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 :</head><label>6</label><figDesc>Correction-style mechanism against FR vs. protection-style mechanism against FR for R-GMM-VGAE and R-DGAE on Cora.</figDesc><table><row><cell>Method</cell><cell cols="2">Protection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Correction</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">No delay</cell><cell cols="10">After 10 epochs After 30 epochs After 50 epochs After 100 epochs After 150 epochs</cell></row><row><cell></cell><cell cols="3">ACC NMI ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell></row><row><cell>R-GMM-VGAE</cell><cell>76.7</cell><cell>57.3</cell><cell>74.5</cell><cell>53.9</cell><cell>73.6</cell><cell>54.8</cell><cell>70.4</cell><cell>51.9</cell><cell>71.6</cell><cell>52.7</cell><cell>70.1</cell><cell>51.0</cell></row><row><cell>R-DGAE</cell><cell>73.7</cell><cell>56.0</cell><cell>71.1</cell><cell>52.0</cell><cell>70.4</cell><cell>50.5</cell><cell>69.8</cell><cell>50.1</cell><cell>69.6</cell><cell>50.0</cell><cell>69.7</cell><cell>49.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7 :</head><label>7</label><figDesc>Protection-style mechanism against FD vs. correction-style mechanism against FD for R-GMM-VGAE and R-DGAE on Cora.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Protection</cell><cell></cell><cell>Correction</cell></row><row><cell></cell><cell cols="4">ACC NMI ARI ACC NMI ARI</cell></row><row><cell>R-GMM-VGAE</cell><cell>73.4</cell><cell>52.1 51.6</cell><cell>76.7</cell><cell>57.3 57.9</cell></row><row><cell>R-DGAE</cell><cell>71.3</cell><cell>54.5 50.4</cell><cell>73.7</cell><cell>56.0 54.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8 :</head><label>8</label><figDesc>Performance of R-GMM-VGAE and R-DGAE on Cora, after ablation of the confidence thresholds ? 1 and ? 2 .</figDesc><table><row><cell>Method</cell><cell cols="2">Ablation of ?2</cell><cell cols="2">Ablation of ?1</cell><cell cols="2">Ablation of both</cell><cell cols="2">No Ablation</cell></row><row><cell></cell><cell cols="8">ACC NMI ARI ACC NMI ARI ACC NMI ARI ACC NMI ARI</cell></row><row><cell>R-GMM-VGAE</cell><cell>74.2</cell><cell>53.7 53.7</cell><cell>73.3</cell><cell>52.0 51.8</cell><cell>71.2</cell><cell>52.5 48.3</cell><cell>76.7</cell><cell>57.3 57.9</cell></row><row><cell>R-DGAE</cell><cell>72.7</cell><cell>55.1 52.2</cell><cell>72.8</cell><cell>54.6 52.2</cell><cell>70.5</cell><cell>50.4 47.7</cell><cell>73.7</cell><cell>56.0 54.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9 :</head><label>9</label><figDesc>Performance of R-GMM-VGAE and R-DGAE on Cora, after ablation of "drop_edge" and "add_edge" operations.</figDesc><table><row><cell>Method</cell><cell cols="6">Ablation of "drop_edge" Ablation of "add_edge"</cell><cell cols="2">Ablation of both</cell><cell cols="2">No Ablation</cell></row><row><cell></cell><cell cols="2">ACC NMI</cell><cell>ARI</cell><cell cols="2">ACC NMI</cell><cell>ARI</cell><cell cols="4">ACC NMI ARI ACC NMI ARI</cell></row><row><cell>R-GMM-VGAE</cell><cell>75.4</cell><cell>55.1</cell><cell>55.6</cell><cell>72.8</cell><cell>52.5</cell><cell>50.4</cell><cell>74.0</cell><cell>53.6 52.8</cell><cell>76.7</cell><cell>57.3 57.9</cell></row><row><cell>R-DGAE</cell><cell>72.4</cell><cell>54.6</cell><cell>52.5</cell><cell>72.5</cell><cell>54.4</cell><cell>51.9</cell><cell>71.7</cell><cell>53.5 50.5</cell><cell>73.7</cell><cell>56.0 54.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Operating System: Ubuntu 18.04.5 LTS</figDesc><table><row><cell>? CPU: Intel(R) Xeon(R) CPU E5-2620 V4 @ 2.10GHz</cell></row><row><cell>Software:</cell></row><row><cell>? Python 3.7.4</cell></row><row><cell>? PyTorch 1.3.1</cell></row><row><cell>? Sklearn 0.23.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 10 :</head><label>10</label><figDesc>Settings of DGAE.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Dimension of the first GCN layer</cell><cell>32</cell></row><row><cell>Dimension of the second GCN layer</cell><cell>16</cell></row><row><cell>Number of pretraining epochs</cell><cell>200</cell></row><row><cell>Pretraining optimizer</cell><cell>Adam</cell></row><row><cell>Learning rate for pretraining</cell><cell>0.01</cell></row><row><cell>Number of training epochs</cell><cell>200</cell></row><row><cell>Training optimizer</cell><cell>Adam</cell></row><row><cell>Learning rate for training</cell><cell>0.01</cell></row><row><cell>Balancing coefficient</cell><cell>0.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 11 :</head><label>11</label><figDesc>Hyper-parameter settings on Cora.</figDesc><table><row><cell>Method</cell><cell>?1</cell><cell>M1</cell><cell>M2</cell></row><row><cell>R-GAE</cell><cell cols="3">0.3 20 epochs 10 epochs</cell></row><row><cell>R-VGAE</cell><cell cols="3">0.3 20 epochs 10 epochs</cell></row><row><cell>R-ARGAE</cell><cell cols="2">0.3 50 epochs</cell><cell>1 epoch</cell></row><row><cell>R-ARVGAE</cell><cell cols="2">0.3 50 epochs</cell><cell>1 epoch</cell></row><row><cell>R-DGAE</cell><cell cols="3">0.3 20 epochs 15 epochs</cell></row><row><cell>R-GMM-VGAE</cell><cell cols="3">0.3 20 epochs 10 epochs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 12 :</head><label>12</label><figDesc>Hyper-parameter settings on Citeseer.</figDesc><table><row><cell>Method</cell><cell>?1</cell><cell>M1</cell><cell>M2</cell></row><row><cell>R-GAE</cell><cell cols="3">0.2 20 epochs 10 epochs</cell></row><row><cell>R-VGAE</cell><cell cols="2">0.2 20 epochs</cell><cell>1 epoch</cell></row><row><cell>R-ARGAE</cell><cell cols="2">0.1 50 epochs</cell><cell>1 epoch</cell></row><row><cell>R-ARVGAE</cell><cell cols="2">0.1 50 epochs</cell><cell>1 epoch</cell></row><row><cell>R-DGAE</cell><cell cols="2">0.2 50 epochs</cell><cell>1 epoch</cell></row><row><cell>R-GMM-VGAE</cell><cell cols="2">0.2 50 epochs</cell><cell>1 epoch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 13 :</head><label>13</label><figDesc>Hyper-parameter settings on Pubmed.</figDesc><table><row><cell>Method</cell><cell>?1</cell><cell>M1</cell><cell>M2</cell></row><row><cell>R-GAE</cell><cell cols="3">0.4 50 epochs 5 epochs</cell></row><row><cell>R-VGAE</cell><cell cols="3">0.4 50 epochs 5 epochs</cell></row><row><cell>R-ARGAE</cell><cell cols="2">0.3 50 epochs</cell><cell>1 epoch</cell></row><row><cell>R-ARVGAE</cell><cell cols="2">0.3 50 epochs</cell><cell>1 epoch</cell></row><row><cell>R-DGAE</cell><cell cols="3">0.3 50 epochs 5 epochs</cell></row><row><cell>R-GMM-VGAE</cell><cell cols="3">0.4 50 epochs 5 epochs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 14 :</head><label>14</label><figDesc>Hyper-parameter settings on Brazil Air traffic.</figDesc><table><row><cell>Method</cell><cell>?1</cell><cell>M1</cell><cell>M2</cell></row><row><cell>R-DGAE</cell><cell cols="3">0.25 50 epochs 1 epoch</cell></row><row><cell>R-GMM-VGAE</cell><cell cols="3">0.25 50 epochs 1 epoch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 15 :</head><label>15</label><figDesc>Hyper-parameter settings on Europe Air traffic.</figDesc><table><row><cell>Method</cell><cell>?1</cell><cell>M1</cell><cell>M2</cell></row><row><cell>R-DGAE</cell><cell cols="3">0.08 20 epochs 15 epochs</cell></row><row><cell>R-GMM-VGAE</cell><cell cols="2">0.01 50 epochs</cell><cell>1 epoch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 16 :</head><label>16</label><figDesc>Hyper-parameter settings on USA Air traffic.</figDesc><table><row><cell>Method</cell><cell>?1</cell><cell>M1</cell><cell>M2</cell></row><row><cell>R-DGAE</cell><cell cols="3">0.1 50 epochs 1 epoch</cell></row><row><cell>R-GMM-VGAE</cell><cell cols="3">0.3 50 epochs 1 epoch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE 17 :</head><label>17</label><figDesc>Clustering performance of several graph clustering methods on Cora, Citeseer, and Pubmed. Best in bold, second best underlined.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>Pubmed</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="9">ACC NMI ARI ACC NMI ARI ACC NMI ARI</cell></row><row><cell>TADW [48]</cell><cell>C&amp;S</cell><cell>53.6</cell><cell>36.6</cell><cell>24.0</cell><cell>52.9</cell><cell>32.0</cell><cell>28.6</cell><cell>56.5</cell><cell>22.4</cell><cell>17.7</cell></row><row><cell>GAE [8]</cell><cell>C&amp;S</cell><cell>61.3</cell><cell>44.4</cell><cell>38.1</cell><cell>48.2</cell><cell>22.7</cell><cell>19.2</cell><cell>63.2</cell><cell>24.9</cell><cell>24.6</cell></row><row><cell>VGAE [8]</cell><cell>C&amp;S</cell><cell>64.7</cell><cell>43.4</cell><cell>37.5</cell><cell>51.9</cell><cell>24.9</cell><cell>23.8</cell><cell>69.6</cell><cell>28.6</cell><cell>31.7</cell></row><row><cell>MGAE [16]</cell><cell>C&amp;S</cell><cell>68.1</cell><cell>48.9</cell><cell>43.6</cell><cell>66.9</cell><cell>41.6</cell><cell>42.5</cell><cell>59.3</cell><cell>28.2</cell><cell>24.8</cell></row><row><cell>ARGE [49]</cell><cell>C&amp;S</cell><cell>64.0</cell><cell>44.9</cell><cell>35.2</cell><cell>57.3</cell><cell>35.0</cell><cell>34.1</cell><cell>68.1</cell><cell>27.6</cell><cell>29.1</cell></row><row><cell>ARVGE [49]</cell><cell>C&amp;S</cell><cell>63.8</cell><cell>45.0</cell><cell>37.4</cell><cell>54.4</cell><cell>26.1</cell><cell>24.5</cell><cell>63.5</cell><cell>23.2</cell><cell>22.5</cell></row><row><cell>ARGVA [50]</cell><cell>C&amp;S</cell><cell>71.1</cell><cell>52.6</cell><cell>49.5</cell><cell>58.1</cell><cell>33.8</cell><cell>30.1</cell><cell>69.0</cell><cell>30.5</cell><cell>30.6</cell></row><row><cell>DGI [51]</cell><cell>C&amp;S</cell><cell>71.3</cell><cell>56.4</cell><cell>51.1</cell><cell>68.8</cell><cell>44.4</cell><cell>45.0</cell><cell>53.3</cell><cell>18.1</cell><cell>16.6</cell></row><row><cell>AGC [52]</cell><cell>C&amp;S</cell><cell>68.9</cell><cell>53.7</cell><cell>48.6</cell><cell>67.0</cell><cell>41.1</cell><cell>41.9</cell><cell>69.8</cell><cell>31.6</cell><cell>31.9</cell></row><row><cell>DAEGC [11]</cell><cell>C&amp;S</cell><cell>70.4</cell><cell>52.8</cell><cell>49.6</cell><cell>67.2</cell><cell>39.7</cell><cell>41.0</cell><cell>67.1</cell><cell>26.6</cell><cell>27.8</cell></row><row><cell>GMM-VGAE</cell><cell>C&amp;S</cell><cell>71.5</cell><cell>53.1</cell><cell>47.4</cell><cell>67.5</cell><cell>40.7</cell><cell>42.4</cell><cell>71.1</cell><cell>29.9</cell><cell>33.0</cell></row><row><cell>[10]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AGE [22]</cell><cell>C&amp;S</cell><cell>76.1</cell><cell>59.9</cell><cell>54.5</cell><cell>70.1</cell><cell>44.3</cell><cell>45.4</cell><cell>70.9</cell><cell>30.8</cell><cell>32.9</cell></row><row><cell>R-DGAE</cell><cell>C&amp;S</cell><cell>73.7</cell><cell>56.0</cell><cell>54.1</cell><cell>70.5</cell><cell>45.0</cell><cell>47.1</cell><cell>71.4</cell><cell>34.4</cell><cell>34.6</cell></row><row><cell>R-GMM-VGAE</cell><cell>C&amp;S</cell><cell>76.7</cell><cell>57.3</cell><cell>57.9</cell><cell>68.9</cell><cell>42.0</cell><cell>43.9</cell><cell>74.0</cell><cell>33.4</cell><cell>37.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I PROOF OF PROPOSITION 4</head><p>Proposition 4. The gradient of the clustering loss L clus (Z(?)) w.r.t. the embedded representation z i can be expressed as:</p><p>Proof.</p><p>?L clus (Z(?)) ?z i =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX J PROOF OF THEOREM 2</head><p>Theorem 2. Given two GAE models Q 1 and Q 2 , which have the same GCN architecture and weights. Q 1 optimizes the objective function in Equation <ref type="bibr" target="#b20">(21)</ref> and Q 2 minimizes the loss function in Equation <ref type="formula">(22)</ref>, where f ? N N (d, d , L) and d d. Let ? * 1 be the Lipschitz constant of f ,Z i = (z jj ? z ij ) j,j ? R N ?d , ? i = ( z j ? z i 2 ) j ? R N , and a i is the i th row of A.</p><p>L Q2 = L clus (f (Z(?))) + ?L bce (?(Z(?)), A self ).</p><p>Proof.</p><p>Since ijmin ? 0 and ijmax ? 0, then we conclude that</p><p>We know that f 1 is a Lipschitz function, thus there exists ? 1 such that:</p><p>Based on Lemma 3, we have x i ? h self (x i ). Additionally, D self 1 and D 2 are differentiable functions, we can apply a first-order Taylor expansion with Peano's form of remainder at f 1 (x i ):</p><p>Hence</p><p>And since</p><p>Based on Lemma 2, we can write:</p><p>By applying the law of Cosines to compute f 1 (</p><p>, we obtain:</p><p>We have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX M PROOF OF THEOREM 5</head><p>Theorem 5. Given two models Q 1 and Q 2 , which optimize the same objective function as described by Equation <ref type="bibr" target="#b25">(26)</ref>. Q 1 has a single graph convolutional layer characterized by the function f 1 (X) = ReLU (? self XW 1 ), where W 1 represents the learning weights of this layer. Q 2 has two graph convolutional layers characterized by the function f 2 (X) = ReLU (? self ReLU (? self XW 1 ) W 2 ), where W 2 represents the learning weights of the second layer. We suppose that the Lipschitz constant ? * 1 of the second graph convolutional layer is less or equal to 1.</p><p>Under Assumption 1 and Assumption 2, we have:</p><p>Proof. Similar to Theorem 4, let h be an aggregation function such that h sup (x i ) = j? sup ij x j , and h self (x i ) = j? self ij x j . Let the functions D 1 and D 2 be distance metrics such that D sup</p><p>We add the following null expression to the equation of</p><p>We obtain</p><p>Hence</p><p>Based on Lemma 2</p><p>Then, based on Theorem 4, we can see that J 0.</p><p>The second graph convolutional layer is a Lipschitz function and its Lipschitz constant ? 1 is less or equal to 1. Hence</p><p>We know that f 2 and h self (f 1 ) are Lipschitz functions. Consequently, for all j and j indices, if x j ? x j 2 ? 0 then f 2 (x j ) ? f 2 (x j ) 2 ? 0 and h self (f 1 (x j )) ? h self (f 2 (x j )) 2 ? 0. So, globally, we have</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph embedded nonparametric mutual information for supervised dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arvanitopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="951" to="963" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Line: Largescale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>WWW</publisher>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pairwise constraint propagationinduced symmetric nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6348" to="6361" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gpens: Graph data learning with graph propagation-embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2609" to="2615" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collaborative graph convolutional networks: Unsupervised learning meets semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4215" to="4222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attributed graph clustering: A deep attentional embedding approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2609" to="2615" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1753" to="1759" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial deep embedded clustering: on a better trade-off between feature randomness and feature drift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrabah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bouguessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ksantini</surname></persName>
		</author>
		<idno type="DOI">file:/localhost/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/zero-shot/arxiv_pdf/10.1109/TKDE.2020.2997772</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What do neural networks learn when trained with random labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maennel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baldock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The early phase of neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mgae: Marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intrinsic dimension of data representations in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ansuini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Macke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoccolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="6111" to="6122" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="3355" to="3364" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive graph encoder for attributed graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="976" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">When does self-supervision help graph convolutional networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page">880</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep clustering with a dynamic autoencoder: From reconstruction towards centroids construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrabah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ksantini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lachiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="206" to="228" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="132" to="149" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="321" to="328" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="15" to="509" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5947" to="5956" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A pac-bayesian approach to spectrally-normalized margin bounds for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust large margin deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sokoli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSP</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4265" to="4280" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking graph regularization for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4573" to="4581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="2304" to="2313" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Decoupling &apos;when to update&apos; from &apos;how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="8527" to="8537" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Net: Degree-specific graph neural networks for node and graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="406" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Estimating the intrinsic dimension of datasets by a minimal neighborhood information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Facco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Errico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A critical analysis of selfsupervision, or what we can learn from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2609" to="2615" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning graph embedding with adversarial training methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2475" to="2487" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attributed graph clustering via adaptive graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4327" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
