<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Virtual Event, Israel Intermediate Supervision Signals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>March 8-12, 2021. March 8-12, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaole</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information System</orgName>
								<orgName type="institution">Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<email>jingjiang@smu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information System</orgName>
								<orgName type="institution">Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaole</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
						</author>
						<title level="a" type="main">Virtual Event, Israel Intermediate Supervision Signals</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the Fourteenth ACM International Conference on Web Search and Data Mining (WSDM &apos;21)</title>
						<meeting>the Fourteenth ACM International Conference on Web Search and Data Mining (WSDM &apos;21)						</meeting>
						<imprint>
							<date type="published">March 8-12, 2021. March 8-12, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3437963.3441753</idno>
					<note>ACM Reference Format: * Corresponding author. Virtual Event, Israel. ACM, New York, NY, USA, 10 pages. https: //</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge Base Question Answering</term>
					<term>Teacher-student Network</term>
					<term>Intermediate Supervision Signals</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-hop Knowledge Base Question Answering (KBQA) aims to find the answer entities that are multiple hops away in the Knowledge Base (KB) from the entities in the question. A major challenge is the lack of supervision signals at intermediate steps. Therefore, multi-hop KBQA algorithms can only receive the feedback from the final answer, which makes the learning unstable or ineffective.</p><p>To address this challenge, we propose a novel teacher-student approach for the multi-hop KBQA task. In our approach, the student network aims to find the correct answer to the query, while the teacher network tries to learn intermediate supervision signals for improving the reasoning capacity of the student network. The major novelty lies in the design of the teacher network, where we utilize both forward and backward reasoning to enhance the learning of intermediate entity distributions. By considering bidirectional reasoning, the teacher network can produce more reliable intermediate supervision signals, which can alleviate the issue of spurious reasoning. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our approach on the KBQA task. The code to reproduce our analysis is available at https://github.com/RichardHGL/WSDM2021_NSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Reasoning about belief and knowledge; Search with partial observations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge Base Question Answering (KBQA) is a challenging task that aims at finding answers to questions expressed in natural language from a given knowledge base (KB). Traditional solutions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36]</ref> usually develop a specialized pipeline consisting of multiple machine-learned or hand-crafted modules (e.g., named entity recognition, entity linking). Recently, end-to-end deep neural networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref> become the popular paradigm for this task by automatically learning data representations and network parameters.</p><p>For the KBQA task, there have been growing interests in solving complex questions that require a multi-hop reasoning procedure <ref type="bibr" target="#b19">[20]</ref>, called multi-hop KBQA. Besides the final answer, it is also important that a multi-hop KBQA algorithm can identify a reasonable relation path leading to the answer entities <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>. In some cases, even if the answer was correctly found, the relation path might be spurious. We present an example of spurious multi-hop reasoning in <ref type="figure" target="#fig_1">Fig. 1</ref>. The question is "what types are the films starred by actors in the nine lives of fritz the cat?". Besides the correct path (with red arrows), two spurious paths (with blue arrows) which include entities who are directors at the first step can also reach the correct answer. It is mainly due to the lack of supervision signals at the intermediate reasoning steps <ref type="bibr">(</ref> To address this issue, several studies formulate multi-hop KBQA as a reinforcement learning (RL) task <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>. They set up a policy-based agent to sequentially extend its inference path until it reaches a target entity. Its states are usually defined as tuple of query and current entity, and action as traverse on KB through outgoing edges of current entity. RL-based approaches heavily rely on the terminal reward to bias the search. To prevent spurious paths in the search, reward shaping <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref> and action dropout <ref type="bibr" target="#b21">[22]</ref> have been proposed to improve the model learning. However, these solutions either require expert experience or still lack effective supervision signals at intermediate steps.</p><p>Different from previous studies, our idea is to set up two models with different purposes for multi-hop KBQA. The main model aims to find the correct answer to the query, while the auxiliary model tries to learn intermediate supervision signals for improving the reasoning capacity of the main model. Specifically, the auxiliary model infers which entities at the intermediate steps are more relevant to the question, and these entities are considered as intermediate supervision signals. Although the idea is appealing, it is challenging to learn an effective auxiliary model, since we do not have such labeled data for training.</p><p>Our solution is inspired by the bidirectional search algorithms (e.g., bidirectional BFS <ref type="bibr" target="#b16">[17]</ref>) on graphs, in which an ideal path connecting the source and the destination can be more effectively identified with bidirectional exploration. Indeed, for KBQA we also have two different views to consider the task setting: the forward reasoning that finds the path starting from the topic entities (i.e., entities in the queries) to the answer entities and the backward reasoning that returns from answer entities to the topic entities. Most existing methods only consider forward reasoning. However, it is possible to jointly model the two reasoning processes, since topic entities and answer entities are all known in the training data. Such a bidirectional reasoning mechanism is able to incorporate additional self-supervision signals at intermediate steps. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the entity distribution obtained by forward reasoning at the second step should be similar to that from backward reasoning at the first step. Irrelevant entities "Devil's Doorway" and "Coffin Rock" are likely to be reached at the second reasoning step of forward reasoning but unreachable at the first step of backward reasoning. To maintain the correspondence between the two processes, we should avoid including the director "Robert Taylor" at the first step of forward reasoning. Such a potential correspondence is useful to improve the learning of each individual reasoning process at intermediate steps. That is the key point how we learn reliable intermediate supervision signals.</p><p>To this end, in this paper, we propose a novel teacher-student approach for the multi-hop KBQA task. Specifically, the student network (i.e., the main model), which aims to find the answer, is implemented by adapting the Neural State Machine (NSM) <ref type="bibr" target="#b13">[14]</ref> from visual question answering. In our approach, the student network can improve itself according to intermediate entity distributions learned from the teacher network. The major novelty lies in the design of the teacher network (i.e., the auxiliary model), which provides intermediate supervision signals. We utilize the correspondence between the state information from the forward and backward reasoning processes to enhance the learning of intermediate entity distributions. We further design two reasoning architectures that support the integration between forward and backward reasoning. By considering bidirectional reasoning, the teacher network can alleviate the issue of spurious reasoning, and produce more reliable intermediate supervision signals.</p><p>To evaluate our approach, we conduct extensive experiments on three benchmark datasets. Extensive experiments have demonstrated the effectiveness of our approach on the multi-hop KBQA task, especially for cases lacking training data. To the best of our knowledge, it is the first time that intermediate supervision signals have been explicitly learned with a teacher-student framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is closely related to the studies on KBQA, multi-hop reasoning and teacher-student framework.</p><p>Knowledge Base Question Answering. For the KBQA task, various methods have been developed over the last decade. They can be categorized into two groups: semantic parsing based methods and retrieval based methods. Semantic parsing based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> learn a semantic parser that converts natural language questions into intermediate logic forms, which can be executed against a KB. Retrieval-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref> directly retrieve answers from the KB in light of the information conveyed in the questions.</p><p>Recently, researchers pay more attention to multi-hop based KBQA. Some work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>  Multi-hop Reasoning. In recent years, multi-hop reasoning becomes a hot research topic for both computer vision and natural language processing domains. Min et al. <ref type="bibr" target="#b24">[25]</ref> proposed to decompose complex queries into several 1-hop queries and solved them by turn. Hudson et al. <ref type="bibr" target="#b12">[13]</ref> designed a novel recurrent Memory, Attention, and Composition (MAC) cell, which splits complex reasoning into a series of attention-based reasoning steps. Das et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref> conducted multi-hop reasoning on a graph under the reinforcement learning setting and treated every reasoning step as an edge transition on the graph. Besides, there are quite a few studies that adopt Graph Neural Network (GNN) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref> to conduct explicit reasoning on graph structure <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Teacher-student Framework. Knowledge distillation (KD) is introduced and generalized by early work <ref type="bibr" target="#b9">[10]</ref>. They proposed a teacher-student framework, where a complicated high-performance model and a light-weight model are treated as teacher and student respectively. The predictions of the teacher model are treated as "soft labels" and the student model is trained to fit the soft labels.</p><p>While knowledge distillation was initially proposed for model compression, recent work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39]</ref> found that applying the soft labels as the training target can help the student achieve better performance.</p><p>Several studies also apply the teacher-student framework in question answering task. Yang et al. <ref type="bibr" target="#b34">[35]</ref> designed a multi-teacher knowledge distillation paradigm in a Web Question Answering system. Do et al. <ref type="bibr" target="#b3">[4]</ref> and Hu et al. <ref type="bibr" target="#b10">[11]</ref> applied the teacher-student framework to visual question answering task and reading comprehension task, respectively. In this work, we try to address spurious reasoning caused by weak supervision in the multi-hop KBQA task with an elaborate teacher-student framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>In this section, we introduce the background and define the task.</p><p>Knowledge Base (KB). A knowledge base typically organizes factual information as a set of triples, denoted by G = {? , , ? ?| , ? ? E, ? R}, where E and R denote the entity set and relation set, respectively. A triple ? , , ? ? denotes that relation exists between head entity and tail entity ? . Furthermore, we introduce entity neighborhood to denote the set of triples involving an entity , denoted by N = {? , , ? ? ? G} ? {? ? , , ? ? G}, containing both incoming and outgoing triples for . For simplicity, we replace a triple ? , , ? ? with its reverse triple ? ? , ?1 , ?, so that we can have N = {? ? , , ? ? G}. For convenience, we further use italic bold fonts to denote the embeddings of entities or relations. Let ? R ? | E | and ? R ? | R | denote the embedding matrices for entities and relations in the KB, respectively, and each column vector ? R or ? R is a -dimensional embedding for entity or relation .</p><p>Knowledge Base Question Answering (KBQA). We focus on factoid question answering over a knowledge base. We assume that a KB G is given as the available resource and the answers will be the entities in G. Formally, given a natural language question = { 1 , 2 , ..., } and a KB G, the task of KBQA is to figure out the answer entitie(s), denoted by the set A , to query from the candidate entity set E. The entities mentioned in a question are called topic entities. Specially, we consider solving complex questions where the answer entities are multiple hops away from the topic entities in the KB, called multi-hop KBQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE PROPOSED APPROACH</head><p>In this section, we present the proposed approach for the multi-hop KBQA task under the teacher-student framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>A major difficulty for multi-hop KBQA is that it usually lacks supervision signals at intermediate reasoning steps, since only the answer entities are given as ground-truth information. To tackle this issue, we adopt the recently proposed teacher-student learning framework <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. The main idea is to train a student network that focuses on the multi-hop KBQA task itself, while another teacher network is trained to provide (pseudo) supervision signals (i.e., inferred entity distributions in our task) at intermediate reasoning steps for improving the student network.</p><p>In our approach, the student network is implemented based on Neural State Machine (NSM) <ref type="bibr" target="#b13">[14]</ref>, which was originally proposed for visual question answering on scene graph extracted from image data. We adapt it to the multi-hop KBQA task by considering KB as a graph, and maintain a gradually learned entity distribution over entities during the multi-hop reasoning process. To develop the teacher network, we modify the architecture of NSM by incorporating a novel bidirectional reasoning mechanism, so that it can learn more reliable entity distributions at intermediate reasoning steps, which will be subsequently used by the student network as the supervision signals.</p><p>In what follows, we first describe the adapted architecture of NSM for multi-hop KBQA, and then present the teacher network and model learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural State Machine for Multi-hop KBQA</head><p>We present an overall sketch of NSM in <ref type="figure">Fig</ref>  Embedding update</p><formula xml:id="formula_0">+ (") (") ("&amp;$)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution update</head><p>Embedding update ? { } ( <ref type="figure" target="#fig_3">Figure 2</ref>: Illustration of the two reasoning steps for neural state machine on question "which person directed the movies starred by john krasinski?". In different reasoning steps, the instruction vector focuses on different parts of the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Instruction Component.</head><p>We first describe how to transform a given natural language question into a series of instruction vectors that control the reasoning process. The input of the instruction component consists of a query embedding and an instruction vector from the previous reasoning step. The initial instruction vector is set as zero vector. We utilize GloVe <ref type="bibr" target="#b25">[26]</ref> to obtain the embeddings of the query words. Then we adopt a standard LSTM encoder to obtain a set of hidden states { } =1 , where ? R and is the length of the query. After that, the last hidden state is considered to be the question representation, i.e., = . Let ( ) ? R denote the instruction vector at the -th reasoning step. We adopt the following method to learn the instruction vector ( ) :</p><formula xml:id="formula_1">( ) = ?? =1 ( ) , ( ) = softmax ( ( ) ? ) + , ( ) = ( ) [ ( ?1) ; ] + ( ) ,<label>(1)</label></formula><p>where ( ) ? R ?2 , ? R ? and ( ) , ? R are parameters to learn. The core idea is to attend to specific parts of a query when learning the instruction vectors at different time steps. In such a process, we also dynamically update the query representation, so that it can incorporate the information of previous instruction vectors. By repeating the process above, we can obtain a list of instruction vectors { ( ) } =1 after reasoning steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Reasoning Component.</head><p>Once we obtain the instruction vector ( ) , we can use it as a guiding signal for the reasoning component. The input of the reasoning component consists of the instruction vector of the current step, and the entity distribution and entity embeddings obtained from the previous reasoning step. The output of the reasoning component includes the entity distribution ( ) and the entity embeddings { ( ) }. First, we set the initial entity embeddings by considering the relations involving :</p><formula xml:id="formula_2">(0) = ?? ? ? , , ? ?N ? ,<label>(2)</label></formula><p>where ? R ? are the parameters to learn. Unlike previous studies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31]</ref>, we explicitly utilize the information of related relation types for encoding entities. In the multi-hop KBQA task, a reasoning path consisting of multiple relation types can reflect important semantics that lead to the answer entities. Besides, such a method is also useful to reduce the influence of noisy entities, and easy to apply to unseen entities of known context relations. Note that we do not use the original embedding of when initializing (0) because for intermediate entities along the reasoning path the identifiers of these entities are not important; it is the relations that these intermediate entities are involved in that matter the most.</p><p>Given a triple ? ? , , ?, a match vector ( ) ? ? , , ? is learned by matching the current instruction ( ) with relation vector :</p><formula xml:id="formula_3">( ) ? ? , , ? = ( ) ? ,<label>(3)</label></formula><p>where ? R ? are the parameters to learn. Furthermore, we aggregate the matching messages from neighboring triples and assign weights to them according to how much attention they receive at the last reasoning step:</p><formula xml:id="formula_4">( ) = ?? ? ? , , ? ?N ( ?1) ? ? ( ) ? ? , , ? ,<label>(4)</label></formula><p>where ( ?1) ? is the assigned probability of entity ? at the last reasoning step, which we will explain below. Such a representation is able to capture the relation semantics associated with an entity in the KB. Then, we update entity embeddings as follows:</p><formula xml:id="formula_5">( ) = FFN([ ( ?1) ;?( ) ]),<label>(5)</label></formula><p>where FFN(?) is a feed-forward layer taking as input of both previous embedding ( ?1) and relation-aggregated embedding?( ) . Through such a process, both the relation path (from topic entities to answer entities) and its matching degree with the question can be encoded into node embeddings. The probability distribution over intermediate entities derived at step can be calculated as:</p><formula xml:id="formula_6">( ) = softmax ( ) ,<label>(6)</label></formula><p>where ( ) is a matrix where each column vector is the embedding of an entity at the -th step, and ? R are the parameters that derive the entity distribution ( ) , and ( ) is the updated entity embedding matrix by Eq. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Discussion.</head><p>For our task, the reason that we adopt the NSM model as the student network are twofold. First, our core idea is to utilize intermediate entity distributions derived from the teacher network as the supervision signals for the student network. In contrast, most previous multi-hop KBQA methods do not explicitly maintain and learn such an entity distribution at intermediate steps. Second, NSM can be considered as a special graph neural network, which has excellent reasoning capacity over the given knowledge graph. As shown in Section 4.2.2, the learning of entity distributions and entity embeddings can indeed correspond to the general "propagate-then-aggregate" update mechanism of graph neural networks. We would like to utilize such a powerful neural architecture to solve the current task. The NSM <ref type="bibr" target="#b13">[14]</ref> was proposed to conduct visual reasoning in an abstract latent space. We make two major adaptations for multihop KBQA. First, in Eq. 2, we initialize the node embeddings by aggregating the embeddings of those relations involving the entity. In our task, the given KB is usually very large. An entity is likely to be linked to a large number of other entities. Our initialization method is able to reduce the influence of noisy entities, focusing on the important relational semantics. Besides, it is also easy to generalize to new or unseen entities with known relations, which is especially important to incremental training. Second, in Eq. 5, we update entity embeddings by integrating previous embedding ( ?1) and relation-aggregated embedding?( ) . For comparison, original NSM <ref type="bibr" target="#b13">[14]</ref> separately modeled the two parts, whereas we combine the two factors in a unified update procedure, which is useful to derive more effective node embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Teacher Network</head><p>Different from the student network, the teacher network aims to learn or infer reliable entity distributions at intermediate reasoning steps. Note that there are no such labeled entity distributions for training the teacher network. Instead, inspired by the bidirectional search algorithm (e.g., bidirectional BFS <ref type="bibr" target="#b16">[17]</ref>), we incorporate the bidirectional reasoning mechanism for enhancing the learning of intermediate entity distributions in the teacher network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Bidirectional</head><p>Reasoning for Multi-hop KBQA. Given a knowledge base, the reasoning process for multi-hop KBQA can be considered to be an exploration and search problem on the graph. Most existing multi-hop KBQA methods start from the topic entities and then look for the possible answer entities, called forward reasoning. On the other hand, the opposite search from answer entities to</p><formula xml:id="formula_7">! (#) , ! (#) ! (%) , ! (%) ! (&amp;) , ! (&amp;) ! (') , ! (') ( (%) , ( (%) ( (&amp;) , ( (&amp;) ( (') , ( (') ( (%) ( (&amp;) ( (') ( (&amp;) ( (%) ! (') ! (&amp;) ! (%) ! (%) ! (&amp;) ( (#) , (<label>(#)</label></formula><p>(a) Illustration of 3-hop parallel reasoning.  topic entities (which we refer to as backward reasoning) has been neglected by previous studies. Our core idea is to consider the exploration in both directions and let the two reasoning processes synchronize with each other at intermediate steps. In this way, the derived intermediate entity distributions can be more reliable than those learned from a single direction. More specifically, given a -hop reasoning path, let ( ) and ( ? ) denote the entity distributions from the forward reasoning at the -th step and from the backward reasoning at the ( ? )-th step, respectively. The key point is that the two distributions should be similar or consistent if the two reasoning processes have been stable and accurate, i.e., ( ) ? ( ? ) . We will utilize such a correspondence as constraints in the following models.</p><formula xml:id="formula_8">(") $ (%) , $ (%) $ (") , $ (") (") (&amp;) (&amp;) $ (&amp;) , $ (&amp;) $ (') , $ (') ( (") , (<label>(")</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Reasoning</head><p>Architectures. Based on the idea above, we design two kinds of neural architectures for the teacher network, namely parallel reasoning and hybrid reasoning.</p><p>Parallel Reasoning. The first way is to set up two separate NSMs for both forward and backward reasoning, respectively. These two NSM networks are relatively isolated, and do not share any parameters. We only consider incorporating correspondence constraints on the intermediate entity distributions between them.</p><p>Hybrid Reasoning. In the second way, we share the same instruction component and arrange the two reasoning processes in a cycled pipeline. Besides the correspondence constraints, the two processes receive the same instruction vectors. Furthermore, the derived information at the final step of the forward reasoning is fed into the backward reasoning as initial values. Formally, the following equations hold in this case:</p><formula xml:id="formula_9">(0) = ( ) , (0) = ( ) , ( ) = ( +1? ) , = 1, ..., .<label>(7)</label></formula><p>We present the illustrative examples of the parallel reasoning and hybrid reasoning in <ref type="figure" target="#fig_6">Fig. 3(a)</ref> and <ref type="figure" target="#fig_6">Fig. 3(b)</ref>. Comparing the two reasoning architectures, it can be seen that parallel reasoning has a more loose integration, while hybrid reasoning requires a deeper fusion between the information from both reasoning processes. Unlike bidirectional BFS, in our task, backward reasoning might not be able to exactly mimic the inverse process of forward reasoning, since the two processes correspond to different semantics in multihop KBQA. Considering this issue, we share the instruction vectors and recycle the final state of the forward reasoning for initializing backward reasoning. In this way, backward reasoning receives more information about forward reasoning, so that it can better trace back the reasoning path of forward reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Learning with the Teacher-Student Framework</head><p>In this part, we present the details of model learning with our teacher-student framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.1</head><p>Optimizing the Teacher Network. The two reasoning architectures of the teacher network can be optimized in the same way. We mainly consider two parts of loss, namely reasoning loss and correspondence loss. The reasoning loss reflects the capacity of predicting the accurate entities, which can be decomposed into two directions:</p><formula xml:id="formula_10">L = ( ) , * , L = ( ) , * ,<label>(8)</label></formula><p>where ( ) ( ( ) ) denotes the final entity distribution for forward (backward) reasoning process, * ( * ) denotes the groundtruth entity distribution, and (?, ?) is the Kullback-Leibler divergence <ref type="bibr" target="#b17">[18]</ref>, which measures the difference between the two distributions in an asymmetric way. To obtain * and * , we transform the occurrences of ground-truth entities into a frequency-normalized distribution. Specifically, if entities in the graph are ground-truth entities, they are assigned a probability of 1 in the final distribution.</p><p>The correspondence loss reflects the consistency degree between intermediate entity distributions from the two reasoning processes. It can be computed by summing the loss at each intermediate step:</p><formula xml:id="formula_11">L = ?1 ?? =1 ( ) , ( ? ) ,<label>(9)</label></formula><p>where (?, ?) is the Jensen-Shannon divergence <ref type="bibr" target="#b7">[8]</ref>, which measures the difference between two distributions in a symmetric way.</p><p>To combine the above loss terms, we define the entire loss function of the teacher network L as:</p><formula xml:id="formula_12">L = L + L + L ,<label>(10)</label></formula><p>where ? (0, 1) and ? (0, 1) are hyper-parameters to control the weights of the factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.2</head><p>Optimizing the Student Network. After the teacher model is trained to convergence, we can obtain intermediate entity distributions in the two reasoning processes of the teacher network. We take the average of the two distributions as the supervision signal:</p><formula xml:id="formula_13">( ) = 1 2 ( ) + ( ? ) , = 1, ..., ? 1<label>(11)</label></formula><p>As described before, we adopt the NSM model as the student network to conduct forward reasoning. Besides the reasoning loss, we also incorporate the loss between the predictions of the student network and the supervision signal of the teacher network:</p><formula xml:id="formula_14">L 1 = ( ( )</formula><p>, * ),</p><formula xml:id="formula_15">L 2 = ?1 ?? =1 ( ( ) , ( ) ), L = L 1 + L 2 .<label>(12)</label></formula><p>where ( ) and ( ) denote the intermediate entity distributions</p><p>at the -th step from the teacher network and student network, respectively, and is a hyperparameter to tune. In practice, labeled data for intermediate reasoning steps is seldom available. Most existing methods only rely on the final answer to learn the entire model, which may not be well trained or form spurious reasoning paths. Our approach adopts the teacher network for improving the student network. The main novelty is to utilize both forward and backward reasoning in producing more reliable intermediate entity distributions. Note that we do not incorporate any additional labeled data for training intermediate reasoning steps in the teacher network. Instead, we try to learn such intermediate entity distributions by enforcing the correspondence in the bidirectional reasoning process. To our knowledge, backward reasoning has been seldom considered in multi-hop KBQA task, especially its correspondence with forward reasoning. Such an idea is indeed related to recent progress in self-supervised learning <ref type="bibr" target="#b14">[15]</ref>, in which we leverage internal supervision signal to learn the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>In this section, we perform the evaluation experiments for our approach on the KBQA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We adopt three benchmark datasets for the multi-hop KBQA task:</p><p>MetaQA <ref type="bibr" target="#b37">[38]</ref> contains more than 400k single and multi-hop (up to 3-hop) questions in the domain of movie, containing three datasets, namely MetaQA-1hop, MetaQA-2hop and MetaQA-3hop.</p><p>WebQuestionsSP (webqsp) <ref type="bibr" target="#b35">[36]</ref> contains 4737 natural language questions that are answerable using Freebase as the knowledge base. The questions require up to 2-hop reasoning from knowledge base. We use the same train/dev/test splits as GraftNet <ref type="bibr" target="#b30">[31]</ref>. <ref type="bibr" target="#b31">[32]</ref> is generated from WebQuestionsSP by extending the question entities or adding constraints to answers. There are four types of question: composition (45%), conjunction (45%), comparative (5%), and superlative (5%). The questions require up to 4-hops of reasoning on the KB. Follow- ing <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, we use the topic entities labeled in original datasets and adopt PageRank-Nibble algorithm (PRN) <ref type="bibr" target="#b0">[1]</ref> to find KB entities close to them. With these entities, we can obtain a relatively small subgraph that is likely to contain the answer entity. For CWQ and webqsp datasets, we first obtain the neighborhood graph within two hops of topic entities and then run PRN algorithm on it. We further expand one hop for CVT entities in Freebase to obtain the neighborhood subgraph. As shown in <ref type="table" target="#tab_1">Table 1</ref>, 2-hop graphs are sufficient to cover most of the answer entities. While on MetaQA datasets, we run PRN algorithm on the entire KB. Specifically, we use the PRN algorithm [1] with = 1 ?6 and then select the top-scoring entities. We set = 500 for the smaller MetaQA KB and = 2000 for larger Freebase. For the reserved triples, both their head and tail entities are obtained from the top entities identified by PRN algorithm. We summarize the statistics of the three datasets in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex WebQuestions 1.1 (CWQ)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Evaluation</head><p>Protocol. We follow <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> to cast the multi-hop KBQA task as a ranking task for evaluation. For each test question in a dataset, a list of answers are returned by a model according to their predictive probabilities. We adopt two evaluation metrics widely used in previous works, namely Hits@1 and F1. Specifically, Hits@1 refers to whether the top answer is correct. For all the methods, we learn them using the training set, and optimize the parameters using the validation set and compare their performance on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Methods to</head><p>Compare. We consider the following methods for performance comparison:</p><p>? KV-Mem [24] maintains a memory table for retrieval, which stores KB facts encoded into key-value pairs.</p><p>? GraftNet [31] adopts a variant of graph convolution network to perform multi-hop reasoning on heterogeneous graph.</p><p>? PullNet <ref type="bibr" target="#b29">[30]</ref> utilizes the shortest path as supervision to train graph retrieval module and conduct multi-hop reasoning with Graft-Net on the retrieved sub-graph.</p><p>? SRN <ref type="bibr" target="#b26">[27]</ref> is a multi-hop reasoning model under the RL setting, which solves multi-hop question answering through extending inference paths on knowledge base.</p><p>? EmbedKGQA <ref type="bibr" target="#b28">[29]</ref> conducts multi-hop reasoning through matching pretrained entity embedings with question embedding obtained from RoBERTa <ref type="bibr" target="#b22">[23]</ref>.</p><p>? NSM, NSM + and NSM +? are three variants of our model, which (1) do not use the teacher network, (2) use the teacher network with parallel reasoning, and (3) use the teacher network with hybrid reasoning, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Implementation Details.</head><p>Before training the student network, we pre-train the teacher network on multi-hop KBQA task. To avoid overfitting, we adopt early-stopping by evaluating Hits@1 on the validation set every 5 epochs. We optimize all models with Adam optimizer, where the batch size is set to 40. The learning rate is tuned amongst {0.01, 0.005, 0.001, 0.0005, 0.0001}. The reasoning steps is set to 4 for CWQ dataset, while 3 for other datasets. The coefficient (in Eq. 12) and , (in Eq. 10) are tuned amongst {0.01, 0.05, 0.1, 0.5, 1.0}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The results of different methods for KBQA are presented in <ref type="table">Table 2</ref>. It can be observed that:</p><p>(1) Among the baselines, KV-Mem performs the worst. This is probably because it does not explicitly consider the complex reasoning steps. Most methods perform very well on the MetaQA-1hop and MetaQA-2hop datasets, which require only up to 2 hops of reasoning. On the other hand, the other datasets seem to be more difficult, especially the webqsp and CWQ datasets. Overall, Em-bedKGQA and PullNet are better than the other baselines. PullNet trains an effective subgraph retrieval module based on the shortest path between topic entities and answer entities. Such a module is specially useful to reduce the subgraph size and produce highquality candidate entities.</p><p>(2) Our base model (i.e., the single student network) NSM performs better than the competitive baselines in most cases. It is developed based on a graph neural network with two novel extensions for this task (Sec. 4.2). The gains of teacher-student framework show variance on different datasets. Specifically, on the two most difficult datasets, namely Webqsp and CWQ, the variants of NSM + and NSM +? are substantially better than NSM and other baselines. These results have shown the effectiveness of the teacher network in our approach, which largely improves the student network. Different from SRN and PullNet, our approach designs a novel bidirectional reasoning mechanism to learn more reliable intermediate supervision signals. Comparing NSM + and NSM +? , we find that their results are similar. On Webqsp and CWQ datasets, the hybrid reasoning is slightly better to improve the student network than parallel reasoning. <ref type="table">Table 2</ref> has shown that our approach overall has a better performance. Next, we perform a series of detailed analysis experiments. For clarity, we only incorporate the results of NSM as the reference, since it performs generally well among all the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Detailed Performance Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Ablation Study.</head><p>Previous experiments have indicated that the major improvement is from the contribution of the teacher network. Here, we compare the effect of different implementations <ref type="table">Table 2</ref>: Performance comparison of different methods for KBQA (Hits@1 in percent). We copy the results for KV-Mem, GraftNet and PullNet from <ref type="bibr" target="#b29">[30]</ref>, and copy the results for SRN and EmbedKGQA from <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>. Bold and underline fonts denote the best and the second best methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Webqsp MetaQA- of the teacher network. The compared variants include: (1) NSM + using only the forward reasoning (unidirectional); (2) NSM + using only the backward reasoning (unidirectional); (3) NSM + using the parallel reasoning (bidirectional); (4) NSM +? using the hybrid reasoning (bidirectional); (5) NSM + ,? removing the correspondence loss (Eq. 9) from NSM + ; and (6) NSM +?,? removing the correspondence loss (Eq. 9) from NSM +? . In <ref type="table" target="#tab_3">Table 3</ref>, we can see that unidirectional reasoning is consistently worse than bidirectional reasoning: the variants of NSM + and NSM + have a lower performance than the other variants. Such an observation verifies our assumption that bidirectional reasoning can improve the learning of intermediate supervision signals. Besides, by removing the correspondence loss from the teacher network, the performance substantially drops, which indicates that forward and backward reasoning can mutually enhance each other.    Step 2 Step 1</p><p>Step 3</p><p>(b) The teacher network with hybrid reasoning. Step 2 Step 1</p><p>Step 3 w ri te rs (c) The student network after improvement. <ref type="figure">Figure 5</ref>: A case from the MetaQA-3hop dataset. We use green, red, yellow and grey circles to denote the topic entity, correct answer, intermediate entities and irrelevant entities respectively. The red colored edges denote the actual reasoning paths for different methods. The color darkness indicates the relevance degree of an entity by a method. For simplicity, we only visualize the entities with a probability equal to or above 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Parameter</head><p>Tuning. In our approach, we have several combination coefficients to tune, including in Eq. 12, and and in Eq. 10. We first tune amongst {0.01, 0.05, 0.1, 0.5, 1.0}, which controls the influence of the teacher network on the student network. As shown in <ref type="figure" target="#fig_8">Fig. 4</ref>, hybrid reasoning seems to work well with small (e.g., 0.05), while parallel reasoning works better with relatively large (e.g., 1.0). Similarly, we can tune the parameters of and . Overall, we find that = 0.01 and = 0.1 are good choices for our approach. Another parameter to tune is the embedding dimension (which is set to 100), and we do not observe significant improvement when &gt; 100. The reasoning steps should be adjusted for different datasets. We observe that our approach achieves the best performance on CWQ dataset with = 4, while = 3 for the other datasets with exhaustive search. Due to space limit, we omit these tuning results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Evaluating Intermediate Entities.</head><p>A major assumption we made is that our teacher network can obtain more reliable intermediate entities than the student network. Here, we compare the performance of the two networks in finding intermediate entities.</p><p>Since the MetaQA-3hop dataset is created using pre-defined templates, we can recover the ground-truth entities at intermediate hops. We consider it a retrieval task and adopt the standard Precision, Recall and F1 as evaluation metrics. From <ref type="table" target="#tab_6">Table 4</ref>, we can see that the teacher network is much better than the student network in finding intermediate entities, but has slightly worse performance at the second hop. Note that the results of the third hop have been omitted, since it is the last hop. Since the student network only utilizes forward reasoning, the results of the first hop are more important than those of subsequent hops. These results also explain why our teacher-student approach is better than the single student model. <ref type="table">Table 2</ref>, we have found that the improvement of our approach over the basic NSM model is very small on the MetaQA datasets. We suspect that this is because the amount of training data for MetaQA is more than sufficient: 100 training cases for no more than 300 templates in each dataset. To examine this, we randomly sample a single training case for every question template from the original training set, which forms a oneshot training dataset. We evaluate the performance of our approach trained with this new training dataset. The results are shown in <ref type="table" target="#tab_7">Table 5</ref>. As we can see, our approach still works very well, and the improvement over the basic NSM becomes more substantial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">One-Shot Evaluation. In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study</head><p>The major novelty of our approach lies in the teacher network. Next, we present a case study for illustrating how it helps the student network.</p><p>Given the question "what types are the movies written by the screenwriter of the music lovers", the correct reasoning path is "The Music Lovers" (movie) ? written by "Melvyn Bragg" (screenwriter) ? write "Play Dirty" (movie) ? has genre "War" (genre). Note that "Isadora" is also qualified at the second step. However, its genre is missing in the KB. <ref type="figure">Fig. 5</ref> presents a comparison between the learned results of the student before improvement (i.e., without the teacher network), the teacher network and the student network after improvement.</p><p>As shown in <ref type="figure">Fig. 5(a)</ref>, the original student network has selected a wrong path leading to an irrelevant entity. At the first hop, NSM mainly focuses on the two entities "Ken Russell" and "Melvyn Bragg" with probabilities of 0.48 and 0.51 respectively. Since it mistakenly includes "Ken Russell" (director of "The Music Lovers") at the first reasoning step, it finally ranks "Drama" as the top entity and chooses an irrelevant entity as the answer. In comparison, the teacher network ( <ref type="figure">Fig. 5(b)</ref>) is able to combine forward and backward reasoning to enhance the intermediate entity distributions. As we can see, our teacher assigns a very high probability of 0.99 to the entity "Melvyn Bragg" at the first step. When the supervision signals of the teacher are incorporated into the student, it correctly finds the answer entity "War" with a high probability of 0.99 ( <ref type="figure">Fig. 5(c)</ref>).</p><p>This example has shown that our teacher network indeed provides very useful supervision signals at intermediate steps to improve the student network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we developed an elaborate approach based on teacherstudent framework for the multi-hop KBQA task. In our approach, the student network implemented by a generic neural state machine focuses on the task itself, while the teacher network aims to learn intermediate supervision signals to improve the student network. For the teacher network, we utilized the correspondence between state information from a forward and a backward reasoning process to enhance the learning of intermediate entity distributions. We further designed two reasoning architectures that support the integration between forward and backward reasoning. We conducted evaluation experiments with three benchmark datasets. The results show that our proposed model is superior to previous methods in terms of effectiveness for the multi-hop KBQA task.</p><p>Currently, we adopt the NSM model as the student network. It is flexible to extend our approach to other neural architectures or learning strategies on graphs. In the future, we will also consider enhancing the entity embeddings using KB embedding methods, and obtain better intermediate supervision signals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Intermediate</head><label></label><figDesc>Supervision Signals. In Proceedings of the Fourteenth ACM International Conference on Web Search and Data Mining (WSDM '21), March 8-12, 2021, Virtual Event, Israel. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3437963.3441753</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>A spurious case from MetaQA-3hop dataset. We use green, red, yellow and grey circles to denote the topic entities, correct answer, intermediate entities and irrelevant entities respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>employed classical methods (e.g., Variational Reasoning Network, Key-Value Memory Network and Graph Convolution Network) to conduct multi-hop reasoning within the KB. Moreover, Sun et al. [30] and Saxena et al. [29] leveraged extra corpus and enriched knowledge graph embeddings to boost the performance of multi-hop KBQA. However, these methods take the performance of final prediction as the only objective, which are vulnerable to the spurious examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>. 2 .</head><label>2</label><figDesc>It mainly consists of an instruction component and a reasoning component. The instruction component sends instruction vectors to the reasoning component, while the reasoning component infers the entity distribution and learns the entity representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Illustration of 3-hop hybrid reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the designed teacher architectures. We use blue, yellow and orange squares to denote the instruction component, forward reasoning component and backward reasoning component, respectively. The dotted arrows link the corresponding intermediate entity distributions of the two reasoning processes. We use and as subscripts to distinguish forward reasoning and backward reasoning, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Varying on CWQ dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Performance tuning of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>which we call intermediate supervision signals). For the multi-hop KBQA task, training data is typically in the form of ? , ? instead of the ideal arXiv:2101.03737v2 [cs.CL] 7 Apr 2021 form of ? , ??. Therefore, multi-hop reasoning algorithms can only receive the feedback at the final answer using such datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of all datasets. "#entity" denotes average number of entities in subgraph, and "coverage" denotes the ratio of at least one answer in subgraph.</figDesc><table><row><cell>Datasets</cell><cell>Train</cell><cell>Dev</cell><cell cols="3">Test #entity coverage</cell></row><row><cell>MetaQA-1hop</cell><cell>96,106</cell><cell>9,992</cell><cell>9,947</cell><cell>487.6</cell><cell>100%</cell></row><row><cell cols="4">MetaQA-2hop 118,980 14,872 14,872</cell><cell>469.8</cell><cell>100%</cell></row><row><cell cols="4">MetaQA-3hop 114,196 14,274 14,274</cell><cell>497.9</cell><cell>99.0%</cell></row><row><cell>webqsp</cell><cell>2,848</cell><cell>250</cell><cell cols="2">1,639 1,429.8</cell><cell>94.9%</cell></row><row><cell>CWQ</cell><cell>27,639</cell><cell>3,519</cell><cell cols="2">3,531 1,305.8</cell><cell>79.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of the teacher network (in percent).</figDesc><table><row><cell></cell><cell></cell><cell>Models</cell><cell>Webqsp Hits F1</cell><cell>CWQ Hits F1</cell></row><row><cell></cell><cell></cell><cell>NSM</cell><cell>68.7 62.8 47.6 42.4</cell></row><row><cell></cell><cell></cell><cell>NSM +</cell><cell>70.7 64.7 47.2 41.5</cell></row><row><cell></cell><cell></cell><cell>NSM +</cell><cell>71.1 65.4 47.1 42.7</cell></row><row><cell></cell><cell></cell><cell>NSM + ,?</cell><cell>72.5 66.5 47.7 42.7</cell></row><row><cell></cell><cell></cell><cell>NSM +?,?</cell><cell>73.0 66.9 47.5 42.1</cell></row><row><cell></cell><cell></cell><cell>NSM +</cell><cell>73.9 66.2 48.3 44.0</cell></row><row><cell></cell><cell></cell><cell>NSM +?</cell><cell>74.3 67.4 48.8 44.0</cell></row><row><cell></cell><cell>0.757</cell><cell></cell><cell>NSM+h</cell></row><row><cell></cell><cell>0.744</cell><cell></cell><cell>NSM+p</cell></row><row><cell>Hits@1</cell><cell>0.731</cell><cell></cell></row><row><cell></cell><cell>0.718</cell><cell></cell></row><row><cell></cell><cell>0.705</cell><cell cols="2">? 0.01 0.05 0.1 0.5 1.0</cell></row></table><note>(a) Varying on webqsp dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison w.r.t. different hops on MetaQA-3hop dataset (in percent). 60.6 60.4 99.9 70.2 80.8 Teacher + 80.0 59.0 66.3 95.0 68.9 78.8 Teacher +? 99.9 56.0 70.9 99.7 63.0 75.4</figDesc><table><row><cell>Models</cell><cell>Hop 1 Pre Rec</cell><cell>F1</cell><cell>Hop 2 Pre Rec</cell><cell>F1</cell></row><row><cell>Student</cell><cell>61.0</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results under one-shot setting (in percent). 93.9 98.7 96.4 97.0 79.8 NSM +? 93.9 93.7 98.4 95.8 95.6 81.6</figDesc><table><row><cell>Models</cell><cell>MetaQA-1 Hits F1</cell><cell>MetaQA-2 Hits F1</cell><cell>MetaQA-3 Hits F1</cell></row><row><cell>NSM</cell><cell cols="3">93.3 92.6 97.7 96.0 90.6 74.5</cell></row><row><cell>NSM +</cell><cell>94.3</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We thank Kun Zhou and Junyi Li for the helpful discussions. This work is partially supported by the National Research Foundation, Singapore under its International Research Centres in Singapore </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Local Graph Partitioning using PageRank Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic Parsing on Freebase from Question-Answer Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Compact Trilinear Interaction for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuong</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huy</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erman</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Question answering over freebase with multi-column convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Benefits of Intermediate Annotations in Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5627" to="5634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building Watson: An Overview of the DeepQA Project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Welty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Jensen-Shannon divergence and Hilbert space embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bent</forename><surname>Fuglede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flemming</forename><surname>Tops?e</surname></persName>
		</author>
		<idno>ISIT 2004. 31</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Born-Again Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">Chase</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attention-Guided Answer Distillation for Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Language-Conditioned Graph Networks for Relational Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Compositional Attention Networks for Machine Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning by Abstraction: The Neural State Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5901" to="5914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Divide and Conquer Bidirectional Search: First Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Korf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 99</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1184" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Query Graph Generation for Answering Multihop Complex Questions from Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="969" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-hop Knowledge Base Question Answering with an Iterative Sequence Matching Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-Hop Knowledge Graph Reasoning with Reward Shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Xi Victoria Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3243" to="3253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Key-Value Memory Networks for Directly Reading Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-hop Reading Comprehension through Question Decomposition and Rescoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stepwise Reasoning for Multi-Relation Question Answering over Knowledge Graph with Weak Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="474" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">FitNets: Hints for Thin Deep Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving Multihop Question Answering over Knowledge Graphs using Knowledge Base Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditay</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4498" to="4507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tania</forename><surname>Bedrax-Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4231" to="4242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Web as a Knowledge-Base for Answering Complex Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Question Answering on Freebase via Relation Extraction and Textual Evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wutao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="690" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The Value of Semantic Parse Labeling for Knowledge Base Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Variational Reasoning for Question Answering with Knowledge Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Mutual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
