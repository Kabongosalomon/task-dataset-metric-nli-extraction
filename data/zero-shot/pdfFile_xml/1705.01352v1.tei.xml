<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optical Flow in Mostly Rigid Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
							<email>jonas.wulff@tue.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tue.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optical Flow in Mostly Rigid Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Overview. From three frames (a) our method computes a segmentation of the scene into static (red) and moving (blue) regions (b), the depth structure of the scene (c) , and the optical flow (d). (e) shows ground truth flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The optical flow of natural scenes is a combination of the motion of the observer and the independent motion of objects. Existing algorithms typically focus on either recovering motion and structure under the assumption of a purely static world or optical flow for general unconstrained scenes. We combine these approaches in an optical flow algorithm that estimates an explicit segmentation of moving objects from appearance and physical constraints. In static regions we take advantage of strong constraints to jointly estimate the camera motion and the 3D structure of the scene over multiple frames. This allows us to also regularize the structure instead of the motion. Our formulation uses a Plane+Parallax framework, which works even under small baselines, and reduces the motion estimation to a one-dimensional search problem, resulting in more accurate estimation. In moving regions the flow is treated as unconstrained, and computed with an existing optical flow method. The resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art results on both the MPI-Sintel and KITTI-2015 benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The world is composed of things that move and things that do not. The 2D motion field, which is the projection of the 3D scene motion onto the image plane, arises from observer motion relative to the static scene and the independent motion of objects. A large body of work exists on estimating camera motion and scene structure in purely static scenes, generally referred to as Structure-from-Motion (SfM). On the other hand, methods that estimate general 2D image motion, or optical flow, make much weaker assumptions about the scene. Neither approach fully exploits the mixed structure of natural scenes. Most of what we see in such scenes is static -houses, roads, desks, etc. 1 Here, we refer to these static parts of the scene as the rigid scene, or rigid regions. At the same time, moving objects like people, cars, and animals make up a small but often important part of natural scenes. Despite the long history of both SfM and optical flow, no state-of-the art optical flow method synthesizes both into an algorithm that works on general scenes like those in the MPI-Sintel dataset <ref type="bibr" target="#b7">[9]</ref>  <ref type="figure">(Fig. 1</ref>). In this work, we propose such a method to estimate optical flow in video sequences of generic scenes that contain moving objects within a rigid scene.</p><p>For the rigid scene, the camera motion and depth structure fully determine the motion, which forms the basis of SfM methods. Modern optical flow benchmarks, however, are full of moving objects such as cars or bicycles in KITTI, or humans and dragons in Sintel. Assuming a fully static scene or treating these moving objects as outliers is hence not viable for optical flow algorithms; we want to reconstruct flow everywhere.</p><p>Independent motion in a scene typically arises from well defined objects with the ability to move. This points to a possible solution. Recently, convolutional neural networks (CNN) have achieved good performance on detecting and segmenting objects in images, and have been successfully incorporated into optical flow methods <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b32">34]</ref>. Here we take a slightly different approach. We modify a common CNN and train it on novel data to obtain a rigidity score from the labels, taking into account that some objects (e.g. humans) are more likely to move than others (e.g. houses). This score is combined with additional motion cues to obtain an estimate of rigid and independently moving regions.</p><p>After partitioning the scene into rigid and moving regions, we can deal with each appropriately. Since the motion of moving objects can be almost arbitrary, it is best computed using a classical unconstrained flow method. The flow of the rigid scene, on the other hand, is extremely restricted, and only depends on the depth structure and the camera motion and calibration. In theory, one could use an existing SfM algorithm to reconstruct the camera motion and the 3D structure of the scene, and project this structure back to obtain the motion of the rigid scene regions. Two factors make this hard in practice. First, the number of frames usually considered in optical flow is small; most methods only work on two or three consecutive frames. SfM algorithms, on the other hand, require tens or hundreds of frames to work reliably. Second, SfM algorithms require large camera baselines in order to reliably estimate the fundamental matrices. In video sequences, large baselines are rare, since the camera usually translates very little between frames. An exception to this are automotive scenarios such as the KITTI benchmark, where the recording car often moves rapidly and the frame rate is low.</p><p>Since full SfM is unreliable in general flow scenarios, we adopt the Plane+Parallax (P+P) framework <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b30">32]</ref> In this framework, frames are registered to a common plane, which is aligned in all images after the registration. This removes the motion caused by camera rotation and simple intrinsic camera parameter changes, leaving parallax as the sole source of motion. Since all parallax is oriented towards or away from a common focus of expansion in the frame, computing the parallax is reduced to a 1D search problem and therefore easier than computing the full optical flow.</p><p>Here we show that using the P+P framework brings an additional advantage: the parallax can be factored into a structure component, which is independent of the camera motion and constant across time, and a temporally varying camera component, which is a single number per frame. We integrate the structure information across time; by definition, the structure of the rigid scene does not change. By combining the structure information from multiple frames, our algorithm generates a better structure component for all frames, and fills in areas that are unmatched in a single pair of frames due to occlusion.</p><p>Additionally, the relationship between the structure component and the parallax (and thus, the optical flow) enables us to regularize the flow in a physically meaningful way, since regularizing the structure implicitly regularizes the flow. We use a robust second-order regularizer, which corresponds to a locally planar prior.</p><p>We integrate the regularization into a novel objective function measuring the photometric error across three frames as a function of the structure and camera motion. This allows us to optimize the structure and also to recover from poor initializations. We call the method MR-Flow for Mostly-Rigid Flow and show an overview in <ref type="figure">Fig. 2</ref>.</p><p>We test MR-Flow on MPI-Sintel <ref type="bibr" target="#b7">[9]</ref> and KITTI 2015 <ref type="bibr" target="#b23">[25]</ref>  <ref type="figure">(Fig. 1</ref>). Among published monocular methods, at time of writing, we achieve the lowest error on MPI-Sintel on both passes; on KITTI-2015, our accuracy is second only to <ref type="bibr" target="#b2">[4]</ref>, a method specifically designed for automotive scenarios. Our code, the trained CNN, and all data is available at <ref type="bibr">[1]</ref>.</p><p>In summary, we present three main contributions. First, we show how to segment the scene into rigid regions and independently moving objects, allowing us to estimate the motion of each type of region appropriately. Second, we extend previous plane+parallax methods to express the flow in the rigid regions via its depth structure. This allows us to regularize this structure instead of the flow field and to combine information across more than two frames. Third, we formulate the motion of the rigid regions as a single model. This allows us to iterate between estimating the structure and to recover from unstable initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous work</head><p>SfM and optical flow have both made significant, but mostly independent, progress. Roughly speaking, SfM methods require purely rigid scenes and use sparse point matches, wide baselines between frames, solve for accurate camera intrinsics and extrinsics, and exploit bundle adjustment to optimize over many views at once. In contrast, optical flow is applied to scenes containing generic motion, exploits continuous optimization, makes weak assumptions about the scene (e.g. that it is piecewise smooth), and typically processes only pairs of video frames at a time.</p><p>Combining optical flow and SfM. There have been many attempts to combine SfM and flow methods, dating to the 80's <ref type="bibr" target="#b10">[12]</ref>. For video sequences from narrow-focallength lenses, the estimation of the camera motion is challenging, as it is easy to confuse translation with rotation and difficult to estimate the camera intrinsics <ref type="bibr" target="#b11">[13]</ref>.</p><p>More recently there have been attempts to combine SfM and optical flow <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40]</ref>. The top monocular optical flow method on the KITTI-2012 benchmark estimates the fundamental matrix and computes flow along the epipolar lines <ref type="bibr" target="#b38">[40]</ref>. This approach is limited to fully rigid scenes. Wedel et al. <ref type="bibr" target="#b37">[39]</ref> compute the fundamental matrix and regularize optical flow to lie along the epipolar lines. If they detect independent motion, they revert to standard optical flow for the entire frame. In contrast, we segment static <ref type="figure">Figure 2</ref>: Algorithm overview. Given a triplet of frames, we first compute initial flow and an initial rigidity estimate based on a semantic segmentation CNN. The images are then aligned to a common plane, and the initial flow is converted to an estimate of the structure in the rigid scene using the Plane+Parallax framework. Where the P+P constraints are violated, the rigidity is refined, while at the same time the structure is refined using a variational optimization. To obtain the final flow estimate, the initial flow is used in moving regions, while the refined structure induces the flow in the rigid scene. from moving regions and use appropriate constraints within each type of region. Roussos et al. <ref type="bibr" target="#b29">[31]</ref> assume a known calibrated camera and solve for depth, motion and segmentation of a scene with moving objects. They perform batch processing on sequences of about 30 frames in length, making this more akin to SfM methods. While they have impressive results, they consider relatively simple scenes and do not evaluate flow accuracy on standard benchmarks.</p><p>Plane+Parallax. P+P methods were developed in the mid-90's <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b30">32]</ref>. The main idea is that stabilizing two frames with a planar motion (homography) removes the camera rotation and simplifies the geometric reasoning about structure <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b34">36]</ref>. In the stabilized pair, motion is always oriented towards or away from the epipole and corresponds to parallax, which is related to the distance of the point from the plane in the 3D scene.</p><p>Estimating a planar homography can be done robustly and with more stability than estimating the fundamental matrix <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b17">19]</ref>. While one is not able to estimate metric depth, the planar stabilization simplifies the matching process, turning the 2D optical flow estimation problem into a 1D problem that is equivalent to stereo estimation. Given the practical benefits, one may ask why P+P methods are not more prevalent in the leader boards of optical flow benchmarks. The problem is that such methods work only for rigid scenes. Making the P+P approach usable in general natural scenes is one of our main contributions.</p><p>Moving region segmentation. There have been several attempts to segment moving scenes into regions corresponding to independently moving objects by exploiting 3D motion cues and epipolar motion <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b36">38]</ref>. Several methods use the P+P framework to detect independent motions, but those methods typically only do detection and not flow estimation, and are often applied to simple scenes where there is a dominant motion like the ground plane and small moving objects <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b39">41]</ref>. Irani et al. <ref type="bibr" target="#b14">[16]</ref> develop mosaic representations that include independently moving objects but do not explicitly compute their flow. Given two frames as input, Ranftl et al. <ref type="bibr" target="#b27">[29]</ref> segment a general moving scene into piecewise-rigid components and reason about the depth and occlusion relationships. While they produce impressive depth estimates, they rely on accurate flow estimates between the frames and do not refine the flow itself.</p><p>Combining multiple flow methods. There is also existing work on combining motion estimates from different algorithms into a single estimate <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b21">23]</ref>, but these do not attempt to fuse rigid and general motion. Bergen et al. <ref type="bibr" target="#b4">[6]</ref> define a framework for describing optical flow problems using different constraints from rigid motion to generic flow, but do not combine these models into a single method.</p><p>Recent work combines segmentation and flow. Sevilla et al. <ref type="bibr" target="#b32">[34]</ref> perform semantic segmentation and use different models for different semantic classes. Unlike them, we use semantic segmentation to estimate the rigid scene and then impose stronger geometric constraints in these regions. Hur and Roth <ref type="bibr" target="#b12">[14]</ref> integrate semantic segmentation over time, leading to more accurate flow estimation for objects and better segmentation performance.</p><p>Most similar to our approach is <ref type="bibr" target="#b2">[4]</ref>, which first segments the scene into objects using a CNN. A fundamental matrix is then computed and used to constrain the flow within each object. Our work is different in a number of important ways. (i) Their approach is sequential and cannot recover from an incorrect fundamental matrix estimate. We propose a unified objective function where the parts of the solution inform and improve each other. (ii) <ref type="bibr" target="#b2">[4]</ref> relies exclusively on the CNN to segment moving regions. While this works in specific scenarios such as automotive, it may not generalize to new scenes. We combine semantic segmentation and motion to classify rigid regions and thus require less accurate semantic rigidity estimates. This makes our algorithm both more robust and more general, as demonstrated by the fact that in contrast to <ref type="bibr" target="#b2">[4]</ref> we evaluate on the challenging MPI-Sintel benchmark. (iii) <ref type="bibr" target="#b2">[4]</ref> requires moving objects to be rigid (i.e., rigidly moving vehicles) and assumes a small rotational component of the egomotion. This works for KITTI-2015 but does not apply to more general scenes. (iv) <ref type="bibr" target="#b2">[4]</ref> uses only two frames at a time and extrapolates into occlusions. Our model combines information across time, and thus it is able to compute accurate flow in occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Plane + Parallax background</head><p>The P+P paradigm has been used in rigid scene analysis for a long time. Since it forms the foundation of our algorithm, we briefly review the parts that are important for this work and refer the reader to <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b30">32]</ref> for more details.</p><p>The core idea of P+P is to align two or more images to a common plane ?, so that</p><formula xml:id="formula_0">x = Hx h ?(x, x ) on ? (1)</formula><p>where x and x represent a point in the reference frame and the corresponding point in another frame of the sequence, x h denotes x in homogeneous coordinates, H is the homography mapping the image of ? between frames, and a = (a 1 /a 3 , a 2 /a 3 ) is the perspective normalization. This alignment removes the effects of camera rotation and the effect of camera calibration change (such as a zoom) between the pair of frames <ref type="bibr" target="#b40">[42]</ref>. Getting rid of rotation is especially convenient, since the ambiguity between rotation and translation in case of small displacements is a major source of numerical instabilities in the estimation of the structure of the scene.</p><p>When computing optical flow between aligned images, the flow of the pixels corresponding to points on the plane is zero 2 . For an image point x corresponding to a 3D point X off the plane, the residual motion is given as <ref type="bibr" target="#b30">[32]</ref> u p (x) = 1</p><formula xml:id="formula_1">1 ? d(C2) Tz z d(X) (e ? x) ,<label>(2)</label></formula><p>where d(C 2 ) is the distance of the second camera center to ?, z is the distance of point X to the first camera, T z is the depth displacement of the second camera, d(X) is the distance from point X to ?, and e is the common focus of expansion that coincides with the epipole corresponding to the second camera. This representation has two main advantages. First, instead of an arbitrary 2D vector, each flow is confined to a line; therefore computing the optical flow is reduced to a 1D search problem. Second, when considering the flow of a pixel to different frames t which are registered to the same plane, Eq. (2) can be written as</p><formula xml:id="formula_2">u p (x, t) = A(x)b t A(x)b t ? 1 (e t ? x) ,<label>(3)</label></formula><p>where A(x) = d(X)/z is the structural component of the flow field, which is independent of t. It is hence convenient to accumulate structure over time via A. b t = T z /d(C 2 ), on the other hand, encodes the camera motion to frame t, and is a single number per frame. To simplify notation, we express the residual flow in terms of the parallax field w(x, t), so that</p><formula xml:id="formula_3">u p (x) = w (x, t) q q , w (x, t) = A(x)b t q A(x)b t ? 1 ,<label>(4)</label></formula><p>with q = (e ? x). Here, w denotes the flow in pixels along the line towards e. We can thus parametrize the motion across multiple frames as a common structure component A and per-frame</p><formula xml:id="formula_4">parameters ? t = {H t , b t , e t }.</formula><p>Since we use the center frame of a triplet of frames as the reference and compute the motion to the two adjacent frames, from here on we denote the two parameter sets as ? + = {H + , b + , e + } for the forward direction and ? ? for the backward direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Initialization</head><p>Given a triplet of images and a coarse, image-based rigidity estimation (described in Sec. 5.1), the goal of our algorithm is to compute (i) a segmentation into rigid regions and moving objects and (ii) optical flow for the full frame. We start by computing initial motion estimates using an existing optical flow method <ref type="bibr" target="#b24">[26]</ref>. For a triplet of images {I ? , I, I + }, we compute four initial flow fields, u + 0 from I to I + and u ? 0 from I to I ? , and their respective backwards flows? + 0 and? ? 0 . Due to the non-convex nature of our model (see Sec. 6) we need to compute good initial estimates for the P+P parameters? + ,? ? , visibility maps V + , V ? denoting which pixels are visible and which are occluded in forward and backward directions, and an initial structure estimate?.</p><p>Initial alignment and epipole detection. First we compute the planar alignments (homographies) between frames. Since P+P only holds in the rigid scene, in this section we only consider points that are marked as rigid by the initial semantic rigidity estimation. While computing a homography between two frames is usually easy, two factors make it challenging in our case: (i) when aligning multiple frames, the plane to which the frames are aligned has to be equivalent for each frame for P+P to work, and (ii) the 3D points corresponding to the four points used to estimate the homographies have to be coplanar for Eq. (3) to hold.</p><p>To compute homographies obeying these constraints, we use a two-stage process. First, we compute initial homogra-phiesH + ,H ? using RANSAC. In each iteration, the same random sample is used to fit bothH + ,H ? , and a point is considered an inlier only when its reprojection error is low in both forward and backward directions. This ensures that the computed homographies belong to the same plane. If a computed homography displaces the images corners by more than half the image size, it is considered invalid. If no valid homography is found, our method returns the initial flow field. This happens on average in 2% of the frames.</p><p>The second step is to ensure the coplanarity of the points inducing the homographies. For this, we can turn around Eq. (3), and simultaneously refine the homographies and estimate the epipoles e {+,?} so that Eq. (3) holds. Let u r = H(x +u 0 ) h ? x be the residual flow after registration with H. Each pair x, u r defines a residual flow line, and in the noise-free case, the epipole e is simply the intersection of these lines. Since the computed optical flow contains noise, we compute the epipole using the method described in <ref type="bibr" target="#b22">[24]</ref>, which we found to be sufficiently robust to noise. Therefore, e is a function of the optical flow and of the computed homography. Enforcing coplanarity of the homographies is now equivalent to enforcing that the residual flow lines in both directions each pass through a common point as well as possible. The refined homographies are thus computed a?</p><formula xml:id="formula_5">H + ,? ? = arg min H + ,H ? x z?{+,?} ? (o z (x)) ,<label>(5)</label></formula><p>with o z (x) defining the orthogonal distance of the residual flow line at x to e z . While Eq. <ref type="formula" target="#formula_5">(5)</ref> is highly nonlinear, we found that initializing withH {+,?} and using a standard non-linear minimization package such as L-BFGS <ref type="bibr" target="#b25">[27]</ref> produced results that greatly improved the final flow error compared to using the unrefined homogra-phiesH {+,?} . Throughout the paper, we use the Lorentzian ?(x) = ? 2 log 1 + x 2 /? 2 as the robust function, and compute the scaling parameter ? via the MAD <ref type="bibr" target="#b5">[7]</ref>. The initial epipolar estimates? {+,?} are computed using? {+,?} . To initialize b + , b ? , we first compute the parallax fields by projecting u r onto the parallax flow lines,</p><formula xml:id="formula_6">w = u r q/ q .<label>(6)</label></formula><p>Inserting <ref type="formula" target="#formula_6">(6)</ref> into <ref type="formula" target="#formula_3">(4)</ref> and solving for A, we get</p><formula xml:id="formula_7">A = w/ (b ( q ? w)) .<label>(7)</label></formula><p>Note that Eq. (3) contains a scale ambiguity between the structure A and the camera motion parameter b. Therefore, we can freely choose one of b + , b ? , which only affects the scaling of A; we chooseb + so that the initial forward structure A + defined by Eq. (7) has a MAD of 1. Since A ? is a function of b ? and should be as close as possible to A + , we obtain the estimateb ? by solvin?</p><formula xml:id="formula_8">b ? = arg min b ? x ? ? + (x) ? A ? (x) .<label>(8)</label></formula><p>Usingb ? , we compute the initial backward structure? ? using Eq. <ref type="formula" target="#formula_7">(7)</ref>, and set the full sets of P+P parameters to ? + = {? + ,b + ,? + }, and ? ? accordingly.</p><p>Occlusion estimation. Pixels can become occluded in both directions. In occluded regions, we expect the flow to be wrong, since it can at best be extrapolated. Given the initial flow fields, we compute the visibility masks V + (x), V ? (x) using a forward-backward check <ref type="bibr" target="#b18">[20]</ref>.</p><p>Initial structure estimation. Using the computed structure maps? {+,?} and visibility maps V {+,?} , the initial estimate for the full structure i?</p><formula xml:id="formula_9">A(x) = 1 max(1, V + (x) + V ? (x)) z?{+,?} V z (x)A z (x).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Rigidity estimation</head><p>Different cues provide different, complementary information about the rigidity of a region. The semantic category of an object tells us whether it is capable of independent motion, rigid scene parts have to obey the parallax constraint (3), and the 3D structure of rigid parts cannot change over time. We integrate all of them in a probabilistic framework to estimate a rigidity map of the scene, marking each pixel as belonging to the rigid scene or to a moving object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Semantic rigidity estimation</head><p>We leverage the recent progress of CNNs for semantic segmentation to predict rigid and independently moving regions in the scene. In short, we model the relationship between an object's appearance and its ability to move.</p><p>Obviously object appearance alone does not fully determine whether something is moving independently. A car may be moving, if driving, or static, if parked. However, for the purpose of motion estimation, not all errors are the same. Assuming an object is static when in reality it is not imposes false constraints that hurt the estimation of the global motion, while assuming a rigid region is independently moving does little harm. Thus, when in doubt, we predict a region to be independently moving.</p><p>The main optical flow benchmarks, KITTI-2015 and MPI-Sintel, provide different training data. While the essence of our model is the same for both, our training process varies to adapt to the available data. In both cases we start with the DeepLab architecture <ref type="bibr" target="#b8">[10]</ref>, pre-trained on the 21 classes of Pascal VOC <ref type="bibr" target="#b9">[11]</ref>, substitute all fully connected layers with convolutional layers, and densify the predictions <ref type="bibr" target="#b32">[34]</ref>. Both networks produce a rigidity score between 0 and 1 which we call the semantic rigidity probability p s .</p><p>MPI-Sintel contains many objects that are not contained in Pascal VOC, such as dragons. Thus using the CNN to predict a semantic segmentation is not possible. Also, no ground truth semantic segmentation is provided, so training a CNN to recognize these categories is not possible. However, the dataset provides ground truth camera calibration, depth and optical flow for the training set. With these we estimate rigidity maps that we take as ground truth. We do this by computing a fully rigid motion field, using the depth and camera calibration, and comparing it with the ground truth flow field. Pixels are classified as independently moving if these two fields differ by more than a small amount. We make this data publicly available [1].</p><p>We modify the last layer of the CNN to predict 2 classes, rigid and independently moving, instead of the original 21. We train using the last 30 frames of each sequence in the training set, and validate on the first 5 frames of each sequence. Sequences shorter than 50 frames are only included in the validation set. At test time, the probability of being rigid is computed at each pixel and then thresholded. Examples of the estimated rigidity maps can be seen in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>In KITTI 2015, some independently moving objects (e.g. people) are masked out from the depth and flow ground truth. Therefore, the approach we followed for MPI-Sintel cannot be used. The objects in KITTI, however, appear in standard datasets like the enriched Pascal VOC. We modify the last layer of the network to predict the 22 classes that may be present in KITTI (e.g. person or road) similar to <ref type="bibr" target="#b32">[34]</ref>. We then classify an object as moving if it has the ability to move independently (e.g. cars, or buses) and as rigid otherwise. Training details appear in the Sup. Mat. <ref type="bibr">[1]</ref>.</p><p>Note that the same approach we use for KITTI can be used for general video sequences by using a generic pretrained semantic segmentation network together with a definition of which semantic classes can move and which are static. This allows our method to directly benefit from advances in semantic segmentation and novel, fine-grained semantic segmentation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Physical rigidity estimation</head><p>For objects that have not been seen previously or that exhibit phenomena like motion blur, the semantic rigidity may be wrong. Hence, we use two additional cues, motion direction and temporal consistency of the structure. Moving regions from motion direction. A simple approach to classify a pixel as rigid or independently moving is to test whether its parallax flow points to the epipole <ref type="bibr" target="#b13">[15]</ref>. Here, we employ a probabilistic framework for this classification. Due to space limitations, we just present the final result here; for the derivation, please see the Sup. <ref type="figure">Mat. [1]</ref>.</p><p>For a given point x, our model assumes the measured corresponding point x = x + u r to have a Gaussian error distribution around the true correspondence with covariance matrix ? = ? 2 d I. Let c = u r and ? be the angle between u r and the line connecting x to e. Assuming a uniform distribution of motion directions for moving objects, the likelihood of a point being rigid is then given as</p><formula xml:id="formula_10">p (x is rigid) = exp ?2t sin 2 (?) exp (?t) I 0 (t) + exp ?2t sin 2 (?)<label>(10)</label></formula><p>with t = c 2 /(4? 2 d ) and I 0 (x) the modified Bessel function of the first kind. Solving for both forward and backward directions yields the direction-based rigidity probabilities p + d and p ? d . These are then combined into the final directionbased rigidity probability using the visibility maps</p><formula xml:id="formula_11">p d = 1 V + +V ? z?+,? V z p z d if V ? + V + &gt; 0 1/2 otherwise.<label>(11)</label></formula><p>Moving regions from structure consistency. Another cue for rigidity is the temporal consistency of the structure. This is particularly helpful where semantics and motion direction cannot disambiguate the rigidity, for example when an object such as a car moves parallel to the observer's motion.</p><p>Recall that according to the P+P framework the structure of the rigid scene is independent of time. In rigid regions that are visible in all frames, we assume the forward and backward structure A + and A ? to be close to each other. A structure based rigidity estimate p s can thus be computed as</p><formula xml:id="formula_12">p s = exp ? (A + ? A ? ) 2 /? 2 s if V ? V + = 1 1/2 otherwise.<label>(12)</label></formula><p>Combined rigidity probability from motion. The motionbased probabilities p d , p s can be seen as orthogonal. Surfaces that move independently along the parallax direction are considered to be rigid according to p d , while surfaces that move by small amounts orthogonal to the parallax direction are considered to be rigid according to p s . Hence, for a region to be considered actually rigid, we require both p d and p s to be high. The final motion-based rigidity probability p m is</p><formula xml:id="formula_13">p m = p d p s if V + V ? = 1 (p d + p s )/2 otherwise.<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Combining rigidity estimates</head><p>The previously computed rigidity probabilities p c , p m yield per-pixel rigidity probabilities. To combine those into a coherent estimate, we first compute a rigidity unary</p><formula xml:id="formula_14">p r = ? r,c p c + (1 ? ? r,c ) p m<label>(14)</label></formula><p>and the corresponding energy</p><formula xml:id="formula_15">E r (R, x) = ? log p r (x) if R(x) = 1 ? log (1 ? p r (x)) otherwise,<label>(15)</label></formula><p>with R(x) = 1 if x is rigid, and 0 otherwise. Since we expect the rigidity to be spatially coherent, we estimate the full labelling by solvingR = arg min  where w x,y is the image-based Potts modulation from <ref type="bibr" target="#b28">[30]</ref> and N (x) is the 8-connected neighborhood of x. Eq. <ref type="formula" target="#formula_6">(16)</ref> is solved using TRWS <ref type="bibr" target="#b1">[3]</ref>. <ref type="figure" target="#fig_1">Figure 3</ref> (top) shows the importance of combining different cues to recover from errors and accurately estimate the rigidity. The semantic estimation (b) misses a large part of the dragon's head, while both the direction-based (b) and structure-based estimations misclassify different segments of the scene. Combining cues yields a good estimate (e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Model and optimization</head><p>Model. The final structure should fulfill a number of criteria. First, as in the classical flow approach, warping the images using the flow induced by the structure should result in a low photometric error. Second, we assume that our initial flow fields are reasonable, hence, the final structure should be similar to the structures defined by the initial forward and backward flow. Third, the structure directly corresponds to the surface structure of the world, and thus we can regularize it using a locally planar model. This implicitly regularizes the flow in a more geometrically meaningful way than traditional priors on the flow.</p><p>Under these considerations, the full model for the motion of the rigid parts of the scene is defined as</p><formula xml:id="formula_17">E(A, ? + , ? ? ) = xR (x) (E d + ? c E c + ? 1st E 1st + ? 2nd E 2nd ) . (17)</formula><p>E d is the photometric error, modulated by the estimated visibilities in forward and backward directions:</p><formula xml:id="formula_18">E d =V + (x)? I + a s x, A, ? + ? I a (x) + V ? (x)? I ? a s x, A, ? ? ? I a (x) ,<label>(18)</label></formula><p>where I ? a , I a , I + a are augmented versions of I ? , I, I + , i.e. stacked images containing the respective grayscale images and the gradients in x and y directions. The warping function s(x, A, ?) defines the correspondence of x according to the structure A and the P+P parameters ?,</p><formula xml:id="formula_19">s(x, A, ?) = H ?1 x + A(x)b A(x)b ? 1 (e ? x) h .<label>(19)</label></formula><p>The consistency term E c encourages similarity between A and A {+,?} .</p><formula xml:id="formula_20">E c = V + ? c A ? A + + V ? ? c A ? A ? .<label>(20)</label></formula><p>To ensure a constant error for all A ? [A ? , A + ], we use the Charbonnier function as the robust penalty ? c . The locally-planar regularization uses a 2nd order prior,</p><formula xml:id="formula_21">E 2nd = w x ? (? xx A) + w x w y ? (? xy A) + w y ? (? yy A) .<label>(21)</label></formula><p>Here, w x , w y are again the modulation terms from <ref type="bibr" target="#b28">[30]</ref>, and, using a slight abuse of notation, ? xx , ? xy , ? yy are the second derivative operators. Since the second order prior by itself is highly sensitive to noise, we add a first order prior</p><formula xml:id="formula_22">E 1st = w x ? (? x A) + w y ? (? y A) ,<label>(22)</label></formula><p>where ? x , ? y are the first derivative operators in the horizontal and vertical direction respectively. Optimization. To minimize the energy (17) we employ an iterative scheme, and alternate between optimizing for A with ? {+,?} fixed, and for ? {+,?} with A fixed. When optimizing A, we use a standard warping-based variational optimization <ref type="bibr" target="#b6">[8]</ref> with 1 inner and 5 outer iterations and no downscaling. To optimize for ?, we first optimize for H, b using L-BFGS and then recompute e as described in Sec. 4. We use two iterations, since we found that more do not decrease the error significantly. This yields the final estimates A,? + ,? ? for the structure and the P+P parameters.</p><p>Due to the non-convex nature of (17), a global optimum is not guaranteed. However, in practice we found that our initializations are close to a good optimum, and hence our optimization procedure works well.</p><p>Final flow estimation. Finally, we convert the estimated structure? into an optical flow field</p><formula xml:id="formula_23">u s (x) = s x,?,? + ? x.<label>(23)</label></formula><p>In the moving regions, we use the initial forward flow u + 0 , and compose the full flow field as  </p><formula xml:id="formula_24">u (x) =R(x)u s + 1 ?R(x) u + 0 .<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>To quantify our method, we evaluate on the MPI-Sintel and KITTI-2015 flow benchmarks. The parameters are chosen to minimize errors on the training sets, and are set to {? d , ? s , ? r,c , ? r,p , ? c , ? 1st , ? 2nd } = {0.75, 2.5, 0.1, 1.1, 0, 0.1, 5e3} for Sintel and {1.0, 0.25, 0.5, 1.1, 0.01, 1, 5e4} for KITTI. <ref type="table">Table 1</ref> shows the errors for our method, our initialization (DF), and for top performing methods on MPI-Sintel (FF+) <ref type="bibr" target="#b3">[5]</ref> and KITTI-2015 (SDF) <ref type="bibr" target="#b2">[4]</ref>. Both evaluate only on one dataset; in contrast, our method achieves high accuracy on both datasets. On MPI-Sintel, our method currently outperforms all published works. In particular, the structure estimation gives flow in occluded regions, producing the lowest errors in the unmatched regions of any published or unpublished work. On a 2.2 GHz i7 CPU, our method takes on average 2 minutes per triplet of frames without the initial flow compu-tation, 74s for the initialization and rigidity estimation, and 46s for the optimization.</p><p>In KITTI-2015 the scenes are simpler and contain only automotive situations; however, the images suffer from artifacts such as noise and overexposures. Among published monocular methods, MR-Flow is second after <ref type="bibr" target="#b2">[4]</ref>, which is designed for automotive scenarios and not tested on Sintel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have demonstrated an optical flow method that segments the scene and improves accuracy by exploiting rigid scene structure. We combine semantic and motion information to detect independently moving regions, and use an existing flow method to compute the motion of these regions. In rigid regions of the scene, the flow is directly constrained by the 3D structure of the world. This allows us to implicitly regularize the flow by constraining the underlying structure to a locally planar model. Furthermore, since the structure is temporally coherent, we combine information from multiple frames. We argue that this uses the right constraints in the right place and produces accurate flow in challenging situations and competitive results on Sintel and KITTI.</p><p>This opens several directions for future work. First, the rigidity estimation could be improved using better inference algorithms and training data. Jointly refining the foreground flow with the rigid flow estimation could improve performance. Our method could also use longer sequences, and enforce temporal consistency of the rigidity maps.</p><p>As described in our paper, each of the two datasets provide different data, and thus we used slightly different procedures to estimate rigidity on each of the datasets. Here we describe the details. Additionally, <ref type="figure" target="#fig_4">Fig. 5</ref> and <ref type="figure" target="#fig_5">Fig. 6</ref> show some more examples of the outputs of the networks on images that were not seen during test time.</p><p>KITTI 2015: We followed the same procedure as previous work on semantic segmentation on KITTI <ref type="bibr" target="#b32">[34]</ref>, since it was shown to be successful. We used the DeepLab architecture <ref type="bibr" target="#b8">[10]</ref> modifying the output layer to classify 22 classes: aeroplane, bicycle, bird, boat, building, bus, car, cat, cow, dog, floor, grass, horse, motorbike, road, sheep, sidewalk, sky, train, water, person and background, where background includes anything that is not one of the other 21 classes. We initialized the weights with the VGG model trained on Pascal VOC and then fine-tuned it on our categories using a fixed momentum of 0.9, weight decay of 0.0005 and learning rate of 0.0001 for the first 100K iterations, reduced by 0.1 after every 50K steps, during 200K iterations. We used a dense CRF <ref type="bibr" target="#b19">[21]</ref> on top, where the unaries are the CNN output at each pixel and the pairwise potentials are position and a bilateral kernel with both position and RGB values. The inference in the dense CRF model is performed using 10 steps of mean-field approximate inference. At test time we obtain a probability over the classes. We estimate rigidity by choosing the class with the highest probability, and classifying the pixel as rigid or non-rigid based on whether an object in the class is capable of moving independently (for example, car) or not (for example, building). The accuracy of rigidity classification on the training set is 96.09%, where rigid parts are correctly classified 96.93% of the time, and independent moving parts are correctly classified 91.51% of the time.</p><p>MPI-Sintel: In this dataset there is no previous work on estimating rigidity. Thus, we used one of the latest released versions of the DeepLab architecture, called DeepLab-Coco-LargeFov, which is pretrained including extra annotations from the MS-COCO dataset <ref type="bibr" target="#b1">3</ref> . We modified the output layer to classify pixels as rigid or nonrigid. We fine-tuned all layers using the same parameters as before for 1.4K iterations. This small number of iterations was selected to avoid overfitting. At test time, we obtain a probability of rigidity, and we compute the final estimate of rigidity by thresholding at 0.5. The accuracy of the estimation on a validation set is 94.2%.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Derivation of direction-based rigidity likelihood</head><p>While the accuracy of the CNN-based rigidity estimation is surprisingly high, there are still occasions when it fails. This may happen for example when there is strong motion blur in the rigid regions, causing these to be classified as independently moving, which is a false positive. On the other hand, a noisy or over-saturated appearance of objects can cause the semantic segmentation to fail and result in a false negative. Therefore, we combine the CNN-based rigidity estimation with a motion-based estimation, described in this section.</p><p>For a given point x, our model assumes the measured corresponding point x = x + u to have a Gaussian error distribution around the true correspondence with covariance matrix ? = ? 2 I. For a given rigid point (denoted by the conditioning on r = 1) and a given focus of expansion e, the probability of the true correspondence pointing towards e is then given as</p><formula xml:id="formula_25">p (x |r = 1, e) = y?l(x,e) 1 2?? 2 m exp ? 1 2 y T ? ?1 y dy,<label>(25)</label></formula><p>where l(x, e) denotes the line that goes through x and e. <ref type="figure" target="#fig_6">Figure 7</ref> shows an illustration.</p><p>Since the error distribution is Gaussian, every marginal is also Gaussian. Therefore, the line integral in (25) is given as</p><formula xml:id="formula_26">p (x |r = 1, e) = 1 2?? 2 m exp ? d 2 2? 2 m ,<label>(26)</label></formula><p>where d denotes the distance of x to l(x, e), as shown in <ref type="figure" target="#fig_6">Figure 7</ref>. In the following, it will be convenient to express the correspondence point x in <ref type="bibr" target="#b24">(26)</ref>  </p><p>Abbreviating the prior for rigidity p(r = 1) as p 1 and setting p(?|r = 0, c) = 1 2? (the motion of independently moving regions is supposed to be uniformly distributed), we get p(r = 1|?, c) = p 1 p(?, c|r = 1)</p><formula xml:id="formula_28">Z(1?p1) 2? + p 1 p(?, c|r = 1)<label>(29)</label></formula><p>and for uninformative priors p 1 = 0.5 p(r = 1|?, c) = p(?, c|r = 1) Z 2? + p(?, c|r = 1)</p><p>.</p><p>What remains is to compute Z. Using Eq. <ref type="formula" target="#formula_1">(27)</ref>, it can be computed as </p><formula xml:id="formula_30">Z = p(c|r = 1) = 2? 0 p(?, c|r = 1)d? = 1 2?? 2 m 2? 0 exp ? c 2 sin 2 (?) 2? 2 m d? t = c 2 4? 2 m , sin 2 (x) = 1 2 (1 ? cos(2x)) = 1 2?? 2 m 2? 0 exp (?t(1 ? cos(2?))) d? = 1 2?? 2 m exp (?t) 2? 0 exp (t cos(2?)) d? [? = 2?] = 1 2?? 2 m exp (?t) 1 2 4? 0 exp (t cos(?)) d? = 1 2?? 2 m exp (?t) 2? 0 exp (t cos(?)) d? = 1 2?? 2 m exp (?t) 2?I 0 (t) = ? 2? ? m exp (?t) I 0 (t) ,<label>(31)</label></formula><formula xml:id="formula_31">= p(r = 1|?, c) = exp ?2t sin 2 (?) exp(?t)I 0 (t) + exp ?2t sin 2 (?) .<label>(32)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation study</head><p>This section provides an additional ablation study that had to be omitted from the paper due to space limitations. Here, we test how different subcomponents of our algorithm impact the end result 4 . To assess the impact in different regions of the frame, we provide the errors both on the full frames and only in the ground truth rigid regions.</p><p>For the ablation study, we successively switch on four steps: occlusion reasoning, coplanarity refinement, nonlinear initialization ofb ? , and spatial priors.</p><p>? Occlusion reasoning refers to the estimation of the visibility maps V + , V ? using the forward-backward consistency, as described in Section 4 in the paper. If this step is switched off, we set V ? = V + = 1 everywhere, and therefore do not explicitly exclude occluded pixels from subsequent computations.</p><p>? Coplanarity refinement refers to the second part of the initial alignment (Eq. (5) in the main paper) <ref type="bibr" target="#b3">5</ref> . This step refines the initial homographiesH ? ,H + to ensure that after registration all residual flow vectors meet in the two epipoles e ? , e + . The optimization yields? ? ,? + . If this step is switched off, we sim-</p><formula xml:id="formula_32">ply set? + =H + ,? ? =H ? .</formula><p>? Nonlinear initialization ofb ? refers to initializin? b ? using Eq.(8), i.e. choosingb ? so that the resulting backward? ? structure is as similar as possible under a robust error norm to the initial forward structure? + . Note that, without loss of generality,b + is always chosen so that the MAD of? + is 1.</p><p>If this step is switched off, use Eq. <ref type="formula" target="#formula_7">(7)</ref> to equate? + and? ? . This produces an estimate ofb ? per pixel, of which we take the median to arrive at the global estimate ofb ? .</p><formula xml:id="formula_33">b ? = median x 1 A + (x) w ? (x) e ? ? x ? w ? (x)<label>(33)</label></formula><p>? Spatial priors refer to the 1st-and 2nd-order spatial smoothness regularizers in our objective function <ref type="bibr" target="#b15">(17)</ref>. To disable those, we set ? 1st = ? 2nd = 0. <ref type="table" target="#tab_3">Table 2</ref> shows the improvement when successively switching on more parts of the algorithm. The occlusion reasoning has the largest positive impact on the error, since it allows the algorithm to properly merge the flow in both directions from the reference frame. Following this, the most important parts are the nonlinear initialization and ensuring the coplanarity. The spatial priors serve mostly to remove flow noise near boundaries. This improves the result visually, but has a fairly small numerical impact. <ref type="table" target="#tab_4">Table 3</ref> shows the impact when turning off individual components, but leaving all others intact. Again, we can observe that disabling the occlusion reasoning has the largest negative impact, followed by the nonlinear initialization and ensuring the coplanarity. <ref type="table" target="#tab_2">Table 4</ref> shows the impact of the different terms of the variational refinement <ref type="figure" target="#fig_6">(Eq. (17)</ref>). The cases are as described above. In addition,    <ref type="table" target="#tab_2">Table 4</ref>: Influence of regularization terms intial structure estimate as described by Eq. (9) in the paper, and selective disabling of the individual regularizers (? 1st = 0 for no-1st and ? 2nd = 0 for no-2nd). When using the data term only (no-spatial-priors), the error is higher than when not using any optimization. Due to effects in the Sintel final pass such as motion blur, fog, vignetting etc. this is to be expected. Using the 2nd order regularization improves the results; interestingly, however, the impact of the 1st order regularization is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Failure cases</head><p>This section gives examples of when our algorithm fails. We consider our algorithm to fail when the flow computed by our algorithm is worse than the initial flow <ref type="bibr" target="#b24">[26]</ref>. For each of Sintel clean, Sintel final, and KITTI we give one failure example. All examples are among the worst overall in the respective training sets. In our training set, we observe two primary sources of error, segmentation failures and alignment failures. <ref type="figure">Figures 8 and Figure 9</ref> show examples for the first type of error, segmentation failures. In these cases, moving regions are mistaken as parts of the rigid background, such as the car in <ref type="figure">Fig. 8</ref> or the girl's head in <ref type="figure">Fig. 9</ref>. These failures occur if the CNN does not pick up a region strongly enough and if, at the same time, the motion of the object is consistent with the motion of the rigid scene. In <ref type="figure">Fig. 8</ref>, the CNN picks up only the frontal part of the car. Since in this example the camera is not moving, the focus of expansion is mistakenly determined by the few parts of the frame that move (i.e. the car), and the motion-based rigidity estimation cannot correct the mistake made by the CNN.</p><p>In <ref type="figure">Fig. 9</ref>, the camera pans to the left, and at the same time, the head moves to the right. Since both directions are approximately parallel, the head is considered to be rigid. Note how in this case the estimated flow has a very similar hue to the ground truth flow, even in most of the regions that are misclassified. This confirms that the direction of the flow is approximately consistent with the motion of the rigid parts of the scene; however, since the head still moves slightly out of the rigidity constraints, our method increases the error over the initialization. <ref type="figure" target="#fig_9">Figure 10</ref> shows the second type of error, a failure to align the images. As can be seen in <ref type="figure" target="#fig_9">Fig. 10(a)</ref>, the background in this sequence contains heavy motion blur and a slight vignetting. Together, these two effects cause a high uncertainty of the initial optical flow in the background regions, which in turn causes our initial alignment procedure to fail. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>R x E r (R, x) + ? r,p y?N (x) w x,y [R(x) = R(y)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Results of rigidity estimation on the test sets of MPI-Sintel and KITTI-2015. From an image (a), we estimate a semantic rigidity (b) and combine it with the direction-based rigidity (c) and the structure-based rigidity (d) to obtain the final estimate (e). Likely rigid regions are red, likely moving regions are blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Results on MPI-Sintel and KITTI. From left to right: Overlaid input images, rigidity estimation, estimated structure (moving regions are masked in purple), estimated optical flow, comparison to initial flow (green areas denote improvements</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4 visualizes results; for more results see [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Results of rigidity estimation on the MPI-Sintel test set. Top: Original frame; Bottom: probability map of rigidity (white is likely rigid, black likely moving independently).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Results of rigidity estimation on KITTI 2015. Top: Original frame; Bottom: probability map of rigidity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Illustration</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 m. ( 27 )</head><label>227</label><figDesc>in terms of the angle ? and the displacement magnitude c = u . Then, p (?, c|r = 1, e) We can thus drop e from the equation. The likelihood of a point being rigid given the measurements ?, c is now p(r = 1|?, c) = p(?|r = 1, c)p(r = 1) p(?|r = 0, c)p(r = 0) + p(?|r = 1, c)p(r = 1) = 1 Z p(?, c|r = 1)p(r = 1) p(?|r = 0, c)p(r = 0) + 1 Z p(?, c|r = 1)p(r = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Failure case KITTI: The car on the left is wrongly detected as rigid. Perc. wrong initialization: 3.62%. Perc. wrong MR-Flow: 4.48%. (a) Input images (b) Ground truth flow (c) Estimated rigid regions (d) Estimated structure (e) Estimated flow (f) Comparison to initial flow Failure case Sintel clean: Moving regions are wrongly detected as rigid. EPE initialization: 0.93. EPE MR-Flow: 1.59.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Failure case Sintel final: Strong motion blur destroys the alignment. EPE initialization: 11.17. EPE MR-Flow: 12.21.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>with I 0 (x) the modified Bessel function of the first kind. Inserting Eq. (31) and Eq. (27) into Eq. 30 yields Eq. (11) from the main paper:</figDesc><table /><note>p (x is rigid)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>table 4</head><label>4</label><figDesc>includes a complete omission of the variational refinement (no-opt), which uses only the occlusion coplanarity nonlinear b ? spatial reasoning refinement initialization priors EPE rigid EPE all</figDesc><table><row><cell>Baseline</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Errors when successively switching on parts of the algorithm occlusion coplanarity nonlinear b ? spatial reasoning refinement initialization priors EPE rigid EPE all</figDesc><table><row><cell>no-occlusions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Errors when disabling individual parts of the algorithm</figDesc><table><row><cell>1st order</cell><cell>2nd order</cell><cell></cell></row><row><cell cols="3">Data term regularization regularization EPE rigid EPE all</cell></row><row><cell>No-opt</cell><cell>1.6124</cell><cell>3.6214</cell></row><row><cell>No-spatial-priors</cell><cell>1.6194</cell><cell>3.6285</cell></row><row><cell>No-1st</cell><cell>1.6025</cell><cell>3.6138</cell></row><row><cell>No-2nd</cell><cell>1.6183</cell><cell>3.6274</cell></row><row><cell>Full</cell><cell>1.6024</cell><cell>3.6138</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In KITTI-2015 and MPI-Sintel, independently moving regions make up only 15% and 28% of the pixels, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the plane does not have to correspond to a physical surface, but merely to a rigid, "virtual" plane.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Further details can be found on their webpage http: //ccvl.stat.ucla.edu/software/deeplab/ deeplab-coco-largefov/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The results in this section were obtained using a reduced version of the MPI-Sintel training set containing every 4th frame, and only the final pass.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">All equation numbers refer to equations in the main paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. JW and LS were supported by the Max Planck ETH Center for Learning Systems.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Determining three-dimensional motion and structure from optical flow generated by several moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Adiv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI</title>
		<imprint>
			<date type="published" when="1985-07" />
			<biblScope unit="page" from="384" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reduce, reuse &amp; recycle: Efficiently solving multi-label mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting semantic information and deep matching for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016: 14th European Conference, Amsterdam</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<meeting><address><addrLine>The Netherlands; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="154" to="170" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4015" to="4023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical model-based motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hingorani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV&apos;92</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="page" from="237" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Edges as outliers: Anisotropic smoothing using local image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scale-Space Theories in Computer Vision: Second International Conference, Scale-Space&apos;99</title>
		<editor>M. Nielsen, P. Johansen, O. F. Olsen, and J. Weickert</editor>
		<meeting><address><addrLine>Corfu, Greece; Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2004: 8th European Conference on Computer Vision</title>
		<editor>T. Pajdla and J. Matas</editor>
		<meeting><address><addrLine>Prague, Czech Republic; Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black ; S. Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>A. Fitzgibbon,</editor>
		<imprint>
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="611" to="625" />
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Subspace methods for recovering rigid motion I: Algorithm and implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="117" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Direct methods for recovering motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Weldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="76" />
			<date type="published" when="1988-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint optical flow and temporally consistent semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 Workshops</title>
		<editor>G. Hua and H. J?gou</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="163" to="177" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A unified approach to moving object detection in 2d and 3d scenes. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1998-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient representations of video sequences and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="327" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Direct recovery of planar-parallax from multiple frames. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From reference frames to reference planes: Multi-view parallax geometry and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV&apos;98</title>
		<editor>H. Burkhardt and B. Neumann</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1407</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Heidelberg</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recovery of ego-motion using region alignment. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rousso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="268" to="272" />
			<date type="published" when="1997-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Forward-backward error: Automatic detection of tracking failures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 20th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2756" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fusionflow: Discretecontinuous optimization for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Brostow. Learning a confidence measure for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013-05" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1107" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Removal of translation bias when using subspace methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Maclean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Seventh IEEE International Conference on</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="753" to="758" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) 2015</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discrete optimization for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9358</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Updating quasi-newton matrices with limited storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of computation</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">151</biblScope>
			<biblScope unit="page" from="773" to="782" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Epipolar constrained motion estimation for reconstruction from video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Memin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Labit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">3309</biblScope>
			<biblScope unit="page" from="460" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dense monocular depth estimation in complex dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">grabcut&quot;: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dense multibody motion estimation and reconstruction from a handheld camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<idno>2012. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl Symposium on Mixed and Augmented Reality</title>
		<imprint/>
	</monogr>
	<note>ISMAR 2012</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d geometry from planar parallax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1994-06" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Computer Society Conference on</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Independent motion detection in 3D scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1191" to="1199" />
			<date type="published" when="1999-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optical flow with semantic segmentation and localized layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pong. Detecting moving objects. IJCV</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="39" to="57" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Plane + parallax, tensors and factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV), volume LNCS 1842</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="522" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A variational model for the joint recovery of the fundamental matrix and the optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rigid body segmentation and shape description from dense optical flow under weak perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="143" />
			<date type="published" when="1997-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structureand motion-adaptive regularization for high accuracy optic flow. In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1663" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust monocular epipolar flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1862" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Detecting motion regions in the presence of a strong parallax from a moving camera by multiview geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1627" to="1641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-frame estimation of planar motion. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
			<date type="published" when="2000-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Supplemental Material A.1. Semantic rigidity estimation: CNN architecture details and training procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
