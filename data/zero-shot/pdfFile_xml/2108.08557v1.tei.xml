<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-19">19 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Garau</surname></persName>
							<email>nicola.garau@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<addrLine>Via Sommarive, 9</addrLine>
									<postCode>38123</postCode>
									<settlement>Povo, Trento</settlement>
									<region>TN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niccol?</forename><surname>Bisagno</surname></persName>
							<email>niccolo.bisagno@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<addrLine>Via Sommarive, 9</addrLine>
									<postCode>38123</postCode>
									<settlement>Povo, Trento</settlement>
									<region>TN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Br?dka</surname></persName>
							<email>piotrbrodka95@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<addrLine>Via Sommarive, 9</addrLine>
									<postCode>38123</postCode>
									<settlement>Povo, Trento</settlement>
									<region>TN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Conci</surname></persName>
							<email>nicola.conci@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<addrLine>Via Sommarive, 9</addrLine>
									<postCode>38123</postCode>
									<settlement>Povo, Trento</settlement>
									<region>TN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-19">19 Aug 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) 2D/3D HPE Input (b) 3D HMR (c) Ours Figure 1: [Better seen in color]. Overview of the proposed solution. Two different views of the same subject are shown for each image: (a) 2D/3D Human Pose Estimation (HPE) and (b) 3D Human Mesh Recovery (HMR) methods achieve good accuracy on the front-view (second row). Changing the viewpoint turns into performance degradation (first row). Our method (c) promotes viewpoint equivariance, showing good results in both the RGB and depth domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Human Pose Estimation (HPE) aims at retrieving the 3D position of human joints from images or videos. We show that current 3D HPE methods suffer a lack of viewpoint equivariance, namely they tend to fail or perform poorly when dealing with viewpoints unseen at training time. Deep learning methods often rely on either scaleinvariant, translation-invariant, or rotation-invariant operations, such as max-pooling. However, the adoption of such procedures does not necessarily improve viewpoint generalization, rather leading to more data-dependent methods. To tackle this issue, we propose a novel capsule autoencoder network with fast Variational Bayes capsule routing, named DECA. By modeling each joint as a capsule entity, combined with the routing algorithm, our approach can preserve the joints' hierarchical and geometrical structure in the feature space, independently from the viewpoint. By achieving viewpoint equivariance, we drastically reduce the network data dependency at training time, resulting in an improved ability to generalize for unseen viewpoints. In the experimental validation, we outperform other methods on depth images from both seen and unseen viewpoints, both top-view, and front-view. In the RGB domain, the same network gives state-of-the-art results on the challenging viewpoint transfer task, also establishing a new framework for top-view HPE. The code can be found at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation is key for many applications, such as action recognition, animation, gaming, to name a few <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28]</ref>. State of the art methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref> that rely on RGB images can correctly localize human joints (e.g. torso, elbows, knees) in images, also in presence of occlusions. However, they tend to fail when dealing with challenging scenarios. The top-view perspective, in particular, turns out to be a difficult task; on the one hand, it causes the largest amount of joints occlusions, and on the other hand, it suffers the scarcity of suitable training data, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>When presented with unseen viewpoints, humans display a remarkable ability to estimate human poses, even in the presence of occlusions and unconventional joints configurations. This is not always true in computer vision. In fact, available methods are trained in relatively constrained settings <ref type="bibr" target="#b14">[15]</ref>, with a limited variability between different viewpoints. Limited data, especially from the topviewpoint, along with limited capabilities of modeling the hierarchical and geometrical structure of the human pose, results in poor generalization capabilities.</p><p>This generalization problem, known as the viewpoint problem, depends on how the network activations vary with the change of the viewpoint, usually after a transformation (translation, scaling, rotation, shearing). Convolutional Neural Networks (CNNs) scalar activations are not suitable to effectively manage these viewpoint transformations, thus needing to rely on max-pooling and aggressive data augmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>. By doing so, CNNs aim at achieving viewpoint invariance, defined as f (T x) = f (x) <ref type="bibr" target="#b0">(1)</ref> According to this formulation, applying a viewpoint transformation T on the input image x, does not change the outcome of the network activations.</p><p>However, a more desirable property would be to capture and retain the transformation T applied to the input image x, thus obtaining a network that is aware of the different transformations applied to the input. Being able to model network activations that change in a structured way according to the input viewpoint transformations is also called viewpoint equivariance and it is defined as:</p><formula xml:id="formula_0">f (T x) = T f (x).<label>(2)</label></formula><p>This is achieved by introducing capsules: groups of neurons that explicitly encode the intrinsic viewpoint-invariant relationship existing between different parts of the same object. Capsule networks (CapsNets) can learn part-whole relationships between so-called entities across different viewpoints <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13]</ref>, similarly to how our visual cortex system operates, according to the recognition-by-components theory <ref type="bibr" target="#b0">[1]</ref>. Unlike traditional CNNs, which usually retain viewpoint invariance, capsule networks can explicitly model and jointly preserve a viewpoint transformation T through the network activations, achieving viewpoint equivariance (Eq. 2).</p><p>Developing viewpoint-equivariant methods for 3D HPE networks leads to multiple advantages: (i) the learned model is more robust, interpretable, and suitable for realworld applications, (ii) the viewpoint is treated as a learnable parameter, allowing to disentangle the 3D data of the skeleton from each specific view, (iii) the same annotated data can be used to train a network for different viewpoints, thus less training data is required.</p><p>In this work, we address the problem of viewpointequivariant human pose estimation from single depth or RGB images. Our contribution is summarised as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? We present a novel Deep viewpoint-Equivariant</head><p>Capsule Autoencoder architecture (DECA) which jointly addresses multiple tasks, such as 3D and 2D human pose estimation.</p><p>? We show how our network works with limited training data, no data augmentation, and across different input domains (RGB and depth images).</p><p>? We show how the feature space organization, defined by routing the input information to build capsule entities, improves when the tasks are jointly addressed.</p><p>? We evaluate our method on the ITOP <ref type="bibr" target="#b8">[9]</ref> dataset for the depth domain and on the PanopTOP31K <ref type="bibr" target="#b4">[5]</ref> dataset for the RGB domain. We establish a new baseline for the viewpoint transfer task and in the RGB domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In recent years, human pose estimation has been a subject of multiple studies, particularly for real-time 2D HPE <ref type="bibr" target="#b1">[2]</ref>, 3D HPE <ref type="bibr" target="#b31">[32]</ref> and human mesh recovery (HMR) approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>. In this work, we focus on HPE from single views, using either RGB <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> or depth images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Viewpoint-invariant HPE from RGB images. 3D HPE usually leverages on additional cues, such as 2D predictions <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30]</ref>, multiple images <ref type="bibr" target="#b37">[38]</ref>, pre-trained models <ref type="bibr" target="#b16">[17]</ref> and pose dictionaries <ref type="bibr" target="#b26">[27]</ref>. Other recent works aim at endto-end, learning-based 3D HPE <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b20">21]</ref>. In the RGB domain, common HPE datasets such as Human3.6M <ref type="bibr" target="#b13">[14]</ref>, provide images from multiple views, like front-view or sideview, while the top-view component is generally missing. It is then evident that the lack of suitable multi-view (topview in particular) data implies that state-of-the-art methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref> necessarily perform poorly when presented with an unseen viewpoint at test time, as shown in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>.</p><p>Viewpoint-invariant HPE from depth images. Viewpoint invariant HPE methods have been developed using depth images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref> from top-view and side-view, using datasets like the K2HPD Body Pose Dataset <ref type="bibr" target="#b34">[35]</ref> and the ITOP dataset <ref type="bibr" target="#b8">[9]</ref>. To take advantage of the 3D information encoded in 2D depth images, one recent research trend is to resort to 3D deep learning. The paid efforts can be generally categorized into 3D CNN-based and point-set-based families. To enhance the 3D proprieties of depth data and compute more significant features, current methods rely on 3D CNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref> or 2D CNNs with dense features <ref type="bibr" target="#b35">[36]</ref>.</p><p>3D CNN-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref> perform a voxelization operation on pixels to transform them into 3D objects. To process the 3D data, each network performs costly 3D convolutions on the input data. These operations are responsible for the high computational burden and the difficulty to properly tune a high number of parameters in 3D CNNs. In the domain of 2D CNNs, Xiong et al. <ref type="bibr" target="#b35">[36]</ref> capture the 3D structure by computing dense features in an ensemble way, thus avoiding computationally intensive CNN layers, but they still rely on a backbone pre-trained network to extract 2D features. Still, the above-mentioned approaches usually achieve weak viewpoint-invariance but fail to model viewpoint-equivariance. Moreover, we argue that the 3D geometry of the data should be interpreted by the network without relying on the voxelization embedding, or a 2D pretrained feature extraction network.</p><p>Capsule networks for HPE. Capsule networks have shown the ability to model the geometric nature of training data thanks to the network structure and features <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref>. Sabour et al., introduce a routing algorithm for vector capsules, called routing-by-agreement as a better maxpooling substitute. Hinton et al. <ref type="bibr" target="#b12">[13]</ref> further improve accuracy through a more complex matrix capsule structure and an Expectation-Maximization routing (EM-routing) for capsules. Unfortunately, the EM-routing and the 4 ? 4 pose matrix embedded in the capsule contribute to increasing the training time, when compared to both CNNs and vector CapsNets. Kosiorek et al. <ref type="bibr" target="#b19">[20]</ref> introduce for the first time an unsupervised capsule-based autoencoder. Ribeiro et al. in <ref type="bibr" target="#b23">[24]</ref> build upon the EM-routing version of capsule by proposing for the first time a Variational Bayes capsule routing (VB routing) fitting a mixture of transforming Gaussians. They present state-of-the-art results using ? 50% fewer capsules, achieving both performance gain and network complexity reduction. However, all the mentioned works only consider small datasets, such as MNIST, small-NORB, and CIFAR-10 for benchmarking.</p><p>In the RGB domain, Ram?rez <ref type="bibr" target="#b22">[23]</ref> tackles the problem of RGB HPE using dynamic vector capsule networks <ref type="bibr" target="#b25">[26]</ref> to solve the 3D HPE problem in an end-to-end fashion. However, their work only exploits lateral viewpoints from the Human3.6M dataset and only considering RGB data.</p><p>In this work, we use matrix capsules <ref type="bibr" target="#b12">[13]</ref>, along with a different capsule routing algorithm and a new encodingdecoding pipeline with GELU activations. We argue that matrix capsules are better suited than vector capsules for the 3D HPE task, as the 4 ? 4 pose matrix used for the routing can capture 3D geometry better than a dynamic vector structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We now analyze the proposed autoencoder, DECA, starting with the capsule encoder and the multi-task decoders. DECA can be trained end-to-end, without any pre-training or data augmentation, and it works in real-time in the inference phase. An overview of the proposed architecture is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Capsule encoder</head><p>The encoding module of the network (light blue in <ref type="figure">Fig.</ref> 2) is divided in: (i) an input pre-processor I, (ii) a CNN encoder E and (iii) four layers of Matrix Capsules with Variational Bayes Routing <ref type="bibr" target="#b23">[24]</ref>.</p><p>(i) I is a layer which normalizes the different type of data (RGB images, depth images, top-view, side-view, freeview) in the interval [0, 1].</p><p>(ii) The normalised input is then forwarded to a CNN encoder E, built using four convolutional layers with inputs [N ch , 64, 128, 256], instance normalisation and GELU activations <ref type="bibr" target="#b10">[11]</ref>, as shown in Eq. 3. N ch is the number of channels, which may vary depending on the input.</p><formula xml:id="formula_1">GELU(x) ? 0.5x(1 + tanh 2 ? (x + 0.044715x 3 ) )<label>(3)</label></formula><p>(iii) The output of the CNN encoder E feeds our capsule layers. It has been shown in previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref> that capsules provide a superior understanding of the viewpoint and the relationship between parts and parent objects, thus aiming at true viewpoint equivariance. Given the multiple degrees of freedom of each joint, we adopt the matrix capsules model <ref type="bibr" target="#b12">[13]</ref> instead of vector capsules <ref type="bibr" target="#b25">[26]</ref>, enriching the description of single joints as hierarchically linked capsule entities. We deploy the novel capsule routing based on Variational Bayes (VB) <ref type="bibr" target="#b23">[24]</ref>, which is proven to speed up the training of our matrix capsules layers, at the same time improving performances. The last iteration of the VB routing is also called ClassRouting and it is used to route the highest-level information to the last layer of capsules before the feature space F.</p><p>In our CapsNet, we employ four layers: a primary capsules layer encapsulates the output features of E into 16dimensional capsules, two convolutional capsules layers refine the capsule features, and a final class capsules layer encodes the output into a J-dimensional features in the latent space F, where J is the number of joints, also called entities.</p><p>Given each lower-level capsule i and the corresponding higher-level capsule j, we define M i as the proposed lower level pose matrix and W ij ? R 4?4 as a trainable viewpointequivariant transformation matrix such that:</p><formula xml:id="formula_2">V j|i = M i W ij (4)</formula><p>where V j|i is the vote coming from lower capsules i for higher capsules j. The voting procedure takes place inside the VB routing and it allows each lower capsule i to route its information to a higher capsule j of its choice, thus allowing to build the hierarchical structure typical of CapsNets.</p><p>To promote the viewpoint equivariance in Eq. 2, we introduce an inverse matrix? W in the class capsules, which aims at satisfying the Inverse Graphics constraint:</p><formula xml:id="formula_3">y W W ij = I<label>(5)</label></formula><p>meaning that the learned inverse matrix? W effectively acts as an approximated inverse of the rendering operation, as it is commonly found in computer graphics <ref type="bibr" target="#b11">[12]</ref>.</p><p>At the output of the encoder, each entity corresponding to each joint of the skeleton is defined by a flattened vector of 16 elements, or, in other words, a 4 ? 4 matrix, which is sufficient to grasp the complete pose (translation + rotation) of each joint.</p><p>An overview of the capsule encoder is shown in Algorithm 1. In the algorithm, s 3D , s 2D , s DM , s W are weights used for the self-balancing of the loss, w c are the convolutional layer weights, a are the activations of each Capsule layer, and {?} represents parameters used only when in the RGB domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Capsule encoder</head><p>CapsuleEncoder (x)</p><p>inputs :</p><formula xml:id="formula_4">x = x 0 . . . x BS , BS = batch size of RGB or depth images outputs: F = J 16-dimensional entities; y W = trainable Inverse Graphics matrix s 3D , s 2D , {s DM }, s W ? 1; w c ? xavier unif orm () ?c ? ConvLayers; foreach i ? ConvLayers do x ? Conv2d i (x); x ? InstanceN orm2d i (x); x ? GELU(x);</formula><p>a, x ? P rimaryCapsules(x); foreach j ? ConvCapsuleLayers do a, x ? ConvCapsules j (a, x); a, x ? V BRouting j (a, x);</p><p>a, x,? W ? ClassCapsules(a, x); a, x ? ClassRouting(a, x); F ? entities(x); return F,? W ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-task decoders</head><p>Starting from the 16-dimensional entities in the capsule feature space F, we design a decoding module (light orange block in <ref type="figure" target="#fig_0">Fig. 2</ref>) that allows us to simultaneously retrieve multiple predictions for different tasks from the same feature space F. Each decoder D ? in the decoding module is configured as an independent fully connected block, with 0.5 Dropout and GELU activations <ref type="bibr" target="#b10">[11]</ref>. We employ no weight sharing or layer sharing across the decoders to enforce the multi-task loss, as explained in section 3.3.</p><p>We define different tasks (? ) with different objectives:</p><p>? 3D: minimise the distance between ground truth and predicted 3D joints in 3D space? 3D ;</p><p>? 2D: as above, but without relying on 3D joints predictions, and rather predicting 2D joints? 2D as seen from the current viewpoint in camera frame coordinates;</p><p>? DM: reconstruct the depth map? DM of the input RGB image. It is used only in the RGB domain;</p><p>? W Inverse Graphics loss : learn the inverse graphics matrix? W to promote the de-rendering of input pixels into isolated capsule entities, as explained in Sec. 3.1, Eq. 5.</p><p>For each task ? = 3D, 2D, DM, a decoder D ? takes as input the feature space F and it outputs the prediction? Y = [? 3D ,? 2D , {? DM }] to the loss function. For W, th? y W matrix is forwarded to the loss function directly from the encoder.</p><p>An overview of the capsule decoders is shown in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Capsule decoders</head><formula xml:id="formula_5">CapsuleDecoders (x) inputs : F = J 16-dimensional entities outputs:? = [? 3D ,? 2D , {? DM }] x ? F; foreach i ? Y do x ? Dropout 0.5 (x); x ? Linear(x); y i ? GELU(x); return? = [? 3D ,? 2D , {? DM }];</formula><p>3.3. Self-balancing multi-task loss Tasks are associated to the different input domains, as follows:</p><formula xml:id="formula_6">3D 2D DM W Depth RGB</formula><p>Each task is assigned a loss L ? , defined as:</p><p>? L 2D , L 3D : Mean Square Error (MSE) loss for the 3D and 2D joints prediction tasks.</p><p>? L DM : masked L1 loss for the depth estimation task DM, in the RGB domain, where mask is a function that applies the L1 loss only on pixels over a certain depth threshold, to promote the depth estimation over non-background areas.</p><p>? L W : inverse graphics loss W, which role is to enforce invertibility for the capsule weight matrices. The notation . F defines the Frobenius norm of a matrix.</p><formula xml:id="formula_7">L ? = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? L 2D,3D = 1 BS BS i=0 (y i ?? i ) 2 L DM = BS i=0 mask |y i ?? i | + |y i ?? i | 2 * BS L W = ? W W ij F<label>(6)</label></formula><p>Considering T as the set of the employed tasks ? , the overall balanced loss for all the tasks is expressed as:</p><formula xml:id="formula_8">L = ? ?T s ? + e ?s? L ?<label>(7)</label></formula><p>where s ? = [s 3D , s 2D , s DM , s W ] are the trainable weights associated with each loss in T , initialised to 1 in algorithm 1, and L ? is each loss of the enabled decoders, as defined in Eq. 6. PanopTOP31K dataset of depth and RGB images. The PanopTOP31K dataset <ref type="bibr" target="#b4">[5]</ref> consists of 34k top-view and 34k front view images coming from video sequences of 24 different actors, available both in the RGB and depth domain, for a total of 68k images. The ground truth 3D skeleton consists of 19 joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation metrics</head><p>Following the works of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>, we choose the mean average precision (mAP) as the evaluation metric for the depth domain. It is defined as the percentage of all predicted joints which fall in an interval smaller than 0.10 meters. In the RGB domain, we use the Mean Per Joint Position Error (MPJPE) in millimeters as in many HPE works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b22">23</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation details</head><p>Our network is trained in an end-to-end fashion using Pytorch Lightning. Input images are normalized in the interval [0, 1] with a resolution of 256x256 pixels for depth  <ref type="figure">Figure 3</ref>: 2D representation on the 16-dimensional latent space obtained using t-SNE <ref type="bibr" target="#b32">[33]</ref>. Each dot corresponds to an entity E jt representing a joint jt of the skeleton from the test set of ITOP <ref type="bibr" target="#b8">[9]</ref>. V2V network <ref type="bibr" target="#b21">[22]</ref> relies on CNNs, thus is not able to cluster together samples corresponding to the same entity (a). When trained to satisfy only the 3D prediction constraint our DECA-D1 network performs slightly better than V2V (b). The 15 clusters, corresponding to the 15 joints of the skeleton model, are clearly distinguishable in DECA-D2 (c) and DECA-D3 (d), with (d) displaying better cluster separation and fewer outliers.</p><p>images and 256x256 pixels for RGB ones. We do not perform any augmentations on the input datasets. The batch size is set to 128 for ITOP and 128 for PanopTOP31K. We initialize the weights with the Xavier initialization <ref type="bibr" target="#b5">[6]</ref>. The learning rate is set to 1e ?5 , the weight decay is set to 0, and Adam is the optimizer of choice. We train our network for 20 epochs on the ITOP dataset and 15 epochs on Panop-TOP31K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Feature space entities and ablation study</head><p>We report experiments on the top-view of the ITOP dataset <ref type="bibr" target="#b8">[9]</ref> to validate the 3D representation provided by our network and to show how the multi-tasks decoder influences the overall performances.</p><p>To do so, we deploy 4 configurations, 3 on depth data and 1 on RGB data, with different sets of tasks T of our method:</p><formula xml:id="formula_9">? DECA-D1, with T = [3D] ? DECA-D2, with T = [3D, W] ? DECA-D3, with T = [3D, 2D, W] ? DECA-R4, with T = [3D, 2D, DM, W]</formula><p>where the letter D or R indicates the depth or RGB domains, and the number defines how many tasks are assigned to the network. Since we are evaluating the performances on the 3D HPE, the ? = [3D] is used for all the different configurations.</p><p>Loss effectiveness analysis. The results are reported in the last 3 columns of <ref type="table" target="#tab_0">Table 1</ref>. As shown in the <ref type="table">Table,</ref> increasing the number of tasks in T generally leads to an increase in the network's performances. DECA-D1 already achieves similar results to the state-of-the-art, thanks to the CapsNets' capability to interpret the geometrical nature of the input data. When the inverse graphics loss W is employed (DECA-D2 and DECA-D3), the enforced invertibility of the weights matrix leads to an immediate gain in performances. In DECA-D3, the introduction of the 2D loss leads to an additional improvement in terms of accuracy. Hence, we argue that the network performances improve when more tasks are given because we achieve a better representation of the entities in the latent space.</p><p>Latent space analysis. To analyze the latent space, we use the features of the test set extracted after the capsule modules. Each feature f ? F is linearised to obtain a vector of length L f eat . At this stage, each entity E jt corresponding to each joint jt is defined by dividing each feature vector by the number of joints, resulting in vectors of length L f eat #of joints . For visualisation purposes, we use t-SNE <ref type="bibr" target="#b32">[33]</ref> to project the entities on a 2-dimensional space. The results are displayed in <ref type="figure">Fig. 3</ref>. We compare our latent space against the publicly available version of the V2V <ref type="bibr" target="#b21">[22]</ref> encoder/decoder structure. We show how our DECA network can better cluster and separate each entity E jt with respect to V2V. Our solution provides a better organization of the latent space, with bigger inter-class margins and fewer outliers. The latent space organization improves drastically when we employ the ? = W task (DECA-D2), thus enforcing the inverse graphics constraint. In DECA-D3 we add the ? = 2D task. The resulting organization of the latent space improves, thus further establishing a correlation between the growing number of tasks and the improvement in performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with state-of-the-art methods</head><p>Depth data: ITOP dataset. We compare our DECA against common state-of-the-art method for human pose estimation on depth images <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>. The results are reported in Tab. 1. Our DECA outperforms exist- ing methods on the front-view task, improving the accuracy by a wide margin on the more challenging top viewpoint.</p><p>In general, we consistently perform better than other methods on most of the joints and the average. The gain of our method is particularly large when dealing with the lower body, which is often occluded in the top-view.</p><p>Depth data: Viewpoint-equivariant ITOP. We test DECA on the viewpoint transfer task, meaning training on one viewpoint, either top-view or front-view, and testing on the other one, unseen at training time. The comparison against available state-of-the-art methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref> are reported in Tab. 2. We consistently outperform other methods by a wide margin, thus making a step forward toward viewpoint equivariance. While other methods provide only the best subset of viewpoint transfer results (Tab. 2), omitting entirely the train on top and test on front scenario, we provide results for all the joints and all the viewpoint transfer combinations in Tab. 3. Our DECA achieves better results than the top-most of the other methods on many different joints (e.g. shoulders, lower body). In Tab 3, training DECA on top-view or front-view achieves comparable lower body accuracy. This means that when the network is trained on top view, where the lower body is mostly occluded, it can retrieve the occluded joints from previously unseen front views, and vice versa. This shows how our network has learned the viewpoint as a parameter, and it is thus able to generalize in a similar fashion in all the viewpoint transfer combinations.</p><p>RGB data: Viewpoint-equivariant PanopTop31K. To the best of our knowledge, we are the first to tackle the problem of viewpoint transfer between top-view and front-view in the RGB domain. We report results with training and testing on both seen and unseen viewpoints in Tab. 4. The chosen metric is the mean per-joint projection error (MPJPE). We report results with and without the Procrustes alignment <ref type="bibr" target="#b6">[7]</ref> of the predicted poses. It is interesting to notice how DECA can reduce the gap between the same viewpoint results and the results of the viewpoint transfer tasks. In the case of viewpoint transfer, we train on viewpoint A, validate on the same viewpoint A and test on viewpoint B.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative results</head><p>In <ref type="figure" target="#fig_3">Fig. 4</ref> we show some qualitative results from DECA-R4 configuration on RGB data. We deploy our network training and testing on all the possible viewpoint combinations. The network takes as input either the top-view RGB <ref type="figure" target="#fig_3">(Fig. 4a</ref>) image or the front view <ref type="figure" target="#fig_3">(Fig. 4b)</ref> one. When trained and tested on the same viewpoint <ref type="figure" target="#fig_3">(Fig. 4d, 4e)</ref>, the network produces similar outputs, thus confirming its abil-  <ref type="table">Table 4</ref>: DECA-R4 results on the PanopTOP31K RGB dataset, with and without the Procrustes transformation <ref type="bibr" target="#b6">[7]</ref> (metric: MPJPE). Tasks: (i) 3D pose estimation from the front and top viewpoints (ii) viewpoint transfer for both front and top views. Test data is unseen during validation for both the viewpoint transfer tasks. ity to deal with the challenging top-view scenario. When training on the top view and testing on the front one ( <ref type="figure" target="#fig_3">Fig.  4f)</ref>, the network can accurately retrieve the positions of the lower body joints. DECA can retrieve parts of the body mostly occluded ad training time, thus displaying its generalization capabilities. When training on the front view and testing on the top one ( <ref type="figure" target="#fig_3">Fig. 4g)</ref>, the network can retrieve the positions of the upper body joints, which are visible in both images but from different perspectives, proving that DECA can internally model the viewpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We presented DECA, a deep viewpoint-equivariant method for human pose estimation on single RGB/depth images using capsule autoencoders. We show how Cap-sNets are better suited to deal with the 3D nature of raw data and how they allow taking a step forward to viewpoint equivariance. We have shown how our method can effectively generalize and achieve state-of-the-art results in both RGB and depth domains, as well as in the viewpoint transfer task. In future work, we aim at improving hands pose estimation and employing matrix capsules on bigger RGB datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FFigure 2 :</head><label>2</label><figDesc>[Better seen in color]. Overview of the proposed architecture. In light blue, the encoding module (Input, CNN encoder, Capsule layers), in green the interpretable feature space with capsule entities, in light orange the decoding module (fully connected decoders with multiple tasks and self-balancing loss).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 1</head><label>1</label><figDesc>. Datasets ITOP Dataset of depth images. The ITOP dataset [9] contains depth images from top and front view. The training split and the test split consist of 40k and 10k images, respectively. The depth images display 15 videos of 20 actors in a constrained setting. The dataset is recorded using two Axus Xtion Pro cameras. The 3D skeleton model consists of 15 joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>DECA-D1, T = [3D] (c) DECA-D2, T = [3D, W] (d) DECA-D3, T = [3D, 2D, W]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>DECA-R4 qualitative results on the PanopTOP31K dataset. On the left (a, b) the types of input accepted by DECA (top-view or front-view). DECA can also accept inputs in the depth domain. In the center (c), the corresponding 3D ground truth. On the right, the possible combinations of training/testing experiments. T stands for top and F stands for front. As an example, in (f), {T};{F} means that DECA has been trained exclusively on top data and tested on previously unseen (not even at validation time) front data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the state-of the art for ITOP front-view and top-view (metric: 0.1m mAP).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ITOP front-view</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ITOP top-view</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Body part</cell><cell cols="18">RF[28] RTW[37] IEF[3] VI [9] REN9x6x6[8] V2V[22] A2J[36] DECA-D3 RF[28] RTW[37] IEF[3] VI [9] REN9x6x6[8] V2V[22] A2J[36] DECA-D1 DECA-D2 DECA-D3</cell></row><row><cell>Head</cell><cell>63.80</cell><cell>97.80</cell><cell>96.20</cell><cell>98.10</cell><cell>98.70</cell><cell>98.29</cell><cell>98.54</cell><cell>93.87</cell><cell>95.40</cell><cell>98.40</cell><cell>83.80</cell><cell>98.10</cell><cell>98.20</cell><cell>98.40</cell><cell>98.38</cell><cell>94.41</cell><cell>95.31</cell><cell>95.37</cell></row><row><cell>Neck</cell><cell>86.40</cell><cell>95.80</cell><cell>85.20</cell><cell>97.50</cell><cell>99.40</cell><cell>99.07</cell><cell>99.20</cell><cell>97.90</cell><cell>98.50</cell><cell>82.20</cell><cell>50.00</cell><cell>97.60</cell><cell>98.90</cell><cell>98.91</cell><cell>98.91</cell><cell>98.86</cell><cell>99.16</cell><cell>98.68</cell></row><row><cell>Shoulders</cell><cell>83.30</cell><cell>94.10</cell><cell>77.20</cell><cell>96.50</cell><cell>96.10</cell><cell>97.18</cell><cell>96.23</cell><cell>95.22</cell><cell>89.00</cell><cell>91.80</cell><cell>67.30</cell><cell>96.10</cell><cell>96.60</cell><cell>96.87</cell><cell>96.26</cell><cell>96.12</cell><cell>97.51</cell><cell>96.57</cell></row><row><cell>Elbows</cell><cell>73.20</cell><cell>77.90</cell><cell>45.40</cell><cell>73.30</cell><cell>74.70</cell><cell>80.42</cell><cell>78.92</cell><cell>84.53</cell><cell>57.40</cell><cell>80.10</cell><cell>40.20</cell><cell>86.20</cell><cell>74.40</cell><cell>79.16</cell><cell>75.88</cell><cell>76.86</cell><cell>81.67</cell><cell>84.07</cell></row><row><cell>Hands</cell><cell>51.30</cell><cell>70.50</cell><cell>30.90</cell><cell>68.70</cell><cell>55.20</cell><cell>67.26</cell><cell>68.35</cell><cell>56.49</cell><cell>49.10</cell><cell>76.90</cell><cell>39.00</cell><cell>85.50</cell><cell>50.70</cell><cell>62.44</cell><cell>59.35</cell><cell>44.41</cell><cell>45.97</cell><cell>54.33</cell></row><row><cell>Torso</cell><cell>65.00</cell><cell>93.80</cell><cell>84.70</cell><cell>85.60</cell><cell>98.70</cell><cell>98.73</cell><cell>98.52</cell><cell>99.04</cell><cell>80.50</cell><cell>68.20</cell><cell>30.50</cell><cell>72.90</cell><cell>98.10</cell><cell>97.78</cell><cell>97.82</cell><cell>99.46</cell><cell>99.70</cell><cell>99.46</cell></row><row><cell>Hip</cell><cell>50.80</cell><cell>80.30</cell><cell>83.50</cell><cell>72.00</cell><cell>91.80</cell><cell>93.23</cell><cell>90.85</cell><cell>97.42</cell><cell>20.00</cell><cell>55.70</cell><cell>38.90</cell><cell>61.20</cell><cell>85.50</cell><cell>86.91</cell><cell>86.88</cell><cell>97.84</cell><cell>97.87</cell><cell>97.42</cell></row><row><cell>Knees</cell><cell>65.70</cell><cell>68.80</cell><cell>81.80</cell><cell>69.00</cell><cell>89.00</cell><cell>91.80</cell><cell>90.75</cell><cell>94.56</cell><cell>2.60</cell><cell>53.90</cell><cell>54.00</cell><cell>51.60</cell><cell>70.00</cell><cell>83.28</cell><cell>79.66</cell><cell>88.01</cell><cell>88.19</cell><cell>90.84</cell></row><row><cell>Feet</cell><cell>61.30</cell><cell>68.40</cell><cell>80.90</cell><cell>60.80</cell><cell>81.10</cell><cell>87.60</cell><cell>86.91</cell><cell>92.04</cell><cell>0.00</cell><cell>28.70</cell><cell>62.40</cell><cell>51.50</cell><cell>41.60</cell><cell>69.62</cell><cell>58.34</cell><cell>79.30</cell><cell>83.53</cell><cell>81.88</cell></row><row><cell>Upper Body</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.03</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>78.51</cell><cell>80.60</cell><cell>83.00</cell></row><row><cell>Lower Body</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>67.30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>95.30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.70</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.96</cell><cell>91.27</cell><cell>91.39</cell></row><row><cell>Mean</cell><cell>65.80</cell><cell>80.50</cell><cell>71.00</cell><cell>77.40</cell><cell>84.90</cell><cell>88.74</cell><cell>88.00</cell><cell>88.75</cell><cell>47.40</cell><cell>68.20</cell><cell>51.20</cell><cell>75.50</cell><cell>75.50</cell><cell>83.44</cell><cell>80.5</cell><cell>83.85</cell><cell>85.58</cell><cell>86.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of the art for the ITOP viewpoint transfer task (metric: 0.1m mAP). Training on front-view, validating on front-view, testing on top-view (top-view data is unseen in validation).</figDesc><table><row><cell></cell><cell cols="2">DECA-D3</cell></row><row><cell>Body part</cell><cell>Train on front, test on top</cell><cell>Train on top, test on front</cell></row><row><cell>Head</cell><cell>46.27</cell><cell>18.51</cell></row><row><cell>Neck</cell><cell>73.14</cell><cell>44.77</cell></row><row><cell>Shoulders</cell><cell>69.02</cell><cell>25.18</cell></row><row><cell>Elbows</cell><cell>43.87</cell><cell>16.23</cell></row><row><cell>Hands</cell><cell>9.41</cell><cell>2.19</cell></row><row><cell>Torso</cell><cell>85.94</cell><cell>68.63</cell></row><row><cell>Hip</cell><cell>72.15</cell><cell>64.75</cell></row><row><cell>Knees</cell><cell>49.31</cell><cell>68.15</cell></row><row><cell>Feet</cell><cell>42.46</cell><cell>46.12</cell></row><row><cell>Upper Body</cell><cell>45.00</cell><cell>18.81</cell></row><row><cell>Lower Body</cell><cell>59.11</cell><cell>60.95</cell></row><row><cell>Mean</cell><cell>51.85</cell><cell>38.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>DECA-D3 complete results for the ITOP viewpoint transfer tasks (metric: 0.1m mAP). Test data is unseen during validation for both the cases.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognition-by-components: a theory of human image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irving</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10130</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Spherical cnns. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Panoptop: a framework for generating viewpoint-invariant human pose estimation datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Garau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Martinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Br?dka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niccol?</forename><surname>Bisagno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Conci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
		<respStmt>
			<orgName>Chia Laguna Resort</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Procrustes methods in the statistical analysis of shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Goodall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="321" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards good practices for deep 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cairong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07248</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards viewpoint invariant 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boya</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="160" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida D</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix capsules with EM routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frosst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">Scott</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Late temporal modeling in 3d cnn architectures with bert for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Esat Kalfaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aydin Alatan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="731" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Latent Representations of 3D Human Pose with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bugra</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1326" to="1341" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Adam Roman Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Stacked capsule autoencoders</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature boosting network for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="494" to="501" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern Recognition</title>
		<meeting>the IEEE conference on computer vision and pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian capsule networks for 3d human pose estimation from single 2d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iv?n</forename><surname>Ram?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo</forename><surname>Cuesta-Infante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Schiavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan Jos?</forename><surname>Pantrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Capsule routing via variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Leontidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3749" to="3756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian image based 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Sanzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valsamis</forename><surname>Ntouskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiora</forename><surname>Pirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="566" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mat</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural state machine for character-scene interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="209" to="210" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>M?rquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3941" to="3950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Densely connected attentional pyramid residual network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangsen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<biblScope unit="page" from="13" to="23" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">man pose machines with self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="1069" to="1082" />
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
	<note>3d hu</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human pose estimation from depth images via inference embedded multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A2j: Anchor-tojoint regression network for 3d articulated pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boshen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taidong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="793" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Random tree walk toward instantaneous 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><surname>Ho Yub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soochahn</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Seok</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Dong</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
