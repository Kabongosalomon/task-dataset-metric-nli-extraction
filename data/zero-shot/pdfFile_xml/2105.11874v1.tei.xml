<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Learning with Part Discovery and Augmentation from Unlabeled Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
							<email>chenyang.si@cripac.ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
							<email>zlwang@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Learning with Part Discovery and Augmentation from Unlabeled Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot learning is a challenging task since only few instances are given for recognizing an unseen class. One way to alleviate this problem is to acquire a strong inductive bias via meta-learning on similar tasks. In this paper, we show that such inductive bias can be learned from a flat collection of unlabeled images, and instantiated as transferable representations among seen and unseen classes. Specifically, we propose a novel part-based self-supervised representation learning scheme to learn transferable representations by maximizing the similarity of an image to its discriminative part. To mitigate the overfitting in few-shot classification caused by data scarcity, we further propose a part augmentation strategy by retrieving extra images from a base dataset. We conduct systematic studies on miniImageNet and tieredImageNet benchmarks. Remarkably, our method yields impressive results, outperforming the previous best unsupervised methods by 7.74% and 9.24% under 5-way 1-shot and 5-way 5-shot settings, which are comparable with state-of-the-art supervised methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, great progress in the computer vision community has been achieved with deep learning, which often needs numerous training data. Unfortunately, there are many practical applications where collecting data is very difficult. To learn a novel concept with only few examples, few-shot learning has been recently proposed and gains extensive attention. <ref type="bibr" target="#b1">[Doersch et al., 2020;</ref><ref type="bibr" target="#b0">Afrasiyabi et al., 2020]</ref> A possible solution to few-shot learning is meta-learning <ref type="bibr" target="#b2">[Finn et al., 2017;</ref><ref type="bibr" target="#b4">Ravi and Larochelle, 2017;</ref><ref type="bibr" target="#b4">Snell et al., 2017;</ref><ref type="bibr" target="#b4">Peng et al., 2019]</ref>. It first extracts shared prior knowledge from many similar tasks. After that, this knowledge will be adapted to the target few-shot learning task to restrict the hypothesis space, which makes it possible to learn a novel concept with few examples. Equipped with neural networks, the prior knowledge can be parameterized as an embedding function, or a set of initial parameters of * Contact Author a network, which often needs an extra labeled base dataset for training. Another line of work is based on transfer learning <ref type="bibr" target="#b5">[Tian et al., 2020;</ref><ref type="bibr" target="#b1">Dhillon et al., 2020]</ref>, which extracts prior knowledge as a pre-trained feature extractor. Typically, a standard cross entropy loss is employed to pre-train the feature extractor, which also needs a labeled base dataset. However, collecting a large labeled dataset is time-consuming and laborious. Besides, these labels are sadly discarded when performing a target few-shot learning task, because they belong to different class spaces. Inspired by recent progress of unsupervised learning, a question is naturally asked: can we can learn prior knowledge only from unlabeled images? If yes, it will be a promising approach for the scenario where many unlabeled images are available but the target task is data-scarce. Some remarkable works have made an effort for this purpose, e.g., unsupervised meta-learning <ref type="bibr" target="#b2">[Hsu et al., 2019;</ref><ref type="bibr">Khodadadeh et al., 2019]</ref>. However, these unsupervised methods are hindered by learning effective class-related representations from images, compared to the supervised counterparts. This is because much unrelated or interfering information, e.g., background clutters, may impose adverse impacts on representation learning under label-free unsupervised setting. Selecting discriminative image regions or target parts is an effective way to reduce this interference during representation learning, which has a consistent motivation with traditional part-based models <ref type="bibr">[Felzenszwalb et al., 2009]</ref>.</p><p>In this paper, we propose a part-based self-supervised learning model, namely Part Discovery Network (PDN), to learn more effective representations from unlabeled images.</p><p>The key point of this model is mining discriminative target parts from images. Due to the lack of part labels, multiple image regions are first extracted via random crop, which inevitably contain interfering backgrounds or less informative regions. To eliminate these regions, we choose the image region as the most discriminative target part, which keeps the largest average distance to other images (negative samples). The rationale of this selection is that the discriminative part should be able to distinguish the original image from others <ref type="bibr" target="#b4">[Singh et al., 2012]</ref>, so its average distance to other images should be large enough. With the selected discriminative part, we maximize its similarity to the original image in a similar way to <ref type="bibr" target="#b2">[He et al., 2020]</ref>.</p><p>Another challenge in the few-shot scenario is that the target <ref type="figure">Figure 1</ref>: The main idea of the proposed Part Augmentation Network. The classifier is easy to overfit due to the scarcity of support samples in few-shot scenario. We retrieve relevant parts from unlabeled images to augment support set. The augmented support set can help learn a robust classifier with clear decision boundary.</p><p>classifier is easy to overfit due to the scarcity of support training samples. An effective way to prevent overfitting is data augmentation, which has been explored in the few-shot learning literature <ref type="bibr" target="#b4">[Schwartz et al., 2018;</ref><ref type="bibr" target="#b5">Wang et al., 2018]</ref>. However, most of these methods assume that the variance of seen classes can be directly transferred to unseen classes, which is too strict in most situations. In this paper, we resort to retrieve extra images from a base dataset and extract their part features to augment support set, based on the fact that similar objects generally share some common parts. The core of our method is selecting these part features which match well with image features in the support set. Specifically, we propose a novel Class-Competitive Attention Map (C 2 AM) to guide the relevant part selection from the retrieved images, and then refine target classifier with these selected parts. Our method is also called Part Augmentation Network (PAN), and <ref type="figure">Figure  1</ref> illustrates the main idea of PAN. Our Part Discovery and Augmentation Network (PDA-Net) consisting of both PDN and PAN largely outperforms the stat-of-the-art unsupervised few-shot learning methods, and achieves the comparable results with most of the supervised methods. The contributions of this work can be summarized as:</p><p>? We propose a novel self-supervised Part Discovery Network, which can learn more effective representations from unlabeled images for few-shot learning. ? We propose a Part Augmentation Network to augment few support examples with relevant part features, which mitigates overfitting and leads to more accurate classification boundaries. ? Our method outperforms previous unsupervised methods on two standard few-shot learning benchmarks. Remarkably, our unsupervised PDA-Net is also comparable with supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Few-Shot Learning. Few-shot learning aims at learning a novel concept from few examples. A possible solution is meta-learning, which extracts prior knowledge from many similar tasks (episodes). For example, MAML <ref type="bibr" target="#b2">[Finn et al., 2017]</ref> learns the optimal initial parameters of a network, which can be quickly adapted to a new task via gradient <ref type="bibr">descent. Matching Networks [Vinyals et al., 2016]</ref> classifies an instance based on its nearest labeled neighbor in the learned embedding space. Different from meta-learning, <ref type="bibr" target="#b1">[Dhillon et al., 2020]</ref> demonstrate that a strong transfer learning baseline can achieve competitive performance. In order to train models, all of the above methods need a large labeled base dataset. Unsupervised meta-learning methods loose this constraint by constructing training episodes from unlabeled images. <ref type="bibr" target="#b2">[Hsu et al., 2019]</ref> propose to acquire pseudo labels by clustering in an unsupervised feature space. <ref type="bibr">[Khodadadeh et al., 2019]</ref> use random sampling and augmentation to create synthetic episodes. Our method aims at both learning more effective representation from unlabeled images and increasing support set by data augmentation for unsupervised few-shot learning.</p><p>Self-Supervised Learning. Self-supervised learning aims at leveraging unlabeled images to learn good representations for down-stream tasks. Previous work mainly focuses on mining supervision signals from unlabeled data <ref type="bibr" target="#b2">[Gidaris et al., 2018;</ref><ref type="bibr" target="#b1">Doersch et al., 2015;</ref><ref type="bibr" target="#b4">Zhang et al., 2016]</ref>. Recently, contrastive learning shows superior performance improvement, which maximizes the similarity of two different views of the same image on global level <ref type="bibr">[Chen et al., 2020]</ref> or local level <ref type="bibr">[Ouali et al., 2020]</ref>, or enforces consistency of cluster assignments between two views <ref type="bibr" target="#b0">[Caron et al., 2020]</ref>.</p><p>In this paper, we explicitly mine discriminative parts for contrastive learning, which learns more effective representations from unlabeled images.</p><p>Data Augmentation for Few-Shot Learning. A straightforward way to deal with data deficiency is to synthesize more data. <ref type="bibr" target="#b4">[Schwartz et al., 2018]</ref> propose to extract intra-class variance (deltas) from base classes and use them to synthesize samples for novel classes.</p><p>[Wang et al., 2018] encode the intra-class variance in a hallucinator, which can synthesize new features taking as input a reference feature and noise vectors. To leverage base class samples, <ref type="bibr">[Chen et al., 2019]</ref> propose to generate a deformed image by fusing a probe image and a gallery image. Our method augments support set by retrieving extra images from a base dataset and extracting matched part features with the support samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Notation</head><p>In this section, we briefly illustrate the formulation of the fewshot image classification problem. Given a labeled support set</p><formula xml:id="formula_0">S = {(x i , y i )} Ns i=1</formula><p>where x i ? I is an image and y i ? C novel is its label, we are supposed to predict the labels of a query set</p><formula xml:id="formula_1">Q = {(x i , y i )} Nq i=1</formula><p>, which also belongs to C novel . The number of classes |C novel | is called way and the number of samples in each class is called shot. For few-shot learning, the shot is very small, like 1-shot or 5-shot. Due to the scarcity of support samples, it is very hard to train a classification model from scratch. Therefore, we are given an extra large base dataset D base to mine prior knowledge. Here, D base is from base classes C base , and C base ? C novel = ?. Previous   <ref type="figure">Figure 2</ref>: The framework of the proposed Part Discovery and Augmentation Network. In Part Discovery Network (PDN), we first extract multiple parts from an image, and then select the most discriminative one. Through maximizing the similarity of the global view of the image to the selected discriminative part, we learn effective representations from unlabeled images. In Part Augmentation Network (PAN), we retrieve relevant images to the support set from unlabeled images, and then create a Class-Competitive Attention Map (C 2 AM) to select relevant parts as augmented features. Finally, a classifier is trained on both support features and augmented features. Note that the feature extractor in PAN is transferred from PDN.</p><p>works usually need base labels to construct training episodes or pre-train the classification model. In this paper, we only use unlabeled images in D base , i.e.,</p><formula xml:id="formula_2">D base = {x i } N b i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>Our method follows a transfer learning protocol which includes two stages, namely the representation learning stage and the few-shot learning stage. On the first stage, we aim at learning a feature extractor f from D base . On the second stage, the learned feature extractor f is transferred to the target few-shot learning task, followed by training a linear classifier using support set S. The final classification results of query set Q are predicted by the learned classifier. From the above description we can see that the key is to learn a good feature extractor from base dataset D base and meanwhile obtain a robust classifier with limited data in support set S. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Part Discovery Network</head><p>Our Part Discovery Network (PDN) is a part-based selfsupervised learning model, which maximizes the similarity between representations of the global view of an image and its discriminative part. We will introduce the details of selecting discriminant part as follows.</p><p>Extracting Multiple Parts. Without part labels, we generate parts by randomly cropping a given image x into n patches {x p i } n i=1 . Meanwhile, a larger crop x g is obtained to serve as global context. Random transformations are further applied to increase data diversity. To get latent part representations {h p i } n i=1 and global representation h g , a convolution neural network f is applied followed by global average pooling and a MLP projection head.</p><p>Selecting Discriminative Part. Since we generate multiple parts with random crop, there are inevitably some crops that belong to background. Directly matching these crops to the global view will create bias towards backgrounds and hurt the generalization of the learned representations. To solve this problem, we develop an effective strategy to select the most discriminative part. Given a set of negative samples</p><formula xml:id="formula_3">D ? = {h g? i } N ? i=1</formula><p>that are exclusive with the input image x, we define a sample-set distance metric d(h p i , D ? ) in the feature space, which indicates the distance between a part and all negative samples. We select the part with the maximum distance as the most discriminative one, therefore excluding background crops and less informative ones. The strategy is formulated as</p><formula xml:id="formula_4">h p := h p i * , where i * = arg max i d(h p i , D ? )<label>(1)</label></formula><p>where h p is the selected part. The rationale of this strategy is that the discriminative part should be able to distinguish the original image from others, so its distance to other images should be very large. The distance between h p i and D ? can be defined in many forms. One could use the minimum distance between h p i and all samples in D ? . However, some similar images may exist in D ? , so the minimum distance is severely affected by these similar images and can not reflect the true distance to most negative samples. Here we choose the mean distance to calculate d(h p i , D ? ), which considers more on the samples of other classes. d(h p i , D ? ) is calculated as:</p><formula xml:id="formula_5">d(h p i , D ? ) = 1 |D ? | h g? ?D ? ?s(h p i , h g? )<label>(2)</label></formula><p>where |D ? | is the number of negative samples in this set, and s is the cosine similarity.</p><p>Training. With selected discriminative parts, we train the PDN with a contrastive loss, which is formulated as</p><formula xml:id="formula_6">L ct = ? log exp(s(h p ,h g )/? ) exp(s(h p ,h g )/? + h g? ?D ? exp(s(h p ,h g? )/? ) (3)</formula><p>where ? denotes a temperature hyper-parameter.</p><p>To get a large negative set, we follow [He et al., 2020] to organize D ? as a queue and use a momentum encoder to get consistent negative representations. The momentum encoder is an exponential moving average version of the feature extractor and MLP head. Its parameter ? m is updated as</p><formula xml:id="formula_7">? m ?? m? t + (1 ? m)? m<label>(4)</label></formula><p>where m is a momentum hyper-parameter, and ? t denotes the parameters of the feature extractor and MLP head at training step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Part Augmentation Network</head><p>Our Part Augmentation Network (PAN) aims to augment support set with relevant part features from base dataset. To this end, we first retrieve extra images from base dataset, then create a Class-Competitive Attention Map (C 2 AM) to guide the relevant part selection from the retrieved images, and finally refine target classifier with these selected parts.</p><p>Retrieving Extra Images. Since the size of D base is very large, we propose a simple but effective strategy to select a small number of very similar images from D base , which are more likely to contain relevant parts. Specifically, we first train a linear classifier p(y|z; W , b) on support set S, where z denotes a feature vector, W and b are weight matrix and bias, respectively. Then, we employ this classifier to classify each image in D base as:</p><formula xml:id="formula_8">y = arg max i p(y = i|z = GAP (M ))<label>(5)</label></formula><p>where GAP is a global average pooling operator and M denotes the feature map extracted by the learned feature extractor f in Section 4.1</p><p>Among the images of class k in D base , we keep the N a images with the highest classification probability as the retrieval results. The feature maps of these N a images are denoted as</p><formula xml:id="formula_9">A k = {M k i } Na i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-Competitive Attention Map</head><p>To further extract relevant parts from retrieved feature maps A k , we propose a novel CAM-based <ref type="bibr" target="#b5">[Zhou et al., 2016]</ref> attention mechanism, Class-Competitive Attention Map (C 2 AM), illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. Given a feature map M k ? A k , we obtain a class attention map S k that indicates its relevance to class k at each spatial location:</p><formula xml:id="formula_10">S k (i, j) = W k M k (i, j) + b k (6) where S k (i, j)</formula><p>and M k (i, j) are the classification score for class k and the feature vector at location (i, j), respectively. W k , b k are the classifier weight vector and bias for class k, which have been learned in p(y|z; W , b).</p><p>Although the class attention map S k is very useful to locate class-specific image regions or object parts, it still contains some parts that have high classification scores for all classes. These parts provide less information for classification, and should be further inhibited. We perform softmax over the classification score vector S(i, j) at each spatial location, which provides a class competitive mechanism to highlight the parts which only have higher score for class k. The classcompetitive attention map ? k for class k is as follows:</p><formula xml:id="formula_11">? k (i, j) = exp(S k (i, j)) |C novel | c exp(S c (i, j))<label>(7)</label></formula><p>With this revised attention map ? k , we can extract more relevant part features z k to augment the class k, which is calculated by the weighted sum of feature map M k :</p><formula xml:id="formula_12">z k = i,j ? k (i, j)M k (i, j) i,j ? k (i, j)<label>(8)</label></formula><p>The retrieved feature set for class k is now updated as A k = {z k i } Na i=1 . We denote A = ? k A k as the set of augmented part features for all classes. Refining Target Classifier. With augmented part features, we can now refine the initial classifier with both support set S and augmented set A to get a more robust classifier. Since augmented features are not from the exactly same classes as support samples, we adopt the label smoothing technique <ref type="bibr">[Szegedy et al., 2016]</ref> to prevent overfitting on the augmented features. Specifically, we convert an one-hot label y into a smoothed probability distribution p y :</p><formula xml:id="formula_13">p y (k) = 1 ? , k = y |C novel |?1 , k = y<label>(9)</label></formula><p>where p y (k) is the probability of the class k, and is a hyperparameter in range (0, 1). Finally, the total loss for refining the classifier is:</p><formula xml:id="formula_14">L cls = z?S ? log p(y|z) + ? z?A KL(p(?|z), p y ) (10)</formula><p>where the first term is cross entropy loss for samples in support set S, and the second term is K-L divergence between predicted probability distribution p(?|z) and smoothed ground-truth distribution p y for augmented features.  <ref type="table">Table 1</ref>: Comparison with prior work on miniImageNet and tieredImageNet. Accuracy is reported with 95% confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We conduct experiments on two widely-used few-shot learning datasets, namely miniImageNet and tieredImageNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>For PDN, we transform input images with random crop, horizontal flip, color jitter and Gaussian blur. The crop scales for part view and global view are in range of (0.05, 0.14) and (0.14, 1), respectively. The number of cropped parts is set as n = 6 by cross validation. The size of D ? is 1024 and 10240 for miniImageNet and tieredImageNet, respectively. We use standard ResNet-50 as backbone and resize images to 224 ? 224. We set hyper-parameter m = 0.999 and ? = 0.2. We adopt SGD optimizer with cosine learning rate decay. The learning rate is 0.015 for miniImageNet and 0.03 for tieredImageNet. For PAN, we retrieve N a = 1024 extra images for each class. The label smoothing hyper-parameter is 0.2 for 1shot and 0.7 for 5-shot. The loss weight ? is set as 1. The classifier is trained with Adam and the learning rate is 0.001. Following the similar setting to   <ref type="table">Table 2</ref>: Ablation study on miniImageNet in 5-way 1-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Prior Work</head><p>In <ref type="table">Table 1</ref>, we compare our method with both supervised and unsupervised few-shot learning methods. Overall, our method achieves the best performance under unsupervised setting, and is comparable with state-of-the-art supervised methods.</p><p>Our PDA-Net significantly outperforms unsupervised meta-learning <ref type="bibr" target="#b2">[Hsu et al., 2019]</ref> by 23.91% and 29.14% on miniImageNet in 1-shot and 5-shot, respectively. Compared with the most related method which also employs MoCo for representation learning <ref type="bibr" target="#b5">[Tian et al., 2020]</ref>, we achieve 9.65% and 10.07% improvement in 1-shot and 5-shot on miniIm-ageNet. On the larger tieredImageNet, our method largely outperforms a recent <ref type="bibr">GAN-based method, GdBT2 [Khoi and Sinisa, 2020]</ref>, by 21.15% and 16.50% in 1-shot and 5-shot, respectively. The significant improvements over the compared unsupervised methods verify the effectiveness of our PDA-Net.</p><p>Furthermore, our method outperforms two supervised meta-learning methods, <ref type="bibr">LEO [Rusu et al., 2019]</ref> and MetaOptNet <ref type="bibr">[Lee et al., 2019]</ref>, on both datasets in both 1shot and 5-shot. Compared with the supervised transfer learning method, Distill <ref type="bibr" target="#b5">[Tian et al., 2020]</ref>, the performance of our PDA-Net only drops 0.98% on miniImageNet in 1-shot, which is acceptable considering the expensive label budget. Alignment <ref type="bibr" target="#b0">[Afrasiyabi et al., 2020]</ref> uses base labels to detect the most related class, which achieves better performance in    1-shot, but is comparable with ours in 5-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Further Analysis</head><p>Ablation Study. We evaluate the effectiveness of each key component of PDA-Net on miniImageNet in 1-shot, and show the results in <ref type="table">Table 2</ref>. The baseline is constructed by ablating part discovery and part augmentation, thus only taking as input two global views for contrastive learning. PDN w/o Select means that we use all the 6 parts for contrastive learning, while PAN w/o C 2 AM means that we use class activation map instead <ref type="bibr" target="#b5">[Zhou et al., 2016]</ref>. Compared with the baseline, PDN w/o Select obtains 2.20% performance gain, which verifies the advantage of part-based representation learning. PDN can get further improvement via selecting the most discriminative part. Better results are achieved by adding PAN to PDN or PDN w/o Select, which illustrates the effectiveness of our part augmentation strategy. Compared with CAM in <ref type="bibr" target="#b5">[Zhou et al., 2016]</ref>, our C 2 AM can obtain better performance, demonstrating the effectiveness of our proposed class competitive mechanism.</p><p>Number of Crops. In PDN, we use random crop to get n parts. Here we evaluate the impact of number of cropped parts on representation learning. As shown in <ref type="table" target="#tab_6">Table 3</ref>, the performance rapidly increases with the growing number of cropped parts, because it is more likely to contain discriminative parts with more cropped parts. The performance gets saturated after reaching 6, which indicates that our selection strategy is an effective way to extract discriminative parts.</p><p>Number of Augmented Features. In PAN, we retrieve N a extra images to augment each novel class. Here we experiment with different N a and show the results in <ref type="figure" target="#fig_2">Figure 4</ref>. It can be seen that the benefit of augmented features is more significant for 1-shot than 5-shot. For 1-shot, the performance grows rapidly as N a increases and finally gets saturated after N a = 1024, which is because relevant features in base classes are limited. With a larger base dataset, we can infer that our method can get better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Visualization</head><p>To explore the discriminative part selection, we visualize the cropped parts and their corresponding global view in <ref type="figure" target="#fig_3">Figure  5</ref>. It should be noted that these images are all randomly transformed, so they may have different appearances even from the same input image. We sort the cropped parts based on the distances to negative samples and select the parts with the largest distances within red boxes. For example, the main body of a bird and the head of a dog are selected in the first and fourth rows, respectively. We can see that as the distances decrease, the parts are more likely to belong to background and contain less information about target object.</p><p>To illustrate the rationality of part augmentation strategy, we visualize the retrieved unlabeled images and their classcompetitive attention maps in <ref type="figure" target="#fig_4">Figure 6</ref>. We can see that the retrieved images are usually similar to support images. More interestingly, C 2 AM can locate class-specific image regions which are very relevant to the objects in support set. For example, the antennae and legs of the ladybug are highlighted in the retrieved image, which are very similar to those of the ant in support image. In contrast, its shell attracts less attention due to the distinctive appearance from the ant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a Part Discovery Network, which can extract effective prior knowledge for few-shot learning from a flat collection of unlabeled images. Furthermore, a Part Augmentation Network is proposed to augment support examples with relevant parts, which can mitigate overfitting and lead to better classification boundaries. The experimental results demonstrate that our method significantly outperforms previous unsupervised meta-learning methods and achieves comparable accuracy with state-of-the-art supervised methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of C 2 AM. We omit bias item for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Accuracy with different number of augmented features in PAN on miniImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the global view and cropped parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of retrieved images and their C 2 AMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1. Part Discovery Network 2. Part Augmentation Network Support Features Augmented Features Unlabeled Images Support Set Query Set Query Features Classifier Test ? ? GAP MLP Similarity GAP Transfer Retrieve Images &amp; Extract Part Features Global Parts Select Discriminative Part Train</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>64?0.61 78.63?0.46 65.99?0.72 81.56?0.53 Distill [Tian et al., 2020] ResNet-12 64.82?0.60 82.14?0.43 71.52?0.69 86.03?0.49 Finetune [Dhillon et al., 2020] WRN-28-10 57.73?0.62 78.17?0.49 66.58?0.70 85.55?0.48 LEO [Rusu et al., 2019] WRN-28-10 61.76?0.08 77.59?0.12 66.33?0.05 81.44?0.09 CC+rot [Gidaris et al., 2019] WRN-28-10 62.93?0.45 79.87?0.33 70.53?0.51 84.98?0.36 Align [Afrasiyabi et al., 2020] WRN-28-10 65.92?0.60 82.85?0.55 74.40?0.68 86.61?0.59</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">miniImageNet 5-way</cell><cell cols="2">tieredImageNet 5-way</cell></row><row><cell>Setting</cell><cell>Method</cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell></cell><cell>IDeMe-Net [Chen et al., 2019]</cell><cell>ResNet-12</cell><cell cols="2">59.14?0.86 74.63?0.74</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">Supervised 62.Unsupervised MetaOptNet [Lee et al., 2019] ResNet-12 CACTUs [Hsu et al., 2019] Conv4 39.90?n/a UMTRA [Khodadadeh et al., 2019] Conv4 39.93?n/a Rot [Gidaris et al., 2019] WRN-28-10 43.43?n/a GdBT2 [Khoi and Sinisa, 2020] SN-GAN 48.28?0.77 66.06?0.70 47.86?0.79 67.70?0.75 53.97?n/a --50.73?n/a --60.86?n/a --</cell></row><row><cell></cell><cell>MoCo [Tian et al., 2020]</cell><cell>ResNet-50</cell><cell cols="2">54.19?0.93 73.04?0.61</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>CMC [Tian et al., 2020]</cell><cell>ResNet-50</cell><cell cols="2">56.10?0.89 73.87?0.65</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>PDA-Net (Ours)</cell><cell>ResNet-50</cell><cell cols="4">63.84?0.91 83.11?0.56 69.01?0.93 84.20?0.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>miniImageNet. miniImageNet is a standard benchmark for few-shot learning proposed by. It is a subset of the ImageNet [Russakovsky et al., 2015] and contains 100 classes and 600 examples for each class. We follow the protocol in [Ravi and Larochelle, 2017] to use 64 classes for training, 16 classes for validation and 20 classes for test.</figDesc><table><row><cell>tieredImageNet. tieredImageNet [Ren et al., 2018] is a</cell></row><row><cell>larger subset of ImageNet and contains 608 classes and 1000</cell></row><row><cell>images in each class. Theses classes are grouped into 34</cell></row><row><cell>higher categories, where 20 categories (351 classes) for train-</cell></row><row><cell>ing, 6 categories (97 classes) for validation and 8 categories</cell></row><row><cell>(160 classes) for test. The large semantic difference between</cell></row><row><cell>categories makes it more challenging for few-shot learning.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Accuracy with different number of cropped parts in PDN on miniImageNet.</figDesc><table><row><cell>Global View</cell><cell>Cropped Parts</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by National Natural Science Foundation of China (61976214, 61633021, 61721004 , 61836008), Beijing Municipal Natural Science Foundation (4214075), and Science and Technology Project of SGCC Research on feature recognition and prediction of typical ice and wind disaster for transmission lines based on small sample machine learning method.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Afrasiyabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>NeurIPS, 2020. [Chen et al., 2019] Zitian Chen, Yanwei Fu, Yu-Xiong Wang, Lin Ma, Wei Liu, and Martial Hebert</editor>
		<imprint>
			<publisher>Ting Chen</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ankush Gupta, and Andrew Zisserman. Crosstransformers: Spatially-aware fewshot transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<meeting><address><addrLine>Pedro Felzenszwalb, Ross Girshick, David McAllester, and Deva Ramanan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning. CVPR, 2020</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Khodadadeh et al., 2019] Siavash Khodadadeh, Ladislau Boloni, and Mubarak Shah</editor>
		<meeting><address><addrLine>Subhransu Maji</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>A self-supervised gan for unsupervised few-shot object recognition. Lee et al., 2019] Kwonjoon Lee</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">C?line Hudelot, and Myriam Tami. Spatial contrastive learning for few-shot classification. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Ouali et al., 2020] Yassine Ouali</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sivan Harary, Mattias Marder, Abhishek Kumar, Rogerio Feris, Raja Giryes, and Alex Bronstein. Delta-encoder: an effective sample synthesis method for few-shot object recognition</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>NeurIPS</editor>
		<meeting><address><addrLine>Jake Snell, Kevin Swersky, and Richard Zemel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Prototypical networks for few-shot learning. Szegedy et al., 2016] Christian Szegedy. Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CVPR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: A good embedding is all you need</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Richard Zhang, Phillip Isola, and Alexei A. Efros</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
