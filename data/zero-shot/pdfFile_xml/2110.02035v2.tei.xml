<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FooDI-ML: a large multi-language dataset of food, drinks and groceries images and descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Amat Ol?ndriz</surname></persName>
							<email>david.amat@glovoapp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">glovoapp Carrer de Pallars</orgName>
								<address>
									<addrLine>190</addrLine>
									<postCode>08005</postCode>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pon?</forename><forename type="middle">Palau</forename><surname>Puigdevall</surname></persName>
							<email>ponc.puigdevall@glovoapp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">glovoapp Carrer de Pallars</orgName>
								<address>
									<addrLine>190</addrLine>
									<postCode>08005</postCode>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><forename type="middle">Salvador</forename><surname>Palau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">glovoapp Carrer de Pallars</orgName>
								<address>
									<addrLine>190</addrLine>
									<postCode>08005</postCode>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FooDI-ML: a large multi-language dataset of food, drinks and groceries images and descriptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we introduce the FooDI-ML dataset. This dataset contains over 1.5M unique images and over 9.5M store names, product names descriptions, and collection sections gathered from the Glovo application. The data made available corresponds to food, drinks and groceries products from 37 countries in Europe, the Middle East, Africa and Latin America. The dataset comprehends 33 languages, including 870K samples of languages of countries from Eastern Europe and Western Asia such as Ukrainian and Kazakh, which have been so far underrepresented in publicly available visio-linguistic datasets. The dataset also includes widely spoken languages such as Spanish and English. To assist further research, we include benchmarks over two tasks: text-image retrieval and conditional image generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The COVID19 pandemic has accelerated the digitalisation of restaurants and the growth of the food delivery sector. National lockdowns have made it impossible to go to bars and restaurants, which has prompted many people to discover the possibility of ordering food and drinks online. Therefore, solving tasks such as text-image retrieval for food and drinks search engines has become increasingly important. The lack of large-scale multilingual datasets covering this domain <ref type="bibr" target="#b15">[16]</ref> means that it is hard to build efficient search engines and recommender systems, especially for under-represented languages <ref type="bibr" target="#b20">[21]</ref>. In this paper, we aim to help bridge this gap by offering a large multi-language dataset that covers several of such languages, in addition to some common languages such as Spanish, French and English: the FooDI-ML dataset. Our dataset features data collected by a food and groceries delivery app: Glovo.</p><p>In recent years there has been increased attention to visio-linguistic datasets. Besides classical vision tasks such as image classification and segmentation, image and text datasets can be used to learn multi-modal tasks. For example, image captioning, text to image generation and textimage retrieval. The size of publicly available image and text datasets has grown steadily from the flickr30K dataset <ref type="bibr" target="#b28">[29]</ref>, including 30K English-language examples, to the WIT dataset, a multi-language dataset extracted from Wikipedia that includes over 11M unique examples. To our knowledge, FooDI-ML is the second largest multilingual visiolinguistic dataset available to date.</p><p>The structure of this paper is as follows: in Sec. 2 we review prior work and place our dataset in a broader research context. In Sec. 3 we describe the dataset in detail, including notable examples and proposed tasks. Sec. 4 presents benchmarks over the text-image retrieval and conditional image generation tasks. Finally, Sec. 5 features the conclusion and a discussion on future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Our contribution</head><p>? We present the FooDI-ML dataset, a dataset of over 2.8M food, drinks and grocery images included together with over 9.5M natural language descriptions, store names, product names, and collection sections in which they are included. It is the second largest multilingual dataset of this kind publicly available.</p><p>? We provide a dataset covering a use domain rarely covered (multi-language samples for food, drinks and groceries).</p><p>? The dataset covers languages that despite having a large number of speakers are under-represented in trained datasets. Therefore, it has potential to reduce bias in food, drinks and grocery based search engines.</p><p>? We compute a train/test/validation split and two benchmark tasks to facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior work 2.1. Datasets</head><p>Datasets including images coupled to natural language data are often referred to as visio-linguistic datasets <ref type="bibr" target="#b22">[23]</ref>. The two pioneering datasets in the field were published just a few months apart from each other: the flickr30K and COCO datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref>. Both datasets were collected by asking humans to label tenths of thousands of images manually. This means that these datasets, albeit seminal, do not include an amount of data comparable with vision-only or text-only datasets with sample numbers routinely reaching the order of 100M samples or more <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>In addition to their relatively smaller size, these initial datasets included exclusively English language examples. This shortcoming spun an effort to extend the work to other languages. Soon, new datasets, together with relabeled versions of the same datasets appeared in several languages. Amongst them German <ref type="bibr" target="#b8">[9]</ref>, Japanese <ref type="bibr" target="#b17">[18]</ref>, Chinese <ref type="bibr" target="#b13">[14]</ref>, Dutch <ref type="bibr" target="#b25">[26]</ref>, Italian <ref type="bibr" target="#b1">[2]</ref> and Vietnamese <ref type="bibr" target="#b12">[13]</ref>. In addition to this, crow-sourced translations of the COCO and flickr30K dataset are relatively frequent and can be found in non-peerreviewed github repositories and other sources (see, for example, the Spanish translation available in Kaggle <ref type="bibr" target="#b6">[7]</ref>).</p><p>The problem of dataset size and language coverage was not solved until recently when two approaches (and corresponding datasets) were introduced, both of them based in web crawling: the Conceptual Captions dataset <ref type="bibr" target="#b21">[22]</ref> and the Wikipedia-based Image Text Dataset <ref type="bibr" target="#b22">[23]</ref>. Google's Conceptual Captions (CC) dataset includes 3.3M English examples obtained by web crawling of images and alt-text annotations. The samples in the dataset are then cleaned using a complex funnel of rules <ref type="bibr" target="#b21">[22]</ref>. Another dataset from google, the google Open Images Dataset <ref type="bibr" target="#b11">[12]</ref> provides 9M images annotated with 675k localized narratives. The Wikipediabased Image Text Dataset (WIT) managed to avoid complex cleaning rules by relying on Wikipedia as a highly curated data source. In doing so, it managed to gather a larger amount of high-quality data. This provided the largest publicly available dataset to date, including more than 11M unique examples in more than 100 different languages <ref type="bibr" target="#b22">[23]</ref>.</p><p>The increase in size and language coverage achieved by WIT and CC was especially aimed at obtaining large pretrained cross-modal networks. This, however, did not solve the problem of specificity. The lack of domain-specific datasets made it hard to train high-performant cross-modal networks for some applications. In our dataset's domainfood and drink images and descriptions -this was identified as a problem for the task of image recipe retrieval <ref type="bibr" target="#b19">[20]</ref>. In fact, the largest dataset available until recently contained only 101K samples pertaining to Chinese food recipes <ref type="bibr" target="#b4">[5]</ref>.</p><p>Other food datasets, such as the ISIA Food-500 dataset, contain up to 400K samples but their visio-linguistic value is limited, as each image is related to a class and a food name rather than to a natural language description <ref type="bibr" target="#b16">[17]</ref>. The FoodX-251 dataset (N=118K) <ref type="bibr" target="#b10">[11]</ref>, ChineseFoodNet (180K) <ref type="bibr" target="#b5">[6]</ref>, Food101 (101K) <ref type="bibr" target="#b3">[4]</ref> and MAFood-121 (21K) <ref type="bibr" target="#b0">[1]</ref> are other notable examples of datasets targeting food classification. Only ChineseFoodNet is multilingual including both Chinese and English classes.</p><p>To fill in this void, Recipe1M+, a large dataset containing over 14M images and 1M recipes was made public for English-language recipes <ref type="bibr" target="#b15">[16]</ref>. Despite the number of text samples being relatively small, the large number of food images was an important step towards improving machine learning tasks in the food domain. This dataset covers only one language and is focused in food recipes instead of food descriptions and broader food categories. The latter are more likely to appear in marketplace applications, and therefore might be more useful in an industrial context.</p><p>To compare our dataset with other existing datasets we follow the approach of the WIT paper, where the number of languages and number of samples are the two main factors compared (see Tab. 1). Our dataset is the second biggest available both in terms of language and number of captioned samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The FooDI-ML dataset</head><p>FooDI-ML is a dataset of food, drinks and groceries images and descriptions collected from all partners operating in the Glovo app in the last six years. The dataset contains data from 37 countries, with a significant representation of 33 languages. Amongst them are some common languages such as French, English and Spanish (which is the most common language), but also some rare languages such as Kazakh and Basque. More interestingly perhaps, some largely spoken languages such as Ukrainian but generally underrepresented in existing datasets are also present.</p><p>The dataset's size places it as the second richest publicly available visio-linguistic dataset, featuring 2.8M images (amongst them 1.5M unique), and up to 9.5M unique text samples. Each sample in the dataset corresponds to a different product that has been offered at some point in the Glovo app. There are 2.8M unique products (see <ref type="bibr" target="#b2">[3]</ref> for detailed statistics). Each product has up to five data points associated with it: the store name, the product name, the product description, the product image and the product "collection section". The latter is a meta category included in the app that restaurants and other sellers in the marketplace can use to organise their menu. Examples of collection sections are "drinks", "our pizzas", "desserts", etc. Collection sections are not standardised, and can be in general chosen by the partner. Note that the number of products (2.8M) is larger than the number of unique images (1.5M), as repeated images are only counted once (although they can be repeated many times over the data set).</p><p>See <ref type="figure" target="#fig_0">Fig. 1</ref> to see the store name, collection section, product name, product description and product image in the context of the Glovo application. For details of how this data was collected, we refer to Sec. 3.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection</head><p>We collect data from all Glovo app partners. This data has been obtained and automatically saved in the app's databases during the six years that the app has been in operation. Most of the data included in the dataset is generated organically. That is, Glovo app does not enforce strict com-pliance measures for the names of products, descriptions and collection sections included in the catalog. This means that the partners that offer their menus through the app can freely generate this information as long as it is not offensive or breaks content rules.</p><p>The data featured in the dataset includes only those products present in the app that have an image associated with them. We decided to not include samples that contained no image information in order to have a fully multi-modal dataset. To maximise the usefulness of the dataset, we computed a hash of our samples (including an image hash) and deleted all identical samples. This reduced the dataset size from its original size of 7.5M samples to 2.8M. The reason behind the widespread presence of duplicated samples is the presence of large franchises in the markets where Glovo operates.</p><p>In this dataset we only include grocery partners that have food items in their catalog. This means that we exclude the majority of partners belonging to a new business vertical: q-commerce ("quick commerce"). These are mostly e-commerce partners with very large product selections and much less product variation across languages and countries. We intend to offer a curated version of this data in a future version of the dataset. It is worth mentioning that all data presented here has been publicly available at some point through the Glovo app. This means that it could potentially have been collected through crawling techniques by thirdparty actors. Whether this has happened or not, this dataset has not been made public through other sources.</p><p>In order to maximise the usefulness of our dataset, we decide to engage in minimal data postprocessing. This is limited to (i) reducing the maximum size of the product images, (ii) processing the store names in the dataset so that they can be more useful for ML practitioners.</p><p>The images present in FooDI-ML are scaled so that the largest size of the image is always equal to or smaller than 512 pixels while maintaining the aspect ratio. No other transformation is implemented.</p><p>We also perform cleaning and processing of our store names. The original dataset had a large number of stores that were used by Glovo agents to backup store information after deactivation and to perform other tasks pertaining to the business' operations. This means that in this case the store name field does not add additional information (as it normally contains dummy information such as "test" or "backup"). It also means that hierarchical information obtained from these stores is not always useful (for example, if one wants to understand the relation between the menu and store name). To address this, we rename these store names with a generated string: "AS N". This stands for "Auxiliary Store number N" -where N is a positive integer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Summarised dataset statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Geography and Languages</head><p>Our dataset includes samples belonging to up to 37 different countries in Europe, the Middle East, Africa and Latin America. Some languages such as Spanish (most of Latin America), English (in Nigeria, Ghana, Uganda), Russian (in several countries in Eastern Europe), Portuguese (Brazil, Portugal) are prevalent due to their broad geographical presence. Others like Ukrainian, Georgian and Italian are present due to the big market share that those countries represent in the app.</p><p>We obtain statistics on language using the fastText classifier <ref type="bibr" target="#b9">[10]</ref> and filtering for those languages that we know that are not present in the country of operation. See <ref type="bibr" target="#b2">[3]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Language fields</head><p>The Glovo app does not enforce the presence of product descriptions in the menus of the partners that use the app. This results in only ?34% of the samples (corresponding to ?980k samples) including the product description field. The fact that some samples do not include product descriptions should not discourage researchers. In most cases, the product name and collection section are sufficient to fully describe the product, and concatenating these two fields provides a satisfactory natural language description of the product (see the analysis in <ref type="bibr" target="#b2">[3]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Store name distribution</head><p>Our dataset features samples from small restaurants and shops but also from large multinational food franchises. This means that the 2.8M samples are distributed amongst only 38K unique store names from which 36K (the overwhelming majority) feature less than 150 samples. If we disregard store names marked as Auxiliar Stores we see that 48% of all samples belong to store names featuring less than 150 samples. That is, local restaurants and shops. The remaining 52% belongs to larger chains or restaurants with very large, changing menus. This points towards a wellbalanced dataset where the long tail of stores holds a significant percentage of the samples. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the stores with more samples in descending order. As one would expect, grocery chains and large multinational brands top the list. This is due to the fact that grocery shops have much larger catalogs than restaurants, and that international brands may have the same product repeated many times over in different locations and different languages under the same store name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Notable examples and proposed tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Differential food samples</head><p>We name "differential food samples" the groups of samples that essentially contain the same entity but with different ingredients. This kind of samples coming from the same partner usually have similar image features such as illumination, background, object pose, etc. For example: in a burrito restaurant this can correspond to several pictures of the same burrito containing different combinations of ingredients -see <ref type="figure">Fig. 4</ref>. This can be used to train saliency neural networks, ingredient generation algorithms, segmentation algorithms, etc.</p><p>Differential food samples are common in our dataset, as most pizzerias, burger stores, burrito stores, salad places, etc. featuring descriptions include samples of this type. Although hard to precisely compute, we estimate that at least 11% of our samples that include a description belong to any of these food types and feature more than one sample per store (see <ref type="bibr" target="#b2">[3]</ref> for the analysis). This produces tenths of thousands of examples, enough to train an ML algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Text on image samples</head><p>Another interesting subset of samples present in our dataset are grocery samples, and samples from well-known fastfood companies. Many of such images are high-quality enough to be usable in optical character recognition tasks. Often, the text imprinted on the image also appears in the product description allowing for some interesting applications such as solving saliency tasks -see <ref type="figure" target="#fig_3">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Cross-language samples</head><p>One of the main contributions of this dataset is the multiplicity of languages that it covers. Especially useful is the fact that we offer product descriptions of similar images in different languages. As an example, one can easily find similar burgers for one of our partners in Russian, Spanish, Italian and English with slightly different images and descriptions. See <ref type="figure" target="#fig_4">Fig. 6</ref> as an example. It is hard to calculate the number of occurrences for this type of sample. However, similar to differential food samples, a lower bound can be established from stores that are globally present -such as the big international chains shown in <ref type="figure">Fig. 4</ref>. Our estimation returns a lower bound of 4.5% of samples (see <ref type="bibr" target="#b2">[3]</ref>). This does not account for samples from different partners that still represent the same food type (for example, a pizza Margherita).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Limitations and possible negative impact</head><p>Despite its large sample size and language coverage, our dataset has some shortcomings.  ? Some languages are still underrepresented due to a "lingua franca" effect. This happens both in developed and developing countries. For example in Spain where Catalan and Basque have a much smaller presence than their proportion of speakers. In Nigeria, Ghana and Uganda English is dominant while local languages such as Bantu and Hausa are not present in our dataset. In summary, the dataset succeeds including languages that are dominant in their countries (like Ukrainian) but underrepresented in public datasets, but struggles to appropriately include minority languages. This can lead to the typical problems associated with the lack of training data: under-performance, bias and lack of generalisation.</p><p>? The issue of representation mentioned above is heavily influenced by market dynamics (which population groups are perceived to be more likely to use the app). This means that there is an additional danger of under-representing dialects perceived as representative of lower purchasing power populations.</p><p>? Glovo app partners are given a high degree of freedom when choosing their images and text representations. This causes some problems: (i) a high proportion of the samples do not include product descriptions. In most cases this is not a problem as the product name and collection section include enough information to obtain linguistic embeddings. However, in some cases -such as very well-known local dishes -this can be a problem. (ii) there is no standardisation on product images, which means that in a minority of samples the same image is used to represent different products. We have observed this, for example, in some small pizze-  <ref type="figure">Figure 4</ref>. <ref type="figure">Figure showing</ref> what we call a differential food example. That is, the same typology of plate (here, a burrito) with different ingredients. The photos are taken in a way ideal for saliency or segmentation approaches. Detailed descriptions are included, ideal for multi-modal learning (shown here translated from Spanish). rias where an image with several pizzas is used to represent all pizza choices. (iii) due to the very large amount of data and the freedom given to Glovo app partners and agents, it is possible that there are some low-quality tags and descriptions.</p><p>? Although the dataset is well balanced between small and big volume partners, it is also true that fast food dominates the delivery sector. This means that pizzas, burgers, fries, etc. are over-represented in the dataset compared to the real local diet of the countries where Glovo app operates. Similarly, dishes that are harder to deliver like cocktails, ice-cream cakes, steak, etc. are less present than easily deliverable food like sushi. This could cause the typical underperformance issue favouring fast food and deliverable food in front of other alternatives.</p><p>? Some global food types, such as pizza, burgers, sushi and salads are present in all languages shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. However, food types with a local footprint such asfor example-"coca de recapte" (a traditional Catalan savory pastry) are only present in local languages. In this case, Catalan and Spanish. This can hinder the performance of some cross-language tasks.</p><p>? All images included in our dataset are commercial images. At Glovo, stores are given the freedom to take and upload pictures independently which leads to large image variability. That said, it is true that photos included in a commercial application will differ from images of food taken in a real setting. Therefore, models trained with this dataset might struggle to generalise to all kinds of food pictures. Expanding the dataset with  some other datasets -such as food datasets that rely on web crawling -might help to mitigate the issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Data Split</head><p>We propose a train/test/validation split of 70/15/15 stratified across countries and samples including product descriptions. We choose this stratification because the presence of a product description is a good indicator of a highquality visio-linguistic sample, therefore ensuring that the evaluation and test metrics are representative and in line with the average quality of the samples. After this split, the train dataset contains 2M samples, and the test and validation splits contain 433k samples each. Tab. 2 shows the full statistics for the proposed split. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Benchmark tasks 4.1. Text image retrieval</head><p>Text image retrieval is a sub-task of cross-modal retrieval that consists of retrieving an image given a query text or set of captions <ref type="bibr" target="#b26">[27]</ref>. This retrieval is typically done by minimising a predefined distance between two vectors: one representing the image, and one representing the text. Amongst other applications, text-image retrieval models are used to locate appropriate descriptions for a given image.</p><p>In this work we use two existing SotA approaches: CLIP <ref type="bibr" target="#b18">[19]</ref>, and an adaptation of the dual-tower method used in the WIT dataset (henceforth, WIT) <ref type="bibr" target="#b22">[23]</ref>, to provide benchmark metrics for our dataset. CLIP and WIT are very similar in that both depend on fine-tuning previously independently trained encoders (such as ResNet <ref type="bibr" target="#b7">[8]</ref> or a Transformer encoder). The main difference between the two is that CLIP is trained over a very large private dataset (400M samples). In addition, CLIP is trained maximising a symmetric binary cross-entropy loss while WIT uses only the first component of the loss (corresponding to image to text retrieval). We use CLIP in a zero-shot manner as proposed by the authors. WIT's implementation is not publicly available, so we rewrite it and offer it publicly. Note that in our implementation we use a transformer model instead of bag of words to reflect recent advances in sentence encoding.</p><p>We use retrieval performance (R@N) as evaluation metric. Note that we count as positive examples any retrieved sample containing the correct image hash (t2i) or text string (i2t). Both WIT and CLIP perform much better than random. Scores significantly vary depending on the model: surprisingly, CLIP (zero-shot) performs much better than the WIT approach (trained on our dataset). This serves as an additional experimental proof of the excellent generalization capabilities of the CLIP network.</p><p>The performance of the WIT and CLIP method on our dataset is significantly better than the metrics reported for similar datasets (see <ref type="bibr" target="#b22">[23]</ref> for example) . However, it is much worse than the performances obtained for smaller datasets such as flickr30K and COCO. This is to be expected as it is harder to retrieve the right sample the larger the dataset is. To illustrate the difficulty of the task, we include the result of several image to text and text to image queries (see <ref type="figure">Fig. 7</ref>). It is clear from the figure that the algorithm -in this case, WIT-has learnt correctly. However, it is still very hard to retrieve the appropriate sample amongst a validation set of over 280k examples.  <ref type="table">Table 3</ref>. Performance of the WIT and CLIP methods. "A" stands for annotation (i2t) and "Re" for retrieval (t2i) scores.</p><formula xml:id="formula_0">Method A-R@1 A-R@5 A-R@10 Re-R@1 Re-R@5</formula><p>(a) Example of performance on the image to text retrieval task (illustrated with associated images). Image query marked in red. Here, the algorithm struggles differentiating amongst different burger descriptions. The appropriate text is "beef hamburger + french fries + coca-cola (0.5 l.) pan america" (translated from Russian). The correct sample is at N=10.</p><p>(b) Example of performance on the text to image retrieval task (illustrated with retrieved images). Query text (from Russian): "tomato juice (250ml)". Here, the correct sample is at N=1 (marked in red). <ref type="figure">Figure 7</ref>. Examples of the performance of WIT on the text-image retrieval task. (a) illustrates i2t and (b) illustrates t2i. Note how the strong similarlity between images means that this is a hard task to solve for the trained network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Conditional image generation</head><p>In addition to text image retrieval, we also train a Generative Adversarial Network (GAN) to conditionally generate pizza images. We choose this task amongst many other options to showcase how easy is it to (i) modify the dataset annotations into categorical variables, (ii) use the rich image dataset to train generative models.</p><p>To train our GAN we use a Self-Attention GAN <ref type="bibr" target="#b29">[30]</ref>, a well-established method allows for easy conditioning of generated images. Upon training, we obtain an inception score of 3.58 and realistic images of several common pizza types (here shown, pepperoni and four cheese -see <ref type="figure" target="#fig_5">Fig. 8</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>We present a large multilingual visio-linguistic dataset covering 33 languages, 37 countries, over 2.8M images and 9.5M text samples: FooDI-ML. The dataset is obtained from the partners of a large food and grocery delivery company: Glovo. FooDI-ML is a domain-specific dataset covering mostly the case of food, drinks and grocery products. The data is collected over six years of Glovo's operations.</p><p>This dataset opens the door for several applications, insofar unavailable for the broader research community due to the lack of public datasets. For instance, multilingual image-based search engines based on food and drink examples, refinement of existing pretrained models for the food and drink industry, and improvement of food image embeddings. The only comparable dataset in this domain is the recipe1M+ dataset, limited to the English language.</p><p>In addition to describing the dataset, we include an overview of notable samples. We also provide a train/test/validation split and several benchmarks over two different tasks. As future work, we plan to release the V2 of this dataset, including many more grocery and marketplace products. We estimate that the V2 of the dataset will at least double the size of the dataset that we present here. We also plan to release some specialized tasks for the food and drinks domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>One of the samples included in our dataset as shown in the Glovo app (prices omitted).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Presence of languages in our dataset in raw samples. Shown the top 20 languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Store names with a larger number of samples, excluding auxiliary stores. Only shown the top 40 store names -representing around 9% of the samples. Note how the dataset is well balanced between stores with a lot of samples and stores featuring a much more modest collection .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Image of a "text on image sample" (rotated 90 degrees). The product name coincides partially with the image text. product name: Gel Limpiador Pure 3 En 1 Garnier 150 Ml .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure showinga cross-language sample. That is, a very similar sample (a burger from one of Glovo's partners), present in four different languages and locations. Note how the photography style, ingredients and description vary depending on location and language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Automatically generated images for four cheese pizza (top) and pepperoni pizza (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Statistics of the train test and validation splits.</figDesc><table><row><cell>Parameter</cell><cell>Train</cell><cell>Valid.</cell><cell>Test</cell></row><row><cell>Unique samples</cell><cell cols="3">2021210 433117 433117</cell></row><row><cell>Unique images</cell><cell cols="3">1234794 355500 355965</cell></row><row><cell cols="4">Samp. w/ prod. nam. &amp; coll. sec. 2021210 433117 433117</cell></row><row><cell>Samp. w/ prod. description</cell><cell>481871</cell><cell cols="2">125193 125109</cell></row><row><cell>Unique store names (inc. Aux.)</cell><cell>37833</cell><cell>29647</cell><cell>29731</cell></row><row><cell>Unique coll. sec.</cell><cell>88628</cell><cell>57376</cell><cell>57736</cell></row><row><cell>Unique prod. descriptions</cell><cell>481871</cell><cell cols="2">125193 125109</cell></row><row><cell>Unique prod. names</cell><cell cols="3">1174586 342826 342834</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Regularized uncertainty-based multi-task learning model for food analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bola?os</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petia</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="360" to="370" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale datasets for image and video captioning in italian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scaiella</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCoL. Italian Journal of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5-2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Foodi-ml-dataset/notebooks at main ? glovo/foodi-ml-dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glovo</forename><surname>App</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep-based ingredient recognition for cooking recipe retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02743</idno>
		<title level="m">Chinesefoodnet: A large-scale image dataset for chinese food recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Ms-coco-es: Spanish coco captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Garcia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multimodal pivots for image caption translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigehiko</forename><surname>Schamoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03916</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Foodx-251: A dataset for fine-grained food classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parneet</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06167</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uit-viic: A dataset for the first evaluation on vietnamese image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang Duy</forename><surname>Quan Hoang Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Van Kiet Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Thuy</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Collective Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="730" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coco-cn for crosslingual image tagging, captioning, and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2347" to="2360" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recipe1m+: A dataset for learning cross-modal embeddings for cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritro</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="187" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Isia food-500: A dataset for large-scale food recognition via stacked global-local attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqing</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="393" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-lingual image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuyuki</forename><surname>Shimizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning cross-modal embeddings for cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferda</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3020" to="3028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Using deep learning for ranking in dish search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramkishore</forename><surname>Saravanan</surname></persName>
		</author>
		<ptr target="https://bytes.swiggy.com/using-deep-learning-for-ranking-in-dish-search-4df2772dddce" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01913</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Didec: The dutch image description and eye-tracking corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akos</forename><surname>Emiel Van Miltenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruud</forename><surname>K?d?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3658" to="3669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Camp: Crossmodal adaptive message passing for text-image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5764" to="5773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
