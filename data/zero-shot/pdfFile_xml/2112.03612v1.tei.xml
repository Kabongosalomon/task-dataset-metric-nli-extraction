<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DCAN: Improving Temporal Action Detection via Dual Context Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Dong</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
							<email>lutong@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DCAN: Improving Temporal Action Detection via Dual Context Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>? Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action detection aims to locate the boundaries of action in the video. The current method based on boundary matching enumerates and calculates all possible boundary matchings to generate proposals. However, these methods neglect the long-range context aggregation in boundary prediction. At the same time, due to the similar semantics of adjacent matchings, local semantic aggregation of densely-generated matchings cannot improve semantic richness and discrimination. In this paper, we propose the endto-end proposal generation method named Dual Context Aggregation Network (DCAN) to aggregate context on two levels, namely, boundary level and proposal level, for generating high-quality action proposals, thereby improving the performance of temporal action detection. Specifically, we design the Multi-Path Temporal Context Aggregation (MTCA) to achieve smooth context aggregation on boundary level and precise evaluation of boundaries. For matching evaluation, Coarse-to-fine Matching (CFM) is designed to aggregate context on the proposal level and refine the matching map from coarse to fine. We conduct extensive experiments on ActivityNet v1.3 and THUMOS-14. DCAN obtains an average mAP of 35.39% on ActivityNet v1.3 and reaches mAP 54.14% at IoU@0.5 on THUMOS-14, which demonstrates DCAN can generate high-quality proposals and achieve state-of-the-art performance. We release the code at https://github.com/cg1177/DCAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indroduction</head><p>With the explosive growth of Internet videos, video content's understanding and analysis technology have attracted more academia and industry attention. Temporal action detection is to locate the boundaries of action instances and recognize action categories in untrimmed videos. Video data is a stack of image frames, and the semantic changes between frames are more complicated to capture than the semantic changes between image pixels. Therefore, compared with object detection, temporal action detection focuses more on processing and capturing the temporal information of the video.</p><p>Similar to object detection, the temporal action detection methods can be divided into one-stage and two-stage meth-* These authors contributed equally.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Examples to illustrate the importance and difficulty of context aggregation on two levels. (a) The frame 2 and frame 5 with similar local context are difficult to be distinguished, which will generate many positive false proposals. (b) The temporal intervals of adjacent 3 ? 3 matchings on the matching map are highly overlapped, lacking semantic discrimination and richness, and it is not easy to obtain effective semantic supplements by simple aggregation.</p><p>ods. The one-stage methods simultaneously locate the action boundaries and classify the action. The two-stage methods first generate proposals, then refine the boundaries and classify the action. In order to obtain high-quality detection results, the proposals should precisely cover action with high recalls and reliable confidence scores. There are mainly two types of proposal generation methods, "top-down" and "bottom-up". The "top-down" method <ref type="bibr" target="#b9">(Gao et al. 2017;</ref><ref type="bibr">Long et al. 2019;</ref><ref type="bibr" target="#b8">Gao et al. 2020;</ref><ref type="bibr" target="#b3">Chao et al. 2018;</ref><ref type="bibr" target="#b18">Lin, Zhao, and Shou 2017)</ref> obtains the final proposals by refining the boundaries of anchors or sliding windows with predefined scales and calculating the confidence. The "bottomup" method <ref type="bibr" target="#b40">(Zhao et al. 2017a;</ref><ref type="bibr" target="#b19">Lin et al. 2018</ref><ref type="bibr" target="#b0">Bai et al. 2020;</ref><ref type="bibr" target="#b27">Su et al. 2021;</ref><ref type="bibr" target="#b38">Xu et al. 2020</ref>) generates proposals by calculating the boundary confidence of each position and matching start positions with end positions.</p><p>The above "bottom-up" methods generate proposals by the confidence scores of boundaries and matching maps. However, there are some difficulties on boundary-level and proposal-level context aggregation with this framework. First, on the boundary level, different actions vary at different speeds. The boundary of a slow action usually is not a clear temporal position but a transitional interval. So there is not enough semantic information for the precise predictions of these boundaries without effective local temporal context aggregation. On the other hand, as shown in <ref type="figure">Figure 1(a)</ref>, the start and end boundaries of some actions are so similar that the high confidence of start and end at such positions will generate many invalid matchings without long-range temporal context aggregation. Second, on the proposal level, it is not proper to simply perform context aggregation on the matching map. As shown in <ref type="figure">Figure 1(b)</ref>, aggregating adjacent matchings with different temporal scales and semantic densities will damage the internal semantic representation of the matchings. Moreover, adjacent matchings are highly overlapped so that their semantic information is too similar to obtain sufficient semantic supplement after aggregation. Therefore, it is necessary to design effective context aggregation methods on the temporal and proposal levels.</p><p>To mitigate the above issues, we propose a novel method called Dual Context Aggregation Network (DCAN) for improving temporal proposal generation. Similar to BMN , DCAN has a temporal evaluation branch and a matching evaluation branch. For the temporal evaluation branch, we design the Multi-Path Temporal Context Aggregation (MTCA) to achieve effective and smooth context aggregation on the boundary level. MTCA is a stack of Multi-Path Temporal Convolutions (MPTC). In each MPTC, there is a long-range path equipped with a dilated convolution to expand the receptive field and achieve long-range context aggregation and a short-range path with a regular convolution to aggregate short-range context. In order to alleviate the gridding artifacts of dilated convolution, we adopt a sawtooth wave-like heuristic arrangement for MPTCs to ensure the context of each position can be aggregated smoothly. MTCA gradually expands the receptive field from the frame to the entire video, thereby effectively aggregating the longrange and short-range contexts. For the matching evaluation branch, we propose the Coarse-to-fine Matching (CFM) for effective context aggregation on the proposal level. CFM first generates a coarse matching map using the Group Sampling strategy, then gradually refines the map from coarse to fine through the refinement network. The coarse map ensures the distinction of semantic information between sparse matchings, and at the same time, aggregates the context of adjacent matchings without damaging the semantic representation. During the coarse-to-fine process, the relation between matchings is gradually supplemented and restored. CFM enhances the expressiveness and robustness of the matching context, and the final matching map contains the relation between the matchings.</p><p>We conduct extensive experiments on the THUMOS-14 and ActivityNet v1.3 to demonstrate the effectiveness of our Dual Context Aggregation Network (DCAN). In summary, our contributions are as follows:</p><p>? On the boundary level, we propose the Multi-Path Temporal Context Aggregation to aggregate boundaries context and alleviate the gridding artifacts of dilated convolutions.</p><p>? On the proposal level, we design the Coarse-to-fine Matching to generate and refine matching maps from coarse to fine, which enhances the expressiveness and robustness of the matching context.</p><p>? The experiments prove the high performance of DCAN, which can achieve better performance than other state-ofthe-art methods on ActivityNet v1.3 and THUMOS-14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Action Recognition</head><p>Action recognition is an essential task in video understanding. It performs spatio-temporal modeling on video frames to recognition actions in the video. 2D CNN methods (Simonyan and Zisserman 2014; <ref type="bibr" target="#b34">Wang et al. 2016;</ref><ref type="bibr" target="#b16">Lin, Gan, and Han 2019;</ref><ref type="bibr" target="#b6">Feichtenhofer et al. 2019;</ref><ref type="bibr" target="#b21">Liu et al. 2020;</ref><ref type="bibr" target="#b32">Wang et al. 2021</ref>) take RGB and optical flow as input and perform spatial and temporal modeling, respectively. 3D CNN methods <ref type="bibr" target="#b29">(Tran et al. 2015</ref><ref type="bibr" target="#b30">(Tran et al. , 2017</ref><ref type="bibr" target="#b2">Carreira and Zisserman 2017;</ref><ref type="bibr" target="#b4">Diba et al. 2017</ref>) capture spatio-temporal information between frames by performing 3D convolution on stacked video frames. <ref type="bibr" target="#b25">(Qiu, Yao, and Mei 2017;</ref><ref type="bibr" target="#b36">Xie et al. 2018;</ref><ref type="bibr" target="#b31">Tran et al. 2018</ref>) model spatiotemporal features by decoupling 2D and 1D convolutions for reducing computing resources. In this work, we use 2stream <ref type="bibr" target="#b26">(Simonyan and Zisserman 2014)</ref> to generate the feature sequence of the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Action Detection and Proposals</head><p>Current temporal action detection methods can be divided into one-stage and two-stage methods. The one-stage methods simultaneously generate action proposals and corresponding action labels in a single model. The two-stage methods first generate the proposals, then refine the boundaries and classify actions. <ref type="bibr" target="#b9">(Gao et al. 2017</ref><ref type="bibr" target="#b8">(Gao et al. , 2020</ref><ref type="bibr" target="#b3">Chao et al. 2018)</ref> generate proposals based on a pre-defined sliding window or anchors and train a classifier to filter anchors. TURN <ref type="bibr" target="#b9">(Gao et al. 2017</ref>) utilizes a sliding window and refines boundaries by concatenating the boundary context and internal context of proposals. RapNet <ref type="bibr" target="#b8">(Gao et al. 2020</ref>) proposes a relation-aware module to capture long-range context between frames. RTD-Net <ref type="bibr" target="#b28">(Tan et al. 2021</ref>) utilizes the transformer decoder to generate a sparse proposal set directly, effectively omitting post-processing steps. Although many anchor-based methods use multi-scale anchors to increase the diversity, the generated proposals are still not flexible enough to cover actions of varying temporal scales. <ref type="bibr" target="#b19">(Lin et al. 2018</ref><ref type="bibr" target="#b27">Su et al. 2021</ref>) use a flexible way called boundary matching. They predict each frame's start and end confidence, then match the frames with high start and end confidences to generate the proposals and evaluate their confidence. These methods are more difficult to optimize due to the lack of prior knowledge of the anchor. First, a dual-path convolution layer is used to model local temporal features with RGB and flow features, respectively. Then, we concatenate these two features and feed them into Multi-Path Temporal Context Aggregation Module to aggregate temporal context for boundary confidence evaluation. At the same time, these features are also input into Coarse-to-fine Matching Module, which generates a coarse group map and then refines the map to a fine matching map through a refinement network for matching confidence evaluation. Finally, the boundary and matching confidence will be combined to obtain final proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Modeling in Temporal Action Detection</head><p>Temporal modeling plays an important role in temporal action detection. <ref type="bibr" target="#b5">(Escorcia et al. 2016;</ref><ref type="bibr" target="#b39">Yeung et al. 2016</ref>) use LSTM to generate action proposals. Compared with LSTM, 1D convolution on temporal dimension shows better performance when modeling the long-range temporal structure of actions. <ref type="bibr" target="#b14">(Lea et al. 2017;</ref><ref type="bibr" target="#b10">Gong, Zheng, and Mu 2020)</ref> and ) utilizes temporal convolution and UNet for temporal relationship modeling, respectively. <ref type="bibr" target="#b24">(Qing et al. 2021</ref>) aggregates local and global temporal context by two types of self-attention modules. We use stacked Multi-Path Temporal Convolution to capture the long-term and shortterm dependence of the frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach Problem Definition</head><p>For an untrimmed video, we denote it as U = {u t } lv t=1 , where l v indicates the length of the video and u t is the tth frame. We denote the temporal annotation of action instances as ? g = {? n = (t s , t e )} Ng n=1 in the video S v which has N g instance. t s and t e are the start and end boundaries of the instance ? respectively. The action detection model generates prediction proposals that should cover ? g with high recall and high temporal overlapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of DCAN</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, DCAN is composed of three modules: Base Network, Multi-Path Temporal Context Aggregation Module, and Coarse-to-fine Matching Module. Firstly, the video frames are fed into the Base Network for local temporal modeling. Then the features enter into Multi-Path Temporal Context Aggregation Module and Coarse-tofine Matching Module to perform the boundary-level and proposal-level context aggregation, respectively. The aggregated feature will be used for boundary and matching evaluation, finally generating the proposals. We present the technical details of three modules in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Network</head><p>Following recent proposal generation methods <ref type="bibr" target="#b19">(Lin et al. 2018</ref>, we take the temporal features which are extracted using the action recognition backbone network with a fixed-interval sliding window as input. This feature extracting method only extracts the semantic feature of local temporal sequence frames in an isolated window, resulting in a lack of correlation between adjacent windows, so we design a base network to perform local context aggregation on the temporal feature. The base network has two convolution paths to model RGB features and optical flow features, respectively. Each path consists of N base 1D convolution layers, with 128 filters, kernel size of 3, stride of 1, followed by a ReLU activation layer. Finally, we obtain the feature F rgb base and F flow base , then feed them to the following modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Path Temporal Context Aggregation</head><p>In this section, we introduce our Multi-Path Temporal Context Aggregation (MTCA) to aggregate long-range and short-range temporal context for temporal evaluation effectively. As shown in <ref type="figure" target="#fig_0">Figure 2</ref> and <ref type="figure" target="#fig_1">Figure 3</ref>, MTCA is com- posed of a sequence of stacked Multi-Path Temporal Convolution (MPTC). In each MPTC, Conv L is the long-range path, including a dilated convolution layer with a kernel size of 3 and a dilation of r to aggregate long-range context and extend the receptive field. Conv S is the short-range path including a regular convolution with a kernel size of 3 that aggregates short-range temporal context. In order to enhance the representation ability of features, and at the same time, solve network degradation during training, we introduce a shortcut path to fuse features of different levels. Finally, we combine these three paths in parallel and do the elementwise addition. MPTC can be formulated as follows:</p><formula xml:id="formula_0">MPTC(x) = ?(?(Conv L (x))+?(Conv S (x))+?(x)),<label>(1)</label></formula><p>where ?(?) and ?(?) is the nonlinear activation function and the normalization operation respectively. Previous research ) on the application of dilated convolution in image detection and segmentation tasks proves that too-rapid expansion of the receptive field may lead to information loss. Specifically, simply stacking multiple dilated convolution layers with exponentially increased dilation will cause gridding artifacts, that is, features at some positions cannot participate in the calculation. To alleviate this phenomenon, we design two different types of MPTC. The first, called MPTC-E, is an MPTC with dilation of 2 k for rapidly expanding the receptive field, where k is the exponent to adjust the scale of receptive field expansion and increases with the depth of the network increases. The second, called MPTC-S, is an MPTC with a fixed dilation of r smooth for alleviating the gridding artifacts.</p><p>The numbers of MPTC-E and MPTC-S in MTCA are both</p><formula xml:id="formula_1">N b , and MPTC-E's dilation r i is 2 i where 1 ? i ? N b .</formula><p>To alleviate the gridding artifacts as much as possible, we set the dilations of MPTC-E and MPTC-S are relatively prime and connect these two blocks alternately. In this way, the top layer of MTCA can access information from the entire video while the aggregation of information at each temporal position is smooth and uniform. Finally, after aggregating longrange and short-range context at each temporal position, the start and end probabilities are predicted, respectively.</p><p>Discussion. Some methods use the self-attention mechanism to enhance and aggregate the features of each position, but we argue that the self-attention mechanism is not suitable for boundary evaluation. The self-attention mechanism pays more attention to the correlation between positions but ignores the order and distance. For boundary evaluation, context aggregation around the boundary is more valuable than aggregation at a distance. We hope that the position can focus on aggregating the information of the action instance to which it belongs while also taking into account the aggregation of long-range context rather than the uniform aggregation of the positions of the entire video. Therefore, MTCA is more beneficial to long-range and short-range context aggregation than the self-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-to-fine Matching</head><p>Coarse-to-fine Matching (CFM) aims to achieve proposallevel context aggregation by constructing a coarse matching map and refining the map from coarse to fine. Since adjacent matchings in the matching map have similar sampling intervals and sampling points, which leads lack of distinction and richness in semantics. Context aggregation of such adjacent matchings cannot obtain an effective semantic supplement. At the same time, aggregating adjacent matchings with different temporal scales and different semantic densities on the matching map will damage the semantic representation inside each matching. Hence we propose the Group Sampling strategy to construct the group map. Specifically, the matchings in the range of G ? G on the original matching map  are grouped into a group, and we take the union of their temporal intervals to construct the features of the group. The entire original matching map can be divided into D G ? T G groups without overlap, where T and D is the scale of temporal feature and the maximum duration of any possible action instance. The strategy can be formulated by:</p><formula xml:id="formula_2">P i,j = GroupSample(F p , s i,j , e i,j ),<label>(2)</label></formula><p>where GroupSample(F, s, e) operation is to uniformly sample N sample points from position s (0 ? s &lt; e ? 1.0) to position e of temporal feature F . P i,j ? R 128?N sample is the sampled group feature of the i-th row and j-th column on the group map. We use hyper-parameter G to set group size and obtain the indices of the group map:</p><formula xml:id="formula_3">0 ? i &lt; D G , 0 ? j &lt; T G .<label>(3)</label></formula><p>The corresponding start position s i,j and end position e i,j of sampling and is also formulated by:</p><formula xml:id="formula_4">s i,j = j ? G T , e i,j = s i,j + (i + 1) ? G T .<label>(4)</label></formula><p>Through the above method, we can obtain each group's start and end boundaries and then use the feature construction method of BMN to generate sample-level group map</p><formula xml:id="formula_5">M g ? R D G ? T G ?128?N sample ,</formula><p>where 128 is the dimension of P i,j . We obtain the group map M g ? R D G ? T G ?C using a linear transformation to M g .</p><p>Then, we refine the coarse group map M g to the fine matching map M m ? R D?T ?C using a refinement network. The refining process has two steps. Firstly, we adopt deconvolutions to upsample M g that each deconvolution layer upsamples the map with factor 2 on the temporal and duration dimensions, and each group feature is finally refined to G ? G matching features. Then, a convolution with the kernel size of 3 is adopted to rebuild the relationship of adjacent matchings. While the refinement network gradually refines group features to matching features, it also restores the relation between matchings and realizes the implicit aggregation of context between matchings. Discussion. The final convolution operation is the same as BMN ), but their effects are different. Our matching features are refined from the group features. The construction of the group features weakens the internal temporal scale representation, so it can be considered that the internal temporal semantic representations of different temporal scales matchings are homogeneous. Performing convolution on such features has a smoother context aggregation effect and can better capture the internal relationship between matchings, so its effect is better than PEM of BMN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Inference For DCAN Training</head><p>We train DCAN in the form of a multitask loss function, including a boundary classification loss L b , a proposal evaluation loss L p and a regularization where ? is set to 0.0001:</p><formula xml:id="formula_6">L = L b + L p + ? ? L 2 (?).<label>(5)</label></formula><p>L b is used to classify whether each frame is the start position or end position of the action:</p><formula xml:id="formula_7">L b = L WCE (P start , G start ) + L WCE (P end , G end ),<label>(6)</label></formula><p>where L WCE is the weighted binary cross-entropy loss function, P start is the predicted start probability of frames (same for P end ), G start and G end are the binary ground-truth which are obtained by calculating IoR between action instances and temporal positions. The loss L p is used to evaluate proposals confidence. Following BMN, we predict two confidence map M cls and M reg , which are trained by the weighted binary cross-entropy loss function and mean squared error loss function respectively:</p><formula xml:id="formula_8">L p = L WCE (M cls , G cls ) + ? ? L 2 (M reg , G IoU ),<label>(7)</label></formula><p>where G IoU is the IoU map calculated by proposals and ground truth, G cls is the foreground-background map obtained by binarizing G IoU with a threshold 0.9, and ? is the loss weight, which is set to 10 as default in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference</head><p>Score Fusion. During the inference stage, the temporal evaluation branch generates the start probability P start and the end probability P end for each position. We use these two probabilities as the two boundaries scores of the proposals and fuse them with the matching score maps M cls and M reg obtained by the matching evaluation branch to generate the final score of proposals. Take the proposal ? = [t s , t e ] for example, the combination of final score p ? can be shown as:</p><formula xml:id="formula_9">p ? = P start ts ? P end te ? (M cls te?ts,ts ? M reg te?ts,ts ) ? ,<label>(8)</label></formula><p>where ? is a hyperparameter for adjusting the compatibility of boundary scores and matching scores and is set as 1.5 on THUMOS-14 and 0.8 on ActivityNet v1.3, respectively.</p><p>Post Processing. After score fusion, DCAN generates the proposal candidates set as ? c = {? n = (t s , t e , p)} Nc n=1 and then we post-process the candidate proposals to remove redundant proposals. First, we remove proposals whose start or end probability is lower than half of the corresponding maximum value. Then, we adopt the Soft-NMS <ref type="bibr" target="#b1">(Bodla et al. 2017)</ref>  Limiting the average number (AN) of proposals for each video allows us to calculate the area under the AR vs AN curve to obtain AUC. On ActivityNet v1.3, we set AN from 1 to 100. The quality of temporal action detection requires evaluating mean Average Precision(mAP) under multiple tIoU. On ActivityNet v1.3, the tIoU thresholds are set to {0.5, 0.75, 0.95}, and we also test the average mAP of tIoU thresholds between 0.5 and 0.95 with the step of 0.05. On THUMOS-14, these tIoU thresholds are set to {0.3, 0.4, 0.5, 0.6, 0.7}. Implementation Details. We use pre-extracted features for all datasets and train the network from scratch. For Ac-tivityNet v1.3, we adopt the two-stream network  fine-tuned on the training set of ActivityNet v1.3 with frame interval ? = 16. Each video feature sequence is rescaled to L = 100 snippets using linear interpolation. We set the batch size to 16 and the learning rate to 0.001 for the first 7 epochs and 0.0001 for the following 3 epochs. For THUMOS-14, the features are extracted using TSN ) pre-trained on Kinetics <ref type="bibr" target="#b13">(Kay et al. 2017</ref>) with ? = 5. We crop each video feature sequence with overlapped windows of size L = 256 and stride 128. In training,  we do not use any clips void of actions. We set the batch size to 16 and the learning rate to 0.0004 for all 5 epochs.</p><p>The N b is set to 6 on ActivityNet v1.3 and 7 on THUMOS-14. The N base , N sample , r smooth and G are set to 3, 32, 3 and 2. In the post-processing, the Soft-NMS threshold is set to 0.5 to pick the top N final confident predictions, where N final is 100 for ActivityNet v1.3 and 200 for THUMOS-14.</p><p>Comparison with State-of-the-art Results . We compare DCAN with other state-ofthe-art methods on THUMOS-14 in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>, where DCAN significantly improves the performance of the proposal generation and action detection. We report AR@AN for proposal generation and mAP for action detection. As shown in <ref type="table" target="#tab_0">Table 1</ref>, DCAN improves the AR of all proposals for proposal generation except for @500. Furthermore, for action detection in <ref type="table" target="#tab_1">Table 2</ref>, DCAN can also obtain at least 5.0% improvement when tIoU at any threshold compared to all previous methods. ActivityNet v1.3. We compare DCAN with the other methods with the state-of-the-art methods on ActivityNet v1.3 in <ref type="table" target="#tab_2">Table 3 and Table 4</ref>. We report the AR@AN and AUC for proposal generation and mAP for action detection. For a fair comparison, in the proposal generation task, we  <ref type="bibr" target="#b7">(Gao, Chen, and Nevatia 2018)</ref>, BSN <ref type="bibr" target="#b19">(Lin et al. 2018)</ref>, MGG , BMN    only compare the methods without the re-sampling strategy. On two tasks, DCAN outperforms the other state-ofthe-art proposal generation methods. Since DCAN improves AR@1 to 34.42, which demonstrates that the proposals with high confidence also have high recalls, bringing a significant performance improvement in the action detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>Ablation comparison with other "bottom-up" methods.</p><p>We conduct a direct comparison to other "bottom-up" methods, namely, BSN, BMN, and BSN++ in <ref type="table" target="#tab_5">Table 5</ref> to confirm the effectiveness and superiority of DCAN.</p><p>In the temporal evaluation phase, the TEM of BSN and BMN, which only considers the local details for boundary evaluation, is inferior with limited receptive fields for boundary-level context aggregation. BSN++ adopts a shallow U-shaped network to expand the temporal receptive field, but it does not expand the receptive field to global. MTCA realizes the long-range and short-range smooth context aggregation and expands the receptive field to the entire video. Therefore, only replacing the TEM with MTCA improves the mAP of ActivityNet v1.3 and THUMOS-14 from 33.85% and 38.80% to 35.02% and 52.55%.</p><p>The BMN and BSN++ directly generate a dense matching map. This matching feature construction method causes the semantics of adjacent matchings on the map to be similar as well as lack distinction and richness. In addition, these methods aggregate proposal-level context by applying convolutions or self-attention modules, ignoring that aggregating proposals with multiple different temporal scales and How to choose r smooth of the MPTC-S? We conduct experiments to explore how to choose r smooth of MPTC-S for smoothing the receptive field. In order to alleviate the gridding artifacts caused by dilated convolution, the dilation of MPTC-S must be mutually prime with the dilation of MPTC-E. Since the dilation of MPTC-E is set as 2 k , we choose 1, 3 and 5 as the candidate dilation of MPTC-S. The experimental results are shown in <ref type="table" target="#tab_6">Table 6</ref>. We also give the result without MPTC-S and denote it as * . The experimental results show that MPTC-S can improve performance, and the performance is best when r smooth = 3.   What is the effect of group size G of CFM? We explore the different group size G of CFM, and the experimental results are shown in <ref type="table" target="#tab_7">Table 7</ref>. When G = 1, CFM degenerates into the dense matching map similar to BMN. For G = 2 and G = 4, we use one-layer and two-layer refinement networks to refine the map from coarse to fine, respectively. The experimental results demonstrate that G = 2 is the best choice. G = 4 does not work because when the group range is too large, and the generated map is too coarse, encoding 4 ? 4 proposals into 1 grouped matchings will lead to the loss of semantic information, and it is difficult to restore this part of the lost information through refinement.</p><p>Generalizability. To evaluate the generalizability of proposals, we follow BMN and choose two different subsets on ActivityNet v1.3 for generalization ability analysis. The experiment extracts two un-overlapped action subsets from ActivityNet v1.3: "Sports, Exercise, and Recreation" as seen subsets and "Sports, Exercise, and Recreation" as unseen subsets separately. <ref type="table" target="#tab_8">Table 8</ref> shows the results of DCAN on 2stream features. The results reveal that there is only a slight performance drop on unseen categories, suggesting that DCAN achieves great generalizability to generate highquality proposals for unseen actions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization</head><p>We visualize the start and end probability sequences of BMN's TEM and MTCA in <ref type="figure" target="#fig_3">Figure 4</ref>. It can be observed that for some boundary positions, TEM is difficult to distinguish whether they are the start or the end boundaries, and some non-boundary positions are recognized as boundaries, which demonstrates only using the local context is difficult to evaluate temporal boundary. The probability curve of MTCA is smoother and more distinguishable than TEM, and the probabilities of non-boundary position are significantly lower than those of boundaries. This indicates that long-range and short-range context aggregation on the boundary level can improve the model's ability to distinguish confusing positions and suppress non-boundary positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we have proposed a novel Dual Context Aggregation Network (DCAN) for high-quality proposal generation and action detection. DCAN aggregating context on boundary and proposal level, respectively. On the boundary level, Multi-Path Temporal Context Aggregation (MTCA) uses multiple paths to aggregate long-range and short-range context smoothly. Coarse-to-fine Matching (CFM) refines the matching map from coarse to fine and achieves effective context aggregation on the proposal level. Extensive experiments on ActivityNet v1.3 and THUMOS-14 demonstrate two-level context aggregation can significantly improve proposal generation and action detection performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The network architecture of DCAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of MPTC. First, temporal features are fed into MPTC-E with an increasing dilation to expand the receptive field. Then, MPTC-S with a fixed dilation smooths the features from MPTC-E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>to eliminate the redundant proposals and obtain the final proposals set ? final = {? n = (t s , t e , p)} N final n=1 , where the number of final proposals to N final . Experiments Datasets and Setup THUMOS-14. (Jiang et al. 2014) contains 413 temporal annotated untrimmed videos with 20 action categories. We use 200 videos in the validation set for training and 213 videos in the testing set for evaluation. ActivityNet v1.3. (Heilbron et al. 2015) is a largescale action understanding dataset, which consists of 19,994 videos for training, 4,728 for validation, and 5,044 for testing, with 200 action classes. The total duration of the videos is about 600 hours. ActivityNet v1.3 only contains 1.5 occurrences per video on average, and most videos contain a single action category with 36% background on average. Evaluation Metrics. Average Recall (AR) is the average recall under specified tIoU thresholds for measuring the quality of proposals. On ActivityNet v1.3, these thresholds are set to [0.5 : 0.05 : 0.95]. On THUMOS-14, they are set to [0.5 : 0.05 : 1.0].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The start and end probability sequences generated by TEM and MTCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of DCAN with other state-of-the-art methods on THUMOS-14 in terms of AR@AN. All models use the two-stream feature as input.</figDesc><table><row><cell cols="4">Method @50 @100 @200 @500 @1000</cell></row><row><cell>TAG</cell><cell>18.55 29.00 39.61</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CTAP 32.49 42.61 51.97</cell><cell>-</cell><cell>-</cell></row><row><cell>BSN</cell><cell cols="3">37.46 46.06 53.23 61.35 65.10</cell></row><row><cell cols="4">MGG 39.93 47.75 54.65 61.36 64.06</cell></row><row><cell cols="4">BMN 39.36 47.72 54.84 62.19 65.49</cell></row><row><cell cols="4">BSN++ 42.44 49.84 57.61 65.17 66.83</cell></row><row><cell cols="4">TCANet 42.05 50.48 57.13 63.61 66.88</cell></row><row><cell cols="4">DCAN 42.65 51.05 57.95 64.58 68.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="6">: Comparison between DCAN with other state-of-</cell></row><row><cell cols="6">the-art methods on THUMOS-14 dataset. The results are</cell></row><row><cell cols="6">measured by mAP(%) at different tIoU thresholds. Propos-</cell></row><row><cell cols="6">als from all methods are combined with video-level classifier</cell></row><row><cell cols="4">UntrimmedNet (Wang et al. 2017).</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>0.7</cell><cell>0.6</cell><cell>0.5</cell><cell>0.4</cell><cell>0.3</cell></row><row><cell>TURN</cell><cell cols="5">6.3 14.1 24.5 35.3 46.3</cell></row><row><cell>BSN</cell><cell cols="5">20.0 28.4 36.9 45.0 53.5</cell></row><row><cell>MGG</cell><cell cols="5">21.3 29.5 37.4 46.8 53.9</cell></row><row><cell>BMN</cell><cell cols="5">20.5 29.7 38.8 47.4 56.0</cell></row><row><cell cols="6">G-TAD 23.4 30.8 40.2 47.6 54.5</cell></row><row><cell cols="6">BSN++ 22.8 31.9 41.3 49.5 59.9</cell></row><row><cell cols="6">TCANet 26.7 36.8 44.6 53.2 60.6</cell></row><row><cell cols="6">DCAN 32.6 43.9 54.1 62.7 68.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison with other state-of-the-art methods CTAP</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>on ActivityNet v1.3 in terms of AR@AN and AUC.</figDesc><table><row><cell>Method</cell><cell cols="5">CTAP BSN MGG BMN DCAN</cell></row><row><cell>AR@1(val)</cell><cell>-</cell><cell>32.17</cell><cell>-</cell><cell>-</cell><cell>34.42</cell></row><row><cell cols="5">AR@100(val) 73.17 74.16 74.54 75.01</cell><cell>75.71</cell></row><row><cell>AUC(val)</cell><cell cols="4">65.72 66.17 66.43 67.10</cell><cell>67.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">: Comparison between DCAN with other state-of-</cell></row><row><cell cols="5">the-art methods on ActivityNet v1.3. The results are mea-</cell></row><row><cell cols="5">sured by mAP(%) at different tIoU thresholds and average</cell></row><row><cell cols="5">mAP(%). We combined our proposals with video-level clas-</cell></row><row><cell cols="4">sification results from (Zhao et al. 2017b).</cell><cell></cell></row><row><cell>Method</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell>Average</cell></row><row><cell>SSN</cell><cell>39.12</cell><cell>23.48</cell><cell>5.49</cell><cell>23.98</cell></row><row><cell>BSN</cell><cell>46.45</cell><cell>29.96</cell><cell>8.02</cell><cell>30.03</cell></row><row><cell>BMN</cell><cell>50.07</cell><cell>34.78</cell><cell>8.29</cell><cell>33.85</cell></row><row><cell>G-TAD</cell><cell>50.36</cell><cell>34.60</cell><cell>9.02</cell><cell>34.09</cell></row><row><cell>BC-GNN</cell><cell>50.56</cell><cell>34.75</cell><cell>9.37</cell><cell>34.26</cell></row><row><cell>BSN++</cell><cell>51.27</cell><cell>35.70</cell><cell>8.33</cell><cell>34.88</cell></row><row><cell>DCAN</cell><cell>51.78</cell><cell>35.98</cell><cell>9.45</cell><cell>35.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study results on the validation set of Ac-tivityNet v1.3 and the test set of THUMOS-14. TEB and MEB denote the modules in Temporal Evaluation Branch and Matching Evaluation Branch. We show experimental results on ActivityNet v1.3 in terms of average mAP(%) and on THUMOS-14 in terms of mAP@0.5(%). matchings. CFM refines the coarse group map into the fine matching map. The semantic distinction and richness between adjacent matchings are better, and the temporal scale representation inside the matching is weakened, so the context aggregation is more effective. After equipping BMN with CFM, the mAPs of the two datasets are increased by 0.95% and 10.62%, respectively.When MTCA and CFM work together, the results of DCAN reach 35.39% and 54.41%, which fully demonstrates the importance of context aggregation at two levels and the effectiveness of MTCA and CFM.</figDesc><table><row><cell>Model</cell><cell>TEB</cell><cell cols="3">MEB ANet-1.3 THU-14</cell></row><row><cell>BSN</cell><cell>TEM</cell><cell>PEM</cell><cell>30.03</cell><cell>36.90</cell></row><row><cell>BMN</cell><cell>TEM</cell><cell>PEM</cell><cell>33.85</cell><cell>38.80</cell></row><row><cell>BSN++</cell><cell>CBG</cell><cell>PRB</cell><cell>34.88</cell><cell>41.30</cell></row><row><cell cols="3">DCAN MTCA PEM</cell><cell>35.02</cell><cell>52.55</cell></row><row><cell>DCAN</cell><cell>TEM</cell><cell>CFM</cell><cell>34.80</cell><cell>49.42</cell></row><row><cell cols="3">DCAN MTCA CFM</cell><cell>35.39</cell><cell>54.14</cell></row><row><cell cols="5">different semantic densities will damage the semantic rep-</cell></row><row><cell cols="2">resentation inside the</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The effect of different r smooth of the MPTC-S on ActivityNet v1.3 and THUMOS-14 in terms of mAP(%).</figDesc><table><row><cell>Dataset</cell><cell>mAP</cell><cell>*</cell><cell>r = 1 r = 3 r = 5</cell></row><row><cell></cell><cell>0.5</cell><cell cols="2">51.49 51.56 51.78 51.86</cell></row><row><cell>ActivityNet</cell><cell cols="3">0.75 35.61 35.92 35.98 36.00 0.95 8.55 8.87 9.45 8.86</cell></row><row><cell></cell><cell cols="3">Avg 35.01 35.18 35.39 35.28</cell></row><row><cell></cell><cell>0.3</cell><cell cols="2">30.58 32.11 32.62 31.48</cell></row><row><cell></cell><cell>0.4</cell><cell cols="2">41.40 42.17 43.91 42.90</cell></row><row><cell>THUMOS-14</cell><cell>0.5</cell><cell cols="2">51.03 53.67 54.14 53.19</cell></row><row><cell></cell><cell>0.6</cell><cell cols="2">59.40 61.59 62.73 61.36</cell></row><row><cell></cell><cell>0.7</cell><cell cols="2">65.16 66.80 68.21 67.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The effect of different group size G of CFM on ActivityNet v1.3 dataset in terms of mAP(%).</figDesc><table><row><cell>G</cell><cell>0.5</cell><cell cols="2">0.75 0.95 Average</cell></row><row><cell cols="3">1 51.34 35.82 8.90</cell><cell>35.11</cell></row><row><cell cols="3">2 51.78 35.98 9.45</cell><cell>35.39</cell></row><row><cell cols="3">4 51.51 35.71 8.83</cell><cell>35.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Generalizability evaluation on ActivityNet v1.3.</figDesc><table><row><cell>train</cell><cell cols="2">Seen(val)</cell><cell cols="2">Unseen(val)</cell></row><row><cell></cell><cell cols="4">AR@100 AUC AR@100 AUC</cell></row><row><cell>Seen+Unseen</cell><cell>74.34</cell><cell>66.65</cell><cell>75.55</cell><cell>67.76</cell></row><row><cell>Seen</cell><cell>73.43</cell><cell>65.10</cell><cell>72.58</cell><cell>64.86</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Natural Science Foundation of China (No. 62076119, No. 61921006), Program for Innovative Talents and Entrepreneur in Jiangsu Province, and Collaborative Innovation Center of Novel Software Technology and Industrialization.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Boundary Content Graph Neural Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<editor>Vedaldi, A.</editor>
		<editor>Bischof, H.</editor>
		<editor>Brox, T.</editor>
		<editor>and Frahm, J.</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12373</biblScope>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Soft-NMS -Improving Object Detection with One Line of Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking the Faster R-CNN Architecture for Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Karami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1711.08200</idno>
		<title level="m">Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">DAPs: Deep Action Proposals for Action Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<editor>Leibe, B.</editor>
		<editor>Matas, J.</editor>
		<editor>Sebe, N.</editor>
		<editor>and Welling, M.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Slow-Fast Networks for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">CTAP: Complementary Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<editor>Ferrari, V.</editor>
		<editor>Hebert, M.</editor>
		<editor>Sminchisescu, C.</editor>
		<editor>and Weiss, Y.</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11206</biblScope>
			<biblScope unit="page" from="70" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate Temporal Action Proposal Generation with Relation-Aware Pyramid Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10810" to="10817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3648" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scale Matters: Temporal Scale Aggregation Network For Precise Action Localization In Untrimmed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ActivityNet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS Challenge: Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<title level="m">The Kinetics Human Action Video Dataset. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal Convolutional Networks for Action Segmentation and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TEA: Temporal Excitation and Aggregation for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="906" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TSM: Temporal Shift Module for Efficient Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7082" to="7092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BMN: Boundary-Matching Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3888" to="3897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single Shot Temporal Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">BSN: Boundary Sensitive Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<editor>Ferrari, V.</editor>
		<editor>Hebert, M.</editor>
		<editor>Sminchisescu, C.</editor>
		<editor>and Weiss, Y.</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11208</biblScope>
			<biblScope unit="page" from="3" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-Granularity Generator for Temporal Action Proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TEINet: Towards an Efficient Architecture for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11669" to="11676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gaussian Temporal Awareness Networks for Action Localization</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal Context Aggregation Network for Temporal Action Proposal Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="485" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ghahramani</title>
		<editor>Lawrence, N. D.</editor>
		<editor>and Weinberger, K. Q.</editor>
		<meeting><address><addrLine>Cortes, C</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2602" to="2610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relaxed Transformer Decoders for Direct Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13526" to="13535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<title level="m">Learning Spatiotemporal Features with 3D Convolutional Networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1708.05038</idno>
	</analytic>
	<monogr>
		<title level="j">ConvNet Architecture Search for Spatiotemporal Feature Learning. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TDN: Temporal Difference Networks for Efficient Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1895" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">UntrimmedNets for Weakly Supervised Action Recognition and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding Convolution for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<editor>Ferrari, V.</editor>
		<editor>Hebert, M.</editor>
		<editor>Sminchisescu, C.</editor>
		<editor>and Weiss, Y.</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1608.00797</idno>
		<title level="m">CUHK &amp; ETHZ &amp; SIAT Submission to ActivityNet Challenge</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">G-TAD: Sub-Graph Localization for Temporal Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10153" to="10162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Action Detection from Frame Glimpses in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal Action Detection with Structured Segment Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08011</idno>
		<title level="m">Cuhk &amp; ethz &amp; siat submission to activitynet challenge 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
