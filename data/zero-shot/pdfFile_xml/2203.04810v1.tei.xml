<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarking Graphormer on Large-Scale Molecular Modeling Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-09">9 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>You</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyan</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Benchmarking Graphormer on Large-Scale Molecular Modeling Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-09">9 Mar 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This technical note describes the recent updates of Graphormer, including architecture design modifications, and the adaption to 3D molecular dynamics simulation. With these simple modifications, Graphormer could attain better results on large-scale molecular modeling datasets than the vanilla one, and the performance gain could be consistently obtained on 2D and 3D molecular graph modeling tasks. In addition, we show that with a global receptive field and an adaptive aggregation strategy, Graphormer is more powerful than classic message-passing-based GNNs. Empirically, Graphormer could achieve much less MAE than the originally reported results on the PCQM4M quantum chemistry dataset used in KDD Cup 2021. In the meanwhile, it greatly outperforms the competitors in the recent Open Catalyst Challenge, which is a competition track on NeurIPS 2021 workshop, and aims to model the catalyst-adsorbate reaction system with advanced AI models. All codes could be found at https://github.com/Microsoft/Graphormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graphormer <ref type="bibr" target="#b18">(Ying et al., 2021)</ref> is a recently proposed deep learning model built upon the standard Transformer, which aims to break through the limitations of conventional graph neural networks <ref type="bibr" target="#b17">(Xu et al., 2019;</ref><ref type="bibr" target="#b7">Hamilton et al., 2017)</ref> on expressiveness and over-smoothing. The architecture design of Graphormer is quite simple, where a standard Transformer <ref type="bibr" target="#b15">(Vaswani et al., 2017)</ref> is equipped by three structural encodings (i.e., centrality, spatial, and edge encodings), which is attractively effective on a wide range of graph representation tasks. Graphormer enjoys the great power of expressiveness from Transformer architecture, while also incorporates the structural information of the graph with high efficiency.</p><p>In this note, we report several design improvements built in the Graphormer framework. We first investigate the change of placement of layer normalization, and find that the Post-LN variant could lead to significantly better results on molecular property prediction tasks. Furthermore, we extend Graphormer to 3D molecule graph modeling by designing specific centrality encoding, spatial encoding, and 3D attention sub-layer. After a series of architecture design modifications, the upgraded Graphormer establishes stronger and more feasible baselines on large-scale molecular modeling datasets, i.e., PCQM4M <ref type="bibr" target="#b8">(Hu et al., 2021)</ref> and OC20 .</p><p>Finally, we endeavor to obtain a better theoretical understanding of Graphormer via the lens of distributed computing theory. Specifically, we show that with a global receptive field, Graphormer enjoys a greater expressiveness compared to classic message-passing-based GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Empirical Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Placement of Layer Normalization Matters</head><p>The originally designed Graphormer follows the architecture of a prevailing variant of Transformer, i.e., the Pre-LN Transformer, where layer normalization is placed inside the residual blocks. Recent literature <ref type="bibr" target="#b16">(Xiong et al., 2020)</ref> shows that with this modification, the gradients are well-behaved at initialization, leading to a much faster convergence compared to the vanilla Post-LN Transformer <ref type="bibr" target="#b15">(Vaswani et al., 2017</ref>). Yet, we observe that the latter leads to a better generalization performance on the large-scale quantum chemical property prediction dataset PCQM4M <ref type="bibr" target="#b8">(Hu et al., 2021)</ref>. Therefore, we adopt this simple modification to Graphormer and establish a stronger baseline built in the Graphromer framework.  <ref type="bibr" target="#b8">(Hu et al., 2021)</ref>, which contains 3.8M graphs and 55.4M edges in total. It is a quantum chemistry dataset aiming to accelerate quantum physical property calculation based on the Density Functional Theory (DFT) by advanced machine learning methods. This dataset has been recently updated to v2 with several modifications 1 and 3D molecular structures. In this section, we report the performance on both versions, but without using any 3D geometric information.</p><p>Settings. We maintain the model configuration for the 12layer Graphormer Base model, then we further scale up it to a 24-layer Graphormer Large model:</p><p>1. Graphormer Base : L = 12, d = 768, H = 32, 2. Graphormer Large : L = 24, d = 1024, H = 32,</p><p>where d and H represents the hidden dimension and the number of attention heads, respectively. We keep the hyperparameter the same as <ref type="bibr" target="#b18">(Ying et al., 2021)</ref>, except that a small learning rate 8 ? 10 ?5 is used for PostLN Large to improve training stability.</p><p>From <ref type="table">Table 1</ref>, we can see the PostLN variant could attain better performance than PreLN on the large-scale molecular property prediction task. Interestingly, although the optimization error is lower, the generalization ability of Graphormer Large is worse on this dataset. Thus, there is still potential to develop deep Graphormer models with better generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adaptation to 3D Molecular Modeling</head><p>A molecule can be represented by a 3D molecular graph G = (V, P ), where V = {v 1 , v 2 , ? ? ? , v n } denotes the set of atoms, each of which holding a feature vector x i , and P = {r 1 , r 2 , ? ? ? , r n } is the set of 3D Cartesian coordinates of atoms which contains 3D spatial information.</p><p>1 https://ogb.stanford.edu/docs/lsc/pcqm4mV2/.</p><p>Given a 3D molecular graph as the input of Graphormer, new designs of structural encoding (i.e., spatial encoding and centrality encoding in <ref type="bibr" target="#b18">Ying et al. (2021)</ref>) are desired for modeling the graph spatial information. First, we choose ?(v i , v j ) in the spatial encoding to be the Euclidean distance between v i and v j , and adopt a set of Gaussian basis functions <ref type="bibr" target="#b14">(Shuaibi et al., 2021)</ref> to encode ?(v i , v j ) in order to model the spatial relation between atoms. Second, all spatial encodings of each node are simply summed up to obtain the centrality encoding which describes the importance of the atom in the 3D molecular graph.</p><p>Periodic boundary condition (PBC) is the common boundary condition for crystal systems, where a set of atoms in the 3D unit cell is periodically repeated. Typically, a radius graph with PBC is constructed <ref type="bibr" target="#b19">(Zitnick et al., 2020)</ref> to capture the local 3D structure surrounding each atom, where the replicated atoms among different unit cells are reduced to a single atom, may result in multiple edges between two atoms (i.e. multigraph). Since message passing is done by attention layers in Graphormer, rather than constructing a multigraph, we prefer to simply physically duplicate all atoms if they lie within the cutoff distance in multiple repeated cells.</p><p>In addition, we design a new attention layer to replace the original node-level projection head for generating 3D outputs. Concretely, the attention probability in a standard selfattention layer is decomposed into three directions by multiplying the normalized relative position offset rij rij ? R 3 between query and key atoms. Then three linear projection heads are applied to each component of the 3D attention layer's output in the three directions respectively. Specifically, it could maintain rotational equivariance of the model's final estimation layer if the parameters of the three linear projections are shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">MOLECULAR DYNAMICS SIMULATION</head><p>Dataset. We verify the effectiveness of 3D molecular modeling on the recent electrocatalysts dataset -the Open Catalyst 2020 (OC20) . It aims to accelerate the catalyst discovery process for solar fuels synthesis, long-term energy storage, and renewable fertilizer production, by using machine learning models to find lowcost electrocatalysts to drive the electrochemical reactions at high rates. The OC20 dataset contains more than 660k catalyst-adsorbate reaction systems (over 140M structureenergy estimation) produced by molecular dynamics simulation using density functional theory. In this section, we report the results of Graphormer for predicting the relaxed energies from initial structures.</p><p>Settings. We employ a 12-layer Graphormer Base as the basic model for energy prediction. Inspired by Jumper et al. <ref type="formula">(2021)</ref>, we repeatedly feed the outputs to this basic model by four times, which contributes markedly to accuracy with minor extra training time. We optimize the model using Adam with learning rate 3e-4 and weight decay 1e-3. We train the model using batch size 64 for 1 million steps.</p><p>In addition to predicting the relaxed energy of the entire system, inspired by <ref type="bibr">Godwin et al. (2022)</ref>, we further adopt an auxiliary node-level objective to predict the displacement of each atom between the initial and relaxed structures. In <ref type="table">Table 2</ref>.1.1 we report the performance on the IS2RE Direct track, which directly estimates the relaxed energy from the initial structure. As shown in the table, the energy prediction of unseen element compositions for catalysts (Out of Domain (OOD) Catalyst) is much accurate than OOD Adsorbates, and OOD Both, which implies that Graphormer may have the potential to help the catalyst discovery process for well-known but important chemical species involved in the chemical reactions of interest, such as OH, O 2 , or H 2 O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Understanding Graphormer from a Theoretical Perspective</head><p>In this section, we develop a better theoretical understanding of Graphormer based on distributed computing theory. Compared with the classic message-passing-based GNNs (MPGNNs), Graphormer enjoys two unique characteristics: a global receptive field and an adaptive aggregation strategy. We first analyze the impact of the global receptive field on expressiveness. In literature, the k-Weisfeiler-Lehman (k-WL) graph isomorphism test is often adopted as a metric to characterize the expressiveness of MPGNNs. For example, it was shown in <ref type="bibr" target="#b17">(Xu et al., 2019)</ref> that the anonymous MPGNN is equivalent to 1-WL test, which implies that anonymous MPGNN has a very limited ability in graph isomorphism test. Higher-order GNNs have been developed to go beyond 1-WL test <ref type="bibr" target="#b12">(Morris et al., 2019;</ref><ref type="bibr" target="#b1">Balcilar et al., 2021)</ref> and achieve the same power as 3-WL test. In these works, one essential assumption is that the nodes share the same feature. For wide and deep MPGNNs with discriminative features, their expressive power goes beyond k-WL (for any k) and becomes universal <ref type="bibr">(Abboud et al., 2021)</ref>. The discriminative features can be easily achieved by adding random features <ref type="bibr" target="#b13">(Sato et al., 2021)</ref> or unique identifiers <ref type="bibr">(Loukas, 2020)</ref>, which is often adopted in practice. Thus, to characterize the expressiveness of more practical MPGNNs, a new metric is needed.</p><p>In <ref type="bibr">(Loukas, 2020)</ref>, the expressiveness of GNNs with discriminative features was studied via the metrics in distributed computing theory. Specifically, it was proved that a d layer MPGNN with width w has equivalent expressiveness to a CONGEST model with d communication round and w log n communication bits in each round, where n is the number of nodes in the graph. Thus, the expressiveness bound of CONGEST models in distributed computing literature can be used for characterizing the expressiveness of MPGNNs. Due to the locality of the messagepassing scheme in CONGEST models, they lose a significant power when d and w is limited. Specifically, several graph problems cannot be solved unless the product of d and w, i.e., communication complexity, is large enough. By repurposing these results in CONGEST models, it was shown in <ref type="bibr">(Loukas, 2020)</ref> that several graph problems cannot be solved unless the product of MPGNN's depth d and width w, i.e., model capacity, exceeds a polynomial of the graph size. The impossibility results in <ref type="bibr">(Loukas, 2020)</ref> are summarized in the column "Local MP" in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>In contrast to the local message-passing in MPGNNs, each node is allowed to send different messages to any other node in Graphormer. This paradigm is called CONGESTED CLIQUE in distributed computing literature <ref type="bibr" target="#b5">(Drucker et al., 2014)</ref> and breaks the expressiveness barrier of the CONGEST model. We summarize the corresponding possibility results of CONGESTED CLIQUE in the column "Non-local MP" of <ref type="table" target="#tab_2">Table 3</ref>. Detailed theorems are listed in Appendix A. These bounds demonstrate that a global receptive field qualitatively improves the expressiveness of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem</head><p>Local MP Non-local MP 4-cycle detection dw = ?( ? n/log n) dw = O(1) subgraph verification d ? w = ?( ? n/ log n) dw = O(1) diam. 3 /2-approx. dw = ?( ? n/log n) dw = O(n 0.158 ) 5-cycle detection dw = ?(n/log n) dw = O(n 0.158 ) diam. computation dw = ?(n/log n) dw = O(n 1/3 log n) min. vertex cover dw = ?(n 2 /log 2 n) for w = O(1) dw = O(n 1?2/n ) for w = O(1) max. indep. set dw = ?(n 2 /log 2 n) for w = O(1) dw = O(n 1?2/n ) for w = O(1) A. Theorems for <ref type="table" target="#tab_2">Table 3</ref> Theorem A.1. (4-cycle detection) <ref type="bibr">(Theorem 4 in (Censor-Hillel et al., 2019)</ref>) Given w = O(1), the existance of 4-cycle can be detected in O(1) rounds.</p><p>Theorem A.2. (5-cycle detection) (Theorem 3 in <ref type="bibr" target="#b2">(Censor-Hillel et al., 2019)</ref>) Given w = O(1), for directed and undirected graphs, the existence of k cycles can be detected in d = 2 O(k) n 1?2/? log n rounds, where ? is the matrix multiplication time.</p><p>For 5-cycle detection problem, taking k = 5 results in the desired bound.</p><p>Theorem A.3.</p><p>(Diameter computation) (Corollary 6 in <ref type="bibr" target="#b2">(Censor-Hillel et al., 2019)</ref>) Given w = O(1), for weighed and directed graphs with integer weights {0, ?1, ? ? ? , ?M }, all-pair shortest path can be computed in d = O(n 1/3 log n?log M/ log n?).</p><p>Theorem A.4. (Diameter approximation) <ref type="bibr">(Theorem 9 in (Censor-Hillel et al., 2019)</ref>) Given w = O(1), for directed graphs with integer weights in {0, 1, ? ? ? , 2 n o(1) }, we can compute (1 + o(1))-approximate all-pairs shortest path in d = O(n 1?2/?+o(1) ) rounds.</p><p>Theorem A.5. (Subgraph verification) (Corollary 1 in <ref type="bibr" target="#b10">(Jurdzi?ski &amp; Nowicki, 2018)</ref>) Given w = O(1), there are randomized distributed algorithms that solve the following verification problems in congested clique model in O(1) rounds with high probability: bipartiteness verifcation, cut verification, s-t connectivity, and cycle containment.</p><p>Theorem A.6. (Subgraph Detection) <ref type="bibr" target="#b4">(Dolev et al., 2012)</ref> Given w = O(1), there are distributed algorithms that count the number of d-vertex subgraph in d = O(n (d?2)/d / log n) rounds.</p><p>The bound of the maximum independent set can be obtained by using the relationship between k-independent set and maximum independent set, and the minimum vertex cover is equivalent to the maximum independent set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Title Suppressed Due to Excessive Size</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Enery MAE (eV) on IS2RE Task (Direct)</cell></row><row><cell>case</cell><cell>ID</cell><cell cols="3">OOD Ads. OOD Cat. OOD Both</cell><cell>avg.</cell></row><row><cell cols="2">0.4329 Graphormer Base (ensemble) 0.3976 Graphormer Base *</cell><cell>0.5850 0.5719</cell><cell>0.4441 0.4166</cell><cell>0.5299 0.5029</cell><cell>0.4980 0.4722</cell></row><row><cell cols="3">Table 2. Results on IS2RE task by direct approach.</cell><cell></cell><cell></cell></row></table><note>* denotes evaluation on the OC20 validation split.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>A comparison of local message passing and non-local message passing in the model capacity for solving different graph problems. Results for local MP are from(Loukas, 2020)  and results for non-local MP are shown in Appendix A.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Microsoft Research Asia 2 HKUST 3 Tsinghua University 4 USTC 5 Peking University. Correspondence to: Shuxin Zheng &lt;shuz@microsoft.com&gt;.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The surprising power of graph neural networks with random node initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Breaking the limits of message passing graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gauzere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="599" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algebraic methods in the congested clique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Censor-Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lenzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suomela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="461" to="478" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heras-Domingo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<title level="m">The open catalyst 2020 (oc20) dataset and community challenges. arxiv. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">tri, tri again&quot;: Finding triangles and small subgraphs in a distributed setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dolev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lenzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peled</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Distributed Computing (DISC)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="195" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the power of the congested clique model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Oshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM symposium on Principles of distributed computing (PODC)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple gnn regularisation for 3d molecular property prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schaarschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Representation Learning (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>??dek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mst in o (1) rounds of congested clique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jurdzi?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nowicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-SIAM Symposium on Discrete Algorithms (SODA)</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2620" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining (SDM)</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="333" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolluru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ulissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09575</idno>
		<title level="m">Rotation invariant graph neural networks using spin convolutions</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Do transformers really perform bad for graph representation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An introduction to electrocatalyst design using machine learning for renewable energy storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heras-Domingo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palizhati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riviere</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09435</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
