<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CV:HCI Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CV:HCI Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxiang</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rei?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CV:HCI Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Carl Zeiss AG</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyu</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CV:HCI Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CV:HCI Lab</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Panoramic images with their 360 ? directional view encompass exhaustive information about the surrounding space, providing a rich foundation for scene understanding. To unfold this potential in the form of robust panoramic segmentation models, large quantities of expensive, pixelwise annotations are crucial for success. Such annotations are available, but predominantly for narrow-angle, pinholecamera images which, off the shelf, serve as sub-optimal resources for training panoramic models. Distortions and the distinct image-feature distribution in 360 ? panoramas impede the transfer from the annotation-rich pinhole domain and therefore come with a big dent in performance. To get around this domain difference and bring together semantic annotations from pinhole-and 360 ? surround-visuals, we propose to learn object deformations and panoramic image distortions in the Deformable Patch Embedding (DPE) and Deformable MLP (DMLP) components which blend into our Transformer for PAnoramic Semantic Segmentation (Trans4PASS) model. Finally, we tie together shared semantics in pinhole-and panoramic feature embeddings by generating multi-scale prototype features and aligning them in our Mutual Prototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor Stan-ford2D3D dataset, our Trans4PASS with MPA maintains comparable performance to fully-supervised state-of-thearts, cutting the need for over 1, 400 labeled panoramas. On the outdoor DensePASS dataset, we break state-of-theart by 14.39% mIoU and set the new bar at 56.38%. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Panoramic 360 ? cameras have received an increasing amount of attention in fields, such as omnidirectional sensing in automated vehicles <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b74">75]</ref> and bringing immersive viewing experiences to augmented-and virtual reality displays <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b70">71]</ref>. Opposed to images captured with pinhole cameras, that occupy narrow Fields of View (FoV), panoramic images offer omni-range perception, benefiting the detection of road scene objects and indoor scene elements <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>. In particular, dense semantic segmentation on panoramic images, facilitates a high-level holistic pixelwise understanding of surrounding environments <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b72">73]</ref>.</p><p>Panoramic semantic segmentation is usually performed on 2D panoramas that were transformed using equirectangular projection <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b74">75]</ref>, which is accompanied by image distortions and object deformations (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Further, in the 360 ? image domain, labeled data is scarce which necessitates model training to be carried out on semantically matching narrow-FoV pinhole datasets. These two circumstances culminate in a significantly degraded performance on panoramic segmentation as compared to the pinhole counterpart <ref type="bibr" target="#b71">[72]</ref> and as such they have to be adequately addressed. Considering the intricacies of panoramas, convolution variants <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59]</ref> and attention-augmented models <ref type="bibr" target="#b74">[75]</ref> were proposed to mitigate image distortions and enlarge receptive fields of Convolutional Neural Networks (CNNs). However, they remain sub-optimal in handling the severe deformations from pinhole-to panoramic data, and fail in establishing long-range contextual dependencies in the ultra-wide 360 ? images, which prove essential for accurate semantic segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b93">94]</ref>.</p><p>In light of these challenges, we propose a Transformer for PAnoramic Semantic Segmentation (Trans4PASS) architecture, and overcome image distortions and object deformations with two novel design choices: Our Deformable Patch Embedding (DPE) is located at the early image sequentialization-and intermediate feature interpretation stages empowering the model to learn characteristic panoramic image distortions and preserve semantics. Secondly, with the Deformable MLP (DMLP) module in the feature parsing stage, we mix patches with learned spatial offsets to enhance global context modeling.</p><p>The challenging mismatch between the label-rich pinhole-and the label-scarce panoramic domain can also be addressed by unsupervised domain adaptation (UDA), considering labeled 2D Pinhole images as source-and 360 ? Panoramas as target domain. Following previous works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b74">75]</ref>, we refer to this scenario as PIN2PAN. Taking this view on the learning problem, shows to be a vital ingredient for circumventing the expensive panoramic image annotation process while satisfying the need for largescale annotated data <ref type="bibr" target="#b93">[94]</ref> to train robust segmentation transformers. Unlike common adversarial-learning <ref type="bibr" target="#b43">[44]</ref> and pseudo-label self-learning <ref type="bibr" target="#b96">[97]</ref> methods for UDA, we put forward Mutual Prototypical Adaptation (MPA), which generates mutual prototypes for pinhole-and panoramic multiscale feature embeddings, distilling prototypical knowledge of both domains, which proves advantageous to domainseparate distillation <ref type="bibr" target="#b83">[84]</ref>. On top, we show MPA works with pseudo-labels in a joint manner and provides a complementary alignment incentive in the feature space.</p><p>To verify the capability for generalization to diverse scenarios of our solution, we evaluate Trans4PASS on both indoor-and outdoor panoramic-view datasets, i.e., Stanford2D3D <ref type="bibr" target="#b0">[1]</ref> and DensePASS <ref type="bibr" target="#b44">[45]</ref> benchmarks. On DensePASS, it outperforms the previous best result <ref type="bibr" target="#b87">[88]</ref> by &gt;10.0% in mIoU. Our solution achieves top performance among unsupervised methods on Stanford2D3D and even ranks higher than many competing supervised methods.</p><p>In summary, we deliver the following contributions: <ref type="bibr" target="#b0">(1)</ref> We consider panoramic deformations in our distortionaware Transformer for Panoramic Semantic Segmentation (Trans4PASS) with deformable patch embeddingand deformable MLP modules. <ref type="bibr" target="#b1">(2)</ref> We present Mutual Prototypical Adaptation to transfer models via distilling dual-domain prototypical knowledge, boosting performance by coupling it with pseudo-labels in feature-and output space. (3) Our framework for transferring models from PIN2PAN yields excellent results on two competitive benchmarks: On Stanford2D3D we circumvent using 1, 400 expensive panorama labels while achieving comparable results and on DensePASS we boost state-of-the-art performance by an absolute 14.39% in mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic-and panoramic segmentation. Dense semantic segmentation is experiencing steep progress since FCN <ref type="bibr" target="#b42">[43]</ref> addressed it end-to-end. Following works built upon FCN to improve performance by enlarging receptive fields <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b92">93]</ref> and refining context priors <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b79">80]</ref>. Driven by non-local blocks <ref type="bibr" target="#b65">[66]</ref>, self-attention <ref type="bibr" target="#b62">[63]</ref> is integrated to learn longrange dependencies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref> within FCNs. Currently, architectures which replace convolutional-with transformerbased backbones <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b60">61]</ref> emerge. Then, image perception is viewed from the lens of sequence-to-sequence learning with dense prediction transformers <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b81">82]</ref> and semantic segmentation transformers <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b93">94]</ref>. Recently, MLP-like architectures <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b59">60]</ref> which alternate spatial-and channel mixing sparked interest for recognition tasks. Most methods are designed for narrow-FoV images and often have large accuracy drops in the 360 ? domain. In this work, we address panoramic segmentation, with a novel Transformer architecture which considers a broad FoV already in its design and handles the panorama-specific semantic distribution via MLP-based mixing. By capturing wide-FoV scenes, panoramic images can serve as starting point for a more holistic scene understanding. Outdoor panorama segmentation works rely on fisheye cameras <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79]</ref> or panoramic images <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b73">74]</ref> for seamless 360 ? parsing. Indoor methods on the other hand focus on either distortion-mitigated representations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b54">55]</ref> or multi-task schemes <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b84">85]</ref>. Most of these works assume that labeled images are available in the target panorama domain. We cut this requirement for labeled target data and circumvent the prohibitively expensive annotation process of determining pixel-wise semantics in complex real-world surroundings. Therefore, unlike previous works, we look through the lens of unsupervised transfer learning and introduce a pinhole-to panorama (PIN2PAN) adaptation method to profit from rich, readily available annotated pinhole datasets. In experiments, our panoramic segmentation transformer architecture generalizes to both indoor and outdoor scenes. Unsupervised domain adaptation. Domain adaptation has been thoroughly investigated to enhance model generalization to unseen domains, with two predominant paradigms based either on self-training <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b91">92]</ref> or adversarial learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b61">62]</ref>. Self-training methods generally create pseudo-labels to gradually adapt through iterative improvement <ref type="bibr" target="#b36">[37]</ref>, whereas adversarial solutions leverage the idea of GANs <ref type="bibr" target="#b17">[18]</ref> to perform image translation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>, or enforce alignment in layout matching <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref> and feature agreement <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. Further adaptation flavors, consider uncertainty reduction <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b94">95]</ref>, model ensembling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b75">76]</ref>, category-level alignment <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b45">46]</ref>, or adversarial entropy minimization <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b63">64]</ref>. Relevant to our work, PIT <ref type="bibr" target="#b18">[19]</ref> addresses the camera gap with FoV-based adaptation, whereas  P2PDA <ref type="bibr" target="#b44">[45]</ref> first tackles PIN2PAN transfer by learning attention correspondences. Aside from distortion-adaptive architecture design, we revisit PIN2PAN segmentation from a feature prototype adaptation-based perspective where we distill panoramic knowledge through class-wise prototypes. Different from methods using individual prototypes for source and target domains <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b90">91]</ref>, we present mutual prototypical adaptation, which jointly exploits source and target feature embeddings to boost transfer beyond the FoV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Here, we put forward our panoramic semantic segmentation framework. In Sec. 3.1, we introduce the Trans4PASS architecture for capturing distortion-aware features and long-range dependencies, with detailed descriptions of deformable patch embeddings and the deformable MLP module in Sec. 3.2 and 3.3. Finally, we outline our domain adaptation method using mutual prototype features in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Trans4PASS Architecture</head><p>To investigate the transformer model on panoramic semantic segmentation, we create two versions of Trans4PASS models (T: Tiny and S: Small). We build both with four stages, where for the tiny model, each stage encompasses 2 layers, for the small version the stages have 3, 4, 6, and 3 layers. As shown in <ref type="figure">Fig. 2</ref>, the pyramidal stages are inspired by recent transformers <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b67">68]</ref>, which reduce the feature scales in deeper layers. Given an input image with H?W ?3, Trans4PASS makes use of a Patch Embedding (PE) module <ref type="bibr" target="#b67">[68]</ref> to split the image into patches. To deal with the severe distortions in panoramas, a special Deformable Patch Embedding (DPE) module is proposed and applied in the encoder and decoder <ref type="figure">(Fig. 2c</ref>). In the encoder, each feature map f l ?{f 1 , f 2 , f 3 , f 4 } in the l th stage is down-sampled by the l th stride ?{4, 8, 16, 32}. The channel dimensions C l ?{64, 128, 320, 512} grow successively. Different from the FPN-like decoder <ref type="bibr" target="#b93">[94]</ref> and vanilla-MLP based decoder <ref type="bibr" target="#b67">[68]</ref> in <ref type="figure">Fig. 2</ref>, we propose the Deformable MLP (DMLP) decoder structure, which mixes feature patches extracted via DPE. Given the extracted fea-ture hierarchy in multiple scales from the encoder, four deformable decoder layers process the feature hierarchy into a consistent shape of H 4 ? W 4 ?C emb , where we set the number of resulting embedding channels C emb =128. An ensuing linear layer transforms the 128 channel output to contain the number of semantic classes of the respective task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deformable Patch Embedding</head><p>Spherical topological images captured by 360 ? cameras occupy a polar coordinate system with ??[0, 2?) and ??[0, ?]. To represent it in 2D space, the spherical data is usually converted into a panoramic format in euclidean-like space through the equirectangular projection. This process leads to severe shape distortions in the projected panoramic image, as seen in <ref type="figure" target="#fig_0">Fig. 1</ref>. Therefore, a common PE module with fixed sampling positions does not respect these shape distortions of objects and the overall scene. Inspired by deformable convolution <ref type="bibr" target="#b11">[12]</ref> and overlapping PE <ref type="bibr" target="#b67">[68]</ref>, we propose Deformable Patch Embeddings (DPE) and employ them on the input to the encoder and the decoder, splitting panoramic images and features. Given an input image or feature map f ?R H?W ?Cin , a standard PE module <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b67">68]</ref> splits it into a flattened 2D patch sequence z?R ( HW s 2 )?(s 2 ?Cin) , where HW s 2 is the number of patches and s is the width and height of each patch. Each element in this sequence is passed through a linear projection layer transforming it into C out dimensional embeddings.</p><p>Consider a single patch in z representing a rectangle of size s?s with s 2 positions. We can define a position offset relative to a location (i, j)|i, j? <ref type="bibr">[1, s]</ref> in the patch as ? (i,j) ?N 2 . In standard PE, these offsets are fixed and lie in ? (i,j) ?[?? s 2 ?, ?+ s 2 ?] 2 . Take e.g. a 3?3 patch, offsets ? (i,j) relative to the center will lie in [?1, 1]?[?1, 1].</p><p>As we want to process panoramic images, which inherit distortions from the equirectangular projection, we can directly address this degradation in the PE. To this end, in our Deformable Patch Embedding (DPE), we enable the model to learn a data-dependent offset ? DP E ?N H?W ?2 that can better cope with the spatial connections of objects, as present in distorted patches. DPE is learnable and pre-dicts relative offsets based on the original input f . The offset ? DP E (i,j) is calculated as depicted in Eq. (1).</p><formula xml:id="formula_0">? DP E (i,j) = min(max(? H r , g(f ) (i,j) ), H r ) min(max(? W r , g(f ) (i,j) ), W r ) ,<label>(1)</label></formula><p>where g(?) is the offset prediction function, which we implement via the deformable convolution operation <ref type="bibr" target="#b11">[12]</ref>. The hyperparameter r puts a constraint onto the offsets and is set as 4 in our experiments. The learned offsets make DPE adaptive and as a result distortion-aware.</p><p>In earlier works, DPT <ref type="bibr" target="#b6">[7]</ref> applies non-overlapping PE with anchor-based offsets at later stages, PS-ViT <ref type="bibr" target="#b82">[83]</ref> uses a progressive sampling module coupled with previous iterations, and Deformable DETR <ref type="bibr" target="#b95">[96]</ref> leverages deformable attention to enhance feature maps. Unlike these previous works, our proposed DPE is designed for pixel-dense prediction tasks and is flexible to replace the raw PE without having to couple previous iterations. Intuitively, a model supplied with DPE, can profit from pinhole images and better adapt to distortions in panoramic images by learning to counteract severe deformations in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deformable MLP</head><p>Apart from the specific design of the encoder, the decoder with an adaptive feature parsing capacity is crucial in segmentation transformers <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b88">89]</ref>. As shown in <ref type="figure">Fig. 2a</ref>, some transformers <ref type="bibr" target="#b93">[94]</ref> borrow a FPN-like decoder from the CNN counterpart <ref type="bibr" target="#b37">[38]</ref>, whose receptive field is limited to the feature resolution in its final stage <ref type="bibr" target="#b64">[65]</ref>. SegFormer <ref type="bibr" target="#b67">[68]</ref> takes inspiration from Multilayer Perceptron-based (MLP) models <ref type="bibr" target="#b59">[60]</ref> and integrates a vanilla MLP to combine features ( <ref type="figure">Fig. 2b</ref>), but does not consider potential distortions in the imaging data. Next, we propose a mechanism to associate self-attention in Transformers and deformationproperties in 360 ? imagery. Linking both of these enables profiting from long-range dependencies for dense scene parsing and keeping this improvement when processing panoramic scenes. Achieving this distortion-aware property at manageable computational complexity, we put forward the Deformable MLP (DMLP) module. Within each stage of the decoder, DMLP mixes patches across the channel dimension, but with a particularly large receptive field, which improves the interpretation of features delivered by the aforementioned DPE. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the difference in MLP-based modeling: while the vanilla MLP (see <ref type="figure" target="#fig_2">Fig. 3a</ref>) performs traditional linear projection without learning any spatial context, Cy-cleMLP (see <ref type="figure" target="#fig_2">Fig. 3b</ref>) has a limited spatial receptive field by hand-crafted, fixed offsets in mixing patches and their channels. In <ref type="figure" target="#fig_2">Fig. 3c</ref>, the proposed DMLP generates a learned spatial offset (top) in a wider range and an adaptive manner. Given the input feature map f ?R H?W ?Cin , the spatial offset ? DM LP (i,j,c) is predicted channel-wise as in Eq. (1) and is , where k?HW and c?C in , for mixing the flattened patch features z?R HW ?Cin , as:</p><formula xml:id="formula_1">z (k,c) = HW k=1 Cin c=1 w T (k,c) ? z (k+? DM LP (k,c) ,c) ,<label>(2)</label></formula><p>where w?R Cin?Cout is the weight matrix of a fullyconnected (FC) layer. As shown in <ref type="figure">Fig 2c,</ref> the decoder has a similar structure as a MLP-Mixer block <ref type="bibr" target="#b59">[60]</ref>, consisting of DPE, DMLP, and MLP modules. The residual connections are kept. Formally, the four-stage decoder is denoted as:</p><formula xml:id="formula_2">z l = DPE(C l , C emb )(z l ), ? l ?{1, 2, 3, 4} z l = DMLP(C emb , C emb )(? l ) +? l , ? l z l = MLP(C emb , C emb )(? l ) +? l , ? l z l = Up(H/4, W/4)(? l ), ? l p = LN(C emb , C K )( l=1? l ),<label>(3)</label></formula><p>where Up(?) and LN(?) refer to the Upsample-and Layer-Norm operations, and p is the prediction of K classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Mutual Prototypical Adaptation</head><p>Due to the lack of large-scale training data in panoramas, we look into PIN2PAN domain adaptation from a perspective of semantic prototypes <ref type="bibr" target="#b90">[91]</ref>. We propose the Mutual Prototypical Adaptation (MPA) method to enable distilling knowledge via prototypes which we cultivate through source ground truth labels and target pseudo labels. Pseudo-labels depend on the few remaining mutual properties from pinhole and panoramic images, e.g., scene distribution at the frontal viewing angle <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b74">75]</ref>. While the related PCS <ref type="bibr" target="#b83">[84]</ref> performs inter-and intra-domain instanceprototype learning, our mutual prototypes are learned from source-and target feature embeddings f s and f t , projected to a shared latent space, and stored in a dynamic bank, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. The key differences to PCS lie in that (1) the mutual prototypes are built by joining embeddings from both domains, and (2) our method leverages multi-scale pyramidal features using different input scales in computing the embeddings which yields more robust prototypes.</p><p>Given the source (pinhole) dataset with annotated images D s ={(x s , y s )|x s ?R H?W ?3 , y s ?{0, 1} H?W ?K } and the target (panoramic) dataset D t ={(x t )|x t ?R H?W ?3 } without annotations, the goal of domain adaptation is to learn semantics from the source domain and transfer it to the target domain with K shared classes. The network is trained in D s based on the segmentation loss:</p><formula xml:id="formula_3">L s SEG = ? H,W,K i,j,k=1 y s (i,j,k) log(p s (i,j,k) ),<label>(4)</label></formula><p>where p s (i,j,k) indicates the probability of pixel x s (i,j) predicted as k-th class on the source domain. To generalize the source pre-trained model to the target data, a typical Self-Supervised Learning (SSL) scheme optimizes the model based on the pseudo labels? t (i,j,k) of pixels x t (i,j) in the target domain:</p><formula xml:id="formula_4">L t SSL = ? H,W,K i,j,k=1? t (i,j,k) log(p t (i,j,k) ),<label>(5)</label></formula><p>where the pseudo label is given by the most probable class in the model predictions:? t</p><formula xml:id="formula_5">(i,j,k) = 1 k . =arg max p t (i,j,:)</formula><p>. However, training with hard pseudo-labels leaves the model sensitive and fragile against errors in its own prediction and has only a limited positive effect on performance. Therefore, we advocate prototype-based alignment in the feature space, which brings two benefits: (1) it softens the hard pseudo-labels by using them in feature space instead of as direct targets and (2) it performs complementary alignment of semantic similarities in feature space.</p><p>Specifically, given a set with all n s source-and n t target feature maps F ={f s 1 , . . . , f s ns } {f t 1 , . . . , f t nt }, with feature maps f fused from four-stage multi-scale features f = 4 l=1 f l . Each feature map is associated either with its respective source ground-truth label or a target pseudo-label. To compute the mutual prototype memory M={P 1 , ..., P K } with prototypes P k we take the mean of all feature vectors (pixel-embeddings) from all feature maps in F that share the class label k. We initialize M by computing the class-wise mean embeddings through the whole dataset and while training we update the prototype P k at timestep t online by P t+1</p><formula xml:id="formula_6">k ?mP t?1 k +(1?m)P t k with a mo- mentum m= 0.999, where P t</formula><p>k is the mean pixel-embedding among embeddings that share the class-label k in the current mini-batch. An overview of this procedure is displayed in <ref type="figure" target="#fig_3">Fig. 4</ref>. The mutual prototypical adaptation loss is inspired by the knowledge distillation loss <ref type="bibr" target="#b5">[6]</ref>, which drives the feature embedding f to be aligned with the prototypical feature mapf which is set up, by stacking the prototypes P k ?M according to the pixel-wise class distribution in either the source label or the pseudo-label. The resulting targetf has the same shape as f . For brevity, only the source domain is   <ref type="formula" target="#formula_7">6)</ref>, which is similar to the target domain.</p><formula xml:id="formula_7">L s M P A = ? ?T 2 KL(?(f s /T )||?(f s /T )) ? (1 ? ?)CE(y s , ?(f s )),<label>(6)</label></formula><p>where KL(?), CE(?), and ?(?) are Kullback-Leibler divergence, Cross-Entropy, and Softmax function, respectively. The temperature T and hyper-parameter ? are 20 and 0.9 in our experiments. The final loss is combined with a weight of ?=0.001 as:   </p><formula xml:id="formula_8">L=L s SEG +L t SSL +?(L s M P A +L t M P A ).<label>(7</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Trans4PASS Structural Analysis</head><p>Effect of DPE. We compare DPE against DePatch from DPT <ref type="bibr" target="#b6">[7]</ref>. While the object-aware offsets and scales in DPT make patches shift around the object, our DPE is flexible to split image patches and is decoupled from object proposals. As shown in the first group of <ref type="table" target="#tab_3">Table 3</ref>, compared with DPT, our DPE-based Trans4PASS adds +3.01% and +9.39% mIoU on Cityscapes and DensePASS, respectively.</p><p>Effect of DMLP. To ablate the effect of different MLP-like modules embedded in the decoder of Trans4PASS, we substitute DMLP by CycleMLP <ref type="bibr" target="#b4">[5]</ref> and ASMLP <ref type="bibr" target="#b35">[36]</ref> modules. DMLP is lighter than ASMLP with fewer GFLOPs, parameters and it is more adaptive as opposed to the fixed offsets in CycleMLP. The first group of <ref type="table" target="#tab_3">Table 3</ref> shows that DMLP outperforms both modules with 3% to 5% in mIoU.</p><p>Effect of encoders and decoders. With the same encoder as PVT, a DMLP-based decoder brings a +3.98% improvement compared to the FPN-and MLP-based decoders, as shown in the second group of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">PIN2PAN Adaptation</head><p>Ablations in outdoor scenarios. To verify the generalization ability of applying Trans4PASS in adaptation methods, FANet and DANet used in P2PDA <ref type="bibr" target="#b44">[45]</ref> are replaced by Trans4PASS-T/-S, as visible in     88, 97] methods. Following <ref type="bibr" target="#b87">[88]</ref>, we also involve multisupervision methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b89">90]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative analysis</head><p>Panoramic semantic segmentation visualizations. <ref type="figure" target="#fig_7">Fig. 6a</ref> and <ref type="figure" target="#fig_7">Fig. 6c</ref> demonstrate that Trans4PASS handles the distortion of panoramic images very well as compared to indoor <ref type="bibr" target="#b64">[65]</ref> and outdoor <ref type="bibr" target="#b67">[68]</ref> baseline models. Especially, the segmentation results for sidewalks and pedestrians from Trans4PASS have more accurate classifications and boundary distinctions, while the baseline model is confused by the distorted shape and space, due to the lacking capacity to learn long-range contexts and distortion-aware features. In the indoor case of <ref type="figure" target="#fig_7">Fig. 6c</ref>, the door and chair categories are barely detected by the baseline model, but our Trans4PASS can output precise segmentation masks on both objects. DPE and DMLP visualizations. <ref type="figure" target="#fig_7">Fig. 6b</ref> and <ref type="figure" target="#fig_7">Fig. 6d</ref> visualize effects of Deformable PE from four stages of Trans4PASS. The red dots denote the centers of a selected patch (size of s?s) sequence. Given learned offsets from DPE, s 2 yellow sampling dots are shifted to semanticrelevant areas in a flexible way, where each pixel is adaptive to distorted objects and space, like the deformed building and sidewalk (see Stage-4 DPE in <ref type="figure" target="#fig_7">Fig. 6b</ref>). Besides, to verify the effect of Deformable MLP, two feature map pairs from the 75 th channel before and after DMLP are displayed in <ref type="figure" target="#fig_7">Fig. 6e and 6e</ref>. The feature maps (indoors/outdoors) after DMLP present semantically recognizable responses, e.g. on regions of distorted sidewalks or doors, as compared to those before the DMLP module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>To revitalize 360 ? scene understanding, we introduce a universal framework with a Transformer for PAnoramic Semantic Segmentation (Trans4PASS) model and a Mutual Prototypical Adaptation (MPA) method for transferring semantic information from the label-rich pinhole domain to the label-scarce panoramic domain. The Deformable Patch Embedding (DPE) and the Deformable MLP (DMLP) module endow Trans4PASS with distortion awareness. The framework elevates state-of-the-art performances on the competitive Stanford2D3D and DensePASS benchmarks. Limitations. We note that the accuracy of some classes are still impacted by the partition boundary of panoramas at 180 ? . Transferring models between pinhole-, fisheye-, and panoramic domains, fusing modalities, and solving various tasks of 360 ? imagery are opportunities for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative analysis A.1. Analysis of hyper-parameters</head><p>As the spatial correspondence problem indicated in <ref type="bibr" target="#b11">[12]</ref>, if the deformable convolution is applied to the lower or middle layers, the spatial structures are susceptible to fluctuation <ref type="bibr" target="#b13">[14]</ref>. To overcome this problem, we propose the regional restriction of learned offsets to stabilize the training of our early-stage and four-stage Deformable Patch Embedding (DPE) module. <ref type="table" target="#tab_9">Table 5</ref> shows that r=4 has a better result. Thus, the constraint r applied in the offset prediction module is set as 4 in our experiments.</p><p>To investigate the effect of various hyper-parameters in the proposed Trans4PASS framework, we analyze the weight ? and the temperature T as shown in <ref type="figure" target="#fig_8">Fig. 7a</ref> and <ref type="figure" target="#fig_8">Fig. 7b</ref>. The weight ? is used to combine the Mutual Prototypical Adaptation (MPA) loss and the source-and target segmentation losses. As ? decreases from 0.1 to 0, we set the temperature T =35 in the MPA loss and evaluate the mIoU(%) results on the target (DensePASS <ref type="bibr" target="#b44">[45]</ref>) dataset. If ?=0, the final loss is equivalent to that of the SSL-based method, i.e., the MPA loss is excluded. When ?=0.001 for combining both, MPA and SSL, Trans4PASS obtains a better performance.</p><p>Apart from the combination weight ?, we further investigate the effect of the temperature T , which is used in the MPA loss. As shown in <ref type="figure" target="#fig_8">Fig. 7b</ref>, the performance is not sensitive to the distillation temperature, which illustrates the robustness of our MPA method. Nevertheless, we found that MPA performs better when the temperature is lower, so T =20 is set as the default setting in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Computational complexity</head><p>We reported the complexity of Deformable Patch Embedding (DPE) and Deformable MLP (DMLP) and compared with other methods on DensePASS in <ref type="table" target="#tab_11">Table 6</ref>. The results indicate that our methods have significant improvement with the same order of complexity. <ref type="table" target="#tab_14">Table 7</ref> shows the per-class IoU results on DensePASS dataset. The first group of experiments is conducted to compare the performance of different backbones in P2PDA <ref type="bibr" target="#b87">[88]</ref> method. Additionally, the adaptation process of the original FANet <ref type="bibr" target="#b23">[24]</ref> and DANet <ref type="bibr" target="#b16">[17]</ref> are shown in more detail, i.e., the performance of the source-only model and that without using the SSL-based method are included. The experiments in the second and third groups are based on Trans4PASS-T and -S model, respectively. As shown in the third group, Trans4PASS-S obtains new state-off-the-art performance in mean IoU (56.38%). In addition, it achieves top scores on 7 out of 19 classes in per-class IoU, including pole, traffic light, person, car, truck, motorcycle, and bicycle.    GFLOPs are calculated @512?512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Detailed results in outdoor scenarios</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Detailed results in indoor scenarios</head><p>Apart from the detailed results in outdoor scenarios, per-class results on the outdoor Stanford2D3D-Panoramic dataset <ref type="bibr" target="#b0">[1]</ref> are shown in <ref type="table" target="#tab_15">Table 8</ref>. The experiments are conducted on the fold-1 dataset setting of Stanford2D3D <ref type="bibr" target="#b0">[1]</ref>. Our proposed framework with the Trans4PASS-S backbone and the MPA method obtains the best performance in the domain adaptation setting, reaching 52.15% in mean IoU. It also achieves best IoU scores on 7 out of 13 classes in the indoor scenario, especially on the ceiling, column, and door categories. In the supervised learning setting, Trans4PASS-S surpasses the CNN-based DANet by a large margin, achieving a score of 53.31% in mean IoU. Besides, its performance in per-class IoU is better than DANet in almost all categories, which lacks the capacity to learn longrange contexts and distortion-aware features in panoramas.</p><p>The comparison of segmentation performance with stateof-the-art methods on Stanford2D3D-Panoramic dataset is shown in <ref type="table">Table 9</ref>. Since the results of these experiments are based on the average of all 3 data-splitting settings, we show the results of each individual split setting and its per-class IoU in detail (in gray). The small version of the Trans4PASS backbone is used in this experiment. Compared with the previous best fully-supervised method equipped with ResNet-101, Trans4PASS-S has much fewer parameters and is an order of magnitude smaller than ResNet-101. Still, our method obtains the new state-of-the-art performance on Stanford2D3D-Panoramic dataset, reaching 53.0% in mean IoU. Within all 13 classes, Trans4PASS obtains a total of 8 best per-class IoUs. In the setting of unsupervised domain adaptation (UDA), our proposed method achieves +2.7% in the average of three folds, and +3.1% when using multi-scale evaluation. It obtains best per-class scores on 9 out of 13 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative analysis B.1. More visualizations in indoor scenarios</head><p>Similar to the visualization in outdoor scenarios, more qualitative comparisons between the baseline and the proposed Trans4PASS are displayed in <ref type="figure" target="#fig_9">Fig. 8</ref>, which are from the evaluation set of Stanford2D3D-Panoramic <ref type="bibr" target="#b0">[1]</ref> in the fold-1 setting. In <ref type="figure" target="#fig_9">Fig. 8(a)</ref>, Trans4PASS can produce higher quality segmentation results in those categories highlighted by the black dashed rectangles, such as column and bookcase categories, while the baseline model can hardly identify these severely deformed objects. In <ref type="figure" target="#fig_9">Fig. 8(b)</ref>, the doors are incorrectly segmented as part of the wall by the baseline model, and the correct segmentation results can be generated by our Trans4PASS model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. More visualizations in outdoor scenarios</head><p>To fully demonstrate the effect of Trans4PASS in dealing with image distortions and object deformations, more qualitative comparisons between the baseline and the proposed Trans4PASS are displayed in <ref type="figure" target="#fig_10">Fig. 9</ref>, which are generated from the evaluation set of DensePASS dataset <ref type="bibr" target="#b44">[45]</ref>. Specifically, Trans4PASS can better classify and segment deformed foreground objects with accurate boundaries, such as the segmentation results of cars and trucks highlighted by the blue dashed rectangles in <ref type="figure" target="#fig_10">Fig. 9(a)</ref>, while the baseline model without deformable PE and deformable MLP modules is likely to be confused or fail in these categories. Apart from the foreground object, the ultra-wide arranged background is particularly distorted and challenging. Thanks to the two distortion-aware modules, our Trans4PASS yields high-quality segmentation results in these categories, e.g., terrain, sidewalk, and wall in <ref type="figure" target="#fig_10">Fig. 9(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Broader Impact.</head><p>This work promotes panoramic semantic segmentation of indoor and outdoor scenes, which benefits ultra-wide scene understanding. However, the proposed method has not been verified in practical applications such as those in intelligent vehicles and mobility assistive systems. As the experiments are conducted based on the referred datasets, there are still data biases in different test fields. If the learned model is directly applied to real scenarios, it may cause negative social impacts such as less reliable decision with less accurate segmentation, which should be considered in the downstream applications.    <ref type="table">Table 9</ref>. Comparison on Stanford2D3D-Panoramic dataset. 'F-i' is the result of the fold-i (in gray) setting of Stanford2D3D <ref type="bibr" target="#b0">[1]</ref>. 'Avg'</p><p>is the averaged result of all 3 folds. 'MS' is multi-scale evaluation. 'UDA' is short for unsupervised domain adaptation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Semantic segmentation of (a) narrow-angle pinhole image and (b) 360 ? panoramic image. Compared to (c) standard Patch Embeddings, our (d) Deformable Patch Embedding partitions 360 ? images while considering distortions, e.g. in sidewalks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Transformer with FPN-like decoder (b) Transformer with vanilla-MLP (c) Trans4PASS with DPE and DMLP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of MLP blocks. The spatial offsets of DMLP are learned adaptively from the input feature map. then flattened as ? DM LP (k,c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Diagram of mutual prototypical adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>displayed in Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Omnidiretional segmentation. To showcase the effectiveness of MPA on omnidiretional segmentation, the panoramic image is divided into 8 directions and evaluated individually. The polar diagram inFig. 5demonstrates that MPA brings uniform improvement to omnidirectional seg-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of omnidirectional segmentation before and after mutual prototypical adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparisons, DPE and DMLP visualizations. (a) and (c) show segmentation comparisons, where the baseline has neither DPE/DMLP nor MPA. The ? dots in (b) and (d) are sampling points shifted by learned offsets w.r.t. the ? patch center of DPE (from decoder). (e) and (f) show the #75 channel maps of stage-3 before and after DMLP. Zoom in for better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Analysis of hyper-parameters. The performance (mIoU) is evaluated in the outdoor target dataset (DensePASS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative comparisons in indoor scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative comparisons in outdoor scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>SPin for short) has 70, 496 pinhole images. The dataset is collected in indoor areas and annotated with 13 categories. Results are averaged over 3 official folds, unless otherwise stated. Indoor pan(oramic) dataset. Stanford2D3D<ref type="bibr" target="#b0">[1]</ref> (SPan for short) has 1, 413 panoramic images. The images are annotated with the same 13 categories as its pinhole dataset. Outdoor pin(hole) dataset. Cityscapes<ref type="bibr" target="#b10">[11]</ref> (CS for short) dataset comprises 2, 979 and 500 images for training and validation. Images are annotated with 19 categories. Outdoor pan(oramic) dataset. DensePASS<ref type="bibr" target="#b44">[45]</ref> (DP for short) collected from cities around the world has 2, 000 images for transfer optimization and 100 labeled images for testing, annotated with the same 19 classes as Cityscapes. Implementation settings. We train Trans4PASS models with 4 1080Ti GPUs with an initial learning rate of 5e?5, scheduled by the poly strategy with power 0.9 over 200 epochs. AdamW<ref type="bibr" target="#b30">[31]</ref> is the optimizer with epsilon 1e?8, weight decay 1e?4 and batch size is 4 on each GPU. The image augmentations include random resize with ratio 0.5-2.0, random horizontal flipping, and random cropping to 512?512. For outdoor datasets, the resolution is 1080?1080 and batch size is 1. When adapting the models from PIN2PAN, the resolution of indoor pinhole and panoramic images are 1080?1080 and 1024?512 for training, while the outdoor images are set to 1024?512 and 2048?400. The image size of indoor and outdoor validation</figDesc><table><row><cell>)</cell></row></table><note>Indoor pin(hole) dataset. Stanford2D3D [1] (. Performance gaps of CNN-and transformer-based models from Cityscapes (CS) @ 1024?512 to DensePASS (DP).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance The proposed Trans4PASS architecture has a high performance on pinhole image segmentation and also outperforms other methods on panoramic segmentation with 44.8% mIoU without any adaptation strategy. It indicates that distortion-aware features and long-range cues maintained in both low and high levels of Transformers as opposed to the context learned in higher-levels of CNNs, are important for wide-FoV panoramic segmentation.Trans4PASS MiT-B1* DMLP 13.11 13.10 69.48 36.50 Trans4PASS MiT-B1 ? CycleMLP [5] 9.83 13.60 73.49 40.16 Trans4PASS MiT-B1 ? ASMLP [36] 13.40 14.19 73.65 42.05 Trans4PASS MiT-B1 ? DMLP 12.02 13.93 72.49 45.89 (+9.39)</figDesc><table><row><cell>Network</cell><cell cols="2">Encoder Decoder</cell><cell>GFLOPs</cell><cell>#P</cell><cell>CS DP</cell></row><row><cell cols="3">(1) Compare PEs and MLPs:</cell><cell></cell><cell></cell></row><row><cell cols="3">(2) Compare encoders and decoders:</cell><cell></cell><cell></cell></row><row><cell>PVT [65]</cell><cell>PVT-T</cell><cell>FPN</cell><cell cols="3">11.17 12.76 71.46 31.20</cell></row><row><cell>PVT [65]</cell><cell>PVT-T</cell><cell cols="4">Vanilla MLP 14.56 12.84 70.60 32.85</cell></row><row><cell>PVT [65]</cell><cell>PVT-T</cell><cell>DMLP</cell><cell cols="3">13.11 13.10 71.75 35.18 (+3.98)</cell></row><row><cell cols="3">Trans4PASS PVT-T ? DMLP</cell><cell cols="3">13.18 13.10 69.62 36.50 (+5.30)</cell></row><row><cell cols="6">SegFormer [68]MiT-B1 Vanilla MLP 13.27 13.66 74.93 39.02</cell></row><row><cell cols="3">SegFormer [68]MiT-B1 FPN</cell><cell cols="3">9.88 13.58 73.96 41.14</cell></row><row><cell cols="3">SegFormer [68]MiT-B1 DMLP</cell><cell cols="3">11.82 13.92 73.10 45.14 (+6.12)</cell></row><row><cell cols="3">Trans4PASS MiT-B1 ? DMLP</cell><cell cols="3">12.02 13.93 72.49 45.89 (+6.87)</cell></row></table><note>gaps from Stanford2D3D-Pinhole (SPin) to Stanford2D3D-Panoramic (SPan) dataset on fold-1.sets are 2048?1024 and 2048?400, respectively. Adapta- tion models are trained within 10K iterations on one GPU.4.2.PIN2PAN Gaps Domain gap in outdoor scenarios. To quantify the PIN2PAN domain gap in outdoor scenarios, we evalu- ate over 15 off-the-shelf segmentation models trained on Cityscapes.1 Table 1 summarizes the results tested on Cityscapes and DensePASS validation sets. Although previ- ous transformers [68, 94] reduce the mIoU gap from ?50% of CNN-based counterparts to ?40%, the PIN2PAN gap re- mains large.Domain gap in indoor scenarios. Table 2 shows PIN2PAN domain gaps in indoor scenarios. As pinhole and panoramic images from Stanford2D3D are captured under the same1 MMSegmentation: https://github.com/open-mmlab/mmsegmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Trans4PASS structural analysis. * and ? denote DPT [7] and our DPE. "#P" is short for #Parameters in millions.</figDesc><table /><note>Models are trained on Cityscapes (CS) @ 512?512 and tested on DensePASS (DP) @ 2048?400.setting, the PIN2PAN gap is smaller compared to the out- door scenario. Still, in light of other CNN-and transformer- based methods, the small Trans4PASS version achieves 50.20% and 48.34% mIoU in pinhole-and panoramic im- age segmentation, yielding the smallest performance drop.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>When our DPE is applied in the early stage of the PVT encoder, further improvements of +5.30% can be made. Similar improvement results (+6.12% and +6.87%) are evident in exper-63.59 18.22 47.01 9.45 12.79 17.00 8.12 6.41 34.24 10.15 18.43 4.96 2.31 46.03 3.19 0.59 0.00 8.30 5.55 PASS (ERFNet) [72] 23.66 67.84 28.75 59.69 19.96 29.41 8.26 4.54 8.07 64.96 13.75 33.50 12.87 3.17 48.26 2.17 0.82 0.29 23.76 19.46 ECANet (Omni-supervised) [75] 43.02 81.60 19.46 81.00 32.02 39.47 25.54 3.85 17.38 79.01 39.75 94.60 46.39 12.98 81.96 49.25 28.29 0.00 55.36 29.47 CLAN (Adversarial) [44] 31.46 65.39 21.14 69.10 17.29 25.49 11.17 3.14 7.61 71.03 28.19 55.55 18.86 2.76 71.60 26.42 17.99 59.53 9.44 15.91 CRST (Self-training) [97] 31.67 68.18 15.72 76.78 14.06 26.11 9.90 0.82 2.66 69.36 21.95 80.06 9.71 1.25 65.12 38.76 27.22 48.85 7.10 18.08 P2PDA (Adversarial) [88] 41.99 70.21 30.24 78.44 26.72 28.44 14.02 11.67 5.79 68.54 38.20 85.97 28.14 0.00 70.36 60.49 38.90 77.80 39.85 24.02 SIM (Self-training) [67] 44.58 68.16 32.59 80.58 25.68 31.38 23.60 19.39 14.09 72.65 26.41 87.88 41.74 16.09 73.56 47.08 42.81 56.35 47.72 39.30 PCS (Self-training) [84] 53.83 78.10 46.24 86.24 30.33 45.78 34.04 22.74 13.00 79.98 33.07 93.44 47.69 22.53 79.20 61.59 67.09 83.26 58.68 39.80 USSS (IDD) [30] 26.98 68.85 5.41 67.39 15.10 21.79 13.18 0.12 7.73 70.27 8.84 85.53 22.05 1.71 58.69 16.41 12.01 0.00 23.58 13.90 USSS (Mapillary) [30] 30.87 71.01 31.85 76.79 12.13 23.61 11.93 3.23 10.15 73.11 31.24 89.59 16.05 3.86 65.27 24.46 18.72 0.00 9.08 14.48 Seamless (Mapillary) [50] 34.14 59.26 24.48 77.35 12.82 30.91 12.63 15.89 17.73 75.61 33.30 87.30 19.69 4.59 63.94 25.81 57.16 0.00 11.59 19.04 SwiftNet (Cityscapes) [48] 25.67 50.73 32.76 70.24 12.63 24.02 18.79 7.18 4.01 64.93 23.70 84.29 14.91 0.97 43.46 8.92 0.04 4.45 12.77 8.77 SwiftNet (Merge3) [90] 32.04 68.31 38.59 81.48 15.65 23.91 20.74 5.95 0.00 70.64 25.09 90.93 32.66 0.00 66.91 42.30 5.97 0.07 6.85 12.66 Trans4PASS-S (ours) 55.25 78.39 41.62 86.47 31.56 45.47 34.02 22.98 18.33 79.63 41.35 93.80 49.02 22.99 81.05 67.43 69.64 86.04 60.85 39.20 Trans4PASS-S (ours)* 56.38 79.91 42.68 86.26 30.68 42.32 36.61 24.81 19.64 78.80 44.73 93.84 50.71 24.39 81.72 68.86 66.18 88.62 63.87 46.62 Adaptation results on SPan @ fold-1.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell></row><row><cell cols="21">ERFNet [52] 16.65 (a) Per-class results on DensePASS. Comparison with state-of-the-art panoramic segmentation [72, 75], domain adaptation [44, 67, 84,</cell></row><row><cell cols="15">88, 97], and multi-supervision methods [30, 50, 90]. * denotes performing multi-scale (MS) evaluation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Network FANet DANet Trans4PASS-T Trans4PASS-S Trans4PASS-T Trans4PASS-T Trans4PASS-T Trans4PASS-T Trans4PASS-T Trans4PASS-T MPA + SSL + MS Method P2PDA P2PDA P2PDA P2PDA -Warm-up SSL MPA MPA + SSL Trans4PASS-S -Trans4PASS-S Warm-up Trans4PASS-S SSL Trans4PASS-S MPA Trans4PASS-S MPA + SSL Trans4PASS-S MPA + SSL + MS (b) Adaptation results on DensePASS. mIoU(%) 35.67 41.99 51.05 52.91 45.89 50.56 51.86 51.93 53.26 54.72 48.73 52.59 54.67 54.77 55.25 56.38</cell><cell></cell><cell cols="15">Network DANet DANet PVT-Tiny PVT-Tiny PVT-Small PVT-Small Trans4PASS-T Trans4PASS-T Trans4PASS-S Trans4PASS-S DANet Trans4PASS-S (c) Method Method mIoU(%) -40.28 P2PDA 42.26 -24.45 P2PDA 39.66 -23.11 P2PDA 43.10 -46.08 MPA 47.48 -48.34 MPA 52.15 Supervised 44.15 Supervised 53.31 Supervised StdConv [59] CubeMap [59] DistConv [59] UNet [53] GaugeNet [10] UGSCNN [28] HexRUNet [86] Tangent [16] (ResNet-101) HoHoNet [58] (ResNet-101) RGB Input mIoU(%) RGB 32.6 RGB 33.8 RGB 34.6 RGB-D 35.9 RGB-D 39.4 RGB-D 38.3 RGB-D 43.3 RGB 45.6 52.0 Trans4PASS (Small) RGB 52.1 Trans4PASS (Small+MS) RGB 53.0 Trans4PASS (Source only) RGB 48.1 Trans4PASS (MPA) RGB 50.8 UDA Trans4PASS (MPA+MS) RGB 51.2</cell><cell></cell></row></table><note>iments with a SegFormer encoder. Overall, these results show that DPE and DMLP can be integrated into diverse backbones, significantly improving distortion-adaptability for panoramic scene segmentation.(d) Comparison on SPan averaged by 3 folds.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparisons and ablation studies of PIN2PAN domain adaptation in indoor and outdoor scenarios.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4b .</head><label>4b</label><figDesc>Trans4PASS brings &gt;10% performance gains due to the captured longrange contexts and distortion-aware features. Without the advantage of a superior network architecture, MPA achieves 51.93% and 54.77% with Trans4PASS-T and -S models, surpassing 51.05% and 52.91% of P2PDA. The second and third ablation groups ofTable 4bshow how Trans4PASS-T and -S match up against each other. Individually, MPA is on par with the SSL-based method. When combining both, MPA and SSL, Trans4Pass-S obtains new state-of-theart performance on DensePASS, reaching 55.25% in mIoU and 56.38% with multi-scale evaluation. This verifies that MPA works collaboratively with pseudo labels and provides a complementary feature alignment incentive.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>mentation. Apart from benefiting the stuff classes (road, sidewalk, and terrain), MPA improves the segmentation of object classes, such as person and truck. Due to the panorama boundary at 180 ? , IoUs of motorcycle and bicycle are impacted, still consistent and large accuracy boosts with MPA in all directions for different classes are observed.</figDesc><table><row><cell>RGB</cell><cell>Stage-1 DPE</cell><cell>RGB</cell><cell>Stage-1 DPE</cell></row><row><cell>Baseline</cell><cell>Stage-2 DPE</cell><cell>Baseline</cell><cell>Stage-2 DPE</cell></row><row><cell>Trans4PASS</cell><cell>Stage-3 DPE</cell><cell>Trans4PASS</cell><cell>Stage-3 DPE</cell></row><row><cell>GT</cell><cell>Stage-4 DPE</cell><cell>GT</cell><cell>Stage-4 DPE</cell></row><row><cell>(a) Segmentation outdoors</cell><cell>(b) DPE outdoors</cell><cell>(c) Segmentation indoors</cell><cell>(d) DPE indoors</cell></row><row><cell>Before DMLP</cell><cell>After DMLP</cell><cell>Before DMLP</cell><cell>After DMLP</cell></row><row><cell cols="2">(e) DMLP outdoors</cell><cell cols="2">(f) DMLP indoors</cell></row></table><note>Comparison with outdoor state-of-the-art methods. In Table 4a, we compare our solution with recent panoramic segmentation [72, 75] and domain adaptation [44, 67, 84,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>Effect of regional restriction (r) on DensePASS.</figDesc><table><row><cell></cell><cell></cell><cell>Sheet2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>None</cell><cell>r=1</cell><cell>r=2</cell><cell>r=4</cell><cell>r=8</cell></row><row><cell>mIoU(%)</cell><cell cols="3">45.74 44.51 45.59</cell><cell>45.89</cell><cell>45.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .</head><label>6</label><figDesc>Computational complexity of DPE and DMLP.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>62.98 10.64 72.41 7.80 20.74 11.77 6.85 3.75 68.11 21.56 87.00 23.73 5.33 49.61 10.65 0.54 16.76 24.15 6.62 FANet P2PDA 33.52 57.16 25.66 78.43 16.02 26.88 12.76 2.30 7.34 68.73 26.92 87.45 36.51 1.20 62.83 20.16 0.00 68.46 17.86 20.19 FANet P2PDA + SSL 35.67 58.08 28.75 78.19 16.47 26.86 13.78 4.76 7.62 69.01 34.58 87.51 36.12 0.90 64.06 27.50 0.00 84.99 18.13 20.35 DANet -28.50 70.68 8.30 75.80 9.49 21.64 15.91 5.85 9.26 71.08 31.50 85.13 6.55 1.68 55.48 24.91 30.22 0.52 0.53 17.00 DANet P2PDA 40.52 62.90 25.58 76.62 24.45 30.37 14.45 16.75 9.96 67.87 19.70 82.04 34.18 22.95 56.99 54.27 44.15 47.75 46.98 31.86 DANet P2PDA + SSL 41.99 70.21 30.24 78.44 26.72 28.44 14.02 11.67 5.79 68.54 38.20 85.97 28.14 0.00 70.36 60.49 38.90 77.80 39.85 24.02 Trans4PASS-T -45.89 72.42 32.53 84.43 20.13 35.20 24.45 15.37 12.59 78.85 31.65 90.87 42.42 14.12 74.07 39.66 35.45 90.32 50.31 26.95 Trans4PASS-T P2PDA 51.05 74.82 36.53 85.93 30.23 34.83 33.70 20.36 20.40 77.43 34.87 93.65 46.01 20.89 76.85 58.19 51.20 82.19 56.84 35.09 Trans4PASS-S -48.73 70.28 25.52 84.98 29.10 39.00 29.05 17.77 13.21 78.26 29.89 91.00 42.16 13.43 78.26 47.25 63.82 78.06 60.31 34.38 Trans4PASS-S P2PDA 52.91 76.29 41.02 86.86 31.96 42.15 35.15 20.98 19.49 79.44 29.26 93.64 49.62 17.47 78.77 62.80 66.38 77.98 59.23 36.73 Trans4PASS-T -45.89 72.42 32.53 84.43 20.13 35.20 24.45 15.37 12.59 78.85 31.65 90.87 42.42 14.12 74.07 39.66 35.45 90.32 50.31 26.95 Trans4PASS-T Warm-up 50.56 76.54 38.94 84.99 27.1 33.61 30.75 18.75 16.73 79.15 41.43 92.19 43.1 18.49 78.42 59.0 51.09 79.9 58.88 31.54 Trans4PASS-T SSL 51.86 78.24 41.16 85.82 27.86 36.01 30.92 21.26 17.70 79.11 46.44 93.47 44.72 17.66 79.44 63.69 48.14 81.56 59.09 32.96 Trans4PASS-T MPA 51.93 77.27 45.61 85.66 23.57 37.10 31.22 20.13 15.35 79.91 43.81 93.95 46.37 21.63 79.34 62.09 56.05 78.43 56.31 32.89 Trans4PASS-T MPA + SSL 53.26 78.14 41.24 85.99 30.21 37.28 32.60 21.71 19.05 79.05 45.70 93.87 48.71 18.15 79.63 64.69 54.71 84.57 59.26 37.31 Trans4PASS-T MPA + SSL + MS 54.72 78.42 42.26 85.88 30.97 38.10 33.83 21.57 20.92 78.26 44.90 93.57 48.43 22.53 79.90 66.00 66.32 85.10 60.54 42.09 Trans4PASS-S -48.73 70.28 25.52 84.98 29.10 39.00 29.05 17.77 13.21 78.26 29.89 91.00 42.16 13.43 78.26 47.25 63.82 78.06 60.31 34.38 Trans4PASS-S Warm-up 52.59 75.28 37.08 86.21 31.34 38.84 34.6 20.92 17.13 79.18 34.86 93.81 49.15 24.12 80.01 55.38 62.2 77.8 61.14 40.2 Trans4PASS-S SSL 54.67 79.72 44.34 85.28 28.88 43.46 34.08 22.63 17.21 78.93 43.98 92.84 49.58 26.28 81.04 65.92 67.37 76.96 59.90 40.25 Trans4PASS-S MPA 54.77 80.55 51.12 87.12 25.87 45.55 34.64 23.44 14.45 79.60 31.77 93.98 49.55 22.98 78.97 66.73 66.28 88.65 61.09 38.25 Trans4PASS-S MPA + SSL 55.25 78.39 41.62 86.47 31.56 45.47 34.02 22.98 18.33 79.63 41.35 93.80 49.02 22.99 81.05 67.43 69.64 86.04 60.85 39.20 Trans4PASS-S MPA + SSL + MS 56.38 79.91 42.68 86.26 30.68 42.32 36.61 24.81 19.64 78.80 44.73 93.84 50.71 24.39 81.72 68.86 66.18 88.62 63.87 46.62</figDesc><table><row><cell>Network</cell><cell>Method</cell><cell>mIoU</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bicycle</cell></row><row><cell>FANet</cell><cell>-</cell><cell>26.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 .</head><label>7</label><figDesc>Per-class results on DensePASS dataset. 'SSL' represents the self-supervised learning with pseudo-labels. '-' means no adaptation. 'MS' denotes multi-scale evaluation.</figDesc><table><row><cell>Network</cell><cell>Method</cell><cell>mIoU</cell><cell>beam</cell><cell>board</cell><cell>bookcase</cell><cell>ceiling</cell><cell>chair</cell><cell>clutter</cell><cell>column</cell><cell>door</cell><cell>floor</cell><cell>sofa</cell><cell>table</cell><cell>wall</cell><cell>window</cell></row><row><cell>DANet</cell><cell>-</cell><cell>40.28</cell><cell>0.00</cell><cell>56.07</cell><cell>52.09</cell><cell>72.05</cell><cell>35.72</cell><cell>20.54</cell><cell>5.81</cell><cell>19.43</cell><cell>72.84</cell><cell>31.76</cell><cell>41.80</cell><cell>68.43</cell><cell>47.13</cell></row><row><cell>DANet</cell><cell>P2PDA</cell><cell>42.26</cell><cell>0.22</cell><cell>57.49</cell><cell>50.92</cell><cell>73.09</cell><cell>44.63</cell><cell>21.72</cell><cell>9.09</cell><cell>24.02</cell><cell>83.18</cell><cell>30.94</cell><cell>41.36</cell><cell>65.43</cell><cell>47.24</cell></row><row><cell>PVT-Tiny</cell><cell>-</cell><cell>24.45</cell><cell>0.06</cell><cell>28.05</cell><cell>32.99</cell><cell>58.97</cell><cell>13.68</cell><cell>12.97</cell><cell>3.03</cell><cell>2.46</cell><cell>76.56</cell><cell>0.00</cell><cell>28.65</cell><cell>51.20</cell><cell>9.23</cell></row><row><cell>PVT-Tiny</cell><cell>P2PDA</cell><cell>39.66</cell><cell>0.38</cell><cell>60.55</cell><cell>54.08</cell><cell>75.14</cell><cell>33.99</cell><cell>26.20</cell><cell>7.23</cell><cell>12.66</cell><cell>82.58</cell><cell>9.14</cell><cell>42.74</cell><cell>65.75</cell><cell>45.12</cell></row><row><cell>PVT-Small</cell><cell>-</cell><cell>23.11</cell><cell>0.42</cell><cell>29.82</cell><cell>26.20</cell><cell>58.65</cell><cell>5.89</cell><cell>12.62</cell><cell>3.57</cell><cell>1.80</cell><cell>77.11</cell><cell>0.00</cell><cell>28.49</cell><cell>48.24</cell><cell>7.58</cell></row><row><cell>PVT-Small</cell><cell>P2PDA</cell><cell>43.10</cell><cell>0.00</cell><cell>66.24</cell><cell>55.31</cell><cell>76.92</cell><cell>40.95</cell><cell>28.99</cell><cell>5.60</cell><cell>13.62</cell><cell>88.35</cell><cell>14.53</cell><cell>52.08</cell><cell>68.26</cell><cell>49.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 .</head><label>8</label><figDesc>Per-class results on Stanford2D3D-Panoramic dataset according to the fold-1 data setting<ref type="bibr" target="#b0">[1]</ref>.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Input</cell><cell>mIoU</cell><cell>beam</cell><cell>board</cell><cell>bookcase</cell><cell>ceiling</cell><cell>chair</cell><cell>clutter</cell><cell>column</cell><cell>door</cell><cell>floor</cell><cell>sofa</cell><cell>table</cell><cell>wall</cell><cell>window</cell></row><row><cell></cell><cell>StdConv [59]</cell><cell>RGB</cell><cell>32.6</cell><cell>0</cell><cell>46.6</cell><cell>44.9</cell><cell>60.8</cell><cell>32.4</cell><cell>18.8</cell><cell>0</cell><cell>13.0</cell><cell>78.0</cell><cell>0</cell><cell>32.6</cell><cell>54.8</cell><cell>40.1</cell></row><row><cell></cell><cell>CubeMap [59]</cell><cell>RGB</cell><cell>33.8</cell><cell>0.2</cell><cell>48.3</cell><cell>48.5</cell><cell>61.3</cell><cell>33.4</cell><cell>23.4</cell><cell>0</cell><cell>15.4</cell><cell>72.7</cell><cell>0</cell><cell>33.8</cell><cell>61.7</cell><cell>36.9</cell></row><row><cell></cell><cell>DistConv [59]</cell><cell>RGB</cell><cell>34.6</cell><cell>0.3</cell><cell>50.8</cell><cell>47.1</cell><cell>61.5</cell><cell>35.4</cell><cell>19.5</cell><cell>0</cell><cell>13.8</cell><cell>83.4</cell><cell>0</cell><cell>34.5</cell><cell>57.1</cell><cell>42.6</cell></row><row><cell></cell><cell>UNet [53]</cell><cell>RGB-D</cell><cell>35.9</cell><cell>8.5</cell><cell>27.2</cell><cell>30.7</cell><cell>78.6</cell><cell>35.3</cell><cell>28.8</cell><cell>4.9</cell><cell>33.8</cell><cell>89.1</cell><cell>8.2</cell><cell>38.5</cell><cell>58.8</cell><cell>23.9</cell></row><row><cell></cell><cell>GaugeNet [10]</cell><cell>RGB-D</cell><cell>39.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Supervised</cell><cell>UGSCNN [28] HexRUNet [86] Tangent (ResNet-101) [16] HoHoNet (ResNet-101) [58] Trans4PASS (F-1) Trans4PASS (F-2)</cell><cell>RGB-D RGB-D RGB RGB RGB RGB</cell><cell>38.3 43.3 45.6 52.0 53.3 45.7</cell><cell>8.7 10.9 --0.4 12.5</cell><cell>32.7 39.7 --69.5 46.9</cell><cell>33.4 37.2 --62.2 32.6</cell><cell>82.2 84.8 --82.8 82.3</cell><cell>42.0 50.5 --58.5 64.7</cell><cell>25.6 29.2 --34.3 37.5</cell><cell>10.1 11.5 --21.9 20.1</cell><cell>41.6 45.3 --44.9 42.7</cell><cell>87.0 92.9 --91.2 86.6</cell><cell>7.6 19.1 --40.8 17.7</cell><cell>41.7 49.1 --57.7 45.2</cell><cell>61.7 63.8 --74.8 70.3</cell><cell>23.5 29.4 --54.2 35.1</cell></row><row><cell></cell><cell>Trans4PASS (F-3)</cell><cell>RGB</cell><cell>57.2</cell><cell>21.4</cell><cell>65.4</cell><cell>58.3</cell><cell>80.2</cell><cell>55.8</cell><cell>41.9</cell><cell>28.6</cell><cell>76.3</cell><cell>88.6</cell><cell>45.4</cell><cell>58.8</cell><cell>59.3</cell><cell>63.6</cell></row><row><cell></cell><cell>Trans4PASS (Avg)</cell><cell>RGB</cell><cell>52.1</cell><cell>11.4</cell><cell>60.6</cell><cell>51.1</cell><cell>81.8</cell><cell>59.7</cell><cell>37.9</cell><cell>23.5</cell><cell>54.6</cell><cell>88.8</cell><cell>34.6</cell><cell>53.9</cell><cell>68.1</cell><cell>51.0</cell></row><row><cell></cell><cell>Trans4PASS (F-1, MS)</cell><cell>RGB</cell><cell>54.2</cell><cell>0.7</cell><cell>72.1</cell><cell>64.1</cell><cell>83.4</cell><cell>61.3</cell><cell>35.5</cell><cell>22.4</cell><cell>42.2</cell><cell>92.0</cell><cell>41.6</cell><cell>59.4</cell><cell>75.3</cell><cell>54.4</cell></row><row><cell></cell><cell>Trans4PASS (F-2, MS)</cell><cell>RGB</cell><cell>46.4</cell><cell>3.1</cell><cell>48.2</cell><cell>32.1</cell><cell>82.9</cell><cell>66.4</cell><cell>37.8</cell><cell>20.3</cell><cell>42.7</cell><cell>87.2</cell><cell>16.8</cell><cell>45.9</cell><cell>71.3</cell><cell>38.0</cell></row><row><cell></cell><cell>Trans4PASS (F-3, MS)</cell><cell>RGB</cell><cell>58.4</cell><cell>1.7</cell><cell>67.1</cell><cell>60.1</cell><cell>81.3</cell><cell>56.8</cell><cell>42.6</cell><cell>29.8</cell><cell>77.6</cell><cell>89.5</cell><cell>45.3</cell><cell>59.9</cell><cell>60.1</cell><cell>67.3</cell></row><row><cell></cell><cell>Trans4PASS (Avg, MS)</cell><cell>RGB</cell><cell>53.0</cell><cell>1.8</cell><cell>62.5</cell><cell>52.1</cell><cell>82.6</cell><cell>61.5</cell><cell>38.6</cell><cell>24.2</cell><cell>54.2</cell><cell>89.5</cell><cell>34.5</cell><cell>55.1</cell><cell>68.9</cell><cell>53.2</cell></row><row><cell></cell><cell>Trans4PASS (F-1)</cell><cell>RGB</cell><cell>48.6</cell><cell>0.1</cell><cell>65.8</cell><cell>58.3</cell><cell>80.5</cell><cell>54.2</cell><cell>29.1</cell><cell>17.4</cell><cell>23.7</cell><cell>89.0</cell><cell>34.3</cell><cell>54.9</cell><cell>73.2</cell><cell>51.6</cell></row><row><cell></cell><cell>Trans4PASS (F-2)</cell><cell>RGB</cell><cell>40.6</cell><cell>10.2</cell><cell>38.3</cell><cell>28.9</cell><cell>77.8</cell><cell>54.6</cell><cell>32.5</cell><cell>15.7</cell><cell>32.9</cell><cell>83.2</cell><cell>13.7</cell><cell>38.0</cell><cell>67.9</cell><cell>33.6</cell></row><row><cell></cell><cell>Trans4PASS (F-3)</cell><cell>RGB</cell><cell>55.2</cell><cell>17.4</cell><cell>64.7</cell><cell>60.2</cell><cell>76.4</cell><cell>58.3</cell><cell>41.4</cell><cell>5.0</cell><cell>76.6</cell><cell>84.5</cell><cell>47.2</cell><cell>57.3</cell><cell>63.8</cell><cell>64.5</cell></row><row><cell></cell><cell>Trans4PASS (Avg)</cell><cell>RGB</cell><cell>48.1</cell><cell>9.2</cell><cell>56.3</cell><cell>49.1</cell><cell>78.2</cell><cell>55.7</cell><cell>34.3</cell><cell>12.7</cell><cell>44.4</cell><cell>85.6</cell><cell>31.8</cell><cell>50.1</cell><cell>68.3</cell><cell>49.9</cell></row><row><cell>UDA</cell><cell>Trans4PASS (F-1, MPA) Trans4PASS (F-2, MPA) Trans4PASS (F-3, MPA)</cell><cell>RGB RGB RGB</cell><cell>52.2 41.8 58.5</cell><cell>1.0 11.0 24.5</cell><cell>68.0 35.1 70.4</cell><cell>61.4 30.9 59.0</cell><cell>82.2 78.6 81.3</cell><cell>58.7 59.3 58.5</cell><cell>35.2 32.7 43.3</cell><cell>17.4 14.3 4.6</cell><cell>36.4 45.6 76.1</cell><cell>90.3 80.1 89.6</cell><cell>46.2 22.9 53.3</cell><cell>56.8 37.0 62.0</cell><cell>73.5 66.2 65.7</cell><cell>50.9 29.6 72.0</cell></row><row><cell></cell><cell>Trans4PASS (Avg, MPA)</cell><cell>RGB</cell><cell>50.8</cell><cell>12.2</cell><cell>57.8</cell><cell>50.4</cell><cell>80.7</cell><cell>58.8</cell><cell>37.1</cell><cell>12.1</cell><cell>52.7</cell><cell>86.7</cell><cell>40.8</cell><cell>51.9</cell><cell>68.4</cell><cell>50.8</cell></row><row><cell></cell><cell>Trans4PASS (F-1, MPA, MS)</cell><cell>RGB</cell><cell>52.6</cell><cell>0.8</cell><cell>70.7</cell><cell>63.3</cell><cell>82.2</cell><cell>60.8</cell><cell>36.2</cell><cell>16.4</cell><cell>33.4</cell><cell>90.5</cell><cell>45.9</cell><cell>58.4</cell><cell>73.1</cell><cell>51.5</cell></row><row><cell></cell><cell>Trans4PASS (F-2, MPA, MS)</cell><cell>RGB</cell><cell>42.6</cell><cell>11.7</cell><cell>35.5</cell><cell>31.6</cell><cell>79.2</cell><cell>60.8</cell><cell>33.2</cell><cell>15.6</cell><cell>46.5</cell><cell>78.8</cell><cell>24.1</cell><cell>38.0</cell><cell>66.2</cell><cell>32.5</cell></row><row><cell></cell><cell>Trans4PASS (F-3, MPA, MS)</cell><cell>RGB</cell><cell>58.3</cell><cell>22.6</cell><cell>70.6</cell><cell>59.4</cell><cell>81.5</cell><cell>58.8</cell><cell>43.9</cell><cell>4.2</cell><cell>76.7</cell><cell>89.5</cell><cell>52.8</cell><cell>62.0</cell><cell>66.0</cell><cell>70.7</cell></row><row><cell></cell><cell>Trans4PASS (Avg, MPA, MS)</cell><cell>RGB</cell><cell>51.2</cell><cell>11.7</cell><cell>58.9</cell><cell>51.4</cell><cell>81.0</cell><cell>60.1</cell><cell>37.7</cell><cell>12.0</cell><cell>52.2</cell><cell>86.2</cell><cell>40.9</cell><cell>52.8</cell><cell>68.4</cell><cell>51.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui-Po</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsiao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation for semantic segmentation with maximum squares loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CycleMLP: A MLP-like architecture for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoufa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DPT: Deformable patch-based transformer for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual path learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cars can&apos;t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungha</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanne</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gauge equivariant convolutional networks and the icosahedral CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkay</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Eliminating the blind spot: Adapting 3D object detection and monocular depth estimation to 360 ? panoramic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gr?goire Payen De La Garanderie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Amir Atapour Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Restricted deformable convolution-based road scene semantic segmentation using surround view cameras. T-ITS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tangent images for mitigating spherical distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Eder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PIT: Position-invariant transform for cross-FoV domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuequan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What&apos;s in my room? Object recognition on indoor panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Guerrero-Viu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Fernandez-Labrador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Demonceaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">J</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MetaCorrection: Domain-aware meta loss correction for unsupervised domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Strip pooling: Rethinking spatial pooling for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Real-time semantic segmentation with fast attention. RA-L, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contextual-relation consistent domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CCNet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Panoramic panoptic segmentation: Towards complete surrounding understanding via unsupervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Jaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spherical CNNs on unstructured grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Chiyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mining contextual information beyond image for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenchao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Universal semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarun</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SpherePHD: Applying CNNs on a spherical PolyHeDron representation of 360?images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeonkun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeseok</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseob</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjune</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Content-consistent matching for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">AS-MLP: An axial shifted MLP architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08391</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Constructing self-motivated pyramid curriculums for crossdomain semantic segmentation: A non-adversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pay attention to MLPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pano-SfMLearner: Self-Supervised multi-task learning of depth and semantics in panoramic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">BAPA-net: Boundary adaptation and prototype alignment for cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DensePASS: Dense panoramic semantic segmentation via unsupervised domain adaptation with attention-augmented context exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Coarseto-fine domain adaptive semantic segmentation with photometric alignment and category-center regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semantic segmentation of outdoor panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Orhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalin</forename><surname>Bastanlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIVP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">In defense of pre-trained ImageNet architectures for realtime semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Seokju Lee, and In So Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkyu</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Seamless scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Colovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast-SCNN: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized ConvNet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">Miguel</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The OmniScape dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed Rida</forename><surname>Sekkat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Equivariant networks for pixelized spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Uncertainty reduction for model adaptation in semantic segmentation</title>
	</analytic>
	<monogr>
		<title level="m">Prabhu Teja Sivaprasad and Fran?ois Fleuret</title>
		<imprint/>
	</monogr>
	<note>CVPR, 2021</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">HoHoNet: 360 indoor holistic understanding with latent horizontal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Distortion-aware convolutional filters for dense prediction in panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">MLP-mixer: An all-MLP architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">ADVENT: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">SegFormer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NeurIPS, 2021. 3, 4, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Predicting head movement in panoramic video: A deep reinforcement learning approach. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglang</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyu</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zulin</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Semantic segmentation of panoramic images using a synthetic dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyou</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Spherical DNNs and their applications in 360 ? images and videos. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">PASS: Panoramic annular semantic segmentation. T-ITS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">Miguel</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">DS-PASS: Detail-sensitive panoramic annular semantic segmentation through SwaftNet for surrounding sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaite</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Is context-aware CNN ready for the surroundings? Panoramic semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Capturing omni-range context for omnidirectional segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rei?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">FDA: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Universal semantic segmentation for fisheye urban driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaozu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaite</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SMC, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Senthil Kumar Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjaya</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saquib</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padraig</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Derek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Dea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciar?n</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumanth</forename><surname>Sistu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uric?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">HRFormer: Highresolution transformer for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Vision transformer with progressive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno>ICCV, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Prototypical cross-domain self-supervised learning for few-shot unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Sangiovanni</forename><surname>Vincentelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">DeepPanoContext: Panoramic 3D scene understanding with holistic scene context graph and relation-based optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Orientation-aware semantic segmentation on icosahedron spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">ResNeSt: Splitattention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Transfer beyond the field of view: Dense panoramic semantic segmentation via unsupervised domain adaptation. T-ITS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Trans4Trans: Efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">IS-SAFE: Improving semantic segmentation in accidents by fusing event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>ICLR, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-T -</forename><surname>Trans4pass</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
