<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021">2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>St?phane D&amp;apos;ascoli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">Ecole Normale Sup?rieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Leavitt</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Biroli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">Ecole Normale Sup?rieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 38 th International Conference on Machine Learning</title>
						<meeting>the 38 th International Conference on Machine Learning						</meeting>
						<imprint>
							<date type="published" when="2021">2021</date>
						</imprint>
					</monogr>
					<note>Correspondence to: St?phane d&apos;Ascoli &lt;stephane.dascoli@ens.fr&gt;.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional architectures have proven extremely successful for vision tasks. Their hard inductive biases enable sample-efficient learning, but come at the cost of a potentially lower performance ceiling. Vision Transformers (ViTs) rely on more flexible self-attention layers, and have recently outperformed CNNs for image classification. However, they require costly pre-training on large external datasets or distillation from pretrained convolutional networks. In this paper, we ask the following question: is it possible to combine the strengths of these two architectures while avoiding their respective limitations? To this end, we introduce gated positional self-attention (GPSA), a form of positional self-attention which can be equipped with a "soft" convolutional inductive bias. We initialize the GPSA layers to mimic the locality of convolutional layers, then give each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. The resulting convolutionallike ViT architecture, ConViT, outperforms the DeiT (Touvron et al., 2020) on ImageNet, while offering a much improved sample efficiency. We further investigate the role of locality in learning by first quantifying how it is encouraged in vanilla self-attention layers, then analyzing how it is escaped in GPSA layers. We conclude by presenting various ablations to better understand the success of the ConViT. Our code and models are released publicly at https://github.com/ facebookresearch/convit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hard inductive bias (CNN)</head><p>Helpful Harmful Soft inductive bias (ConViT)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of deep learning over the last decade has largely been fueled by models with strong inductive biases, allowing efficient training across domains <ref type="bibr" target="#b25">(Mitchell, 1980;</ref><ref type="bibr" target="#b14">Goodfellow et al., 2016)</ref>. The use of Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b23">(LeCun et al., 1998;</ref><ref type="bibr" target="#b22">1989)</ref>, which have become ubiquitous in computer vision since the success of AlexNet in 2012 <ref type="bibr" target="#b21">(Krizhevsky et al., 2017)</ref>, epitomizes this trend. Inductive biases are hard-coded into the architectural structure of CNNs in the form of two strong constraints on the weights: locality and weight sharing. By encouraging translation equivariance (without pooling layers) and translation invariance (with pooling layers) <ref type="bibr" target="#b30">(Scherer et al., 2010;</ref><ref type="bibr" target="#b31">Schmidhuber, 2015;</ref><ref type="bibr" target="#b14">Goodfellow et al., 2016)</ref>, the convolutional inductive bias makes models more sampleefficient and parameter-efficient <ref type="bibr">(Simoncelli &amp; Olshausen, 2001;</ref><ref type="bibr" target="#b29">Ruderman &amp; Bialek, 1994)</ref>. Similarly, for sequencebased tasks, recurrent networks with hard-coded memory cells have been shown to simplify the learning of long-range dependencies (LSTMs) and outperform vanilla recurrent neural networks in a variety of settings <ref type="bibr" target="#b13">(Gers et al., 1999;</ref><ref type="bibr" target="#b36">Sundermeyer et al., 2012;</ref><ref type="bibr" target="#b15">Greff et al., 2017)</ref>.</p><p>However, the rise of models based purely on attention in recent years calls into question the necessity of hard-coded inductive biases. First introduced as an add-on to recurrent neural networks for Sequence-to-Sequence models <ref type="bibr" target="#b2">(Bahdanau et al., 2014)</ref>, attention has led to a breakthrough in Natural Language Processing through the emergence of Transformer models, which rely solely on a particular kind of attention: Self-Attention (SA) <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref>. The strong performance of these models when pre-trained on large datasets has quickly led to Transformer-based approaches becoming the default choice over recurrent models like LSTMs <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref>.</p><p>In vision tasks, the locality of CNNs impairs the ability to capture long-range dependencies, whereas attention does not suffer from this limitation. <ref type="bibr" target="#b5">Chen et al. (2018)</ref> and <ref type="bibr" target="#b3">Bello et al. (2019)</ref> leveraged this complementarity by augmenting convolutional layers with attention. More recently, <ref type="bibr" target="#b28">Ramachandran et al. (2019)</ref> ran a series of experiments replacing some or all convolutional layers in ResNets with attention, and found the best performing models used convolu- <ref type="figure">Figure 1</ref>. Soft inductive biases can help models learn without being restrictive. Hard inductive biases, such as the architectural constraints of CNNs, can greatly improve the sample-efficiency of learning, but can become constraining when the size of the dataset is not an issue. The soft inductive biases introduced by the ConViT avoid this limitation by vanishing away when not required. tions in early layers and attention in later layers. The Vision Transformer (ViT), introduced by <ref type="bibr" target="#b11">Dosovitskiy et al. (2020)</ref>, entirely dispenses with the convolutional inductive bias by performing SA across embeddings of patches of pixels. The ViT is able to match or exceed the performance of CNNs but requires pre-training on vast amounts of data. More recently, the Data-efficient Vision Transformer (DeiT) <ref type="bibr" target="#b39">(Touvron et al., 2020)</ref> was able to reach similar performances without any pre-training on supplementary data, instead relying on Knowledge Distillation <ref type="bibr" target="#b17">(Hinton et al., 2015)</ref> from a convolutional teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft inductive biases</head><p>The recent success of the ViT demonstrates that while convolutional constraints can enable strongly sample-efficient training in the small-data regime, they can also become limiting as the dataset size is not an issue. In data-plentiful regimes, hard inductive biases can be overly restrictive and learning the most appropriate inductive bias can prove more effective. The practitioner is therefore confronted with a dilemma between using a convolutional model, which has a high performance floor but a potentially lower performance ceiling due to the hard inductive biases, or a self-attention based model, which has a lower floor but a higher ceiling. This dilemma leads to the following question: can one get the best of both worlds, and obtain the benefits of the convolutional inductive biases without suffering from its limitations (see <ref type="figure">Fig. 1</ref>)?</p><p>In this direction, one successful approach is the combination of the two architectures in "hybrid" models. These models, which interleave or combine convolutional and selfattention layers, have fueled successful results in a variety of tasks <ref type="bibr" target="#b4">(Carion et al., 2020;</ref><ref type="bibr" target="#b18">Hu et al., 2018a;</ref><ref type="bibr" target="#b28">Ramachandran et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2020;</ref><ref type="bibr" target="#b24">Locatello et al., 2020;</ref><ref type="bibr" target="#b35">Sun et al., 2019;</ref><ref type="bibr" target="#b33">Srinivas et al., 2021;</ref><ref type="bibr" target="#b44">Wu et al., 2020</ref>). Another approach is that of Knowledge Distillation <ref type="bibr" target="#b17">(Hinton et al., 2015)</ref>, which has recently been applied to transfer the inductive bias of a convolutional teacher to a student transformer <ref type="bibr" target="#b39">(Touvron et al., 2020)</ref>. While these two methods offer an interesting compromise, they forcefully induce convolutional inductive biases into the Transformers, potentially affecting the Transformer with their limitations.</p><p>Contribution In this paper, we take a new step towards bridging the gap between CNNs and Transformers, by presenting a new method to "softly" introduce a convolutional inductive bias into the ViT. The idea is to let each SA layer decide whether to behave as a convolutional layer or not, depending on the context. We make the following contributions:</p><p>1. We present a new form of SA layer, named gated positional self-attention (GPSA), which one can initialize as a convolutional layer. Each attention head then has the freedom to recover expressivity by adjusting a gating parameter. 2. We then perform experiments based on the DeiT <ref type="bibr" target="#b39">(Touvron et al., 2020)</ref>, with a certain number of SA layers replaced by GPSA layers. The resulting Convolutional Vision Transformer (ConViT) outperforms the DeiT while boasting a much improved sample-efficiency ( <ref type="figure" target="#fig_1">Fig. 2</ref>). 3. We analyze quantitatively how local attention is naturally encouraged in vanilla ViTs, then investigate the inner workings of the ConViT and perform ablations to investigate how it benefits from the convolution initialization.</p><p>Overall, our work demonstrates the effectiveness of "soft" inductive biases, especially in the low-data regime where the learning model is highly underspecified (see <ref type="figure">Fig. 1</ref>), and motivates the exploration of further methods to induce them.</p><p>Related work Our work is motivated by combining the recent success of pure Transformer models  with the formalized relationship between SA and convolution. Indeed, <ref type="bibr" target="#b7">Cordonnier et al. (2019)</ref> showed that a SA layer with N h heads can express a convolution of kernel size ? N h , if each head focuses on one of the pixels in the kernel patch. By investigating the qualitative aspect of attention maps of models trained on CIFAR-10, it is shown that SA layers with relative positional encodings naturally converge towards convolutional-like configurations, suggesting that some degree of convolutional inductive bias is desirable.</p><p>Conversely, the restrictiveness of hard locality constraints has been proven by <ref type="bibr" target="#b12">Elsayed et al. (2020)</ref>. A breadth of approaches have been taken to imbue CNN architectures with nonlocality <ref type="bibr" target="#b19">(Hu et al., 2018b;</ref><ref type="bibr" target="#b43">Wang et al., 2018;</ref><ref type="bibr" target="#b44">Wu</ref>    <ref type="table">Tab.</ref> 1) with that of the DeiT-S by training them on restricted portions of ImageNet-1k, where we only keep a certain fraction of the images of each class. Both models are trained with the hyperparameters reported in <ref type="bibr" target="#b39">(Touvron et al., 2020)</ref>. We display the the relative improvement of the ConViT over the DeiT in green. Right: we compare the top-1 accuracies of our ConViT models with those of other ViTs (diamonds) and CNNs (squares) on ImageNet-1k. The performance of other models on ImageNet are taken from <ref type="bibr" target="#b39">(Touvron et al., 2020;</ref><ref type="bibr" target="#b16">He et al., 2016;</ref><ref type="bibr" target="#b37">Tan &amp; Le, 2019;</ref><ref type="bibr" target="#b44">Wu et al., 2020;</ref><ref type="bibr" target="#b45">Yuan et al., 2021</ref><ref type="bibr">Yuan et al., ). et al., 2020</ref>. Another line of research is to induce a convolutional inductive bias is different architectures. For example, Neyshabur (2020) uses a regularization method to encourage fully-connected networks (FCNs) to learn convolutions from scratch throughout training.</p><p>Most related to our approach, d' <ref type="bibr">Ascoli et al. (2019)</ref> explored a method to initialize FCNs networks as CNNs. This enables the resulting FCN to reach much higher performance than achievable with standard initialization. Moreover, if the FCN is initialized from a partially trained CNN, the recovered degrees of freedom allow the FCN to outperform the CNN it stems from. This method relates more generally to "warm start" approaches such as those used in spiked tensor models <ref type="bibr" target="#b1">(Anandkumar et al., 2016)</ref>, where a smart initialization, containing prior information on the problem, is used to ease the learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>We provide an open-source implementation of our method as well as pretrained models at the following address: https://github.com/ facebookresearch/convit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>We begin by introducing the basics of SA layers, and show how positional attention can allow SA layers to express convolutional layers.</p><p>Multi-head self-attention The attention mechanism is based on a trainable associative memory with (key, query) vector pairs. A sequence of L 1 "query" embeddings Q ? R L1?D h is matched against another sequence of L 2 "key" embeddings K ? R L2?D h using inner products. The result is an attention matrix whose entry (ij) quantifies how semantically "relevant" Q i is to K j :</p><formula xml:id="formula_0">A = softmax QK ? D h ? R L1?L2 ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">(softmax [X]) ij = e Xij / k e X ik .</formula><p>Self-attention is a special case of attention where a sequence is matched to itself, to extract the semantic dependencies between its parts. In the ViT, the queries and keys are linear projections of the embeddings of 16 ? 16 pixel patches</p><formula xml:id="formula_2">X ? R L?D emb . Hence, we have Q = W qry X and K = W key X, where W key , W qry ? R D emb ?D h .</formula><p>Multi-head SA layers use several self-attention heads in parallel to allow the learning of different kinds of interdependencies. They take as input a sequence of L embeddings of dimension D emb = N h D h , and output a sequence of L embeddings of the same dimension through the following mechanism:</p><p>MSA(X) := concat</p><formula xml:id="formula_3">h?[N h ] [SA h (X)] W out + b out ,<label>(2)</label></formula><p>where W out ? R D emb ?D emb , b out ? R D emb . Each selfattention head h performs the following operation:</p><formula xml:id="formula_4">SA h (X) := A h XW h val ,<label>(3)</label></formula><p>where W h val ? R D emb ?D h is the value matrix. However, in the vanilla form of Eq. 1, SA layers are positionagnostic: they do not know how the patches are located according to each other. To incorporate positional information, there are several options. One is to add some positional information to the input at embedding time, before propagating it through the SA layers:  use this approach in their ViT. Another possibility is to replace the vanilla SA with positional self-attention (PSA), using encodings r ij of the relative position of patches i and j <ref type="bibr" target="#b28">(Ramachandran et al., 2019)</ref>:</p><formula xml:id="formula_5">A h ij := softmax Q h i K h j + v h pos r ij<label>(4)</label></formula><p>Each attention head uses a trainable embedding v h pos ? R Dpos , and the relative positional encodings r ij ? R Dpos only depend on the distance between pixels i and j, denoted denoted by a two-dimensional vector ? ij . <ref type="bibr" target="#b7">Cordonnier et al. (2019)</ref> show that a multi-head PSA layer with N h In the above,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-attention as a generalized convolution</head><p>? The center of attention ? h ? R 2 is the position to which head h pays most attention to, relative to the query patch. For example, in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>, the four heads correspond, from left to right, to ? 1 = (?1, 1), ? 2 = (?1, ?1), ? 3 = (1, 1), ? 4 = (1, ?1).</p><p>? The locality strength ? h &gt; 0 determines how focused the attention is around its center ? h (it can also by understood as the "temperature" of the softmax in Eq. 1). When ? h is large, the attention is focused only on the patch(es) located at ? h , as in <ref type="figure" target="#fig_2">Fig. 3(d)</ref>; when ? h is small, the attention is spread out into a larger area, as in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>.</p><p>Thus, the PSA layer can achieve a strictly convolutional attention map by setting the centers of attention ? h to Because GPSA layers involve positional information, the class token is concatenated with hidden representation after the last GPSA layer. In this paper, we typically take 10 GPSA layers followed by 2 vanilla SA layers. FFN: feedforward network (2 linear layers separated by a GeLU activation); Wqry: query weights; W key : key weights; vpos: attention center and span embeddings (learned); r qk : relative position encodings (fixed); ?: gating parameter (learned); ?: sigmoid function. each of the possible positional offsets of a ? N h ? ? N h convolutional kernel, and sending the locality strengths ? h to some large value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Building on the insight of <ref type="bibr" target="#b7">(Cordonnier et al., 2019)</ref>, we introduce the ConVit, a variant of the ViT  obtained by replacing some of the SA layers by a new type of layer which we call gated positional self-attention (GPSA) layers. The core idea is to enforce the "informed" convolutional configuration of Eqs. 5 in the GPSA layers at initialization, then let them decide whether to stay convolutional or not. However, the standard parameterization of PSA layers (Eq. 4) suffers from two limitations, which lead us two introduce two modifications.</p><p>Adaptive attention span The first caveat in PSA is the vast number of trainable parameters involved, since the number of relative positional encodings r ? is quadratic in the number of patches. This led some authors to restrict the attention to a subset of patches around the query patch <ref type="bibr" target="#b28">(Ramachandran et al., 2019)</ref>, at the cost of losing long-range information.</p><p>To avoid this, we leave the relative positional encodings r ? fixed, and train only the embeddings v h pos which determine the center and span of the attention heads; this approach relates to the adaptive attention span introduced in <ref type="bibr" target="#b34">Sukhbaatar et al. (2019)</ref> for Language Transformers. The initial values of r ? and v h pos are given by Eq. 5, where we take D pos = 3 to get rid of the useless zero components. Thanks to D pos D h , the number of parameters involved in the positional attention is negligible compared to the number of parameters involved in the content attention. This makes sense, as content interactions are inherently much simpler to model than positional interactions.</p><p>Positional gating The second issue with standard PSA is the fact that the content and positional terms in Eq. 4 are potentially of different magnitudes, in which case the softmax will ignore the smallest of the two. In particular, the convolutional initialization scheme discussed above involves highly concentrated attention scores, i.e. high-magnitude values in the softmax. In practice, we observed that using a convolutional initialization scheme on vanilla PSA layers gives a boost in early epochs, but degrades late-time performance as the attention mechanism lazily ignores the content information (see SM. A).</p><p>To avoid this, GPSA layers sum the content and positional terms after the softmax, with their relative importances governed by a learnable gating parameter ? h (one for each attention head). Finally, we normalize the resulting sum of matrices (whose terms are positive) to ensure that the resulting attention scores define a probability distribution. The resulting GPSA layer is therefore parametrized as follows (see also <ref type="figure" target="#fig_3">Fig. 4</ref>):</p><formula xml:id="formula_6">GPSA h (X) := normalize A h XW h val (6) A h ij := (1 ? ?(? h )) softmax Q h i K h j + ?(? h ) softmax v h pos r ij ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_7">(normalize [A]) ij = A ij / k A ik and ? : x ? 1 /(1+e ?x )</formula><p>is the sigmoid function. By setting the gating parameter ? h to a large positive value at initialization, one has ?(? h ) 1 : the GPSA bases its attention purely on position, dispensing with the need of setting W qry and W key to zero as in Eq. 5. However, to avoid the ConViT staying stuck at ? h 1, we initialize ? h = 1 for all layers and all heads.</p><p>Architectural details The ViT slices input images of size 224 into 16 ? 16 non-overlapping patches of 14 ? 14 pixels and embeds them into vectors of dimension D emb = 64N h using a convolutional stem. It then propagates the patches through 12 blocks which keep their dimensionality constant. Each block consists in a SA layer followed by a 2-layer Feed-Forward Network (FFN) with GeLU activation, both equipped with residual connections. The ConViT is simply a ViT where the first 10 blocks replace the SA layers by GPSA layers with a convolutional initialization.</p><p>Similar to language Transformers like BERT <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref>, the ViT uses an extra "class token", appended to the sequence of patches to predict the class of the input. Since this class token does not carry any positional information, the SA layers of the ViT do not use positional attention: the positional information is instead injected to each patch before the first layer, by adding a learnable positional embedding of dimension D emb . As GPSA layers involve positional attention, they are not well suited for the class token approach. We solve this problem by appending the class token to the patches after the last GPSA layer, similarly to what is done in <ref type="bibr" target="#b41">(Touvron et al., 2021b</ref>) (see <ref type="figure" target="#fig_3">Fig. 4</ref>) 1 .</p><p>For fairness, and since they are computationally cheap, we keep the absolute positional embeddings of the ViT active in the ConViT. However, as shown in SM. F, the ConViT relies much less on them, since the GPSA layers already use relative positional encodings. Hence, the absolute positional embeddings could easily be removed, dispensing with the need to interpolate the embeddings when changing the input resolution (the relative positional encodings simply need to be resampled according to Eq. 5, as performed automatically in our open-source implementation).</p><p>Training details We based our ConVit on the DeiT (Touvron et al., 2020), a hyperparameter-optimized version of the ViT which has been open-sourced 2 . Thanks to its ability to achieve competitive results without using any external data, the DeiT both an excellent baseline and relatively easy to train: the largest model (DeiT-B) only requires a few days of training on 8 GPUs.</p><p>To mimic 2 ? 2, 3 ? 3 and 4 ? 4 convolutional filters, we consider three different ConViT models with 4, 9 and 16 attention heads (see Tab. 1). Their number of heads are slightly larger than the DeiT-Ti, ConViT-S and ConViT-B of <ref type="bibr" target="#b39">Touvron et al. (2020)</ref>, which respectively use 3, 6 and 12 attention heads. To obtain models of similar sizes, we use two methods of comparison.</p><p>? To establish a direct comparison with Touvron et al.</p><p>(2020), we lower the embedding dimension of the Con-ViTs to D emb /N h = 48 instead of 64 used for the DeiTs. Importantly, we leave all hyperparameters (scheduling, data-augmentation, regularization) presented in <ref type="bibr" target="#b39">(Touvron et al., 2020)</ref>  achieve a fair comparison. The resulting models are named ConViT-Ti, ConViT-S and ConViT-B.</p><p>? We also trained DeiTs and ConViTs using the same number of heads and D emb /N h = 64, to ensure that the improvement due to ConViT is not simply due to the larger number of heads <ref type="bibr" target="#b41">(Touvron et al., 2021b)</ref>. This leads to slightly larger models denoted with a "+" in Tab. 1. To maintain stable training while fitting these models on 8 GPUs, we lowered the learning rate from 0.0005 to 0.0004 and the batch size from 1024 to 512. These minimal hyperparameter changes lead the DeiT-B+ to perform less well than the DeiT-S+, which is not the case for the ConViT, suggesting a higher stability to hyperparameter changes. This result can be directly compared to <ref type="bibr" target="#b46">(Zhai et al., 2019)</ref>, which after testing several thousand convolutional models reaches a top-1 accuracy of 56.4%; the ConViT is therefore highly competitive in terms of sample efficiency. These findings confirm our hypothesis that the convolutional inductive bias is most helpful on small datasets, as depicted in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Investigating the role of locality</head><p>In this section, we demonstrate that locality is naturally encouraged in standard SA layers, and examine how the ConViT benefits from locality being imposed at initialization.</p><p>SA layers are pulled towards locality We begin by investigating whether the hypothesis that PSA layers are naturally encouraged to become "local" over the course of training <ref type="bibr" target="#b7">(Cordonnier et al., 2019)</ref> holds for the vanilla SA layers used in ViTs, which do not benefit from positional attention. To quantify this, we define a measure of "nonlocality" by summing, for each query patch i, the distances ? ij to all the key patches j weighted by their attention score A ij . We average the number obtained over the query patch to obtain the nonlocality metric of head h, which can then be averaged over the attention heads to obtain the nonlocality of the whole layer :</p><formula xml:id="formula_8">D ,h loc := 1 L ij A h, ij ? ij , D loc := 1 N h h D ,h loc<label>(8)</label></formula><p>Intuitively, D loc is the number of patches between the center of attention and the query patch: the further the attention heads look from the query patch, the higher the nonlocality.</p><p>In <ref type="figure">Fig. 5 (left panel)</ref>, we show how the nonlocality metric evolves during training across the 12 layers of a DeiT-S trained for 300 epochs on ImageNet. During the first few epochs, the nonlocality falls from its initial value in all layers, confirming that the DeiT becomes more "convolutional". During the later stages of training, the nonlocality metric stays low for lower layers, and gradually climbs back up for upper layers, revealing that the latter capture long range dependencies, as observed for language Transformers <ref type="bibr" target="#b34">(Sukhbaatar et al., 2019)</ref>.</p><p>These observations are particularly clear when examining the attention maps ( <ref type="figure" target="#fig_10">Fig. 15 of the SM)</ref>, and point to the beneficial effect of locality in lower layers. In <ref type="figure">Fig. 10</ref> of the SM., we also show that the nonlocality metric is lower when training with distillation from a convolutional network as in <ref type="bibr" target="#b39">Touvron et al. (2020)</ref>, suggesting that the locality of the teacher is partly transferred to the student <ref type="bibr" target="#b0">(Abnar et al., 2020)</ref>. GPSA layers escape locality In the ConViT, strong locality is imposed at the beginning of training in the GPSA layers thanks to the convolutional initialization. In <ref type="figure">Fig. 5  (right panel)</ref>, we see that this local configuration is escaped throughout training, as the nonlocality metric grows in all the GPSA layers. However, the nonlocality at the end of training is lower than that reached by the DeiT, showing that some information about the initialization is preserved throughout training. Interestingly, the final nonlocality does not increase monotonically throughout the layers as for the DeiT. The first layer and the final layers strongly escape locality, whereas the intermediate layers (particularly the second layer) stay more local.</p><p>To gain more understanding, we examine the dynamics of the gating parameters in <ref type="figure">Fig. 6</ref>. We see that in all layers, the average gating parameter E h ?(? h ) (in black), which reflects the average amount of attention paid to positional information versus content, decreases throughout training. This quantity reaches 0 in layers 6-10, meaning that positional information is practically ignored. However, in layers 1-5, some of the attention heads keep a high value of ?(? h ), hence take advantage of positional information. Interestingly, the ConViT-Ti only uses positional information up to layer 4, whereas the ConViT-B uses it up to layer 6 (see App. D), suggesting that larger models -which are more under-specified -benefit more from the convolutional prior. These observations highlight the usefulness of the gating parameter in terms of interpretability.</p><p>The inner workings of the ConViT are further revealed by the attention maps of <ref type="figure" target="#fig_5">Fig. 7</ref>, which are obtained by propagating an embedded input image through the layers and selecting a query patch at the center of the image 3 . In layer 10, (bottom row), the attention maps of DeiT and ConViT look qualitatively similar: they both perform content-based attention. In layer 2 however (top row), the attention maps of the ConViT are more varied: some heads pay attention to content (heads 1 and 2) whereas other focus mainly on position (heads 3 and 4). Among the heads which focus on position, some stay highly localized (head 4) whereas others broaden their attention span (head 3). The interested reader can find more attention maps in SM. E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ref.</head><p>Train  <ref type="table">Table 3</ref>. Gating and convolutional initialization play nicely together. We ran an ablation study on the ConViT-S+ trained for 300 epochs on the full ImageNet training set and on 10% of the training data. From the left column to right column, we experimented freezing the gating parameters to 0, removing the convolutional initialization, freezing the GPSA layers and removing them altogether.</p><p>Strong locality is desirable We next investigate how the performance of the ConViT is affected by two important hyperparameters of the ConViT: the locality strength, ?, which determines how focused the heads are around their center of attention, and the number of SA layers replaced by GPSA layers. We examined the effects of these hyperparameters on ConViT-S, trained on the first 100 classes of ImageNet. As shown in <ref type="figure">Fig. 8(a)</ref>, final test accuracy increases both with the locality strength and with the number of GPSA layers; in other words, the more convolutional, the better. <ref type="bibr">3</ref> We do not show the attention paid to the class token in the SA layers Left: input image which is embedded then fed into the models. The query patch is highlighted by a red box and the colormap is logarithmic to better reveal details. Center: attention maps obtained by a DeiT-Ti after 300 epochs of training on ImageNet. Right: Same for ConViT-Ti. In each map, we indicated the value of the gating parameter in a color varying from white (for heads paying attention to content) to red (for heads paying attention to position). Attention maps for more images and heads are shown in SM. E.</p><p>In <ref type="figure">Fig. 8(b)</ref>, we show how performance at various stages of training is impacted by the presence of GPSA layers. We see that the boost due to GPSA is particularly strong during the early stages of training: after 20 epochs, using 9 GPSA layers leads the test-accuracy to almost double, suggesting that the convolution initialization gives the model a substantial "head start". This speedup is of practical interest in itself, on top of the boost in final performance.  <ref type="figure">Figure 8</ref>. The beneficial effect of locality. Left: As we increase the locality strength (i.e. how focused each attention head is its associated patch) and the number of GPSA layers of a ConViT-S+, the final top-1 accuracy increases significantly. Right: The beneficial effect of locality is particularly strong in the early epochs.</p><p>of the convolutional initialization ([a], +3.1), and is unhelpful otherwise. These mild improvements due to gating and convolutional initialization (likely due to performance saturation above 80% top-1) become much clearer in the low data regime. Here, GPSA alone brings +6.8, with an extra +2.3 coming from gating, +2.8 from convolution initialization and +5.1 with the two together, illustrating their complementarity.</p><p>We also investigated the performance of the ConViT with all GPSA layers frozen, leaving only the FFNs to be trained in the first 10 layers. As one could expect, performance is strongly degraded in the full data regime if we initialize the GPSA layers randomly ([f], -5.4 compared to the DeiT). However, the convolutional initialization remarkably enables the frozen ConViT to reach a very decent performance, almost equalling that of the DeiT ([e], -0.5). In other words, replacing SA layers by random "convolutions" hardly impacts performance. In the low data regime, the frozen ConViT even outperforms the DeiT by a margin (+6.5). This naturally begs the question: is attention really key to the success of ViTs <ref type="bibr" target="#b10">(Dong et al., 2021;</ref><ref type="bibr" target="#b38">Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b40">Touvron et al., 2021a)</ref>?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and perspectives</head><p>The present work investigates the importance of initialization and inductive biases in learning with vision transformers. By showing that one can take advantage of convolutional constraints in a soft way, we merge the benefits of architectural priors and expressive power. The result is a simple recipe that improves trainability and sample efficiency, without increasing model size or requiring any tuning.</p><p>Our approach can be summarized as follows: instead of interleaving convolutional layers with SA layers as done in hybrid models, let the layers decide whether to be convolutional or not by adjusting a set of gating parameters. More generally, combining the biases of varied architectures and letting the model choose which ones are best for a given task could become a promising direction, reducing the need for greedy architectural search while offering higher interpretability.</p><p>Another direction which will be explored in future work is the following: if SA layers benefit from being initialized as random convolutions, could one reduce even more drastically their sample complexity by initializing them as pre-trained convolutions?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The importance of positional gating</head><p>In the main text, we discussed the importance of using GPSA layers instead of the standard PSA layers, where content and positional information are summed before the softmax and lead the attention heads to focus only on the positional information. We give evidence for this claim in <ref type="figure" target="#fig_6">Fig. 9</ref>, where we train a ConViT-B for 300 epochs on ImageNet, but replace the GPSA by standard PSA. The convolutional initialization of the PSA still gives the ConViT a large advantage over the DeiT baseline early in training. However, the ConViT stays in the convolutional configuration and ignores the content information, as can be seen by looking at the attention maps (not shown). Later in training, the DeiT catches up and surpasses the performance of the ConViT by utilizing content information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The effect of distillation</head><p>Nonlocality In <ref type="figure">Fig. 10</ref>, we compare the nonlocality curves of <ref type="figure">Fig. 5</ref> of the main text with those obtained when the DeiT is trained via hard distillation from a RegNetY-16GF (84M parameters) <ref type="bibr" target="#b27">(Radosavovic et al., 2020)</ref>, as in <ref type="bibr" target="#b39">Touvron et al. (2020)</ref>.</p><p>In the distillation setup, the nonlocality still drops in the early epochs of training, but increases less at late times compared to without distillation. Hence, the final internal states of the DeiT are more "local" due to the distillation. This suggests that knowledge distillation transfers the locality of the convolutional teacher to the student, in line with the results of <ref type="bibr" target="#b0">(Abnar et al., 2020)</ref>.</p><p>Performance The hard distillation introduced in <ref type="bibr" target="#b39">Touvron et al. (2020)</ref> greatly improves the performance of the DeiT. We have verified the complementarity of their distillation methods with our ConViT. In the same way as in the DeiT paper, we used a RegNet-16GF teacher and experimented hard distillation during 300 epochs on ImageNet. Just like the DeiT, the ConViT benefits from distillation, albeit somewhat less than the DeiT, as can be seen from the DeiT-B performing less well than the ConViT-S+ without distillation but better with distillation. This hints to the fact that the convolutional inductive bias transferred from the teacher is redundant with its own convolutional prior. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further performance results</head><p>In <ref type="figure" target="#fig_7">Fig. 11</ref>, we display the time evolution of the top-1 accuracy of our ConViT+ models on CIFAR100, ImageNet and subsampled ImageNet, along with a comparison with the corresponding DeiT+ models.</p><p>For CIFAR100, we kept all hyperparameters unchanged, but rescaled the images to 224 ? 224 and increased the number of epochs (adapting the learning rate schedule correspondingly) to mimic the ImageNet scenario. After 1000 epochs, the ConViTs shows clear signs of overfitting, but reach impressive performances (82.1% top-1 accuracy with 10M parameters, which is better than the EfficientNets reported in <ref type="bibr" target="#b47">(Zhao et al., 2020)</ref>). In <ref type="figure" target="#fig_1">Fig. 12</ref>, we study the impact of the various ingredients of the ConViT (presence and number of GPSA layers, gating parameters, convolutional initialization) on the dynamics of learning.  <ref type="figure" target="#fig_1">Figure 12</ref>. Impact of various ingredients of the ConViT on the dynamics of learning. In both cases, we train the ConViT-S+ for 300 epochs on first 100 classes of ImageNet. Left: ablation on number of GPSA layers, as in <ref type="figure">Fig. 8</ref>. Right: ablation on various ingredients of the ConViT, as in Tab. 3. The baseline is the DeiT-S+ (pink). We experimented (i) replacing the 10 first SA layers by GPSA layers ("GPSA") (ii) freezing the gating parameter of the GPSA layers ("frozen gate"); (iii) removing the convolutional initialization ("conv"); (iv) freezing all attention modules in the GPSA layers ("frozen"). The final top-1 accuracy of the various models trained is reported in the legend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effect of model size</head><p>In <ref type="figure" target="#fig_2">Fig. 13</ref>, we show the analog of <ref type="figure">Fig. 5</ref> of the main text for the tiny and base models. Results are qualitatively similar to those observed for the small model. Interestingly, the first layers of DeiT-B and ConViT-B reach significantly higher nonlocality than those of the DeiT-Ti and ConViT-Ti.</p><p>In <ref type="figure" target="#fig_3">Fig. 14, we</ref> show the analog of <ref type="figure">Fig. 6</ref> of the main text for the tiny and base models. Again, results are qualitatively similar: the average weight of the positional attention, E h ?(? h ), decreases over time, so that more attention goes to the content of the image. Note that in the ConViT-Ti, only the first 4 layers still pay attention to position at the end of training (average gating parameter smaller than one), whereas for ConViT-S, the 5 first layers still do, and for the ConViT-B, the 6 first layers still do. This suggests that the larger (i.e. the more underspecified) the model is, the more layers make use of the convolutional prior.   <ref type="figure" target="#fig_2">Figure 13</ref>. The bigger the model, the more non-local the attention. We plotted the nonlocality metric defined in Eq. 8 of the main text (the higher, the further the attention heads look from the query pixel) throughout 300 epochs of training on ImageNet-1k.  <ref type="figure" target="#fig_3">Figure 14</ref>. The bigger the model, the more layers pay attention to position. We plotted the gating parameters of various heads and various layers, as in <ref type="figure">Fig. 6</ref> of the main text (the lower, the less attention is paid to positional information) throughout 300 epochs of training on ImageNet-1k. Note that the ConViT-Ti only has 4 attention heads whereas the ConViT-B has 16, hence the different number of curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Attention maps</head><p>Attention maps of the DeiT reveal locality In <ref type="figure" target="#fig_10">Fig. 15</ref>, we give some visual evidence for the fact that vanilla SA layers extract local information by averaging the attention map of the first and tenth layer of the DeiT over 100 images. Before training, the maps look essentially random. After training, however, most of the attention heads of the first layer focus on the query pixel and its immediate surroundings, whereas the attention heads of the tenth layer capture long-range dependencies.  Attention maps of the ConViT reveal the diversity of the attention heads In <ref type="figure">Fig. 16</ref>, we show a comparison of the attention maps of Deit-Ti and ConViT-Ti for different images of the ImageNet validation set. In <ref type="figure" target="#fig_5">Fig. 17</ref>, we compare the attention maps of DeiT-S and ConViT-S.</p><p>In all cases, results are qualitatively similar: the DeiT attention maps look similar across different heads and different layers, whereas those of the ConViT perform very different operations. Notice that in the second layer, the third and forth head focus stay local whereas the first two heads focus on content. In the last layer, all the heads ignore positional information, focusing only on content.  <ref type="figure">Figure 16</ref>. Left: input image which is embedded then fed into the models. The query patch is highlighted by a red box and the colormap is logarithmic to better reveal details. Center: attention maps obtained by a DeiT-Ti after 300 epochs of training on ImageNet. Right: Same for ConViT-Ti. In each map, we indicated the value of the gating parameter in a color varying from white (for heads paying attention to content) to red (for heads paying attention to position).</p><p>ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 ( ) = 0.00 (c) ConViT <ref type="figure" target="#fig_5">Figure 17</ref>. Attention maps obtained by a DeiT-S and ConViT-S after 300 epochs of training on ImageNet. In each map, we indicated the value of the gating parameter in a color varying from white (for heads paying attention to content) to red (for heads paying attention to position).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Further ablations</head><p>In this section, we explore masking off various parts of the network to understand which are most crucial.</p><p>In Tab. 5, we explore the importance of the absolute positional embeddings injected to the input in both the DeiT and ConViT. We see that masking them off at test time a mild impact on accuracy for the ConViT, but a significant impact for the DeiT, which is expected as the ConViT already has relative positional information in each of the GPSA layers. This also shows that the absolute positional information contained in the embeddings is not very useful.</p><p>In Tab. 6, we explore the relative importance of the positional and content information by masking them off at test time. To do so, we manually set the gating parameter ?(?) to 1 (no content attention) or 0 (no positional attention). In the first GPSA layers, both procedures affect performance similarly, signalling that positional and content information are both useful. However in the last GPSA layers, masking the content information kills performance, whereas masking the positional information does not, confirming that content information is more crucial. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The ConViT outperforms the DeiT both in sample and parameter efficiency. Left: we compare the sample efficiency of our ConViT-S (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Convolutional initialization, strength ? = 2 Positional self-attention layers can be initialized as convolutional layers. (a): Input image from ImageNet, where the query patch is highlighted by a red box. (b),(c),(d): attention maps of an untrained SA layer (b) and those of a PSA layer using the convolutional-like initialization scheme of Eq. 5 with two different values of the locality strength parameter, ? (c, d). Note that the shapes of the image can easily be distinguished in (b), but not in (c) or (d), when the attention is purely positional. heads and learnable relative positional encodings (Eq. 4) of dimension D pos ? 3 can express any convolutional layer ?? h 1, ?2? h 1 , ?2? h 2 , 0, . . . 0 r ? := ? 2 , ? 1 , ? 2 , 0, . . . 0 W qry = W key := 0, W val := I (5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Architecture of the ConViT. The ConViT (left) is a version of the ViT in which some of the self-attention (SA) layers are replaced with gated positional self-attention layers (GPSA; right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>SA layers try to become local, GPSA layers escape locality. We plot the nonlocality metric defined in Eq. 8, averaged over a batch of 1024 images: the higher, the further the attention heads look from the query pixel. We trained the DeiT-S and ConViT-S for 300 epochs on ImageNet. Similar results for DeiT-Ti/ConViT-Ti and DeiT-B/ConViT-B are shown in SM. D. The gating parameters reveal the inner workings of the ConViT. For each layer, the colored lines (one for each of the 9 attention heads) quantify how much attention head h pays to positional information versus content, i.e. the value of ?(? h ), see Eq. 7. The black line represents the value averaged over all heads. We trained the ConViT-S for 300 epochs on ImageNet. Similar results for ConViT-Ti and ConViT-B are shown in SM D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The ConViT learns more diverse attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Convolutional initialization without GPSA is helfpul during early training but deteriorates final performance. We trained the ConViT-B along with its DeiT-B counterpart for 300 epochs on ImageNet, replacing the GPSA layers of the ConViT-B by vanilla PSA layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>The convolutional inductive bias is particularly useful for large models applied to small datasets. Each of the three panels displays the top-1 accuracy of the ConViT+ model and their corresponding DeiT+ throughout training, as well as the relative improvement between the best top-1 accuracy reached by the DeiT+ and that reached by the ConViT+. Left: tiny, small and base models trained for 3000 epochs on CIFAR100. Middle: tiny, small and base models trained for 300 epochs on ImageNet-1k. The relative improvement of the ConViT over the DeiT increases with model size. Right: small model trained on a subsampled version of ImageNet-1k, where we only keep a fraction f ? {0.05, 0.1, 0.3, 0.5, 1} of the images of each class. The relative improvement of the ConViT over the DeiT increases as the dataset becomes smaller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 .</head><label>15</label><figDesc>The averaged attention maps of the DeiT reveal locality at the end of training. To better visualise the center of attention, we averaged the attention maps over 100 images. Top: before training, the attention patterns exhibit a random structure. Bottom: after training, most of the attention is devoted to the query pixel, and the rest is focused on its immediate surroundings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>0.15 ( ) = 0.00 ( ) = 0.59 ( ) = 0.00 ( ) = 0.04 ( ) = 0.02 ( ) = 0.04 ( ) = 0.00 ( ) = 0.00Layer 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2</head><label>12</label><figDesc>Performance of the models considered, trained from scratch on ImageNet. Speed is the number of images processed per second on a Nvidia Quadro GP100 GPU at batch size 128. Top-1 accuracy is measured on ImageNet-1k test set without distillation (see SM. B for distillation). The results for DeiT-Ti, DeiT-S and DeiT-B are reported from<ref type="bibr" target="#b39">(Touvron et al., 2020)</ref>.</figDesc><table><row><cell>unchanged in order to</cell></row></table><note>. The convolutional inductive bias strongly improves sample efficiency. We compare the top-1 and top-5 accuracy of our ConViT-S with that of the DeiT-S, both trained using the original hyperparameters of the DeiT (Touvron et al., 2020), as well as the relative improvement of the ConViT over the DeiT. Both models are trained on a subsampled version of ImageNet-1k, where we only keep a variable fraction (leftmost column) of the images of each class for training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Sample efficiency of the ConViT In Tab. 2, we investigate the sample-efficiency of the ConViT in a systematic way, by subsampling each class of the ImageNet-1k dataset by a fraction f = {0.05, 0.1, 0.3, 0.5, 1} while multiplying the number of epochs by 1/f so that the total number images presented to the model remains constant. As one might expect, the top-1 accuracy of both the DeiT-S and its ConViT-S counterpart drops as f decreases. However, the ConViT suffers much less: while training on only 10% of</figDesc><table /><note>ConViT In Tab. 1, we display the top-1 accuracy achieved by these models evaluated on the ImageNet test set after 300 epochs of training, alongside their number of parameters, number of flops and throughput. Each ConViT outperforms its DeiT of same size and same number of flops by a margin. Importantly, although the positional self-attention does slow down the throughput of the ConViTs, they also outperform the DeiTs at equal throughput. For example, The ConViT-S+ reaches a top- 1 of 82.2%, outperforming the original DeiT-B with less parameters and higher throughput. Without any tuning, the ConViT also reaches high performance on CIFAR100, see SM. C where we also report learning curves. Note that our ConViT is compatible with the distillation methods introduced in Touvron et al. (2020) at no extra cost. As shown in SM. B, hard distillation improves performance, enabling the hard-distilled ConViT-S+ to reach 82.9% top-1 accuracy, on the same footing as the hard-distilled DeiT- B with half the number of parameters. However, while distillation requires an additional forward pass through a pre-trained CNN at each step of training, ConViT has no such requirement, providing similar benefits to distillation without additonal computational requirements.the data, the ConVit reaches 59.5% top-1 accuracy, com- pared to 46.5% for its DeiT counterpart.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 4. Top-1 accuracies of the ConViT-S+ compared to the DeiT-S and DeiT-B, both trained for 300 epochs on ImageNet.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>The results we obtain are</cell></row><row><cell>summarized in Tab. 4.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">DeiT-S (22M) DeiT-B (86M) ConViT-S+ (48M)</cell></row><row><cell>No distillation</cell><cell>79.8</cell><cell>81.8</cell><cell>82.2</cell></row><row><cell>Hard distillation</cell><cell>80.9</cell><cell>83.0</cell><cell>82.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Nevertheless, the performance improvement obtained by the ConViT with hard distillation demonstrates that instantiating soft inductive biases directly in a model can yield benefits on top of those obtained by instantiating such biases indirectly, in Distillation pulls the DeiT towards a more local configuration. We plotted the nonlocality metric defined in Eq. 8 throughout training, for the DeiT-S trained on ImageNet. Left: regular training. Right: training with hard distillation from a RegNet teacher, by means of the distillation introduced in<ref type="bibr" target="#b39">(Touvron et al., 2020)</ref>.</figDesc><table><row><cell>Non-locality</cell><cell>5.75 6.00 6.25 6.50 6.75 7.00 7.25 7.50</cell><cell>0</cell><cell>100 200 300 Epochs</cell><cell>5.75 6.00 6.25 6.50 6.75 7.00 7.25 7.50</cell><cell>0</cell><cell>100 200 300 Epochs</cell><cell>Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12</cell></row><row><cell>Figure 10.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>this case via distillation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Table 5. Performance on ImageNet with the positional embeddings masked off at test time. Performance of ConViT-Ti on ImageNet with positional or content attention masked off at test time.</figDesc><table><row><cell>Model</cell><cell cols="2">Mask pos embed No mask</cell><cell></cell></row><row><cell>DeiT-Ti</cell><cell>38.3</cell><cell>72.2</cell><cell></cell></row><row><cell>ConViT-Ti</cell><cell>67.1</cell><cell>73.1</cell><cell></cell></row><row><cell cols="4"># layers masked Mask content Mask position No mask</cell></row><row><cell>3</cell><cell>62.3</cell><cell>63.5</cell><cell>73.1</cell></row><row><cell>5</cell><cell>35.0</cell><cell>53.1</cell><cell>73.1</cell></row><row><cell>10</cell><cell>1.3</cell><cell>46.8</cell><cell>73.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also experimented incorporating the class token as an extra patch of the image to which all heads pay attention to at initialization, but results were worse than concatenating the class token after the GPSA layers (not shown).2 https://github.com/facebookresearch/deit</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">To remove gating, we freeze all gating parameters to ? = 0 so that the same amount of attention is paid to content and position.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Herv? J?gou and Francisco Massa for helpful discussions. SD and GB acknowledge funding from the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Transferring inductive biases through knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00555</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09322</idno>
		<title level="m">Homotopy analysis for tensor pca</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11579</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03584</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding the needle in the haystack with convolutions: on the benefits of architectural bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9334" to="9345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attention is not all you need: Pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03404</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting spatial invariance with low-rank local connectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2868" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LSTM: A Search Space Odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2016.2582924</idno>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>nett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9401" to="9411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00745</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="2575" to="7075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Object-centric learning with slot attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15055</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The need for biases in learning generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Laboratory for Computer Science Research . . .</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards learning convolutions from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Statistics of natural images: Scaling in the woods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">814</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15825-410</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks -ICANN 2010</title>
		<editor>Diamantaras, K., Duch, W., and Iliadis, L. S.</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2014.09.003</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0893608014002135" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Natural image statistics and neural representation. Annual review of neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases Simoncelli</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1193" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Bottleneck Transformers for Visual Recognition. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joulin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07799</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Videobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resmlp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nonlocal Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00813</idno>
		<ptr target="https://ieeexplore.ieee.org/document/8578911/" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<idno>arXiv: 2006.03677</idno>
		<ptr target="http://arxiv.org/abs/2006.03677" />
		<title level="m">Visual Transformers: Tokenbased Image Representation and Processing for Computer Vision</title>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">S4l: Selfsupervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Splitnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14660</idno>
		<title level="m">Divide and co-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
