<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">D 3 TW: Discriminative Differentiable Dynamic Time Warping for Weakly Supervised Action Alignment and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yi</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">D 3 TW: Discriminative Differentiable Dynamic Time Warping for Weakly Supervised Action Alignment and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address weakly supervised action alignment and segmentation in videos, where only the order of occurring actions is available during training. We propose Discriminative Differentiable Dynamic Time Warping (D 3 TW), the first discriminative model using weak ordering supervision. The key technical challenge for discriminative modeling with weak supervision is that the loss function of the ordering supervision is usually formulated using dynamic programming and is thus not differentiable. We address this challenge with a continuous relaxation of the min-operator in dynamic programming and extend the alignment loss to be differentiable. The proposed D 3 TW innovatively solves sequence alignment with discriminative modeling and end-toend training, which substantially improves the performance in weakly supervised action alignment and segmentation tasks. We show that our model is able to bypass the degenerated sequence problem usually encountered in previous work and outperform the current state-of-the-art across three evaluation metrics in two challenging datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video action understanding has gained increasing interest over recent years because of the large amount of video data. In contrast to fully annotated approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39]</ref> which require annotations of the exact start and end time of each action, weakly supervised approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21]</ref> significantly reduce the required annotation effort and improve the applicability to real-world data. In particular, we focus on one type of weak label commonly referred to as action order or transcript, which uses an ordered list of actions occurring in the video as supervision.</p><p>The major challenge of using only the action order as supervision is that the ground truth target, frame-wise action label is not available at training time. Previous work resorts to using a variety of surrogate loss functions that maximize the posterior probability of the weak labels or the action ordering given the video. However, as shown in <ref type="bibr" target="#b14">[15]</ref>, using surrogate loss functions can easily lead to  <ref type="figure">Figure 1</ref>. We use only the ordered list of actions or the transcript as weak supervision for training. This setting is challenging as the desired output is not available at training. We address this challenge by proposing the first discriminative model for this task. The cost ??( + , X) of aligning the video X (middle) to the ground truth or positive transcript + (top) should be smaller than that of the negative transcript ? (bottom) that are randomly sampled.</p><p>degenerated results that align some occurring actions to a single frame in the video. Such degenerated results are far from the ground truth we desire because each action usually spans many frames during its execution. While previous works have attempted to address this challenge using frame-to-frame similarity <ref type="bibr" target="#b14">[15]</ref>, fine-to-coarse strategy <ref type="bibr" target="#b27">[28]</ref>, and segment length modeling <ref type="bibr" target="#b28">[29]</ref>, these approaches still consider the degenerated results that align to single frames as valid solutions subject to the surrogate loss functions.</p><p>The main contribution of this paper is to address the challenge by proposing the first discriminative model using order supervision. As illustrated in <ref type="figure">Figure 1</ref>, the idea is that the probability of having the correct alignment with the positive or ground truth transcript should be higher than that of negative transcripts. In contrast to previous works that only maximize the posterior probability of the weak labels <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, our discriminative formulation does not suffer from the degenerated alignment as it is no longer an obvious and trivial solution to the newly proposed discriminative loss. Further, minimizing the discriminative loss directly contributes to the improvement of our target in contrast to previous work. Similar ideas have been studied in other research areas, such as multiple-instance learning for image tagging, and have been shown to be successful <ref type="bibr" target="#b36">[37]</ref>.</p><p>While the idea of applying discriminative modeling to weakly supervised action labeling problem is seemingly intuitive, the key technical challenge is that the computation of loss functions in previous methods usually involves non-differentiable structural prediction algorithms such as dynamic programming (DP). We address this challenge by proposing Discriminative Differentiable Dynamic Time Warping (D 3 TW), where we directly optimize for better outputs by minimizing a discriminative loss function obtained by continuous relaxation of the minimum operator in DP <ref type="bibr" target="#b25">[26]</ref>. The use of D 3 TW allows us to incorporate the advantage of discriminative modeling with structural prediction model, which was not possible in previous approaches.</p><p>We evaluate D 3 TW on two weakly supervised tasks in two popular benchmark datasets, the Breakfast Action <ref type="bibr" target="#b19">[20]</ref> and the Hollywood Extended <ref type="bibr" target="#b2">[3]</ref>. The first task is action segmentation, which refers to predicting frame-wise action labels, where the test video is given without any further annotation. The second task is action alignment, as proposed in <ref type="bibr" target="#b2">[3]</ref>, which refers to aligning a test video sequence to a given action order sequence. We show that our D 3 TW significantly improves the performance on both tasks.</p><p>In summary, our key contributions are: (i) We introduce the first discriminative model for ordering supervision to address the degenerate sequence problem. (ii) We propose D 3 TW, a novel framework that incorporates the advantage of discriminative modeling and end-to-end training for structural sequence prediction with weak supervision. (iii) We apply our method in two challenging real-world video datasets and show that it achieves state-of-the-art for both weakly supervised action segmentation and alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Action Recognition and Segmentation. Action recognition has been an important task for video understanding <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. As performances on trimmed video datasets advance <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4]</ref>, recent focus of video understanding has shifted towards longer and untrimmed video data, such as VLOG <ref type="bibr" target="#b9">[10]</ref>, Charades <ref type="bibr" target="#b34">[35]</ref>, and EPIC-Kitchens <ref type="bibr" target="#b5">[6]</ref>. This has led to the development of action segmentation approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39</ref>] that aim to label every frame in the video and not just to classify trimmed video clips. Our goal is also to densely label each frame of the video, but without the dense supervision for training.</p><p>Weakly Supervised Learning in Vision. For images, weakly supervised learning has been studied in classification <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b23">24]</ref>, semantic segmentation <ref type="bibr" target="#b39">[40]</ref>, object detection <ref type="bibr" target="#b21">[22]</ref>, and visual grounding <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref>. The ordering constraint has been used widely as weak supervision in videos <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. The closest to our work is the NN-Viterbi <ref type="bibr" target="#b28">[29]</ref>, where the it combines a neural network and a non-differentiable Viterbi process to learn from ordering supervision iteratively. In contrast, the proposed D 3 TW is end-to-end differentiable and uses discriminative modeling to directly optimize for the best alignment under ordering supervision. Using Language as Supervision for Videos. As the ordering supervision can be automatically extracted from language, our work is related to using language as supervision for videos. The supervision usually comes from movie scripts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b40">41]</ref> or transcription of instructional videos <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14]</ref>. Unlike these approaches, we assume the discrete action labels are already extracted and focus on leveraging the ordering information as supervision. Continuous Relaxation. Our D 3 TW is related to recent progress on continuous relaxation of discrete operations, including theorem proving <ref type="bibr" target="#b29">[30]</ref>, softmax function <ref type="bibr" target="#b15">[16]</ref>, logic programming <ref type="bibr" target="#b8">[9]</ref>, and dynamic programming <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b4">5]</ref>. We use the same principle and further enable discriminative modeling of dynamic programming based alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our goal is to learn to temporally align and segment video frames using only weak supervision, where only the order of occurring actions is available at training. The major challenge for weakly supervised problem is that the ground truth target, i.e., frame-wise action labels are not available at training. We address this challenge by proposing Discriminative Differentiable Dynamic Time Warping (D 3 TW), which is to our best knowledge, the first discriminative modeling framework with ordering supervision. The use of discriminative modeling and differentiable dynamic programming sets our approach apart from previous work that involves non-differentiable forward-backward algorithms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref> and dramatically alleviates the problem of degenerated alignments that aligns each action label to a single frame. <ref type="figure">Figure 2</ref> shows the outline of our model.</p><p>In the following, we describe our framework in detail, starting with the problem statement. We then define our model and show how it can be used at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Weakly Supervised Action Learning</head><p>We start with the definition of the weakly supervised action alignment and segmentation. Here the weak supervision means that only the transcript, or an ordered list of the actions is provided at training time. A video of frying eggs, for example, might consist of taking eggs, breaking eggs, and frying eggs. While the full supervision would provide the fine-grained temporal boundary of each action, in our weakly supervised setup, only the action order sequence [take_egg, break_egg, fry_egg] is given.</p><p>We address two tasks in this paper: action segmentation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outputs</head><p>(c) Testing Segmentation <ref type="figure">Figure 2</ref>. (a) During training, only the transcript + is given. The input video is first forwarded through a GRU to generate the posterior probabilities p(k|X) of each action for each frame. D 3 TW is a discriminative model with a fully differentiable loss function, which allows us to learn p(k|X) via backpropagation and sets our approach apart from previous work. (b) For alignment, at test time our D 3 TW loss can directly be used to align the given transcript + with the video sequence. (c) For segmentation, at test time no transcript is given. We reduce segmentation to alignment by aligning the video to a set of candidate transcripts ? and output the best candidate as the segmentation result. and action alignment. We aim to learn both with weak supervision. As shown in <ref type="figure">Figure 2</ref>(b) and (c), the difference between the two tasks is that at test time, action alignment uses both transcript and test video frames as input, while action segmentation only requires test video frames as inputs. We observe that action segmentation can be formulated as an action alignment task given a set of possible transcripts at test time. We will first explain how to tackle action alignment using weak supervision, and explain how action segmentation can be reduced to the action alignment problem.</p><p>Formally, given an input sequence of video frames X = [x 1 , ? ? ? , x T ] ? R d?T , the goal of action alignment is to predict an output alignment sequence of frame-wise action labels? = [? 1 , ? ? ? ,? T ] ? A 1?T , under the constraint that a i follows the action order in the transcript</p><formula xml:id="formula_0">+ = [ + 1 , ? ? ? , + L ] ? A 1?L .</formula><p>Here, A is the set of pos-sible actions. In other words, we want to learn a model f (X, + ) =?. The key challenge of weak supervision is that we only have the inputs (X, + ) as supervision for training f (?) without access to the ground truth action labels a + 1:T . For action segmentation, we observe that segmentation can be formulated as alignment given a set of possible transcripts. Formally, given a set of possible transcripts L, let ?(a, X) ? R be a score function that measures the goodness of predicted action labels a given input video X, action segmentation task can be solved by exhaustive searc?</p><formula xml:id="formula_1">a = argmax a=f ( ,X), ?L ? (a, X) .<label>(1)</label></formula><p>This finds the candidate transcript that gives the best alignment measured by ?(?, X) for transcripts in L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discriminative Differentiable DTW (D 3 TW)</head><p>We have discussed what is weakly supervised action alignment and how we can solve action segmentation based on alignment. Now we discuss how we use discriminative modeling to learn a model that aligns the transcript + and the video frames X using just + and X at training.</p><p>We pose action alignment as a Dynamic Time Warping (DTW) <ref type="bibr" target="#b31">[32]</ref> problem, which has been widely applied to sequence alignment in speech recognition. Given a distance function d( + i , x j ) that measures the cost of aligning the frame x j to a label in the transcript + i , DTW uses dynamic programming to efficiently find the best alignment that minimizes the overall cost. The key challenge of weakly supervised learning is that there is no frame-to-frame alignment label to train this distance function d( + i , x j ). We address this challenge by proposing Discriminative Differentiable Dynamic Time Warping (D 3 TW), which allows us to learn d( + i , x j ) using only weak supervision. In the following, we will first discuss how we formulate video alignment as DTW and next how we learn the distance function d( + i , x j ) using D 3 TW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Video Alignment as Dynamic Time Warping</head><p>Given two sequences and X of lengths L and T corresponding to the transcript and the video, we define Y ? {0, 1} L?T to be the set of possible binary alignment matrices. Here ?Y ? Y, Y ij = 1 if video frame x j is labeled as i and Y ij = 0 otherwise. We impose rigid constraints on eligible warping paths based on the observation that each video frame can only be aligned to a single action label, such that the alignment from X to is strictly one-to-one. In other words, Y ? {0, 1} L?T is the set of binary matrices with exactly T nonzero elements and column pivots. Given an alignment matrix Y , we can derive its corresponding action label a 1:T as:  Here we use a trellis diagram to show the computational graph of the optimal transcript-video alignment Y * as defined in Eq. <ref type="formula" target="#formula_3">(2)</ref>. Bellman recursion guarantees that Y * , ? ? Y , ? , ?Y ? Y and the action order in the transcript is strictly preserved.</p><formula xml:id="formula_2">a j = i , if Y ij = 1.</formula><p>Given the constraints on the eligible alignments, the goal of DTW is to find the best alignment Y * ? Y</p><formula xml:id="formula_3">Y * = argmin Y ?Y Y, ?( , X) ,<label>(2)</label></formula><p>that minimizes the inner product between the alignment matrix Y and the distance matrix ?( , X) between transcript and video X, where ?( , X) :</p><formula xml:id="formula_4">= [d( i , x j )] ij ? R L?T .</formula><p>Given the distance function d( i , x j ), we can solve Eq. (2) using dynamic programming. A simplified example of such process is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. Of all paths that connect the upper left entry ? 11 to the lower right entry ? LT using only ??, moves, Y * is the optimal alignment that minimizes the alignment cost between transcript sequence and video frames. In this case, we can efficiently obtain the best alignment between video X and transcript .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Discriminative Modeling with Weak Supervision</head><p>We have discussed how we obtain the best alignment Y * given the distance function d( i , x j ) using DTW. However, the problem remains that how can we learn this distance function without access to the ground truth alignment.</p><p>An approach used in prior work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> maximizes the probability of the video X given the transcript :</p><formula xml:id="formula_5">p(X| ) = a t p(xt|at)p(at| ),<label>(3)</label></formula><p>where a t ? A is the action label for frame t. By optimizing the objective in Eq. (3), we can learn p(x t |k), the probability of observing x t given action k ? A. In order to maximize the probability, we define the distance d( i , x j ) = ? log p(x j | i ) as the negative log-likelihood.  <ref type="figure">Figure 4</ref>. We introduce discriminative modeling to weakly supervised action alignment. The loss ??( + , X) of aligning the video X to the correct transcript + should be lower than that of any other randomly sample negative transcript ? , which prevents degenerated alignments issue commonly seen in previous work.</p><p>One should notice that the alignment a t in Eq. <ref type="formula" target="#formula_5">(3)</ref> is latent and the number of possible alignments grows exponentially with the length of the video. Therefore, previous work either uses dynamic programming <ref type="bibr" target="#b14">[15]</ref>, or uses a hard EM approach <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> to infer a t and iteratively maximize the objective in Eq. (3). The key drawback of such approaches is that they can easily lead to a degenerate or trivial solution as the space of alignments is too large. While one can impose constraints by enforcing heuristic priors on the possible alignments p(a t | ), this does not directly address the drawback that maximizing this objective does not necessarily lead to the correct alignment.</p><p>Our key insight here is to introduce discriminative modeling to the weak ordering supervision problem. We enforce a discriminative constraint that should hold for any input tuple ( + , X), that</p><formula xml:id="formula_6">p(X| + ) &gt; p(X| ? ), ? ? ? L \ + ,<label>(4)</label></formula><p>where the probability of observing the video based on the ground truth or positive transcript + should always be higher than the probability observing the video from the negative transcript ? , as illustrated in <ref type="figure">Figure 4</ref>. This discriminative constraint was not explicitly used in previous work. Using the hinge loss with margin ? ? 0, the loss function can be written as:</p><formula xml:id="formula_7">? ?L\ + max(p(X| + ) ? p(X| ? ), ?).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Differentiable Loss with Continuous Relaxation</head><p>While the above discriminative modeling is intuitive, the technical challenge is that p(X| + ) and p(X| ? ) in Eq. <ref type="formula" target="#formula_7">(5)</ref> are generally not differentiable with respect to the distance function d( i , x j ) = ? log p(x j | i ) we aim to learn. One way of optimizing it is to use hard EM <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> and iteratively optimize this loss given the current distance function d( i , x j ). However, hard EM is numerically unstable because it uses a hard maximum operator in its interactions to update model parameters <ref type="bibr" target="#b25">[26]</ref>. The key technical contribution of our approach is proposing a continuous relaxation of the DTW-based video alignment loss function. Instead of iteratively updating the model parameters by solving Eq. (2) to find the best alignment given the current d( i , x j ) with hard EM, we can solve the following continuous relaxation:</p><formula xml:id="formula_8">??( , X) = min ? { Y, ?( , X) , Y ? Y}.<label>(6)</label></formula><p>Here min ? {} is the continuous relaxation of regular minimum operator regularized by negative entropy H(q) = ? q log(q) with a smoothing parameter ? ? 0, such that</p><formula xml:id="formula_9">min ? {a1, ? ? ? , an} = min i?n ai, ? = 0 ?? log n i=1 e ?a i /? , ? &gt; 0 .<label>(7)</label></formula><p>This transforms the dynamic programming based DTW loss function into a differentiable one with respect to d( i , x j ) when ? &gt; 0. The smoothing parameter ? empirically helps the optimization although it does not explicitly convexify the objective function. The gradient of Eq. (6) can be derived using the chain rule:</p><formula xml:id="formula_10">?X ??( , X) = ??( , X) ?X T Y ?Y e ? Y,?( ,X) /? Y Y ?Y e ? Y,?( ,X) /? ,<label>(8)</label></formula><p>where the second term on the right can be interpreted as the average alignment matrix under the Gibbs distribution p ? ? e ? Y,?( ,X) /? , ?Y ? Y. Algorithm 1 summarizes the procedure for computing ? ? ( , X) and its gradient. We can interpret ? ? ( , X) as the expectation cost over all possible alignments between transcript and video X. Its gradient ? X ? ? can be seen as a relaxed version of the hard alignment Y * in Eq. <ref type="bibr" target="#b1">(2)</ref>. With the continuous relaxation in Eq. (6), we can directly compute the gradient and optimize for Eq. (5). This addresses the challenge of getting degenerated alignments due to numerically unstable operations in hard EM. By substituting p(X| ) in Eq. (5) with our relaxed alignment cost ? ? ( , X), we obtain the discriminative and differentiable loss function L D 3 TW :</p><formula xml:id="formula_11">L D 3 TW ( + , X) = ? ?L\ + max(??( + , X) ? ??( ? , X), ?).<label>(9)</label></formula><p>Directly minimizing Eq. (9) enables our model to simultaneously optimize for finding the best alignment and discriminating the most accurate transcript given the observed video sequence. The differentiablity of Eq. (9) allows gradients to backpropogate through the entire model and finetune the distance function d( i , x j ) for the distance matrix ?( , X) in the alignment task with end-to-end training.</p><p>Algorithm 1 Compute alignment cost ? ? ( , X) and its gradient ? X ? ? ( , X) 1: Inputs: , X, smoothing parameter ? ? 0, distance function for j = [T, ? ? ? , 1]; i = [L, ? ? ? , 1] do 13: </p><formula xml:id="formula_12">d 2: procedure FORWARD PASS 3: v [0,0] ? 0 4: v [:,0] , v [0,:] ? inf 5: for i = [1, ? ? ? , L]; j = [1, ? ? ? , T ] do 6: v [i,j] ? d [i,j] + min?(v [i,j?1] , v [i?1,j?1] ) 7: q [i,j,:] ? ?min?(v [i,j?1] , v [i?1,j?1]</formula><formula xml:id="formula_13">r [i,j] ? q [i,j+1,1] r [i,j+1] + q [i+1,j+1,2] r [i+1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Learning and Inference</head><p>Distance Function Parameterization. In this paper, we use a Recurrent Neural Network (RNN) with a softmax output layer to parameterize our distance function d( i , x j ) given video frames as input. Let Z = [z 1 , ? ? ? , z T ] ? R A?T be the RNN output at each frame, where A = |A| is the number of possible actions. p(k|x t ) = z k t can be interpreted as the posterior probability of action k at time t. We follow <ref type="bibr" target="#b28">[29]</ref> and approximate emission probability p(x t |k) ? p(k|xt) p(k) , where p(k) is the action class prior. Action class priors are uniformly initialized to 1 A and updated after every batch of iterations by counting and normalizing the number of occurrences of each action class that have been processed so far during the training process. Inference for Action Segmentation. At test time we want our model to predict the best action labels a = [a 1 , ? ? ? , a T ] given only an unseen test video X test = [x 1 , ? ? ? , x T ]. We disentangle the action segmentation task into two components: First, we generate a set of candidate transcripts ? = { 1 , ? ? ? , m } ? L following <ref type="bibr" target="#b28">[29]</ref>, where L represents the set of all possible transcripts. Then we align each of the candidate transcripts to the unseen test video X test to find the transcript? that minimizes the alignment cost ? ? : </p><p>The predicted alignment? and associated frame-level action labels? is given by ?? ? (? , X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The key contribution of D 3 TW is to apply discriminative, differentiable, and dynamic alignment between weak labels and video frames. In this section, we evaluate the proposed model on two challenging weakly supervised tasks, action segmentation and alignment in two real-world datasets. In addition, we study how our model's segmentation performance varies with more supervision. Through ablation study, we further investigate the effectiveness of the proposed D 3 TW and compare our approach to current state-of-the-art methods. <ref type="bibr" target="#b19">[20]</ref> consists of 1,712 untrimmed videos of 52 participants cooking 10 dishes, such as fried eggs, in 18 different kitchens. Overall, there are around 3.6M frames labeled with 48 possible actions. The dataset has been used widely for weakly supervised action labeling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. For a fair comparison, we use the pre-computed features and data split provided by <ref type="bibr" target="#b19">[20]</ref>. Hollywood Extended <ref type="bibr" target="#b2">[3]</ref> consists of 937 videos containing 2 to 11 actions in each video. Overall, there are about 0.8M frames labeled with 16 possible actions, such as open_door. We use the feature and follow the data split in <ref type="bibr" target="#b2">[3]</ref> for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Features. Breakfast Action</head><p>Network Architecture. We use single layer GRU <ref type="bibr" target="#b11">[12]</ref> with 512 hidden units. We optimize with Adam <ref type="bibr" target="#b17">[18]</ref> and cross-validate the hyperparameters such as learning rate and batch size.</p><p>Frame Sub-sampling. For faster training and inference, we temporally sub-sample feature vectors in Breakfast Action. Following <ref type="bibr" target="#b14">[15]</ref>, we cluster visually similar and temporally adjacent frames using k-means, where T M centers are temporally uniformly distributed as initialization. We empirically pick M = 20, which is much shorter than the average length of action (?400 frames in the Breakfast dataset). No further pre-processing is required for Hollywood Extended dataset as the feature vectors are already sub-sampled.</p><p>Baselines. We compare to the following six baselines: -ECTC <ref type="bibr" target="#b14">[15]</ref> does not rely on hard-EM. However, it uses non-differentiable DP based algorithm to compute its gradients. In addition, it does include explicit models for the context between classes.</p><p>-GRU reest. <ref type="bibr" target="#b27">[28]</ref> uses hidden Markov models and train their systems iteratively to reestimate the output.</p><p>-TCFPN <ref type="bibr" target="#b6">[7]</ref> is also based on action alignment. However, it uses an iterative framework that is neither differentiable nor discriminative like D 3 TW.</p><p>-NN-Viterbi <ref type="bibr" target="#b28">[29]</ref> is the most similar to ours, and can be seen as an ablation without discriminative modeling and without differentiable loss. However, our RNN takes the whole video as input instead of segments of the videos.</p><p>-Ours w/o D 3 TW is our model without using D 3 TW but instead uses an iterative strategy similar to NN-Viterbi <ref type="bibr" target="#b28">[29]</ref>. This ablation shows our model's performance without discriminative and differentiable modeling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Weakly Supervised Action Segmentation</head><p>In the segmentation task, the goal is to predict framewise action labels for unseen test videos without any an-Recipe ?Facc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct Predictions False Positives False Negatives</head><p>Sandwich +24.7%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cereals</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+19.9%</head><p>Pancake +0.2%</p><p>Scrambled Egg ?0.8% <ref type="figure">Figure 6</ref>. Qualitative results show the importance of discriminative modeling. We calculate ?Facc., the absolute difference in frame accuracy between Ours D 3 TW and Ours w/o Discriminative. Discriminative modeling is able to improve the performances on almost all recipes or activities in the Breakfast dataset. In Pancake (row 3) and Scrambled Egg (row 4) where D 3 TW does not achieve a significant improvement, we see the challenge of cooking steps that are extremely similar from a further viewpoint. When cooking steps are distinct such as Sandwich (row 1) and Cereals (row 2), our D 3 TW is able to substantially improve the performance of frame accuracy by over 20%.</p><p>notation. Weakly supervised action segmentation is challenging as the target output is never used in training. As discussed in Section 3.2.4, we reduce the segmentation task to the alignment task by first finding the predicted transcript that maximizes the likelihood in Eq. (10) given a set of candidate transcripts ?, and then deriving the frame-wise labels from the alignment between? and video X. For a fair comparison, we follow <ref type="bibr" target="#b28">[29]</ref> and set ? to be the set of all transcripts seen in training time. Metrics. We follow the metrics used in the previous work <ref type="bibr" target="#b19">[20]</ref> to evaluate predicted frame-wise action labels. The first is frame accuracy, the percentage of frames that are correctly labeled. The second is unit accuracy, which is metric similar to the word error rate in speech recognition <ref type="bibr" target="#b18">[19]</ref>. The output action label sequence is first aligned to the ground truth label sequence by vanilla dynamic time warping (DTW) before the error rate is computed. Results. The results of weakly supervised action segmentation are shown in <ref type="table" target="#tab_2">Table 1</ref>. First, by explicitly modeling for the context between classes and their temporal progression, both GRU reest <ref type="bibr" target="#b27">[28]</ref> and NN-Viterbi <ref type="bibr" target="#b28">[29]</ref> are able to outperform ECTC by a large margin <ref type="bibr" target="#b14">[15]</ref>. In addition, we can see that using alignment is an effective strategy based on TCFPN <ref type="bibr" target="#b6">[7]</ref>. Ours w/o D 3 TW is able to combine these strengths and perform reasonably well compared to the state-of-the-art approaches. Ours w/o Discriminative further improves on all metrics by using the differentiable relaxed loss function with better numerical stability. Most importantly, our full model using D 3 TW is able to combine the benefits of differentiable loss with discriminative modeling and significantly outperforms all the baselines and achieve state-of-the-art results on all metrics. This shows the importance of both components of our proposed D 3 TW model. <ref type="figure" target="#fig_6">Fig. 5</ref> shows a qualitative comparison of models on a video making sandwich. Colors indicate different actions, and the horizontal axis is time. Ours D 3 TW is the only model that correctly captures all the occurring actions with discriminative modeling. In addition, this also leads to more accurate boundaries of actions. Comparing NN-Viterbi and Ours w/o Discriminative shows the benefit of the differentiable model that leads to better action boundaries. In addition, we further illustrate the importance of discriminative modeling in <ref type="figure">Fig. 6</ref> by comparing our full model with Ours w/o Discriminative and show the Correct Prediction, False Positives, and False Negatives of our model. As shown in the figure, discriminative modeling almost improves all 10 dishes in the Breakfast dataset, with the only exception of Scrambled Egg that the D 3 TW is lower by a neglectable 0.2% for the frame accuracy. We can see that for the dishes or activities of Pancake and Scrambled Egg that our D 3 TW does not improve much, the false positives are visually very similar to the correct prediction and lead to challenges of aligning the video with the transcript. On the other hand, for activities such as Sandwich and Cereals that involves distinct steps, our D 3 TW significantly improves the performance of the model by over 20% of frame accuracy. In addition, if we look at the False Positives of Cereals, it is only fails because it is inherently difficult to distinguish visually similar actions of pouring cereals versus pouring flour from an obstructed viewing angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semi-Supervised Action Segmentation</head><p>In contrast to most baselines, our formulation of weakly supervised action alignment based on DTW can easily incorporate any additional frame supervision by imposing path constraints in the calculation of ? ? . This is also called the frame-level semi-supervised setting, as proposed in <ref type="bibr" target="#b14">[15]</ref>. In semi-supervised setting, only a few frames in the video are sparsely annotated with the ground truth action, which is much easier for the annotator to annotate.</p><p>In this setting, we only compare to ECTC as it is the only baseline that allows this experiment. We further compare to  . Frame and unit accuracy are plotted against a fraction of labeled data in the frame-level semi-supervised setting for Breakfast dataset. Our DTW based formulation allows the frame-level supervision to be easily incorporated as the path constraints in dynamic programming. Our differentiable and discriminative modeling is able to lead to better performances on both metrics even in the semi-supervised setting.</p><p>the "Uniform" baseline that was discussed in <ref type="bibr" target="#b14">[15]</ref>, where the model uses pseudo labels generated by uniformly distributing the transcript following the order. The results for frame-level semi-supervised action segmentation is shown in <ref type="figure" target="#fig_8">Fig. 7</ref>. We can see that the proposed D 3 TW is also able to significantly improve performances in the semi-supervised setting. This again shows the importance of both the differentiable loss function and the discriminative modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Weakly Supervised Action Alignment</head><p>In this task, the goal is to align the given transcript to its proper temporal location in the test video. Our D 3 TW formulation is designed to directly optimize for action alignment with only weak supervision. In this case, we always have the ground truth transcript + and does not have to search using Eq. (10). It is noteworthy that the result from alignment can be interpreted as an empirical upper bound for our model's performance in action segmentation. Metrics. The primary goal of this experiment is to evaluate our model on aligning ground truth transcript to input video frames. We use metrics such as frame accuracy that measures the exact temporal boundaries in predictions. We drop unit accuracy as its use of DTW inevitably obfuscates the exact temporal boundaries. In addition to frame accuracy, we also measure the alignment quality with intersection over detection (IoD) following <ref type="bibr" target="#b2">[3]</ref>. Given a groundtruth action interval I * and a prediction interval I, IoD is defined as |I?I * | |I| . Readers should note that IoD is some-  <ref type="table">Table 2</ref>. Weakly supervised action alignment results. Compared to segmentation, the ground-truth transcript is given for the alignment, and thus the performances are higher. Nevertheless, both the differentiable relaxation and discriminative modeling are still beneficial for this task and lead to state-of-the-art results.</p><p>times referred as Jaccard measure <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref>. The value of IoD is between 0 to 1 and the higher the better. We report the IoD averaged across all ground-truth intervals in the test set.</p><p>Results. The results for weakly supervised action alignment are shown in <ref type="table">Table 2</ref>. We can see that the performance of all the baselines improves in terms of frame accuracy, this is because we have more information about the video in action alignment at test time. This also implies that the gap between different methods might be smaller. However, we observe the same trend as seen in action segmentation that the proposed D 3 TW is able to significantly outperform all the baselines on the metrics and achieve state-of-the-art result. This experiment once again validates that the use of both differentiable loss and discriminative modeling is important for our model's success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose D 3 TW, the first discriminative framework for weakly supervised action alignment and segmentation.</p><p>The key observation of our work is to use discriminative modeling between the positive and negative transcripts and bypass the problem of the degenerated sequence. The major challenge is that the dynamic programming based loss is often non-differentiable. We address this by proposing a continuous relaxation that allows D 3 TW to directly optimize for the discriminative objective with end-to-end training. Our results and ablation studies show that both the discriminative modeling and the differentiable relaxation are crucial for the success of D 3 TW, which achieves state-ofthe-art results in both segmentation and alignment on two challenging real-world datasets. Our D 3 TW framework is general and can be extended to other tasks that require prior structures in the output and end-to-end differentiability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>" , $ + ) &lt; % &amp; ? # , $ ?? #~. ? ? "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Dynamic Time Warping formulation for video alignment. The 5 ? 8 colored grid represents distance matrix ?( , X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>) 8 :</head><label>8</label><figDesc>procedure BACKWARD PASS 9: q [:,T +1,:] , q [L+1,:,:] ? 0 10: r [:,T +1] , r [L+1,:] ? 0 11: q [L+1,T +1,:] , r [L+1,T +1] ? 1 12:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>j+1] 14 :</head><label>14</label><figDesc>Returns: ?? = v [L,T ] , ?X ?? = r [1:L,1:T ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FramesFigure 5 .</head><label>5</label><figDesc>Qualitative results on the Breakfast dataset. Colors indicate actions and the horizontal axis is time. While both Ours w/o Discriminative and NN-Viterbi introduce additional actions not appearing in the ground truth, Ours w/o Discriminative has better action boundaries because of the differentiable loss. Ours D 3 TW is the only model that correctly captures all the occurring actions with discriminative modeling. In addition, this also leads to more accurate boundaries of actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>-</head><label></label><figDesc>Ours w/o Discriminative is compared to show the importance of discriminative modeling for weakly supervised learning. Compared to Ours w/o D 3 TW, this model use a differentiable relaxation of Eq. (3) as the objective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7</head><label>7</label><figDesc>Figure 7. Frame and unit accuracy are plotted against a fraction of labeled data in the frame-level semi-supervised setting for Breakfast dataset. Our DTW based formulation allows the frame-level supervision to be easily incorporated as the path constraints in dynamic programming. Our differentiable and discriminative modeling is able to lead to better performances on both metrics even in the semi-supervised setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Weakly supervised action segmentation results in the Breakfast and Hollywood datasets. The use of both differentiable relaxation and discriminative modeling leads to the success of our D 3 TW and set our approach apart from previous approaches using ordering supervision.</figDesc><table><row><cell></cell><cell cols="2">Breakfast</cell><cell cols="2">Hollywood</cell></row><row><cell></cell><cell cols="4">Facc. Uacc. Facc. Uacc.</cell></row><row><cell>ECTC[15]</cell><cell>27.7</cell><cell>35.6</cell><cell>-</cell><cell>-</cell></row><row><cell>GRU reest.[28]</cell><cell>33.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TCFPN[7]</cell><cell>38.4</cell><cell>-</cell><cell>28.7</cell><cell>-</cell></row><row><cell>NN-Viterbi[29]</cell><cell>43.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours w/o D 3 TW</cell><cell>34.9</cell><cell>36.1</cell><cell>25.9</cell><cell>24.3</cell></row><row><cell>Ours w/o Discriminative</cell><cell>38.0</cell><cell>38.4</cell><cell>30.0</cell><cell>28.3</cell></row><row><cell>Ours (D 3 TW)</cell><cell>45.7</cell><cell>47.4</cell><cell>33.6</cell><cell>30.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was partially funded by Toyota Research Institute (TRI). This article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly-supervised alignment of video with text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lagugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Soft-dtw: a differentiable loss function for time-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="894" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epickitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning explanatory rules from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From lifestyle vlogs to everyday interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>In ICML</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding &quot;it&quot;: Weakly-supervised, referenceaware visual grounding in instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Categorical reparametrization with gumble-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Testing the correlation of word error rate and perplexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Track and transfer: Watching videos to simulate strong human supervision for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y. Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00932</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What&apos;s cookin&apos;? interpreting cooking videos using text, speech and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Differentiable dynamic programming for structured prediction and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neuralnetwork-viterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end differentiable proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithm optimization for spoken word recognition. Acoustics, Speech and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing of video collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Asynchronous temporal fields for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3460" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly-supervised visual grounding of phrases with linguistic structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y. Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation for social images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
