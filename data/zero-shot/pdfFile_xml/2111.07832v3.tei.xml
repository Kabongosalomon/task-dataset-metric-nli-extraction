<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2022 IBOT : IMAGE BERT PRE-TRAINING WITH ONLINE TOKENIZER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">UC Santa Cruz</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2022 IBOT : IMAGE BERT PRE-TRAINING WITH ONLINE TOKENIZER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM) <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref>, where texts are first tokenized into semantically meaningful pieces. In this work, we study masked image modeling (MIM) and indicate the advantages and challenges of using a semantically meaningful visual tokenizer. We present a self-supervised framework iBOT that can perform masked prediction with an online tokenizer. Specifically, we perform self-distillation on masked patch tokens and take the teacher network as the online tokenizer, along with self-distillation on the class token to acquire visual semantics. The online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage training pipeline where the tokenizer needs to be pretrained beforehand. We show the prominence of iBOT by achieving an 82.3% linear probing accuracy and an 87.8% fine-tuning accuracy evaluated on ImageNet-1K. Beyond the state-of-the-art image classification results, we underline emerging local semantic patterns, which helps the models to obtain strong robustness against common corruptions and achieve leading results on dense downstream tasks, e.g., object detection, instance segmentation, and semantic segmentation. The code and models are publicly available at https://github.com/bytedance/ibot.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We compare iBOT with other unsupervised baselines.</p><p>Masked Language Modeling (MLM), which first randomly masks and then reconstructs a set of input tokens, is a popular pre-training paradigm for language models. The MLM pre-trained Transformers <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref> have demonstrated their scalability to largecapacity models and datasets, becoming a defacto standard for lingual tasks. However, its potential for Vision Transformer (ViT), which recently started to revolutionize computer vision research <ref type="bibr">(Touvron et al., 2021;</ref><ref type="bibr">Dosovitskiy et al., 2021)</ref>, has been largely underexplored. Most popular unsupervised pretraining schemes in vision deal with the global views <ref type="bibr">(Chen et al., 2021;</ref><ref type="bibr">Caron et al., 2021)</ref>, neglecting images' internal structures, as opposed to MLM modeling local tokens. In this work, we seek to continue the success of MLM and explore Masked Image Modeling (MIM) for training better Vision Transformers such that it can serve as a standard component, as it does for NLP.</p><p>One of the most crucial components in MLM is the lingual tokenizer which splits language into semantically meaningful tokens, e.g., WordPiece <ref type="bibr" target="#b47">(Wu et al., 2016)</ref> in BERT. Similarly, the crux of MIM lies in a proper design of visual tokenizer, which transforms the masked patches to supervisory Published as a conference paper at ICLR 2022 signals for the target model, as shown in <ref type="figure">Fig. 2</ref>. However, unlike lingual semantics arising naturally from the statistical analysis of word frequency <ref type="bibr" target="#b38">(Sennrich et al., 2016)</ref>, visual semantics cannot be extracted such easily due to the continuous property of images. Empirically, visual semantics emerges progressively by bootstrapping online representation that enforces a similarity of distorted image views <ref type="bibr" target="#b19">(He et al., 2020;</ref><ref type="bibr" target="#b17">Grill et al., 2020;</ref><ref type="bibr" target="#b6">Caron et al., 2020)</ref>. This property intuitively indicates a multi-stage training pipeline, where we need to first train an off-the-shelf semantic-rich tokenizer before training the target model. However, since acquiring visual semantics is a common end for both the tokenizer and target model, a single-stage training pipeline where the tokenizer and target model can be jointly optimized awaits further exploration.</p><p>Previous works partially tackle the above challenges. Several works use identity mapping as the visual tokenizer, i.e., predicting the raw pixel values <ref type="bibr" target="#b34">(Pathak et al., 2016;</ref><ref type="bibr" target="#b2">Atito et al., 2021)</ref>. Such paradigm struggles in semantic abstraction and wastes the capacity at modeling high-frequency details, yielding less competitive performance in semantic understanding <ref type="bibr" target="#b28">(Liu et al., 2021a)</ref>. Recently, BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref> proposes to use a pre-trained discrete VAE <ref type="bibr">(Ramesh et al., 2021)</ref> as the tokenizer. Though providing some level of abstraction, the discrete VAE is still found only to capture low-level semantics within local details (as observed by Tab. 9). Moreover, the tokenizer needs to be offline pre-trained with fixed model architectures and extra dataset <ref type="bibr">(Ramesh et al., 2021)</ref>, which potentially limits its adapativity to perform MIM using data from different domains.</p><p>ViT I Tok.</p><p>distill <ref type="figure">Figure 2</ref>: Masked image modeling. I denotes an image and Tok. denotes a visual tokenizer.</p><p>To this end, we present iBOT , short for image BERT pre-training with Online Tokenizer, a new framework that performs MIM with a tokenizer handling above-mentioned challenges favorably. We motivate iBOT by formulating the MIM as knowledge distillation (KD), which learns to distill knowledge from the tokenizer, and further propose to perform self-distillation for MIM with the help of twin teacher as online tokenizer. The target network is fed with a masked image while the online tokenizer with the original image. The goal is to let the target network recover each masked patch token to its corresponding tokenizer output. Our online tokenizer naturally resolves two major challenges. On the one hand, our tokenizer captures highlevel visual semantics progressively learned by enforcing the similarity of cross-view images on class tokens. On the other hand, our tokenizer needs no extra stages of training as pre-processing setup since it is jointly optimized with MIM via momentum update.</p><p>The online tokenizer enables iBOT to achieve excellent performance for feature representation. Specifically, iBOT advances ImageNet-1K classification benchmark under k-NN, linear probing and fine-tuning protocols to 77.1%, 79.5%, 84.0% with ViT-Base/16 respectively, which is 1.0%, 1.3%, 0.4% higher than previous best results. When pre-trained with ImageNet-22K, iBOT with ViT-L/16 achieves a linear probing accuracy of 82.3% and a fine-tuning accuracy of 87.8%, which is 1.0% and 1.8% higher than previous best results. Beyond that, the advancement is also valid when transferring to other datasets or under semi-supervised and unsupervised classification settings. Of particular interest, we have identified an emerging part-level semantics that can help the model with image recognition both on global and local scales. We identify that the semantic patterns learned in patch tokens, which sufficiently lack in the off-line tokenizer as in BEiT <ref type="bibr" target="#b3">(Bao et al., 2021</ref>), helps the model to be advanced in linear classification and robustness against common image corruptions. When it is transferred to downstream tasks, we show that in downstream tasks related to image classification, object detection, instance segmentation, and semantic segmentation, iBOT surpasses previous methods with nontrivial margins. All of the evidence demonstrates that iBOT has largely closed the gap of masked modeling pre-training between language and vision Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES 2.1 MASKED IMAGE MODELING AS KNOWLEDGE DISTILLATION</head><p>Masked image modeling (MIM), which takes a similar formulation as MLM in BERT, has been proposed in several recent works <ref type="bibr" target="#b3">(Bao et al., 2021;</ref><ref type="bibr" target="#b40">Tan et al., 2021)</ref>. Specifically, for an image token sequence x = {x i } N i=1 , MIM first samples a random mask m ? {0, 1} N according to a prediction ratio r, where N is the number of tokens. The patch token x i where m i being 1, denoted asx {x i | m i = 1}, are then replaced with a mask token e <ref type="bibr">[MASK]</ref> , yielding a corrupted imag?</p><formula xml:id="formula_0">x {x i | (1 ? m i )x i + m i e [MASK] } N i=1</formula><p>. MIM is to recover the masked tokensx from the corrupted imagex, i.e., to maximize: log q ? (x|x) ? N i=1 m i ? log q ? (x i |x), where ? holds with an independence assumption that each masked token can be reconstructed separately. In BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref>, q ? is modelled as a categorical distribution and the task is to minimize</p><formula xml:id="formula_1">? N i=1 m i ? P ? (x i ) T log P ? (x i ),<label>(1)</label></formula><p>where P (?) transforms the input to a probability distribution over K dimensions, and ? is parameters of a discrete VAE <ref type="bibr">(Ramesh et al., 2021)</ref> that clusters image patches into K categories and assigns each patch token a one-hot encoding identifying its category. We note this loss is formulated similarly to knowledge distillation <ref type="bibr" target="#b23">(Hinton et al., 2015)</ref>, where knowledge is distilled from a pre-fixed tokenizer parameterized by ? to current model parameterized by ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SELF-DISTILLATION</head><p>Self-distillation, proposed recently in DINO <ref type="bibr">(Caron et al., 2021)</ref>, distills knowledge not from posterior distributions P ? (x) but past iterations of model itself P ? (x) and is cast as a discriminative self-supervised objective. Given the training set I, an image x ? I is sampled uniformly, over which two random augmentations are applied, yielding two distorted views u and v. The two distorted views are then put through a teacher-student framework to get the predictive categorical dis-</p><formula xml:id="formula_2">tributions from the [CLS] token: v [CLS] t = P [CLS] ? (v) and u [CLS] s = P [CLS] ? (u).</formula><p>The knowledge is distilled from teacher to student by minimizing their cross-entropy, formulated as</p><formula xml:id="formula_3">L [CLS] = ?P [CLS] ? (v) T log P [CLS] ? (u).</formula><p>(2)</p><p>The teacher and the student share the same architecture consisting of a backbone f (e.g., ViT) and a projection head h <ref type="bibr">[CLS]</ref> . The parameters of the student network ? are Exponentially Moving Averaged (EMA) to the parameters of teacher network ? . The loss is symmetrized by averaging with another cross-entropy term between v [CLS] s and u <ref type="bibr">[CLS]</ref> t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IBOT</head><p>We motivate our method by identifying the similar formulation of Eq. (1) and Eq. (2). A visual tokenizer parameterized by online ? instead of pre-fixed ? thus arises naturally. In this section, we present iBOT, casting self-distillation as a token-generation self-supervised objective and perform MIM via self-distillation. We illustrate the framework of iBOT in <ref type="figure">Fig. 3</ref> and demonstrate the pseudocode in Appendix A. In Sec. 3.2, we briefly introduce the architecture and pre-training setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FRAMEWORK</head><p>First, we perform blockwise masking <ref type="bibr" target="#b3">(Bao et al., 2021)</ref> on the two augmented views u and v and obtain their masked views? andv. Taking? as an example for simplicity, the student network outputs for the masked view? projections of its patch tokens? patch s = P patch ? (?) and the teacher network outputs for the non-masked view u projections of its patch tokens u patch t = P patch ? (u). We here define the training objective of MIM in iBOT as</p><formula xml:id="formula_4">L MIM = ? N i=1 m i ? P patch ? (u i ) T log P patch ? (? i ).<label>(3)</label></formula><p>We symmetrize the loss by averaging with another CE term betweenv patch s and v patch t .</p><p>The backbone together with the projection head of teacher network h patch t ? f t is, therefore, a visual tokenizer that generates online token distributions for each masked patch token. The tokenizer used in iBOT is jointly learnable to MIM objective without a need of being pre-trained in an extra stage, a bonus feature of which is now its domain knowledge can be distilled from the current dataset rather than fixed to the specified dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3:</head><p>Overview of iBOT framework, performing masked image modeling with an online tokenizer. Given two views u and v of an image x, each view is passed through a teacher network h t ? f t and a student network h s ? f s . iBOT minimizes two losses. The first loss L <ref type="bibr">[CLS]</ref> is selfdistillation between cross-view [CLS] tokens. The second loss L MIM is self-distillation between in-view patch tokens, with some tokens masked and replaced by e <ref type="bibr">[MASK]</ref> for the student network. The objective is to reconstruct the masked tokens with the teacher networks' outputs as supervision.</p><p>To ensure that the online tokenizer is semantically-meaningful, we perform self-distillation on [CLS] token of cross-view images such that visual semantics can be obtained via bootstrapping, as achieved by the majority of the self-supervised methods <ref type="bibr" target="#b19">(He et al., 2020;</ref><ref type="bibr" target="#b17">Grill et al., 2020;</ref><ref type="bibr">Caron et al., 2021)</ref>. In practice, iBOT works with L <ref type="bibr">[CLS]</ref> in Eq. (2) proposed in DINO <ref type="bibr">(Caron et al., 2021)</ref>, except that now we have? <ref type="bibr">[CLS]</ref> s instead of u <ref type="bibr">[CLS]</ref> s as input for the student network. To further borrow the capability of semantics abstraction acquired from self-distillatin on [CLS] token, we share the parameters of projection heads for [CLS] token and patch tokens, i.e., h [CLS]</p><formula xml:id="formula_5">s = h patch s , h [CLS] t = h patch t .</formula><p>We empirically find that it produces better results than using separate heads.</p><p>Unlike tokenized words whose semantics are almost certain, image patch is ambiguous in its semantic meaning. Therefore, tokenization as one-hot discretization can be sub-optimal for images. In iBOT, we use the token distribution after softmax instead of the one-hot token id as a supervisory signal, which plays an important role in iBOT pre-training as shown in Tab. 18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IMPLEMENTATION</head><p>Architecture. We use the Vision Transformers <ref type="bibr">(Dosovitskiy et al., 2021)</ref> and Swin Transformers <ref type="bibr" target="#b29">(Liu et al., 2021b)</ref> with different amounts of parameters, ViT-S/16, ViT-B/16, ViT-L/16, and Swin-T/{7,14} as the backbone f . For ViTs, /16 denotes the patch size being 16. For Swins, /{7, 14} denotes the window size being 7 or 14. We pre-train and fine-tune the Transformers with 224-size images, so the total number of patch tokens is 196. The projection head h is a 3-layer MLPs with l 2 -normalized bottleneck following DINO <ref type="bibr">(Caron et al., 2021)</ref>. Towards a better design to acquire visual semantics, we studied different sharing strategies between projection heads h <ref type="bibr">[CLS]</ref> and h patch , considering that semantics obtained in distillation on [CLS] token helps the training of MIM on patch tokens. We empirically find that sharing the entire head prompts the best performance. We set the output dimension of the shared head to 8192.</p><p>Pre-Training Setup. We by default pre-train iBOT on ImageNet-1K <ref type="bibr" target="#b13">(Deng et al., 2009</ref>) training set with AdamW <ref type="bibr" target="#b30">(Loshchilov &amp; Hutter, 2019)</ref> optimizer and a batch size of 1024. We pre-train iBOT with ViT-S/16 for 800 epochs, ViT-B/16 for 400 epochs, ViT-L/16 for 250 epochs, and Swin-T/{7,14} for 300 epochs. We also pre-train on ImageNet-22K training set with ViT-B/16 for 80 epochs and ViT-L/16 for 50 epochs. The learning rate is linearly ramped up during the first 10 epochs to its base value scaled with the total batch size: lr = 5e ?4 ? batch size/256. We use random MIM, with prediction ratio r set as 0 with a probability of 0.5 and uniformly sampled from range [0.1, 0.5] with a probability of 0.5. We sum L <ref type="bibr">[CLS]</ref> and L MIM up without scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>We first transfer iBOT to downstream tasks, following the standard evaluation protocols adopted in prior arts, the details of which are delayed in Appendix C. We then study several interesting   properties of Transformers pre-trained with iBOT. Finally, we give a brief ablation study on the crucial composing of iBOT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CLASSIFICATION ON IMAGENET-1K</head><p>We consider five classification protocols on ImageNet-1K: k-NN, linear probing, fine-tuning, semisupervised learning, and unsupervised learning.</p><p>k-NN and Linear Probing. To evaluate the quality of pre-trained features, we either use a k-nearest neighbor (k-NN) classifier or a linear classifier on the frozen representation. We follow the evaluation protocols in DINO <ref type="bibr">(Caron et al., 2021</ref>  . A linear probing accuracy of 79.5% with ViT-B/16 is comparable to 79.8% by SimCLRv2 with RN152 (3?) ? but with 10? less parameters. We underline that the performance gain over DINO gets larger (0.9% w/ ViT-S versus 1.3% w/ ViT-B) with more parameters, suggesting iBOT is more scalable to larger models.</p><p>Fine-Tuning. We study the fine-tuning on ImageNet-1K and focus on the comparison with selfsupervised methods for Transformers and its supervised baseline (Rand.) <ref type="bibr">(Touvron et al., 2021)</ref>. As shown in Tab. 2, iBOT achieves an 82.3%, 84.0%, and 84.8% top-1 accuracy with ViT-S/16, ViT-B/16, and ViT-L/16, respectively. As shown in Tab. 3, iBOT pre-trained with ImageNet-22K achieves 84.4% and 86.6% top-1 accuracy with ViT-B/16 and ViT-L/16, respectively, outperforming ImageNet-22K pre-trained BEiT by 0.7% and 0.6%. When fine-tuned on an image size of 512, we achieve 87.8% accuracy. We note that, with ViT-L/16, iBOT is 0.4% worse than BEiT using 1K data but 0.6% better using 22K data. This implies that iBOT requires more data to train larger model.</p><p>Semi-Supervised and Unsupervised Learning. For semi-supervised learning, we focus our comparison with methods following the unsupervised pre-train, supervised fine-tune paradigm. As   shown in Tab. 4, iBOT advances DINO by 1.6% and 0.8% using 1% and 10% data, respectively, suggesting a higher label efficiency. For unsupervised learning, we use standard evaluation metrics, including accuracy (ACC), adjusted random index (ARI), normalized mutual information (NMI), and Fowlkes-Mallows index (FMI). We compare our methods to SimCLRv2 (Chen et al., 2020b), Self-label , InfoMin <ref type="bibr" target="#b41">(Tian et al., 2020)</ref>, and SCAN (Van Gansbeke et al., 2020). As shown in Tab. 5, we achieve a 32.8% NMI, outperforming the previous state of the art by 1.8%, suggesting MIM helps the model learn stronger visual semantics on a global scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DOWNSTREAM TASKS</head><p>Object Detection and Instance Segmentation on COCO. Object detection and instance segmentation require simultaneous object location and classification.We consider Cascade Mask R-CNN <ref type="bibr" target="#b4">(Cai &amp; Vasconcelos, 2019;</ref><ref type="bibr" target="#b18">He et al., 2017</ref>) that produces bounding boxes and instance masks simultaneously on COCO dataset <ref type="bibr" target="#b27">(Lin et al., 2014)</ref>. Several recent works <ref type="bibr" target="#b29">(Liu et al., 2021b;</ref> proposes Vision Transformers that suit dense downstream tasks. To compare, we include the results of supervised Swin-T <ref type="bibr" target="#b29">(Liu et al., 2021b)</ref> which shares approximate parameter numbers with ViT-S/16 and its self-supervised counterpart MoBY  in Tab. 6. iBOT improves ViT-S's AP b from 46.2 to 49.4 and AP m from 40.1 to 42.6, surpassing both supervised Swin-T and its self-supervised counterpart by a nontrivial margin. With ViT-B/16, iBOT achieves an AP b of 51.2 and an AP m of 44.2, surpassing previous best results by a large margin.</p><p>Semantic Segmentation on ADE20K. Semantic segmentation can be seen as a pixel-level classification problem. We mainly consider two segmentation settings on ADE20K dataset <ref type="bibr" target="#b55">(Zhou et al., 2017)</ref>. First, similar to linear evaluation protocol in classification, we evaluate on the fixed patch features and only fine-tune a linear layer, which gives us a more explicit comparison of the quality of representations. Second, we use the task layer in UPerNet <ref type="bibr" target="#b50">(Xiao et al., 2018)</ref> and fine-tune the entire network. From Tab. 6, we can see that iBOT advances its supervised baseline with ViT-S/16 with a large margin of 0.9 on mIoU, surpassing Swin-T. With ViT-B/16, iBOT advances previous best methods DINO by 3.2 on mIoU with UperNet. We notice a performance drop of BEiT using linear head, indicating BEiT's features lack local semantics. As analyzed later, the property of strong local semantics induces a 2.9 mIoU gain compared to the supervised baseline with a linear head.</p><p>Transfer Learning. We study transfer learning where we pre-train on ImageNet-1K and fine-tune on several smaller datasets.We follow the training recipe and protocol used in <ref type="bibr">(Dosovitskiy et al., 2021)</ref>. The results are demonstrated in Tab. 7. While the results on several datasets (e.g., CIFAR10, CIFAR100, Flowers, and Cars) have almost plateaued, iBOT consistently performs favorably against other SSL frameworks, achieving state-of-the-art transfer results. We observe greater performance  gain over DINO in larger datasets like iNaturalist18 and iNaturalist19, indicating the results are still far from saturation. We also find that with larger models, we typically get larger performance gain compared with DINO (e.g., 1.7% with ViT/S-16 versus 2.0% with ViT-B/16 on iNaturalist18, and 0.3% with ViT/S-16 versus 1.0% with ViT-B/16 on iNaturalist19).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PROPERTIES OF VIT TRAINED WITH MIM</head><p>In the previous sections, we have shown the priority of iBOT on various tasks and datasets. To reveal the strengths of iBOT pre-trained Vision Transformers, we analyze its property from several aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">DISCOVERING THE PATTERN LAYOUT OF IMAGE PATCHES</head><p>What Patterns Does MIM Learn? The output from the projection head used for self-distillation depicts for patch token a probabilistic distribution. To help understand what patterns MIM induces to learn, we visualize several pattern layouts. We use 800-epoch pre-trained ViT-S/16 and visualize the top-36 patches with the highest confidence on ImageNet-1K validation set. We visualize a 5? context for each 16 ? 16 patch (colored orange). We observe the emergence of both high-level semantics and low-level details. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, several patches are grouped with clear semantic meaning, e.g., headlight and dog's ear. Such behavior stands a distinct contrast with the offline tokenizer used in BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref>, which encapsulates mostly low-level details as shown in <ref type="figure" target="#fig_1">Fig. 16</ref>. Apart from patch patterns that share high-level semantics, we also observe clusters accounting for low-level textures, indicating the diversity of learned part patterns. The comparison with previous work <ref type="bibr">(Caron et al., 2021;</ref><ref type="bibr" target="#b3">Bao et al., 2021)</ref> and the visualization of more pattern layouts are provided in Appendix G.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How Does MIM Help Image Recognition?</head><p>To illustrate how the property of better part semantics can help image recognition, we use part-wise linear classification to study the relationship between representations of patch tokens and [CLS] token. Specifically, we average k patch tokens with the top-k highest self-attention scores. The results are demonstrated in <ref type="figure" target="#fig_3">Fig. 5</ref>. While the performance gap between DINO and iBOT is only 0.9% in the standard setting (77.9% v.s. 77.0%) with [CLS] token, we observe that iBOT outperforms DINO when using the patch representations directly. We observe that using top-56 patch tokens yields an optimal result, and iBOT is 5.9% higher than DINO. The performance gap becomes more prominent when using fewer patch tokens. When using only the patch token with the highest self-attention score, iBOT advances by 17.9%. These results reveal much semantic information in iBOT representations for patch tokens, which helps the model to be more robust to the loss of local details and further boosts its performance on image-level recognition.   To analyze, we visualize the self-attention map with ViT-S/16. We choose [CLS] token as the query and visualize attention maps from different heads of the last layer with different colors, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. Of particular interest, we indicate that iBOT shows a solid ability to separate different objects or different parts of one object apart. For example, in the leftmost figure, we observe iBOT fairly distinct the bird from the tree branch. Also, iBOT focuses mainly on the discriminative parts of the object (e.g., the wheel of the car, the beak of the bird). These properties are crucial for iBOT to excel at image recognition, especially in complicated scenarios with object occlusion or distracting instances. While these properties are not unique strengths brought by MIM and we observe similar behaviors in DINO, we show in Appendix G.2 that iBOT generally gives better visualized results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">ROBUSTNESS</head><p>The  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Visual Representation Learning. Most self-supervised methods assume an augmentation invariance of images and achieve so by enforcing similarity over distorted views of one image while avoiding model collapse. Avoiding collapse can be achieved by noise-contrastive estimation with negative samples <ref type="bibr" target="#b48">(Wu et al., 2018;</ref><ref type="bibr" target="#b19">He et al., 2020;</ref><ref type="bibr" target="#b8">Chen et al., 2020a)</ref>, introducing asymmetric network <ref type="bibr" target="#b17">(Grill et al., 2020;</ref><ref type="bibr">Chen &amp; He, 2021)</ref>, or explicitly enforcing the distribution of image distribution over the channel to be uniform as well as one-hot <ref type="bibr" target="#b6">(Caron et al., 2020;</ref><ref type="bibr" target="#b0">Amrani &amp; Bronstein, 2021;</ref><ref type="bibr">Caron et al., 2021)</ref>. In fact, the idea of simultaneously enforcing distribution uniform and one-hot is hidden from earlier studies performing representation learning via clustering <ref type="bibr" target="#b5">(Caron et al., 2018;</ref><ref type="bibr" target="#b20">2020;</ref><ref type="bibr" target="#b53">YM. et al., 2020)</ref>, where the cluster assignment naturally meets these two requirements. Other methods rely on handcrafted pretext tasks and assume the image representation should instead be aware of image augmentation by solving image jigsaw puzzle <ref type="bibr" target="#b33">(Noroozi &amp; Favaro, 2016;</ref>, predicting rotation <ref type="bibr" target="#b24">(Komodakis &amp; Gidaris, 2018)</ref> or relative position <ref type="bibr" target="#b15">(Doersch et al., 2015)</ref>.</p><p>Masked Prediction in Images. Predicting masked images parts is a popular self-supervised pretext task drawing on the idea of auto-encoding and has been previously achieved by either recovering raw pixels <ref type="bibr" target="#b34">(Pathak et al., 2016;</ref><ref type="bibr" target="#b2">Atito et al., 2021;</ref> or mask contrastive learning <ref type="bibr" target="#b20">(Henaff, 2020;</ref><ref type="bibr" target="#b54">Zhao et al., 2021)</ref>. Recently, it is formulated into MIM <ref type="bibr" target="#b3">(Bao et al., 2021;</ref><ref type="bibr" target="#b40">Tan et al., 2021)</ref> with a discrete VAE <ref type="bibr" target="#b37">(Rolfe, 2017;</ref><ref type="bibr">Ramesh et al., 2021)</ref> as visual tokenizer. As a counterpart of MLM in NLP, MIM eases masked prediction into a classification problem supervised by labels output from the tokenizer, mitigating the problem of excessive focus on high-frequency details. Concurrently, masked image prediction has been explored in the field of multi-modality, i.e., visionlanguage representation learning. These methods operate on local regions instead of global images thus reply on pre-trained detection models, i.e., Faster- <ref type="bibr">RCNN (Ren et al., 2015)</ref> to propose regions of interest. <ref type="bibr">(Su et al., 2020;</ref><ref type="bibr" target="#b31">Lu et al., 2019;</ref><ref type="bibr" target="#b12">Chen et al., 2020c)</ref> perform masked region classification tasking the category distribution output from the detection model as the ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we study BERT-like pre-training for Vision Transformers and underline the significance of a semantically meaningful visual tokenizer. We present a self-supervised framework iBOT that performs masked image modeling via self-distillation with an online tokenizer, achieving stateof-the-art results on downstream tasks related to classification, object detection, instance segmentation, and semantic segmentation. Of particular interest, we identify an emerging part-level semantics for models trained with MIM that helps for not only recognition accuracy but also robustness against common image corruptions. In the future, we plan to scale up iBOT to a larger dataset (e.g., ImageNet-22K) or larger model size (e.g., ViT-L/16 and ViT-H/16) and investigate whether MIM can help Vision Transformers more scalable to unlabelled data in the wild. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PSEUDOCODE</head><formula xml:id="formula_6">// [n, K], [n, S 2 , K] v [CLS] s ,v patch s = g s (v, return all tok=true) ; // [n, K], [n, S 2 , K] u [CLS] t , u patch t = g t (u, return all tok=true) ; // [n, K], [n, S 2 , K] v [CLS] t , v patch t = g t (v, return all tok=true) ; // [n, K], [n, S 2 , K] L [CLS] = H(? [CLS] s , v [CLS] t , C, ? s , ? t ) / 2 + H(v [CLS] s , u [CLS] t , C, ? s , ? t ) / 2 L MIM = (m u ? H(? patch s , u patch t , C , ? s , ? t ).sum(dim=1) / m u .sum(dim=1) / 2 + (m v ? H(v patch s , v patch t , C , ? s , ? t ).sum(dim=1) / m v .sum(dim=1) / 2 (L [CLS]</formula><p>.mean() +L MIM .mean()).backward() update(g s ) ;</p><p>// student, teacher and center update g t .params = l? g t .params +(1 ? l)? g s .params</p><formula xml:id="formula_7">C = m ? C + (1 ? m)? cat([u [CLS] t , v [CLS] t ]).mean(dim=0) C = m ? C + (1 ? m )? cat([u patch t , v patch t ]).mean(dim=(0, 1)) end def H(s, t, c, ? s , ? t ): t = t.detach(); // stop gradient s = softmax(s / ? s , dim=1) t = softmax((t ? c) / ? t , dim=1);</formula><p>// center + sharpen return ?(t? log(s)).sum(dim=-1);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MULTI-CROP</head><p>The advanced performance of several recent state-of-the-art methods <ref type="bibr">(Caron et al., 2021;</ref><ref type="bibr" target="#b20">2020)</ref> relies on multi-crop augmentation, as well as iBOT. In our early experiments, we find the direct usage of multi-crop augmentation leads to instability issues that degrade accuracy. We reveal that these results can be attributed to the distribution mismatch between masked images and non-masked images and can be resolved by minimal changes in iBOT framework.</p><p>Stability of MIM Pre-trained with Multi-Crop. We first showcase several practices where training instability occurs, shown in <ref type="figure">Fig. 7</ref>. To reveal the instability, we monitor the NMI curves during training for each epoch as shown in <ref type="figure">Fig. 8</ref>. The most intuitive ideas are to compute as (b) or (c). In (b), MIM is only performed on global crops. This pipeline is unstable during training, and we observe a dip in the NMI training curve. We hypothesize that it can be caused by the distribution mismatch of masked global crops and non-masked local crops. To alleviate this, a straightforward solution is to also perform MIM on local crops with an extra computation cost as (c). However, we do not observe this circumvents training instability. We hypothesize that the regions corresponding to patch tokens of the local crops are small in size, in which there exist few meaningful contents to predict. This hypothesis can be supported by the experiments that when we set the local crop scale in (c) from (0.05, 0.4) to (0.2, 0.4), denoted as (e), the performance drop is mitigated.</p><formula xml:id="formula_8">! ! ! " ! " ! ! ! " ! " ! " # ? ! ! ! " ! " ! ! ! " ! $ ? ! " $ ? ! ! " ! " ! " # ? student teacher ? %&amp;% ? [()*] global view , local view , (a) (b) (c) (d)</formula><p>Stabilizing the Training with Non-Masked Global Crops. Another solution to alleviate the distribution mismatch between masked global crops and non-masked local crops is to train with nonmasked global crops, as shown in (d). In other words, we perform random MIM when training ViT with multi-crop augmentation. This computation pipeline is stable and achieves a substantial performance gain. In practice, to include non-masked global crops in training, we use (b) and randomly choose a prediction ratio between [0, r (r &gt; 0)] for each image. When the ratio 0 is chosen, the whole framework excludes MIM and can be seen as DINO. When the ratio r (r &gt; 0) is chosen, MIM is performed for both of the two global crops. We observe the latter practice performs sightly better since it is more flexible in task composition and data in a batch is mutually independent.</p><p>Range of Scales in Multi-Crop. We further study the performance with different local and global scale. Following DINO <ref type="bibr">(Caron et al., 2021)</ref>, we conduct the experiments by tweaking s, where s is the scale deviding the local and global crops. The local crops are sampled from (0.05, s) and the global crops are sampled from (s, 1).  We empirically find that s = 0.32 yields optimal performance for both small-size and base-size models. Therefore, we use an s of 0.32 by default.</p><p>State-of-the-Art Comparison w/o and w/ Multi-Crop. Including iBOT, several recent state-ofthe-art works <ref type="bibr">(Caron et al., 2021;</ref><ref type="bibr" target="#b20">2020)</ref> rely heavily on multi-crop augmentation during pre-training. Except for several specific self-supervised methods <ref type="bibr" target="#b17">(Grill et al., 2020)</ref>, multi-crop works well on most of the self-supervised methods and consistently yields performance gain <ref type="bibr">(Caron et al., 2021)</ref>. While a more fair comparison with our methods without multi-crop augmentation can be conducted, we believe it is a unique strength of iBOT to work well with multi-crop. In Tab. 10, we categorize the state-of-the-art comparison into two parts where one for methods without multi-crop and the other with multi-crop. For the former, we mainly compare our method without multi-crop with MoCov3 <ref type="bibr">(Chen et al., 2021)</ref> and DINO without multi-crop. We observe that our method achieves state-of-theart performance with ViT-S/16 even without multi-crop and comparable performance with ViT-B/16 compared with MoCov3. For the latter, we mainly compare our method with SwAV <ref type="bibr" target="#b6">(Caron et al., 2020)</ref> and DINO with multi-crop augmentation. We observe that iBOT achieves higher performance with 79.4% of linear probing accuracy when using ViT-S/16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effective Training</head><p>Epochs. Due to extra computation costs brought by multi-crop augmentation, different methods with the same pre-training epochs actually see different total numbers of images. To mitigate, we propose to measure the effective training epochs, defined as actual pre-training epochs multiplied with a scaling factor accounting for extra trained images of different resolutions induced by multi-crop augmentation. DINO and iBOT are by default trained with 2 global crops of size 224 ? 224 and 10 local crops of size 96 ? 96. Thus r = 2 + ( 96 224 ) 2 ? 10 = 3.84 ? 4 for DINO and iBOT. r ? 3 for SwAV or DINO with RN50 as the backbone and pre-trained with 2 global crops and 6 local crops. r = 2 for contrastive methods without multi-crop augmentation (e.g., MoCo, SimCLR, BYOL, etc.) and r = 1 for non-contrastive methods (e.g., BEiT, Jigsaw, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL IMPLEMENTATIONS</head><p>Fine-Tuning Recipes of Classification on ImageNet-1K. By default, we follow the fine-tuning protocol in BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref> to use a layer-wise learning rate decay, weight decay and AdamW optimizer and train small-, base-size models with 200, 100, and 50 epochs respectively. We sweep over four learning rates {8e ?4 , 9e ?4 , 1e ?3 , 2e ?3 }. Comparatively, traditional fine-tuning recipe is is to fine-tune the network for 300 epochs with a learning rate 5e ?4 , no weight decay, and SGD optimizer <ref type="bibr">(Touvron et al., 2021)</ref>  <ref type="figure" target="#fig_1">(Row 1 versus 8)</ref>. For a fair comparison, we compare the impact of different fine-tuning recipes with different methods, shown in Tab. 11. We empirically find that fine-tuning protocol used in BEiT consistently yields better fine-tuning results and greatly reduces the training epochs. By default, we use a layerwise decay of 0.75 with a training epoch of 200 for ViT-S/16, a layerwise decay of 0.65 with a training epoch of 100 for ViT-B/16, and a layerwise decay of 0.75 with a training epoch of 50 for ViT-L/16. We report the higher results between using or not using DS since we find it brings different impacts to different methods.</p><p>Evaluation Protocols of Semi-Supervised Learning on ImageNet-1K. We study the impact of different evaluation protocols for semi-supervised learning. Under conventional semi-supervised evaluation protocol, pre-trained models are end-to-end fine-tuned with a linear classification head.  SimCLRv2 <ref type="bibr" target="#b9">Chen et al. (2020b)</ref> found that keeping the first layer of the projection head can improve accuracy, especially under the low-shot setting. We fine-tune the pre-trained model from the first layer of the projection head and verify this conclusion holds true for Vision Transformers. We empirically find that Vision Transformer performs better with a frozen backbone with 1% of training data (62.5% in row 4 versus 61.9 % in row 7). In DINO, a logistic regressor built upon the frozen features is found to perform better compared with the multi-class linear classifier upon the frozen features, especially with 1% data (65.9% in row 6 versus 62.5% in row 4). When using 10% data, we empirically find that end-to-end fine-tuning from the first layer of the projection layer yields the best performance (75.1% in row 10 versus 73.4% in row 6).</p><p>Fine-Tuning Recipes of Object Detection and Instance Segmentation on COCO. For both small-and base-size models, we utilize multi-scale training (resizing image with shorter size between 480 and 800 while the longer side no larger than 1333), a learning rate 1e ?4 , a weight decay of 0.05, and fine-tune the entire network for 1? schedule (12 epochs with the learning rate decayed by 10? at epochs 9 and 11). We sweep a layer decay rate of {0.65, 0.75, 0.8, 0.9}. Note that a layer decay rate of 1.0 denotes no layer is decayed. To produce hierarchical feature maps, we use the features output from layer 4, 6, 8, and 12, with 2 deconvolutions, 1 deconvolution, identity mapping, and max-pooling appended after, respectively. We do not use multi-scale testing.</p><p>Fine-Tuning Recipes of Semantic Segmentation on ADE20K. For semantic segmentation, we follow the configurations in BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref>, fine-tuning 160k iterations with 512 ? 512 images and a layer decay rate of 0.65. We do not use multi-scale training and testing. We sweep the learning rate {3e ?5 , 8e ?5 , 1e ?4 , 3e ?4 , 8e ?4 }. Similar to object detection and instance segmentation, to produce hierarchical feature maps, we add additional deconvolution layers after ViT.</p><formula xml:id="formula_9">DINO, w/o [LN] DINO, w/ [LN] iBOT, w/o [LN] iBOT, w/ [LN]</formula><p>33.7 34.5 37.8 38.3 When using linear (Lin.) as the task layer, we find that appending the last LayerNorm ([LN]) for [CLS] token to each patch tokens before the decoder consistently yields better performance, while we do not spot the substantial gain when with UperNet as the task layer. By default, we report the segmentation result with [LN] for both linear head for UperNet head.</p><p>Part-Wise Linear Probing. We use the average of the last-layer self-attention map with [CLS] as the query from multiple heads to rank all the patch tokens. We remove the extra LayerNorm (LN) after the final block following MoCov3 <ref type="bibr">(Chen et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL RESULTS</head><p>In this section, we provide detailed results for dense downstream tasks, i.e., object detection, instance segmentation, and semantic segmentation. We give the complete figures for occlusion robustness    analysis. We also provide extra experiments of nearest neighbor retrieval, robustness analysis against occlusion and shuffle.</p><p>Object Detection, Instance Segmentation, and Semantic Segmentation. We here provide more detailed results on object detection, instance segmentation, and semantic segmentation with smalland base-size models, shown in Tab. 13 and Tab. 14 respectively. Specifically, we include AP b 50 and AP b 75 for object detection, AP m 50 and AP m 75 for instance segmentation, mAcc for semantic segmentation. For object detection (Det.) and instance segmentation (Inst. Seg.), we consider Cascade Mask R-CNN as the task layer. For semantic segmentation (Seg.), we consider two evaluation settings where a linear head <ref type="bibr">(Lin.)</ref> and UPerNet are taken as the task layer.</p><p>k-NN and Linear Probing with ImageNet-22K. We further report k-NN and linear probing accuracy on ImageNet-1K with models pre-trained on ImageNet-22K dataset. We empirically observe that ImageNet-1K pre-training incurs better ImageNet-1K k-NN and linear probing performance, which is opposite to the fine-tuning performance observed in <ref type="table" target="#tab_1">Table 2 and Table 3</ref>. We hypothesize that the data distribution plays a more crucial rule under evaluation protocols based on frozen features, such that models pre-trained with smaller ImageNet-1K dataset consistently achieve better results. Nearest Neighbor Retrieval. Nearest neighbor retrieval is considered using the frozen pre-trained features following the evaluation protocol as in DINO <ref type="bibr">(Caron et al., 2021)</ref>. DINO has demonstrated the strong potential of pre-trained ViT features to be directly used for retrieval. To validate, DINO designed several downstream tasks, including image retrieval and video object segmentation, where video object segmentation can be seen as a dense retrieval task by finding the nearest neighbor between consecutive frames to propagate masks. We compare iBOT with DINO on these benchmarks with the same evaluation settings. As demonstrated in Tab. 16, iBOT has comparable results with DINO. While iBOT has higher k-NN results on Imagenet-1K, the performance is not better for iBOT in image retrieval. We empirically find that the results on image retrieval are sensitive to image resolution, multi-scale features, etc., and the performance varies using pre-trained models with minimal differences on hyper-parameter setup. For this reason, we do not further push iBOT for better results.</p><p>Robustness against Background Change. Deep models rely on both foreground objects and backgrounds. Robust models should be tolerant to background changes and able to locate discriminative foreground parts. We evaluate this property on ImageNet-9 (IN-9) dataset <ref type="bibr" target="#b49">(Xiao et al., 2020)</ref>. Robustness against Occlusion. Masked prediction has a natural strength in cases where parts of the image are masked out since the models are trained to predict their original contents. We here provide the detailed results of occlusion with different information loss ratios in <ref type="figure" target="#fig_6">Fig. 9</ref> under three dropping settings: random, salient, and non-salient. We showcase the results of iBOT end-to-end fine-tuned or with a linear head over the pre-trained backbone. We include the results of supervised results with both ViT-S/16 and ResNet-50 for comparison. ViT shows higher robustness compared to its CNN counterpart, i.e., ResNet-50, given that Transformers' dynamic receptive field makes it less dependent on images' spatial structure. We empirically find iBOT has stronger robustness against occlusion compared to its supervised baseline, consolidating that MIM help to model the interaction between the sequence of image patches using self-attention such that discarding proportion of elements does not degrade the performance significantly.</p><p>Robustness against Shuffle. We study the model's sensitivity to the spatial structure by shuffling on input image patches. Specifically, we shuffle the image patches with different grid sizes following <ref type="bibr" target="#b32">(Naseer et al., 2021)</ref>. We showcase the results of iBOT end-to-end fine-tuned or with a linear head over the pre-trained backbone. We include the results of supervised results with both ViT-S/16 and ResNet-50 for comparison. Note that a shuffle grid size of 1 means no shuffle, and a shuffle grid size of 196 means all patch tokens are shuffled. <ref type="figure" target="#fig_1">Fig. 10</ref> suggests that iBOT retain accuracy better than its supervised baseline and ResNet-50. It also indicates that iBOT relies less on positional embedding to preserve the global image context for right classification decisions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL ABLATIONS</head><p>In this section, we study the impact of other parameters that we have conducted experiments on. Without extra illustrations, we use 300-epoch pre-trained ViT-S/16, a prediction ratio r = 0.3 and without multi-crop augmentation for the ablative study.</p><p>[CLS] patch   Architecture of Projection Head. As mentioned earlier, a shared head can transfer the semantics acquired in [CLS] token to patch tokens, slightly improving the performance. We notice that the head for patch tokens in the student network only see the masked tokens throughout the training, the distribution of which mismatches tokens with natural textures. Therefore, we conduct an experiment using a non-shared head for the student network but a shared head for the teacher network denoted as semi-shared head. Their differences are demonstrated in <ref type="figure" target="#fig_1">Fig. 17</ref>, where S and T denotes student and teacher network respectively. The heads with the same index and color denotes they have shared parameters.</p><p>Arch. vanilla shared ? sm. shared sm. shared ? shared k-NN . 68.9 68.0 68.4 68.4 69.1 Lin.</p><p>73.9 73.7 73.7 73.8 74.2 ? denotes only the first 2 layers out of the 3-layer MLP share the parameters. However, we do not observe that semi-shared head is better than shared head. By default, we share the entire projection head for [CLS] token and patch tokens.</p><p>Comparing MIM with Dense Self-Distillation. To identify the superiority of MIM to model internal structure using over its alternatives, we conduct experiments performing self-distillation on original patch tokens along with the [CLS] token. We consider two matching strategies to construct patch token pairs for self-distillation.</p><p>Arch. DINO DINO + pos. DINO + feat.</p><p>iBOT k-NN 67.9 67.1 (?0.8) 68.5 (+0.6) 69.1 (+1.2) Lin.</p><p>72.5 72.5 (+0.0) 73.4 (+0.9) 74.2 (+1.7) Specifically, pos. denotes matching according to the absolute position of two views. Similar to <ref type="bibr" target="#b52">Xie et al. (2021b)</ref>. j is defined as arg min j dist(p i , p j ), where p is the position in the original image space and dist(u, v) is euclidean distance. The losses are only computed for the overlapped regions of two views. We do not observe substantial gain brought by matching via patches' absolute position. feat. denotes matching according to the similarity of the backbone similarity of two views. Similar to <ref type="bibr" target="#b45">Wang et al. (2021b)</ref>, we match for each patch token f i the most similar patch token from another view f j , where j = arg max j sim(f i , f j ). sim(u, v) is cosine distance. Such practice brings a 0.6% performance gain in terms of linear probing accuracy, which is also observed by a concurrent work, EsViT . Comparatively, iBOT prompts an 1.2% gain on linear probing, verifying the necessity and advancement of MIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hard Label versus Soft Label</head><p>We study the importance of using a continuous token distribution (softmax ? ) instead of a discretized id (hardmax) when performing MIM. Results in Tab. 18 indicate continuous tokenization plays a crucial part. We empirically find the improvement brought by centering, whose roles are less important compared to centering in self-distillation on [CLS] token. Only sharpening can produce a k-NN accuracy of 69.4 and a linear probing accuracy of 73.9.</p><p>Centering and Sharpening. Different from the [CLS] token, patch tokens do not have certain semantic cluster and vary more widely from each others. We study the impact of several critical parameters that decide the distillation process and customize them for distillation over the patch tokens. Loss Ratio. We study the impact of different ratio between L [CLS] and L MIM . We keep the base of L <ref type="bibr">[CLS]</ref> to 1 and scale L MIM with different ratios.</p><p>L <ref type="bibr">[CLS]</ref> / L MIM 0.5 2 1 k-NN 68.7 69.4 69.1 Lin.</p><p>73.8 74.1 74.2 We observe that directly adding two losses up without scaling yields the best performance in terms of linear probing accuracy.</p><p>Output Dimension. We follow the structure of projection head in DINO with l2-normalized bottleneck and without batch normalization. We study the impact of output dimension K of the last layer. 74.5 74.0 74.2 While our method excludes large output dimensionality since each patch token has an output distribution, we do not observe substantial performance gain brought by larger output dimensions. Therefore, we choose K = 8192 by default.</p><p>Prediction Ratios. Masked modeling is based on a formulation of partial prediction, the objective of which is to maximize the log-likelihood of the target tokens conditioned on the non-target tokens. We experiment with different prediction ratios for masked image modeling. The results are shown in <ref type="figure" target="#fig_1">Fig. 11</ref>. We observe that the performance is not sensitive to variant prediction ratios between 0.05 and 0.4. Adding a variance upon the fixed value can also consistently bring a performance gain, which can be explained as stronger data augmentation. The teacher output of non-masked images is now pulled together with the student output of masked images with different ratios. By default, we use 0.3 (?0.2) as the prediction ratio. For models with multi-crop augmentation, following the above discussions, we randomly choose a prediction of 0 or 0.3 (?0.2) for each image.  <ref type="figure" target="#fig_1">Figure 11</ref>: Impact of the prediction ratio. ? denotes to randomly sample from a region.   Training Epochs. We provide the linear probing top-1 accuracy with ViT-S/16 pre-trained for different epochs. For comparison, we also include the accuracy curve of other methods with comparable numbers of parameters, i.e., ResNet-50. <ref type="figure" target="#fig_1">From Fig. 12</ref>, we observe that longer training for 800 epochs can improve the model's performance. It's north worthy that iBOT can achieve a Top-1 accuracy of SwAV <ref type="bibr" target="#b6">(Caron et al., 2020)</ref> pre-trained with 800 epochs in less than 100 epochs. iBOT pre-trained with 800 epochs brings a 0.9% improvement over previous state-of-the-art method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ALTERNATIVE TOKENIZERS</head><p>To investigate how different approaches to tokenize the patches affect MIM, we study several alternatives. In BEiT <ref type="bibr" target="#b3">(Bao et al., 2021)</ref>, masked patches are tokenized by a DALL-E encoder. MPP <ref type="bibr">(Dosovitskiy et al., 2021)</ref> tokenizes the masked patches using their 3-bit mean color. For Patch Clustering, we first perform K-Means algorithm to the flattened color vector of each 16 ? 16 patch (d = 768). 10% data of ImageNet-1K training set is sampled and clustered. We set K to 4096. During pre-training, each patch is tokenized by the index of its closest centroids. Lastly, we use 300epoch pre-trained DINO as a standalone tokenizer. Each patch can be tokenized by the argmax of its output from the pre-trained DINO. We use average pooling to aggregate the patch representations. From Tab. 20, we see that all methods achieve decent fine-tuning results compared to the supervised baseline, while only methods tokenized by semantically meaningful tokenizer have proper results on k-NN and linear classification. MPP <ref type="bibr">(Dosovitskiy et al., 2021)</ref> and patch clustering rely purely on offline statistics without the extra stage of online training. We find patch clustering has slightly better performance in all three protocols compared to MPP, suggesting the benefits brought by visual semantics. While BEiT has poor k-NN and linear probing accuracy, a good fine-tuning result also suggests relatively low requirements for fine-tuning protocol on high-level semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G VISUALIZATION</head><p>In this section, we first give more visualized pattern layouts and self-attention maps. Beyond that, we consider an additional task of mining sparse correspondences between two images and illustrating the superiority of ViTs by showcasing several visualized results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 PATTERN LAYOUT</head><p>Pattern Layout for Patch Tokens. To illustrate versatile, interesting behaviors iBOT has learned, we organize the visualization of pattern layout in two figures. In <ref type="figure" target="#fig_1">Fig. 13</ref>, we mainly showcase additional pattern layouts that share high-level semantics. In <ref type="figure" target="#fig_1">Fig. 14,</ref> we mainly showcase additional pattern layouts that share low-level details like color, texture, shape, etc. Top 100 patches with the highest confidence over the validation set are visualized with a 5 ? 5 context around each 16 ? 16 patch token (colored orange).</p><p>Composing Images with Representative Patterns. In <ref type="figure" target="#fig_1">Fig. 15</ref>, we visualize 4 patches with the highest self-attention score (with non-overlapped assigned index) and also show the pattern layout of that assigned index. The visualized results indicate iBOT can only be represented by several representative patches, which helps the model's robustness and performance in recognition. This is also validated by our part-wise linear probing experiments.</p><p>Comparison with Other Methods. We visualize pattern layout for patch tokens using other selfsupervised methods <ref type="bibr" target="#b3">(Bao et al., 2021;</ref><ref type="bibr">Caron et al., 2021)</ref> in <ref type="figure" target="#fig_1">Fig. 16</ref>. For BEiT, the DALL-E encoder generates a discrete number for each patch token. For DINO, we directly use the projection head for [CLS] token and generate a 65536-d probability distribution for each patch token. The index with the highest probability is assigned for the token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern Layout for [CLS]</head><p>Token. We here also provide additional visualization of semantic patterns emerge in [CLS] token, which is obtained via self-distillation on cross-view images. We also observe similar behavior in DINO since it's not a unique property brought by MIM. In fact, semantics are now believed to emerge as long as a similarity between two distorted views of one image is enforced <ref type="bibr" target="#b17">(Grill et al., 2020;</ref><ref type="bibr" target="#b19">He et al., 2020;</ref><ref type="bibr" target="#b6">Caron et al., 2020;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 SELF-ATTENTION VISUALIZATIONS</head><p>Similar to the setting of Sec. 4.3.2, we here provided more self-attention map visualization from multiple heads of the last layer in <ref type="figure" target="#fig_1">Fig. 18</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 SPARSE CORRESPONDENCE.</head><p>We consider a sparse correspondence task where the overlapped patches from two augmented views of one image, or patches from two images labeled as one class, are required to be matched. The correlation is sparse since at most 14 ? 14 matched pairs can be extracted with a ViT-S/16 model. We visualize 12 correspondences with the highest self-attention score extracted from iBOT with ViT-S/16 pre-trained for 800 epochs. The score is averaged between multiple heads of the last layer. Several sampled sets of image pairs are shown in <ref type="figure" target="#fig_1">Fig. 19</ref>. We observe empirically that iBOT perform well for two views drawn from one image, nearly matched the majority of correspondence correctly. In the second column, iBOT can match different parts of two instances from the same class (e.g., tiles and windows of two cars) despite their huge differences in texture or color. We observe the DINO also has comparable visualized effects, illustrating the representation pre-trained with self-distillation also suits well for retrieval in a patch-level scale. <ref type="figure" target="#fig_1">Figure 13</ref>: Visualization for pattern layout of patch tokens that share high-level semantics. In the first row, we visualize different human-related semantic parts. We observe clear patterns accounting for human hair, human shoulder &amp; arm, and human elbow respectively in the left, middle, and right figure. In the figures from the second row and the left figure from the the third row, we visualize animal-related semantic parts. dog's ear, dog's nose, bird's wing, and dragonfly's wing can be observed. In the rest of figures from the third row, we visualize semantic parts related to outdoor scenes. front window of the vehicle and window of the architecture can be observed. In the last row, we visualize indoor objects like ceiling and glass bottle. <ref type="figure" target="#fig_1">Figure 14</ref>: Visualization for pattern layout of patch tokens that share low-level details. In the first two columns, we visualize patches that share similar textures. In the first figure, fur of leopard and the skin of lizard share a similar dotted texture. In the second figure, shell of hedgehog and the skin of elephant share similar striped texture. In the third column, we visualize pattern layouts related to shape. For example, the shape of objects in the left and middle figures share similar curvature. The rightmost patterns clearly depict the shape of a straight line. We visualize pattern layout related to color in the last column, where blue, green and white can be observed. <ref type="figure" target="#fig_1">Figure 16</ref>: Visualization for pattern layout of patch tokens using BEiT (top) and DINO (bottom). In the layout extracted from the DALL-E encoder, we observe minimal semantic patterns. In most cases, patches with similar color (e.g., black area in left figure) or texture (e.g., line in right figure) are clustered. In the layout extracted from DINO, while more complex textures are visible, most patches share similar local details instead of high-level semantics. In the right figure, the semantic part eyes can be somehow observed, yet it is mixed with plenty of irrelevant semantic parts. <ref type="figure" target="#fig_1">Figure 17</ref>: Visualization for pattern layout of [CLS] token. We here indicate the high quality of semantic layout brought by self-distillation of cross-view images on [CLS] token. This property is not brought by MIM and is also prominent in DINO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DINO BEiT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DINO iBOT</head><p>iBOT <ref type="figure" target="#fig_1">Figure 18</ref>: Visualization for self-attention map from Multiple Heads. In the first 8 columns, we showcase iBOT's attention map along with DINO's. In the last 10 columns, we showcase more attention map from iBOT. We indicate that iBOT shows visually stronger ability to separate different objects or different parts of one object apart by giving more attentive visualized results for each part, compared with DINO. For example, in the fifth column, there is an attention head in iBOT accounting for the ear of the fox solely, while in DINO, it emerges with other parts; In the eighth column, iBOT separates the mushroom into more semantically meaningful parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correspondence between two views of one image</head><p>Correspondence between two images of one class <ref type="figure" target="#fig_1">Figure 19</ref>: Visualization for sparse correspondence. The top panel are images pairs sampled from two views of one image. The extracted correspondence from iBOT is mostly correct despite augmentations on scale and color. The bottom panel are image pairs sampled from two images of one class. The first row is images with salient objects but different sizes, positions and textures. The second row are images draw from animals, and we can observe more clearly that iBOT matches the semantic parts of animals correctly (e.g., tails of the fox, beak of the bird). The third row is humancentered images with human bodies or clothing. The fourth row is natural or domestic scenes where salient objects are invisible. Although no explicit semantic parts can be matched visible to human's understanding, we can still observe the iBOT can extract correspondence based on their texture or color (e.g., wooden texture of signboard and boxes. All these visual results demonstrate strong capability for iBOT in part retrieval or matching in a local scale.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Linear probing accuracy on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Pattern layout of patch tokens. Two left figures showcase patterns, headlight of the vehicle and ear of the dog, that share part semantics. Two right figures showcase patterns, stripped and curly surface, that share part textures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Part-wise linear probing accuracy. Top-k tokens with the highest attention scores are averaged for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Visualization for self-attention map. Self-attention map from multiple heads are visualized with different color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Computation pipelines for iBOT with or without multi-crop augmentation. (a) iBOT w/o multi-crop augmentation. (b), (c), and (d) are three pipelines w/ multi-crop augmentation. (b) does not perform MIM for local crops, whereas (c) performs MIM for all crops. (d) only performs MIM for one of the two global crops. iBOT uses (b) with random MIM. Training curves of different multi-crop strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Robustness against occlusion. Model's robustness against occlusion with different information loss ratios is studied. 3 patch dropping settings: Random Patch Dropping (left), Salient Patch Dropping (middle), and Non-Salient Patch Dropping (right) are considered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>IN-9 includes 9 coarse-grained classes and 7 variants by mixing up the foreground and background from different images. Only-FG (O.F.), Mixed-Same (M.S.), Mixed-Rand (M.R.), and Mixed-Next (M.N.) are 4 variant datasets where the original foreground is present but the background is modified, whereas No-FG (N.F.), Only-BG-B (O.BB.), and Only-BG-T (O.BT.) are 3 variants where the foreground is masked. As shown in Tab. 8, we observe a performance gain except for O.BT., indicating iBOT's robustness against background changes. We note in O.BT. neither foreground nor foreground mask is visible, contradicting the pre-training objective of MIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Robustness against shuffle. Model's robustness against shuffle with different grid shuffle sizes is studied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Impact of the training epochs. Models are ViT-S/16 with multi-crop augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>k-NN and linear probing on ImageNet-1K. ? denotes using selective kernel. ? denotes pretraining on ImageNet-22K.</figDesc><table><row><cell cols="2">Method Arch.</cell><cell cols="3">Par. im/s Epo. 1 k-NN Lin.</cell></row><row><cell cols="2">SSL big ResNets</cell><cell></cell><cell></cell></row><row><cell cols="2">MoCov3 RN50</cell><cell>23 1237 1600</cell><cell>-</cell><cell>74.6</cell></row><row><cell>SwAV</cell><cell>RN50</cell><cell cols="3">23 1237 2400 65.7 75.3</cell></row><row><cell>DINO</cell><cell>RN50</cell><cell cols="3">23 1237 3200 67.5 75.3</cell></row><row><cell>BYOL</cell><cell cols="4">RN200w2 250 123 2000 73.9 79.6</cell></row><row><cell cols="5">SCLRv2 RN152w3  ? 794 46 2000 73.1 79.8</cell></row><row><cell cols="2">SSL Transformers</cell><cell></cell><cell></cell></row><row><cell cols="2">MoCov3 ViT-S/16</cell><cell>21 1007 1200</cell><cell>-</cell><cell>73.4</cell></row><row><cell cols="2">MoCov3 ViT-B/16</cell><cell>85 312 1200</cell><cell>-</cell><cell>76.7</cell></row><row><cell>SwAV</cell><cell>ViT-S/16</cell><cell cols="3">21 1007 2400 66.3 73.5</cell></row><row><cell>DINO</cell><cell>ViT-S/16</cell><cell cols="3">21 1007 3200 74.5 77.0</cell></row><row><cell>DINO</cell><cell>ViT-B/16</cell><cell cols="3">85 312 1600 76.1 78.2</cell></row><row><cell>EsViT</cell><cell>Swin-T/7</cell><cell cols="3">28 726 1200 75.7 78.1</cell></row><row><cell>EsViT</cell><cell cols="4">Swin-T/14 28 593 1200 77.0 78.7</cell></row><row><cell>iBOT</cell><cell>ViT-S/16</cell><cell cols="3">21 1007 3200 75.2 77.9</cell></row><row><cell>iBOT</cell><cell>Swin-T/7</cell><cell cols="3">28 726 1200 75.3 78.6</cell></row><row><cell>iBOT</cell><cell cols="4">Swin-T/14 28 593 1200 76.2 79.3</cell></row><row><cell>iBOT</cell><cell>ViT-B/16</cell><cell cols="3">85 312 1600 77.1 79.5</cell></row><row><cell>iBOT</cell><cell>ViT-L/16</cell><cell cols="3">307 102 1200 78.0 81.0</cell></row><row><cell cols="2">iBOT  ? ViT-L/16</cell><cell cols="3">307 102 200 72.9 82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Fine-tuning on ImageNet-1K.</figDesc><table><row><cell>Method</cell><cell>Arch.</cell><cell>Epo. 1</cell><cell>Acc.</cell></row><row><cell>Rand.</cell><cell>ViT-S/16</cell><cell>-</cell><cell>79.9</cell></row><row><cell>MoCov3</cell><cell>ViT-S/16</cell><cell>600</cell><cell>81.4</cell></row><row><cell>DINO</cell><cell>ViT-S/16</cell><cell>3200</cell><cell>82.0</cell></row><row><cell>iBOT</cell><cell>ViT-S/16</cell><cell>3200</cell><cell>82.3</cell></row><row><cell>Rand.</cell><cell>ViT-B/16</cell><cell>-</cell><cell>81.8</cell></row><row><cell>MoCov3</cell><cell>ViT-B/16</cell><cell>600</cell><cell>83.2</cell></row><row><cell>BEiT</cell><cell>ViT-B/16</cell><cell>800</cell><cell>83.4</cell></row><row><cell>DINO</cell><cell>ViT-B/16</cell><cell>1600</cell><cell>83.6</cell></row><row><cell>iBOT</cell><cell>ViT-B/16</cell><cell>1600</cell><cell>84.0</cell></row><row><cell>MoCov3</cell><cell>ViT-L/16</cell><cell>600</cell><cell>84.1</cell></row><row><cell>iBOT</cell><cell>ViT-L/16</cell><cell>1000</cell><cell>84.8</cell></row><row><cell>BEiT</cell><cell>ViT-L/16</cell><cell>800</cell><cell>85.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">: Fine-tuning on ImageNet-1K.</cell></row><row><cell cols="3">Pre-training on ImageNet-22K.</cell><cell></cell></row><row><cell>Method</cell><cell>Arch.</cell><cell>Epo. 1</cell><cell>Acc.</cell></row><row><cell>BEiT</cell><cell>ViT-B/16</cell><cell>150</cell><cell>83.7</cell></row><row><cell>iBOT</cell><cell>ViT-B/16</cell><cell>320</cell><cell>84.4</cell></row><row><cell>BEiT</cell><cell>ViT-L/16</cell><cell>150</cell><cell>86.0</cell></row><row><cell>iBOT</cell><cell>ViT-L/16</cell><cell>200</cell><cell>86.6</cell></row><row><cell>iBOT</cell><cell>ViT 512 -L/16</cell><cell>200</cell><cell>87.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Semi-supervised learning on ImageNet-1K. 1% and 10% denotes label fraction. SD denotes self-distillation.</figDesc><table><row><cell>Method</cell><cell>Arch.</cell><cell>1%</cell><cell>10%</cell></row><row><cell>SimCLRv2</cell><cell>RN50</cell><cell>57.9</cell><cell>68.1</cell></row><row><cell>BYOL</cell><cell>RN50</cell><cell>53.2</cell><cell>68.8</cell></row><row><cell>SwAV</cell><cell>RN50</cell><cell>53.9</cell><cell>70.2</cell></row><row><cell cols="2">SimCLRv2+SD RN50</cell><cell>60.0</cell><cell>70.5</cell></row><row><cell>DINO</cell><cell cols="2">ViT-S/16 60.3</cell><cell>74.3</cell></row><row><cell>iBOT</cell><cell cols="2">ViT-S/16 61.9</cell><cell>75.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Unsupervised learning on ImageNet-1K. ? denotes k-means clustering on frozen features.</figDesc><table><row><cell>Method</cell><cell>Arch.</cell><cell>ACC ARI NMI FMI</cell></row><row><cell cols="2">Self-label  ? RN50</cell><cell>30.5 16.2 75.4 -</cell></row><row><cell cols="2">InfoMin  ? RN50</cell><cell>33.2 14.7 68.8 -</cell></row><row><cell>SCAN</cell><cell>RN50</cell><cell>39.9 27.5 72.0 -</cell></row><row><cell>DINO</cell><cell cols="2">ViT-S/16 41.4 29.8 76.8 32.8</cell></row><row><cell>iBOT</cell><cell cols="2">ViT-S/16 43.4 32.8 78.6 35.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Object detection (Det.) &amp; instance segmentation (ISeg.) on COCO and Semantic segmentation (Seg.) on ADE20K. We report the results of ViT-S/16 (left) and ViT-B/16 (right). Seg. ? denotes using a linear head for semantic segmentation.</figDesc><table><row><cell cols="2">Method Arch.</cell><cell>Param.</cell><cell>Det. ISeg. Seg.</cell><cell>Method</cell><cell cols="3">Det. ISeg. Seg.  ?</cell><cell>Seg.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AP b AP m mIoU</cell><cell></cell><cell cols="4">AP b AP m mIoU mIoU</cell></row><row><cell>Sup.</cell><cell>Swin-T</cell><cell cols="2">29 48.1 41.7 44.5</cell><cell>Sup.</cell><cell>49.8</cell><cell>43.2</cell><cell>35.4</cell><cell>46.6</cell></row><row><cell cols="2">MoBY Swin-T</cell><cell cols="2">29 48.1 41.5 44.1</cell><cell>BEiT</cell><cell>50.1</cell><cell>43.5</cell><cell>27.4</cell><cell>45.8</cell></row><row><cell>Sup.</cell><cell cols="3">ViT-S/16 21 46.2 40.1 44.5</cell><cell>DINO</cell><cell>50.1</cell><cell>43.4</cell><cell>34.5</cell><cell>46.8</cell></row><row><cell cols="4">iBOT ViT-S/16 21 49.4 42.6 45.4</cell><cell>iBOT</cell><cell>51.2</cell><cell>44.2</cell><cell>38.3</cell><cell>50.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Transfer learning by fine-tuning pre-trained models on different datasets. We report Top-1 accuracy of ViT-S/16 (left) and ViT-B/16 (right). Method Cif 10 Cif 100 iNa 18 iNa 19 Flwrs Cars Rand. 99.0 89.5 70.7 76.6 98.2 92.1 BEiT 98.6 87.4 68.5 76.5 96.4 92.1 DINO 99.0 90.5 72.0 78.2 98.5 93.0 iBOT 99.1 90.7 73.7 78.5 98.6 94.0 Method Cif 10 Cif 100 iNa 18 iNa 19 Flwrs Cars Rand. 99.0 90.8 73.2 77.7 98.4 92.1</figDesc><table><row><cell>BEiT</cell><cell>99.0 90.1 72.3 79.2 98.0 94.2</cell></row><row><cell cols="2">DINO 99.1 91.7 72.6 78.6 98.8 93.0</cell></row><row><cell cols="2">iBOT 99.2 92.2 74.6 79.6 98.9 94.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Robustness evaluation of pre-trained models against background change, occlusion, and out-of-distribution examples. O.F. M.S. M.R. M.N. N.F. O.BB. O.BT. IN-9 S .5 N S .5 IN-A IN-C ? IN DINO 89.2 89.2 80.4 78.3 52.0 21.9 18.4 96.4 64.7 42.0 12.</figDesc><table><row><cell>Method</cell><cell>Background Change</cell><cell cols="2">Clean Occlusion Out-of-Dist. Clean</cell></row><row><cell></cell><cell></cell><cell>3 51.7</cell><cell>77.0</cell></row><row><cell>iBOT</cell><cell cols="2">90.9 89.7 81.7 80.3 53.5 22.7 17.4 96.8 65.9 43.4 13.8 48.1</cell><cell>77.9</cell></row><row><cell cols="3">4.3.2 DISCRIMINATIVE PARTS IN SELF-ATTENTION MAP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>above-mentioned properties brought by MIM objective can improve the model's robustness to uncommon examples. We quantitatively benchmark robustness in terms of 3 aspects: background change, occlusion, and out-of-distribution examples, with a ViT-S/16 pre-trained for 800 epochs and then linearly evaluated for 100 epochs. Results are shown in Tab. 8. For background change, we study images under 7 types of change, detailed in Appendix D. iBOT is more robust against background changes except for O.BT.. For occlusion, we study the linear accuracy with salient and non-salient patch dropping following<ref type="bibr" target="#b32">Naseer et al. (2021)</ref> with an information loss ratio of 0.5. iBOT has a smaller performance drop under both settings. For out-of-distribution examples, we study natural adversarial examples in ImageNet-A(Hendrycks et al., 2021)  and image corruptions in ImageNet-C<ref type="bibr" target="#b21">(Hendrycks &amp; Dietterich, 2019)</ref>. iBOT has higher accuracy on the ImageNet-A and a smaller mean corruptions error (mCE) on the ImageNet-C.</figDesc><table /><note>4.4 ABLATION STUDY ON TOKENIZER In this section, we ablate the importance of using a semantically meaningful tokenizer using a 300- epoch pre-trained ViT-S/16 with a prediction ratio r = 0.3 and without multi-crop augmentation. Additional ablations are given in Appendix E. iBOT works with self-distillation on [CLS] token with cross-view images (L [CLS] ) to acquire visual semantics. To verify, we conduct experiments to perform MIM without L [CLS] or with alternative models as visual tokenizer. Specifically, ? denotes a standalone DINO and denotes a pre-tranined DALL-E encoder (Ramesh et al., 2021).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Effect of design choices of semantically meaningful tokenization. Method L MIM L [CLS] SH k-NN Lin. Fin. We find that performing MIM without L [CLS] leads to undesirable results of 9.5% k-NN accuracy and 29.8% linear accuracy, indicating that visual semantics can hardly be obtained with only MIM. While semantics emerges with a standalone DINO as a visual tokenizer, it is still far from reaching a decent result (44.3% versus 69.1% in k-NN accuracy). token and patch tokens, which shares the semantics acquired in [CLS] token to MIM.</figDesc><table><row><cell></cell><cell>iBOT</cell><cell></cell><cell>69.1 74.2 81.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>69.0 73.8 81.5</cell></row><row><cell>Comparing iBOT with multi-tasking of</cell><cell>BEiT</cell><cell>?</cell><cell>-9.5 29.8 79.4 -44.3 60.0 81.7 -6.9 23.5 81.4</cell></row><row><cell>DINO and BEiT (DINO+BEiT), we see</cell><cell>DINO</cell><cell></cell><cell>-67.9 72.5 80.6</cell></row><row><cell>the strengths of merging the semantics ac-quired by self-distillation with the visual tokenizer with an 11.5% advance in linear probing and 0.3% in fine-tuning. More-</cell><cell cols="3">BEiT + DINO : pre-trained DALL-E encoder -48.0 62.7 81.2 ?: standalone DINO (w/o mcrop, 300-epoch)</cell></row><row><cell cols="4">over, we empirically observe a performance improvement using a Shared projection Head (SH) for</cell></row><row><cell>[CLS]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Algorithm 1: iBOT PyTorch-like Pseudocode w/o multi-crop augmentation Input: g s , g t ;</figDesc><table><row><cell></cell><cell></cell><cell>// student and teacher network</cell></row><row><cell>C, C ;</cell><cell></cell><cell>// center on [CLS] token and patch tokens</cell></row><row><cell>? s , ? t ;</cell><cell cols="2">// temperature on [CLS] token for student and teacher network</cell></row><row><cell>? s , ? t ; l ;</cell><cell cols="2">// temperature on patch tokens for student and teacher network // momentum rate for network</cell></row><row><cell>m, m ;</cell><cell cols="2">// momentum rates for center on [CLS] token and patch tokens</cell></row><row><cell cols="2">g t .params = g s .params</cell></row><row><cell cols="2">for x in loader do</cell></row><row><cell cols="2">u, v = augment(x), augment(x) ;</cell><cell>// random view?</cell></row><row><cell cols="2">u, m u = blockwise mask(u) ;</cell><cell>// random block-wise maskin?</cell></row><row><cell cols="2">v, m v = blockwise mask(v) ;</cell><cell>// random block-wise maskin?</cell></row><row><cell>u [CLS] s</cell><cell>,? patch</cell></row></table><note>s = g s (?, return all tok=true) ;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>k-NN and linear probing accuracy on ImageNet-1K without multi-crop augmentation (left) and with multi-crop augmentation (right) multi-crop augmentation. We split the table into results without or with multi-crop augmentation.</figDesc><table><row><cell cols="2">Method Arch</cell><cell cols="3">Param. Epo. k-NN Linear</cell><cell cols="2">Method Arch</cell><cell>Param. Epo. k-NN Linear</cell></row><row><cell>MoCov3</cell><cell cols="2">RN50 ViT-S/16 21 600 23 800</cell><cell>--</cell><cell>74.6 73.4</cell><cell>SwAV</cell><cell>RN50 ViT-S/16 21 800 66.3 73.5 23 800 65.7 75.3</cell></row><row><cell></cell><cell cols="2">ViT-B/16 85 600</cell><cell>-</cell><cell>76.7</cell><cell></cell><cell>RN50</cell><cell>23 800 67.5 75.3</cell></row><row><cell>DINO</cell><cell cols="4">ViT-S/16 21 800 70.0 73.7 ViT-B/16 85 400 68.9 72.8</cell><cell>DINO</cell><cell>ViT-S/16 21 800 74.5 77.0 ViT-B/16 85 400 76.1 78.2</cell></row><row><cell>iBOT</cell><cell cols="4">ViT-S/16 21 800 72.4 76.2 ViT-B/16 85 400 71.2 76.0</cell><cell cols="2">ViT-S/16 21 800 75.2 77.9 iBOT ViT-B/16 85 400 76.8 79.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Different fine-tuning recipes. LD denotes layerwise learning rate decay. DS denotes mixed-precision training with DeepSpeed.</figDesc><table><row><cell></cell><cell cols="5">Epo. LD DS BEiT DINO iBOT</cell></row><row><cell cols="2">ViT-S/16</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>300</cell><cell>1.0</cell><cell>81.5</cell><cell>81.1</cell><cell>81.2</cell></row><row><cell>2</cell><cell cols="2">300 0.75</cell><cell>81.7</cell><cell>82.0</cell><cell>82.3</cell></row><row><cell>3</cell><cell cols="2">200 0.65</cell><cell>80.7</cell><cell>-</cell><cell>-</cell></row><row><cell>4</cell><cell cols="2">200 0.75</cell><cell>81.4</cell><cell>81.9</cell><cell>82.3</cell></row><row><cell>5</cell><cell cols="2">200 0.75</cell><cell>81.4</cell><cell>82.0</cell><cell>82.2</cell></row><row><cell>6</cell><cell cols="2">200 0.85</cell><cell>81.2</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ViT-B/16</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>300</cell><cell>1.0</cell><cell>82.1</cell><cell>82.8</cell><cell>82.4</cell></row><row><cell>8</cell><cell cols="2">200 0.65</cell><cell>82.7</cell><cell>83.1</cell><cell>83.2</cell></row><row><cell>9</cell><cell cols="2">100 0.65</cell><cell>83.4</cell><cell>83.5</cell><cell>84.0</cell></row><row><cell cols="3">10 100 0.65</cell><cell>83.2</cell><cell>83.6</cell><cell>83.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Evaluation protocols for semisupervised learning. Proj. denotes finetuning from the middle layer of the projection head. LR denotes logistic regression.</figDesc><table><row><cell>Method</cell><cell cols="2">Proj. 1% 10%</cell></row><row><cell>frozen features</cell><cell></cell><cell></cell></row><row><cell>1 DINO + k-NN</cell><cell>-</cell><cell>61.3 69.1</cell></row><row><cell>2 iBOT + k-NN</cell><cell>-</cell><cell>62.3 70.1</cell></row><row><cell>3 DINO + Lin.</cell><cell>-</cell><cell>60.5 71.0</cell></row><row><cell>4 iBOT + Lin.</cell><cell>-</cell><cell>62.5 72.2</cell></row><row><cell>5 DINO + LR</cell><cell>-</cell><cell>64.5 72.2</cell></row><row><cell>6 iBOT + LR</cell><cell>-</cell><cell>65.9 73.4</cell></row><row><cell>end-to-end fine-tuning</cell><cell></cell><cell></cell></row><row><cell>7 DINO</cell><cell></cell><cell>50.6 73.2</cell></row><row><cell>8 iBOT</cell><cell></cell><cell>55.0 74.0</cell></row><row><cell>9 DINO</cell><cell></cell><cell>60.3 74.3</cell></row><row><cell>10 iBOT</cell><cell></cell><cell>61.9 75.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Additional object detection, instance segmentation, and semantic segmentation results with small-size models. We pre-train iBOT with ViT-S/16 for 800 epochs.</figDesc><table><row><cell cols="2">Method Arch.</cell><cell>Param.</cell><cell cols="4">Det. &amp; Inst. Seg. w/ Cascade Mask R-CNN Seg. w/ UperNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AP b AP b 50</cell><cell>AP b 75</cell><cell>AP m AP m 50</cell><cell>AP m 75</cell><cell>mIoU</cell><cell>mAcc</cell></row><row><cell>Sup.</cell><cell>Swin-T</cell><cell>29</cell><cell cols="4">48.1 67.1 52.5 41.7 64.4 45.0</cell><cell>44.5</cell><cell>-</cell></row><row><cell cols="2">MoBY Swin-T</cell><cell>29</cell><cell cols="4">48.1 67.1 52.1 41.5 64.0 44.7</cell><cell>44.1</cell><cell>-</cell></row><row><cell>Sup.</cell><cell>ViT-S/16</cell><cell>21</cell><cell cols="4">46.2 65.9 49.6 40.1 62.9 42.8</cell><cell>44.5</cell><cell>55.5</cell></row><row><cell>iBOT</cell><cell>ViT-S/16</cell><cell>21</cell><cell cols="4">49.4 68.7 53.3 42.6 65.6 45.8</cell><cell>45.4</cell><cell>56.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc>Additional object detection, instance segmentation, and semantic segmentation results with base-size models. We pre-train iBOT with ViT-B/16 for 400 epochs.</figDesc><table><row><cell>Method</cell><cell cols="5">Det. &amp; Inst. Seg. w/ Cascade Mask R-CNN</cell><cell cols="2">Seg. w/ Lin.</cell><cell cols="2">Seg. w/ UperNet</cell></row><row><cell></cell><cell>AP b AP b 50</cell><cell>AP b 75</cell><cell cols="2">AP m AP m 50</cell><cell>AP m 75</cell><cell cols="3">mIoU mAcc mIoU</cell><cell>mAcc</cell></row><row><cell>Sup.</cell><cell>49.8 69.6</cell><cell>53.8</cell><cell>43.2</cell><cell>66.6</cell><cell>46.5</cell><cell>35.4</cell><cell>44.6</cell><cell>46.6</cell><cell>57.0</cell></row><row><cell>BEiT</cell><cell>50.1 68.5</cell><cell>54.6</cell><cell>43.5</cell><cell>66.2</cell><cell>47.1</cell><cell>27.4</cell><cell>35.5</cell><cell>45.8</cell><cell>55.9</cell></row><row><cell>DINO</cell><cell>50.1 69.5</cell><cell>54.3</cell><cell>43.4</cell><cell>66.8</cell><cell>47.0</cell><cell>34.5</cell><cell>43.7</cell><cell>46.8</cell><cell>57.1</cell></row><row><cell>iBOT</cell><cell>51.2 70.8</cell><cell>55.5</cell><cell>44.2</cell><cell>67.8</cell><cell>47.7</cell><cell>38.3</cell><cell>48.0</cell><cell>50.0</cell><cell>60.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 :</head><label>15</label><figDesc>k-NN and linear probing on ImageNet-1K with different pre-training datasets.</figDesc><table><row><cell>Arch.</cell><cell>Pre-Train Data</cell><cell>Param.</cell><cell>Epoch</cell><cell>k-NN</cell><cell>Linear</cell></row><row><cell>ViT-S/16</cell><cell>ImageNet-1K</cell><cell>21</cell><cell>800</cell><cell>75.2</cell><cell>77.9</cell></row><row><cell>ViT-S/16</cell><cell>ImageNet-22K</cell><cell>21</cell><cell>160</cell><cell>69.3</cell><cell>76.5</cell></row><row><cell>ViT-B/16</cell><cell>ImageNet-1K</cell><cell>85</cell><cell>400</cell><cell>77.1</cell><cell>79.5</cell></row><row><cell>ViT-B/16</cell><cell>ImageNet-22K</cell><cell>85</cell><cell>80</cell><cell>71.1</cell><cell>79.0</cell></row><row><cell>ViT-L/16</cell><cell>ImageNet-1K</cell><cell>307</cell><cell>300</cell><cell>78.0</cell><cell>81.0</cell></row><row><cell>ViT-L/16</cell><cell>ImageNet-22K</cell><cell>307</cell><cell>50</cell><cell>72.9</cell><cell>82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 16 :</head><label>16</label><figDesc>Effectiveness of pre-trained features on nearest neighbor retrieval. We report the results on different downstream tasks whose evaluation is based on nearest neighbor retrieval.</figDesc><table><row><cell>Method DINO</cell><cell>Image Retrieval ROx RPar M H M 37.2 13.9 63.1 34.4 H</cell><cell cols="2">Vid. Obj. Segment. (J &amp;F) m J m F m 61.8 60.2 63.4</cell></row><row><cell>iBOT</cell><cell>36.6 13.0 61.5 34.1</cell><cell>61.8</cell><cell>60.4 63.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 17 :</head><label>17</label><figDesc>Different head sharing strategy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 18 :</head><label>18</label><figDesc>Hard label versus soft label. Cen. denotes centering. ? denotes smaller temperature for teacher output.</figDesc><table><row><cell cols="4">Method Cen. Post Proc. k-NN Lin.</cell></row><row><cell></cell><cell></cell><cell cols="2">softmax 49.8 63.5</cell></row><row><cell></cell><cell></cell><cell cols="2">hardmax 64.8 71.9</cell></row><row><cell></cell><cell></cell><cell cols="2">softmax  ? 69.4 73.9</cell></row><row><cell></cell><cell></cell><cell cols="2">softmax 67.8 72.9</cell></row><row><cell></cell><cell></cell><cell cols="2">hardmax 68.1 73.3</cell></row><row><cell>iBOT</cell><cell></cell><cell cols="2">softmax  ? 69.1 74.2</cell></row><row><cell>DINO</cell><cell>-</cell><cell>-</cell><cell>67.9 72.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>.07 .04 ? .07 .04 ? .07 .04 ? .06 .05 ? .08 .04 ? Specifically, the smoothing momentum for online centering m and sharpening temperature ? t are studied. Note we keep the parameters for [CLS] token the same as DINO and only study for parameters for the patch tokens.</figDesc><table><row><cell>m</cell><cell>.8</cell><cell>.99</cell><cell>.999</cell><cell>.9</cell><cell>.9</cell><cell>.9</cell></row><row><cell>? t k-NN</cell><cell cols="6">.04 ? .07 68.7 68.8 68.9 68.5 68.7 69.1</cell></row><row><cell>Lin.</cell><cell>74.0</cell><cell>73.8</cell><cell>73.8</cell><cell>73.5</cell><cell>73.9</cell><cell>74.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 19 :</head><label>19</label><figDesc>Time and Memory Requirements. Mem. Lin. 300 Lin. 800 Fin. 800</figDesc><table><row><cell>We detail the actual training time (T) and GPU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 20 :</head><label>20</label><figDesc>Methodology comparison over different approaches to tokenize the patches. We report ImageNet-1K k-NN, linear and fine-tuning validation accuracy. Models are pre-trained with ViT-S/16 and 300 epochs.</figDesc><table><row><cell>Method</cell><cell cols="3">k-NN Linear Fine-Tune</cell></row><row><cell>Rand.</cell><cell>-</cell><cell>-</cell><cell>79.9</cell></row><row><cell cols="3">MPP (Dosovitskiy et al., 2021) 16.4 37.2</cell><cell>80.8</cell></row><row><cell>Patch Clustering</cell><cell cols="2">19.2 40.1</cell><cell>81.3</cell></row><row><cell>BEiT (Bao et al., 2021)</cell><cell>6.9</cell><cell>24.2</cell><cell>81.4</cell></row><row><cell cols="3">Standalone DINO as tokenizer 44.3 60.0</cell><cell>81.7</cell></row><row><cell>iBOT</cell><cell cols="2">70.3 74.8</cell><cell>81.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head></head><label></label><figDesc>Time and Memory Requirements. BEiT is trained with a non-contrastive objective and without multi-crop augmentation, thus it consumes only a memory of 5.6G and takes 90.1h for 800 epochs. Comparing iBOT and DINO with multi-crop augmentation, iBOT with MIM induces 25% more memory requirements and 7.4% more actual training time. Considering pre-training efficiency (accuracy versus time), 800-epochs pre-trained DINO requiring for 180.0h, while 300-epochs iBOT only requires 73.3h with 0.4% higher linear probing accuracy (77.0 versus 77.4).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Effective pre-training epochs accounting for actual trained images/views. See Appenix B for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published as a conference paper at ICLR 2022</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement Tao Kong is the corresponding author. We would like to acknowledge Feng Wang, Rufeng Zhang, and Zongwei Zhou for helpful discussions. We thank Mathilde Caron, Julien Mairal, and Hugo Touvronfor for sharing details of DINO. We thank Li Dong and Hangbo Bao for sharing details of BEiT.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Published as a conference paper at ICLR 2022 Order index 0, 1, 2, 3 are ranked according to its self-attention score. In the top-left corner for each pattern layout subfigure, its order index and cluster index are annotated. In the top panel, we can observe that pattern 0,2,3 show explicit semantic information of nose, eyes, ears respectively. Interestingly, patch 1 also locates around the eyes of the Samoyed but its corresponding pattern share visual similarity in shape instead of semantics. This illustrate the diverse behaviour for each learned pattern. In the bottom panel, a library is represented by 0 two-or multi-color joints, 1,3 knurlling texture, 2 texts. Similarly, we have patterns 0,1,3 focusing more on texture &amp; color and pattern 2 focusing more on semantics. All of these visualized results illustrate versatile behaviour for each index.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-supervised classification network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10994</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SiT: Self-supervised vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Atito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03602</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">BEiT: BERT pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cascade R-CNN: High quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Big selfsupervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient self-supervised vision transformers for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09785</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05656</idno>
		<title level="m">Masked self-supervised transformer for visual representation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Selfsupervised learning: Generative or contrastive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanchana</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10497</idno>
		<title level="m">Intriguing properties of vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discrete variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolfe</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Vimpac: Video pre-training via masked token prediction and contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11250</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingda</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Noise or signal: The role of image backgrounds in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Yuanqing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Selfsupervised learning with swin transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Self-supervised visual representations learning by contrastive mask prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07954</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
