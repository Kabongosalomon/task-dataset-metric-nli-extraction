<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Semantic Scene Completion: a Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Rold?o</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">AKKA Research</orgName>
								<address>
									<settlement>Guyancourt</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><surname>De Charette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Verroust-Blondet</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Semantic Scene Completion: a Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding the 3D surroundings is a natural ability for humans, who are capable to leverage prior knowledge to estimate geometry and semantics, even in large occluded areas. This proves more difficult for computers, which has drawn wide interest from computer vision researchers in recent years <ref type="bibr" target="#b52">[53]</ref>. Indeed, 3D scene understanding is a crucial feature for many applications, such as robotic navigation or augmented reality, where geometrical and semantics understanding is key to leverage interaction with the real world <ref type="bibr" target="#b43">[44]</ref>. Nonetheless, vision sensors only provide partial observations of the world given their limited field of view, sparse sensing, and measurement noise, capturing a partial and incomplete representation of the scene.</p><p>To address this, Scene Completion (SC) has been used for a long time to infer the complete geometry of a scene given one or more 2D/3D observations, historically using a wide variety of more or less sophisticated interpolation methods. With recent advances of 3D deep learning, Semantic Scene Completion (SSC) has been introduced as an extension of SC, where semantics and geometry are jointly inferred for the whole scene, departing from the idea that they are entangled <ref type="bibr" target="#b136">[137]</ref>. Consequently, along with the addition of semantics, the SSC task has significantly departed from original SC in terms of nature and sparsity of the input data. A camera (RGB, RGB-D, Depth) senses dense volumes but produces noisy depth measurements (a), while LiDAR -more accurate -is significantly sparser (b). (Inspired from: <ref type="bibr" target="#b136">[137]</ref>). <ref type="figure">Fig. 1</ref> shows samples of input and ground truth for the four most popular SSC datasets. The complexity of the SSC task lies in the sparsity of the input data (see holes in depth or LiDAR input), and the incomplete ground truth (resulting of frame aggregation) providing a rather weak guidance. Different from object-level completion <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b171">172]</ref> or from scene reconstruction where multiple views are aggregated <ref type="bibr" target="#b108">[109,</ref><ref type="bibr" target="#b181">182]</ref>, SSC requires in-depth understanding of the entire scene heavily relying on learned priors to resolve ambiguities. The increasing number of large scale datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b132">133,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b139">140]</ref> have encouraged new SSC works in the last years. On connex topics, 3D (deep) vision has been thoroughly reviewed <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b90">91]</ref> including surveys on 3D representations <ref type="bibr" target="#b1">[2]</ref> and taskoriented reviews like 3D semantic segmentation <ref type="bibr" target="#b167">[168,</ref><ref type="bibr" target="#b174">175]</ref>, 3D reconstruction <ref type="bibr" target="#b108">[109,</ref><ref type="bibr" target="#b181">182]</ref>, 3D object detection <ref type="bibr" target="#b67">[68]</ref>, etc. Still, no survey seems to exist on this hot SSC topic and navigating the literature is not trivial.</p><p>In this article, we propose the first comprehensive and critical review of the Semantic Scene Completion (SSC) literature, focusing on methods and datasets. With this systematic survey, we expose a critical analysis of the SSC knowledge and highlight the missing areas for future research. We aim to provide new insights to informed readers and help new ones navigate in this emerging field which gained significant momentum in the past few years. To the best of our knowledge this is the first SSC survey, which topic has only been briefly covered in recent 3D surveys <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b90">91]</ref>.</p><p>To study SSC, this paper is organized as follows. We first introduce and formalize the problem of Semantic Scene Completion in Sec. 2, briefly brushing closely related topics. Ad-hoc datasets employed for the task and introduction to common 3D scene representations are covered in Sec. 3. We study the existing works in Sec. 4, highlighting the different input encodings, deep architectures, design choices, and training strategies employed. The section ends with an analysis of the current performances, followed by a discussion in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem statement</head><p>Let x be an incomplete 3D representation of a scene, Semantic Scene Completion (SSC) is the function f (.) inferring a dense semantically labeled scene? such that f (x) =?, best approximates the real 3D scene y. Most often, x is significantly sparser than y and the complexity lies in the inherent ambiguity, especially where large chunks of data are missing, due to sparse sensing or occlusions (see <ref type="figure" target="#fig_0">Fig. 2</ref>). Subsequently, the problem cannot be addressed by interpolating data in x and is most often solved by learning priors from (x, y) pairs of sparse input and dense 3D scenes with semantic labels.</p><p>The nature of the sparse 3D input x greatly affects the task complexity. While 3D data can be obtained from a wide variety of sensors, RGB-D/stereo cameras or LiDARs are commonly employed. The former, for example, provides a dense description of the visible surfaces where missing regions correspond to occluded areas, as shown in <ref type="figure" target="#fig_0">Fig. 2a</ref>. This reduces the SSC task to estimating semantic completion only in the occluded regions <ref type="bibr" target="#b136">[137]</ref>. Conversely, LiDAR data provides considerably sparser sensing, with density decreasing afar and point-wise returns from laser beams cover an infinitesimal portion of the space leading to a high proportion of unknown volume, as shown in <ref type="figure" target="#fig_0">Fig. 2b</ref>.</p><p>The rest of this section provides the reader with a brief overview of related tasks, covering early works and foundations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related tasks</head><p>Semantic scene completion inspires from closely related tasks like shape completion, semantic segmentation, and more recently semantic instance completion. Thereof, SSC benefits from individual insights of these tasks which we briefly review their literature pointing to the respective surveys.</p><p>Completion. Completion algorithms initially used interpolation <ref type="bibr" target="#b28">[29]</ref> or energy minimization <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b138">139]</ref> techniques to complete small missing regions. Completion works were first devoted to object shape completion, which infers occlusionfree object representation. For instance, some trends exploit symmetry <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b133">134,</ref><ref type="bibr" target="#b142">143,</ref><ref type="bibr" target="#b147">148]</ref> and are reviewed in <ref type="bibr" target="#b95">[96]</ref>. Another common approach is to rely on prior 3D models to best fit sparse input <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b122">123,</ref><ref type="bibr" target="#b130">131]</ref>. In recent years, model-based techniques and new large-scale datasets enlarged the scope of action by enabling inference of complete occluded parts in both scanned objects <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b129">130,</ref><ref type="bibr" target="#b134">135,</ref><ref type="bibr">141,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b169">170,</ref><ref type="bibr" target="#b171">172]</ref> and entire scenes <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b180">181]</ref>. Moreover, contemporary research shows promising results on challenging multi-object reconstruction from as single RGB image <ref type="bibr" target="#b33">[34]</ref>. For further analysis, we refer readers to related surveys <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b169">170]</ref>. <ref type="bibr">Dataset</ref> Year Type Nature Input ?Ground truth 3D Sensor # Classes Tasks* #Sequences #Frames SSC DE SPE SS OC SNE NYUv2 <ref type="bibr" target="#b132">[133]</ref> a 2012 Real-world ? Indoor RGB-D ? Mesh/Voxel RGB-D 894 <ref type="bibr" target="#b10">(11)</ref> 1449 795/654 SUN3D <ref type="bibr" target="#b164">[165]</ref> 2013 Real-world Indoor RGB-D ? Points RGB-D -254 -NYUCAD <ref type="bibr" target="#b37">[38]</ref>  <ref type="bibr">b</ref> 2013 Synthetic Indoor RGB-D ? Mesh/Voxel RGB-D 894 <ref type="bibr" target="#b10">(11)</ref> 1449 795/654 SceneNet <ref type="bibr" target="#b59">[60]</ref> 2015 Synthetic Indoor RGB-D ? Mesh RGB-D ? 11 57 -SceneNN <ref type="bibr" target="#b63">[64]</ref> 2016 Real-world Indoor RGB-D ? Mesh RGB-D 22 <ref type="bibr" target="#b99">100</ref> -SUNCG <ref type="bibr" target="#b136">[137]</ref> 2017 Synthetic Indoor Depth ? Mesh/Voxel RGB-D ? 84 <ref type="bibr" target="#b10">(11)</ref> 45622 139368/470 Matterport3D <ref type="bibr" target="#b12">[13]</ref> 2017 Real-world Indoor RGB-D ? Mesh 3D Scanner 40 (11) 707 72/18 ScanNet <ref type="bibr" target="#b23">[24]</ref> 2017 Real-world Indoor RGB-D ? Mesh RGB-D 20 <ref type="bibr" target="#b10">(11)</ref> 1513 1201/312 2D-3D-S <ref type="bibr" target="#b2">[3]</ref> 2017 Real-world Indoor RGB-D ? Mesh 3D Scanner 13 270 -SUNCG-RGBD <ref type="bibr" target="#b85">[86]</ref> c 2018 Synthetic Indoor RGB-D ? Mesh/Voxel RGB-D ? 84 <ref type="bibr" target="#b10">(11)</ref> 45622 13011/499 SemanticKITTI <ref type="bibr" target="#b5">[6]</ref> 2019 Real-world Outdoor Points/RGB ? Points/Voxel Lidar- <ref type="bibr">64 28 (19)</ref> 22 23201/20351 SynthCity <ref type="bibr" target="#b48">[49]</ref> 2019 Synthetic Outdoor Points ? Points Lidar ? 9 9 -CompleteScanNet <ref type="bibr" target="#b163">[164]</ref>   <ref type="table">Table 1</ref>: SSC datasets. We list here datasets readily usable for the SSC task in chronological order. Popular datasets are bold and previewed in <ref type="figure">Fig. 1</ref>. Classes show the total number of semantic classes and when it differs, SSC classes in parenthesis.</p><p>Semantics. Traditional segmentation techniques reviewed in <ref type="bibr" target="#b101">[102]</ref> were based on hand-crafted features, statistical rules, and bottom-up procedures, combined with traditional classifiers. The advances in deep learning have reshuffled the cards <ref type="bibr" target="#b166">[167]</ref>. Initial 3D deep techniques relied on multiviews processed by 2D CNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b141">142]</ref> and were quickly replaced by the use of 3D CNNs which operate on voxels <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b119">120,</ref><ref type="bibr" target="#b144">145,</ref><ref type="bibr" target="#b152">153]</ref>, tough suffering from memory and computation shortcomings. Point-based networks <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b114">115,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b146">147,</ref><ref type="bibr" target="#b157">158]</ref> remedied this problem by operating on points and quickly became popular for segmentation, though generative tasks are still a challenge. Therefore, if the point space is predefined, SSC can be seen as a point segmentation task <ref type="bibr" target="#b179">[180]</ref>. We refer readers to dedicated surveys on semantic segmentation <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b166">167]</ref>.</p><p>Semantic Instance Completion. Different from SSC, semantic instance completion is applied only at the objects instance level and cannot complete scene background like walls, floor, etc. Most existing methods require instance detection and complete the latter with CAD models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b128">129]</ref> or a completion head <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b171">172]</ref>. Notably, <ref type="bibr" target="#b4">[5]</ref> detects both objects and the scene layout to output a complete lightweight CAD representation. These approaches are commonly based on 3D object detection <ref type="bibr" target="#b113">[114,</ref><ref type="bibr" target="#b131">132,</ref><ref type="bibr" target="#b135">136]</ref> or instance segmentation <ref type="bibr" target="#b61">[62]</ref>. Furthermore, some recent methods tackle multiple object completion from single images either through individual object detection and reconstruction <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b171">172]</ref> or joint completion by understanding the scene structure <ref type="bibr" target="#b102">[103,</ref><ref type="bibr" target="#b112">113]</ref>.</p><p>Song et al. <ref type="bibr" target="#b136">[137]</ref> were the first to address semantic segmentation and scene completion jointly, showing that both tasks can mutually benefit. Since then, many SSC works gather ideas from the above described lines of work and are extensively reviewed in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets and representations</head><p>This section presents existing SSC datasets (Sec. 3.1) and commonly used 3D representations for SSC (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>A comprehensive list of all SSC ready datasets is shown in Tab. 1. We denote as SSC ready any dataset containing pairs of sparse/dense data with semantics label. Note that while 14 datasets meet these criteria, only half has been used for SSC among which the four most popular are bold in the table and previewed in <ref type="figure">Fig. 1</ref>. Overall, there is a prevalence of indoor stationary datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b132">133,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b163">164]</ref> as opposed to outdoors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b104">105]</ref>.</p><p>Datasets creation. Synthetic datasets can easily be obtained by sampling 3D object meshes <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b136">137]</ref> or simulating sensors in virtual environments <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b125">126]</ref>. Their evident advantage is the virtually free labeling of data, though transferability of synthetically learned features is arguable. Real datasets are quite costly to record and annotate, and require a significant processing effort. Indoor datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b132">133,</ref><ref type="bibr" target="#b163">164,</ref><ref type="bibr" target="#b164">165]</ref> are commonly captured with RGB-D or stereo sensors. Conversely, outdoor datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b104">105]</ref> are often recorded with LiDAR and optional camera. They are dominated by autonomous driving applications, recorded in (peri)-urban environment, and must subsequently account for dynamic agents (e.g. moving objects, ego-motion, etc.).</p><p>(a) NYUv2 <ref type="bibr" target="#b132">[133]</ref> (b) SemanticKITTI <ref type="bibr" target="#b5">[6]</ref> Fig. 3: Semantics distribution. Class-wise frequencies of the most popular real datasets prove to be highly imbalanced.</p><p>Ground truth generation. An evident challenge is the dense annotation of such datasets. Specifically, while indoor stationary scenes can be entirely captured from multiple views or rotating apparatus, 3D outdoor dynamic scenes are virtually impossible to capture entirely as it would require ubiquitous scene sensing. Subsequently, ground truth y is obtained from the aggregation and labeling of sparse sequential data {y 0 , y 1 , ..., y T } over a small time window T . Multi-frame registration is usually leveraged to that matters, assuming that consecutive frames have an overlapping field of view. For RGB-D datasets, mostly stationary and indoors, it is commonly achieved from Structure from Motion (SfM) <ref type="bibr" target="#b97">[98,</ref><ref type="bibr" target="#b127">128]</ref> or visual SLAM <ref type="bibr" target="#b127">[128]</ref> (vSLAM), which cause holes, missing data, and noisy annotations <ref type="bibr" target="#b39">[40]</ref>. Such imperfections are commonly reduced by inferring dense complete geometry of objects with local matching of CAD models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b163">164]</ref>, or post-processing hole filling techniques <ref type="bibr" target="#b139">[140]</ref>. In outdoor settings, point cloud registration techniques <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b111">112]</ref> enable the co-location of multiple LiDAR measurements into a single reference coordinate system <ref type="bibr" target="#b5">[6]</ref>. Interestingly, while frequently referred to as dense, the ground truth scenes are often noisy and non-continuous in real datasets, being in fact an approximation of the real scene. Firstly, regardless of the number of frames used, some portions of the scene remain occluded, see <ref type="figure">Fig. 4d</ref>, especially in dynamic environments. Secondly, sensors accuracy and density tend to steadily decrease with the distance, as in <ref type="figure">Fig. 4b</ref>. Thirdly, rigid registration can only cope with viewpoint changes, which leads to dynamic objects (e.g. moving cars) producing traces, which impact on the learning priors is still being discussed <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b168">169]</ref>, see <ref type="figure">Fig. 4c</ref>. Finally, another limitation lies in the sensors, which only sense the geometrical surfaces and not the volumes, turning all solid objects into shells. To produce semantics labels, the common practice is to observe the aggregated 3D data from multiple virtual viewpoints to minimize the labeling ambiguity. This process is tedious and (a) Misslabeling <ref type="bibr" target="#b132">[133]</ref> (b) Sparse sensing <ref type="bibr" target="#b5">[6]</ref> (c) Object motion <ref type="bibr" target="#b5">[6]</ref> (d) Occlusions <ref type="bibr" target="#b5">[6]</ref> Fig. 4: Inaccuracies of SSC ground truth. Providing semantics and geometrics annotation in real-world datasets proves to be complex, and the resulting process is imperfect -which lead to noisy supervision signal. Ground truth mislabeling is a well known bias (a). Sparsity is common in LiDAR-based datasets (b). Object motion causes temporal traces (c), and occlusions are inevitable in dynamic scenes (d).</p><p>prone to errors 1 , visible in <ref type="figure">Fig. 4a</ref>. Ultimately, semantics distribution is highly imbalanced as shown in the two most used indoor/outdoor datasets <ref type="figure">Fig. 3</ref>.</p><p>Indoor datasets. From Tab. 1, NYUv2 <ref type="bibr" target="#b132">[133]</ref> (aka NYU-Kinect) is the most popular indoor real-world dataset, composed of mainly office and house room scenery. Despite complete 3D ground truth scene volumes not being originally provided, they have been generated in <ref type="bibr" target="#b51">[52]</ref> by using 3D models and 3D boxes or planes for producing complete synthetically augmented meshes of the scene, generating 1449 pairs of RGB-D images and 3D semantically annotated volumes. Extension of the mesh volumes to 3D grids has been done in <ref type="bibr" target="#b136">[137]</ref>. However, mesh annotations are not always perfectly aligned with the original depth images. To solve this, many methods also report results by using depth maps rendered from the annotations directly as in <ref type="bibr" target="#b37">[38]</ref>. This variant is commonly named as NYUCAD and provides simplified data pairs at the expense of geometric detail loss. Additional indoor real-world datasets as Matterport3D <ref type="bibr" target="#b12">[13]</ref>, SceneNN <ref type="bibr" target="#b63">[64]</ref> and ScanNet <ref type="bibr" target="#b23">[24]</ref> can be used for completing entire rooms from incomplete 3D meshes <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>. The latter has also been synthetically augmented from 3D object models and referred to as CompleteScanNet <ref type="bibr" target="#b163">[164]</ref>, providing <ref type="bibr" target="#b0">1</ref> Authors of SemanticKITTI report that semantic labeling a hectar of 3D data takes approx. 4.5hr <ref type="bibr" target="#b5">[6]</ref>. cleaner annotations. Additionally, smaller SUN3D <ref type="bibr" target="#b164">[165]</ref> provides RGB-D images along with registered semantic point clouds. Note that datasets providing 3D meshes or point clouds easily be voxelized as detailed in <ref type="bibr" target="#b136">[137]</ref>. Additionally, Stanford 2D-3D-S <ref type="bibr" target="#b2">[3]</ref> provides 360 ? RGB-D images, of interest for completing entire rooms <ref type="bibr" target="#b32">[33]</ref>. Due to real datasets small sizes, low scene variability, and annotation ambiguities, synthetic SUNCG <ref type="bibr" target="#b136">[137]</ref> (aka SUNCG-D) was proposed, being a large scale dataset with pairs of depth images and complete synthetic scene meshes. An extension containing RGB modality was presented in <ref type="bibr" target="#b85">[86]</ref> and known as SUNCG-RGBD. Despite its popularity, the dataset is no longer available due to copyright infringement 2 , evidencing a lack of synthetic indoor datasets for SSC, that could be addressed using SceneNet <ref type="bibr" target="#b59">[60]</ref>.</p><p>Outdoor datasets. SemanticKITTI <ref type="bibr" target="#b5">[6]</ref> is the most popular large-scale dataset and currently the sole providing single sparse and dense semantically annotated point cloud pairs from real-world scenes. It derivates from the popular odometry benchmark of the KITTI dataset <ref type="bibr" target="#b44">[45]</ref>, which provides careful registration and untwisted point clouds considering vehicle's ego-motion. Furthermore, voxelized dense scenes were later released as part of an evaluation benchmark with a hidden test set <ref type="bibr" target="#b2">3</ref> . The dataset presents big challenges given the high scene variability and the high class imbalance naturally present in outdoor scenes <ref type="figure">(Fig. 3b</ref>). SemanticPOSS <ref type="bibr" target="#b104">[105]</ref> also provides single sparse semantic point clouds and sensor poses in same format as the latter to ease its implementation. Furthermore, synthetic SynthCity additionally provides dense semantic point clouds and sensor poses. It has the advantage of excluding dynamic objects, which solves the effect of object motion (cf. <ref type="figure">Fig. 4c</ref>), but not occlusions (cf. <ref type="figure">Fig. 4d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scene representations</head><p>We now detail the common 3D representation for SSC output, shown in <ref type="figure">Fig</ref> Point Cloud is a convenient and memory efficient representation, which expresses scene geometry in the 3D continuous world as a set of points lying on its surface. Different from others, point cloud omits definition of the free space and lacks geometrical structure. Few works have in fact applied point-based SSC. On objects, PCN <ref type="bibr" target="#b171">[172]</ref> was the first to address point-based completion followed by <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b145">146,</ref><ref type="bibr" target="#b154">155,</ref><ref type="bibr" target="#b155">156,</ref><ref type="bibr" target="#b160">[161]</ref><ref type="bibr" target="#b161">[162]</ref><ref type="bibr" target="#b162">[163]</ref><ref type="bibr" target="#b165">166,</ref><ref type="bibr" target="#b177">178]</ref>. However, point generation on scenes is more challenging and <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b179">180]</ref> circumvent this need by querying semantic classes at given locations. Occupancy grid encodes scene geometry as a 3D grid, in which cells describe semantic occupancy of the space. Opposed to point clouds, grids conveniently define neighborhood with adjacent cells, and thus enable easy application of 3D CNNs, which facilitates to extend deep learning architectures designed for 2D data into 3D <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b173">174,</ref><ref type="bibr" target="#b176">177]</ref>. However, the representation suffers from constraining limitations and efficiency drawbacks since it represents both occupied and free regions of the scene, leading to high memory and computation needs. Voxels are also commonly used as a support for implicit surface definition which we now describe.</p><p>Implicit surface encodes geometry as a gradient field expressing the signed distance to the closest surface, known as the Signed Distance Function (SDF). While fields are continuous by nature, for ease of implementation they are commonly encoded in a discrete manner. The value of the gradient field is estimated at specific locations, typically at the voxel centers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>, or at the point locations for point clouds <ref type="bibr" target="#b120">[121]</ref>. Implicit surface may also be used as input <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b173">174,</ref><ref type="bibr" target="#b176">177]</ref> to reduce the sparsity of the input data, at the expense of greedy computation. For numerical reason, most works encode in fact a flipped version (cf. f-TSDF, sec. 4.1). Meshes, explained in detail below, can be obtained from the implicit surface, by using meshification algorithms such as marching cubes <ref type="bibr" target="#b89">[90]</ref>.</p><p>Mesh enables an explicit surface representation of the scene by a set of polygons. Implementing deep-learning algorithms directly on 3D meshes is challenging and most works obtain the mesh from intermediate implicit voxel-based TSDF representations <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref> by minimizing distance values within voxels and applying meshification algorithms <ref type="bibr" target="#b89">[90]</ref>. Other alternatives contemplate applying view inpainting as in <ref type="bibr" target="#b58">[59]</ref> or using parametric surface elements <ref type="bibr" target="#b49">[50]</ref>, which are more oriented to object/shape completion. Furthermore, recent learning-based algorithms such as Deep Marching Cubes <ref type="bibr" target="#b82">[83]</ref> enable to obtain continuous meshes end-to-end from well sampled point clouds, but similarly have not been applied to fill large areas of missing information or scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic Scene Completion</head><p>The seminal work of Song et al. <ref type="bibr" target="#b136">[137]</ref> first addressed Semantic Scene Completion (SSC) with the observation that semantics and geometry are 'tightly intertwined'. While there have been great progress lately, the best methods still perform little below 30% mIoU on the most challenging SemanticKITTI benchmark <ref type="bibr" target="#b5">[6]</ref>, advocating that there is a significant margin for improvement. Inferring a dense 3D scene from 2D or sparse 3D inputs is in fact an ill-posed problem since the input data are not sufficient to resolve all ambiguities. As such, apart from <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b178">179]</ref>, all existing works rely on deep learning to learn semantics and geometric priors from large scale datasets reviewed in Sec. 3.1.</p><p>In the following, we provide a comprehensive survey of the SSC literature. Unlike some historical computer vision tasks, for SSC we found little consensus and a wide variety of choices exists. As such we also focus on the remaining avenues of research to foster future works.</p><p>The survey is organized in sections that follow a standard SSC pipeline, with each section analyzing the different line of choices. We start in Sec. 4.1 by reviewing the various input encoding strategies, broadly categorized into 2D/3D, and discuss their influence on the global problem framing. Following this, we study SSC deep architectures in Sec. 4.2. While initially, the task was addressed with vanilla 3D CNNs, other architectures have been leveraged such as 2D/3D CNNs, point-based networks, or various hybrid proposals. Sec. 4.3 presents important design choices, such as contextual aggregation which greatly influences any geometrical task like SSC, or lightweight designs to leverage the burden of 3D networks spanning from compact 3D representations to custom convolutions. We discuss the training strategies in Sec. 4.4, along with the losses and their benefits. Finally, a grouped evaluation of the metrics, methods performance and network efficiency is in Sec. 4.5.</p><p>We provide the reader with a digest overview of the field, chronologically listing methods in Tab. 2 -where columns follow the paper structure. Because SSC definition may overlap with some reconstruction methods that also address semantics, we draw the inclusion line in that SSC must also  <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b159">160]</ref> solves the view dependency but gradient is stronger at farther areas from the surface, being inadequate for learning-based methods.</p><p>In contrast, f-TSDF (d) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b173">174,</ref><ref type="bibr" target="#b176">177]</ref> has strongest gradient near the surface. (Source: <ref type="bibr" target="#b136">[137]</ref>).</p><p>complete semantics and geometry of unseen regions. We indistinctively report as SSC any method meeting these criteria. Looking at Tab. 2, it illustrates both the growing interest in this task and the current lack of consensus on input encoding, architectures, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Input Encoding</head><p>Given the 3D nature of the task, there is an evident benefit of using 3D inputs as it already withholds geometrical insights. As such, it is easier to leverage sparse 3D surface as input in the form of occupancy grid, distance fields, or point clouds. Another line of work uses RGB data in conjunction with depth data since they are spatially aligned and easily handled by 2D CNNs.</p><p>3D grid-based. In most works, 3D occupancy grid (aka Voxel Occupancy) is used <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b163">164,</ref><ref type="bibr" target="#b168">169]</ref>, encoding each cell as either free or occupied. Such representation is conveniently encoded as binary grids, easily compressed (cf. Sec. 4.3.4), but provides little input signal for the network. An alternative richer encoding is the use of TSDF ( <ref type="figure" target="#fig_3">Fig. 6c</ref>), where the signed distance d to the closest surface is computed at given 3D locations (usually, voxel centers), as in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. Instead of only providing input signal at the measurement locations like occupancy grids or point cloud, TSDF provides a richer supervision signal for the network. The sign for example provides guidance on which part of the scene is occluded in the input. The greedy 3D computation can be  <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b5">[6]</ref>, or <ref type="bibr" target="#b123">[124]</ref>. b Includes both NYUv2 <ref type="bibr" target="#b132">[133]</ref>, NYUCAD <ref type="bibr" target="#b37">[38]</ref>. c Includes both SUNCG <ref type="bibr" target="#b136">[137]</ref>, SUNCG-RGBD <ref type="bibr" target="#b85">[86]</ref>. d Includes both ScanNet <ref type="bibr" target="#b23">[24]</ref>, CompleteScanNet <ref type="bibr" target="#b163">[164]</ref>.  avoided using projective TSDF (p-TSDF, cf. <ref type="figure" target="#fig_3">Fig. 6b</ref>) which only computes the distance along the sensing path <ref type="bibr" target="#b100">[101]</ref>, but with the major drawback of being view-dependent. Highlighted by Song et al. <ref type="bibr" target="#b136">[137]</ref>, another limitation of TSDF or p-TSDF lies in the strong gradients being in the free space area rather than close to the surface where the networks need guidance. This is noticeable in <ref type="figure" target="#fig_3">Fig. 6b</ref>, 6c since the red/blue gradients are afar from the surface. To move strong gradients closer to the surface, they introduced flipped TSDF (f-TSDF, <ref type="figure" target="#fig_3">Fig. 6d</ref>) such that f-TSDF = sign(TSDF)(d max ? d) with d max the occlusion boundary, showing improvement in both network guidance and performance <ref type="bibr" target="#b136">[137]</ref>. However, the field yet lacks thorough study on the benefit of TSDF encoding. While f-TSDF is still commonly used in recent SSC <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b173">174,</ref><ref type="bibr" target="#b175">176,</ref><ref type="bibr" target="#b176">177]</ref> or SC <ref type="bibr" target="#b29">[30]</ref>, other approaches stick with original TSDF <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b175">176]</ref>. Furthermore, the benefit of f-TSDF over TSDF is questioned in <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b175">176]</ref> with experiments showing it may slightly harm the performance or bring negligible improvement. Except for the lighter p-TSDF, all TSDF-variants require high computation times and hinder real time implementations.</p><p>3D point cloud. Despite the benefit of such representation, only three recent SSC works <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b179">180]</ref> use point cloud as input encoding. In <ref type="bibr" target="#b179">[180]</ref>, RGB is used in conjunction to augment point data with RGB features, whereas <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b121">122]</ref> reproject point features in the top-view space. Despite the few SSC methods using points input, it is commonly used for object completion <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b154">155,</ref><ref type="bibr" target="#b165">166,</ref><ref type="bibr" target="#b171">172]</ref>.</p><p>2D representations. Alternatively, depth maps or range images (i.e. 2D polar-encoded LiDAR data) are common 2D representations storing geometrical information, therefore suitable candidates for the SSC task. Indeed many works <ref type="bibr">[6, 21, 54, 76-79, 84, 86, 159]</ref> used either representation alone or in conjunction with other modalities. Opposite to point cloud but similarly to grid representation, such encoding enables meaningful connexity of the data and cheap processing with 2D CNNs. Some works propose to transform depth into an HHA image <ref type="bibr" target="#b54">[55]</ref>, which keeps more effective information when compared to the single channel depth encoding <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b85">86]</ref>. Additional non-geometrical modalities such as RGB or LiDAR refraction intensity provide auxiliary signals specifically useful to infer semantic labels <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b179">180]</ref>. In practice, some works have shown that good enough semantic labels can be inferred directly from depth/range images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b168">169]</ref> to guide SSC. Interestingly, the vast majority of the literature only accounts for surface information while ignoring any free space data provided by the sensors (see <ref type="figure" target="#fig_0">Fig. 2</ref>). While free space labels might be noisy, we believe it provides an additional signal for the network which was found beneficial in <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b163">164]</ref>. Conversely, Roldao et al. <ref type="bibr" target="#b123">[124]</ref> relate that encoding unknown information brought little improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Architecture choices</head><p>Directly linked to the choice of input encoding, we broadly categorize architectural choices into 4 groups. In detail: Volume networks leveraging 3D CNNs to convolve volumetric grid representations, Point-based networks which compute features on points locations, View-Volume networks that learn the 2D-3D mapping of data with 2D and 3D CNNs, and Hybrid networks that use various networks to combine modalities of different dimension. All architectures output N ? C data (N the spatial dimensions and C the number of semantic classes) where the last dimension is the probability of either semantic class at the given location. In most works, the final prediction is the softmax output with the class probabilities. We refer to <ref type="figure">Fig. 7</ref> for a visual illustration of either architecture type.</p><p>Volume networks. As they are convenient for processing grid data 3D CNNs <ref type="figure">(Fig. 7b</ref>) are the most popular for SSC <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b163">164,</ref><ref type="bibr" target="#b168">169,</ref><ref type="bibr" target="#b173">174,</ref><ref type="bibr" target="#b175">176,</ref><ref type="bibr" target="#b176">177]</ref>.</p><p>Since completion heavily requires contextual information it is common practice to use a U-Net architecture <ref type="bibr" target="#b124">[125]</ref> (see <ref type="figure">Fig. 7</ref>), i.e. encoder-decoder with skip connections. The benefit of the latter is not only to provide contextual information 3D-backbone</p><formula xml:id="formula_0">: Feature Lifting : Feature Fusion 3D U-Net 2D Encoder FL 3D Encoder FL 2D Encoder Optional Multi-branch 2D 3D 2D 3D 2D Input 2D Input 3D Output Depth F RGB Depth HHA SSC 3D Encoder F FL 2D-backbone 2D U-Net FL 3D 2D FL 2D 3D 3D Decoder 3D Output 3D Input</formula><p>Occupancy fTSDF SSC (a) View-Volume Nets., 3D-backbone: <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b88">89]</ref>, 2Dbackbone.: <ref type="bibr" target="#b123">[124]</ref> 3D U-Net 3D Output 3D Input Occupancy fTSDF SSC (b) Volume Nets. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b163">164,</ref><ref type="bibr" target="#b168">169,</ref><ref type="bibr" target="#b173">174,</ref><ref type="bibr" target="#b175">176,</ref><ref type="bibr" target="#b176">177]</ref> ... ... ... ...  <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b121">122]</ref>, parallel-2D-3D: <ref type="bibr" target="#b20">[21]</ref>, pointaugmented: <ref type="bibr" target="#b179">[180]</ref>, mix-2D-3D: <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b77">78]</ref>  <ref type="figure">Fig. 7</ref>: Architectures for SSC. For compactness, we do not display all connections but rather focus on the global architectures and information exchanges between the different networks. F stands for any type of fusion. but also to enable meaningful coarser scene representation, used in SSC for outputting multi-scale predictions <ref type="bibr" target="#b123">[124,</ref><ref type="bibr" target="#b173">174]</ref> or for enabling coarse-to-fine refinement <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point</head><p>There are two important limitations of 3D CNNs: their cubically growing memory need, and the dilation of the sparse input manifold due to convolutions. To circumvent both, one can use sparse 3D networks like SparseConvNet <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b173">174]</ref> or Minkowski Network <ref type="bibr" target="#b22">[23]</ref> which conveniently operates only on input geometry, thus allowing high grid resolution where each cell contains typically a single point. While they were found highly beneficial for most 3D tasks, they show limited interest for SSC since the output is expected to be denser than the network input. Considering the output to be sparse, Dai et al. <ref type="bibr" target="#b24">[25]</ref> used a sparse encoder and a custom sparse generative decoder to restrict the manifold dilation, applied for SC rather than SSC. This is beneficial but cannot cope with large chunks of missing data. An alternative is <ref type="bibr" target="#b168">[169]</ref> that first performs pointwise semantic labeling using a sparse network. To enable a dense SSC output in <ref type="bibr" target="#b173">[174]</ref>, authors merge the output of multiple shared SparseConvNets applied on sub-sampled non-overlapping sparse grids. Both <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b173">174]</ref> are a clever use of sparse convolutions but somehow limit the memory and computational benefit of the latter. In <ref type="bibr" target="#b21">[22]</ref>, variational optimization is used to regularize the model and avoid the need for greedy high-capacity 3D CNN.</p><p>View-volume networks. To take advantage of 2D CNNs efficiency, a common strategy is to use them in conjunction with 3D CNNs as in <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b123">124]</ref>, see <ref type="figure">Fig. 7a</ref>. Two different schemes have been identified from the literature. The most common, as in <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b88">89]</ref>, employs a 2D CNN encoder to extract 2-dimensional features from 2D texture/geometry inputs (RGB, depth, etc.), which are then lifted to 3D and processed by 3D CNN <ref type="figure">(Fig. 7a</ref>, 3D-backbone). Optional modalities may be added with additional branches and mid-fusion scheme <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b88">89]</ref>. In brief, the use of sequential 2D-3D CNNs conveniently benefits of different neighboring definitions, since 2D neighboring pixels might be far away in 3D and vice versa, thus providing rich feature representation, at the cost of increased processing. To address this limitation, the second scheme ( <ref type="figure">Fig. 7a</ref>, 2D-backbone) projects 3D input data into 2D, then processed with normal 2D CNN <ref type="bibr" target="#b123">[124]</ref> significantly lighter than its 3D counterpart. The resulting 2D features are then lifted back to 3D and decoded with 3D convolutions, retrieving the third dimension before the final prediction. This latter scheme is irrefutably lighter in computation and memory footprint but better suited for outdoor scenes (cf. Sec. 4.5), as the data exhibits main variance along two axes (i.e. longitudinal and lateral).</p><p>Point-based networks. To ultimately prevent discretization of the input data, a few recent works <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b179">180]</ref> employ point-based networks, see <ref type="figure">Fig. 7c</ref>. In 2018, <ref type="bibr" target="#b171">[172]</ref> first proposed to apply permutation-invariant architecture <ref type="bibr" target="#b115">[116]</ref> to object completion with promising results. However, its use for generative SSC is hindered by the limited points generation capacity, the need for fixed size output, and the use of global features extraction. To date, only SPC-Net <ref type="bibr" target="#b179">[180]</ref> relies solely on a point-based network -X Conv [80] -to predict the semantics of observed and unobserved points. The fixed size limitation is circumvented by assuming a regular distribution of unobserved points, addressing the problem as a point segmentation task. Overall, we believe point-based SSC has yet attracted too few works and is a viable avenue of research.</p><p>Hybrid networks. Several other works combine architectures already mentioned, which we refer to as hybrid networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b179">180]</ref>, see <ref type="figure">Fig. 7d</ref>. A common 2D-3D scheme combines 2D and 3D features (e.g. RGB and f-TSDF) through expert modality networks in a common latent space decoded via a 3D CNN <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref>  <ref type="figure">(Fig. 7d</ref>, mix-2D-3D). This enables additional benefit with the combined use of texture and geometrical features. Similarly, IPF-SPCNet <ref type="bibr" target="#b179">[180]</ref> performs semantic segmentation from RGB image on an initial 2D CNN and lifts image labels to augment ad-hoc 3D points <ref type="figure">(Fig. 7d</ref>, point-augmented). In <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b121">122]</ref>, Point-Net <ref type="bibr" target="#b115">[116]</ref> encodes geometrical features from sub-set of points later convolved in a bird eye view (BEV) projection with 2D CNN in a hybrid architecture manner ( <ref type="figure">Fig. 7d</ref>, point-2D). Another strategy employs parallel 2D-3D branches to process the same data with different neighborhoods definition contained in the 2D projected image and 3D grid as in <ref type="bibr" target="#b77">[78]</ref>. Recently, S3CNet <ref type="bibr" target="#b20">[21]</ref> combines 3D voxelized f-TSDF and normal features with a 2D BEV <ref type="bibr" target="#b17">[18]</ref>, passing both modalities through sparse encoder-decoder networks for late fusion <ref type="figure">(Fig. 7d</ref>, parallel-2D-3D), achieving impressive results in outdoor scenes. A similar architecture is proposed by <ref type="bibr" target="#b0">[1]</ref> to perform what they refer to as scene extrapolation <ref type="bibr" target="#b137">[138]</ref>, by performing extrapolation of a half-known scene into a complete one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Design choices</head><p>The significant sparsity difference between the input data and the expected dense output imposes special design choices to be made, specifically to ensure efficient flow of information. In the following, we elaborate on the most important ones such as contextual awareness (Sec. 4.3.1), position awareness (Sec. 4.3.2), and fusion strategies (Sec. 4.3.3). Finally, we detail lightweight designs (Sec. 4.3.4) to efficiently process 3D large extent of sparse data, and the common refinement processes (Sec. 4.3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Contextual awareness</head><p>To correctly complete the missing information in the scene, it is necessary to gather contextual information from multiple scales, which enables to disambiguate between similar objects present in the scene. This makes it possible to capture both local geometric details and high-level contextual information <ref type="bibr" target="#b136">[137]</ref>. A common strategy to accomplish this is to add skip connections between different convolutional layers to aggregate features from different receptive fields <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b173">174,</ref><ref type="bibr" target="#b176">177]</ref>. Additionally, serial context aggregation with multi-scale feature fusion can be used as proposed in <ref type="bibr" target="#b77">[78]</ref>, <ref type="figure" target="#fig_5">Fig. 8a</ref>. In <ref type="bibr" target="#b136">[137]</ref>, the use of dilated convolutions (aka 'atrous') <ref type="bibr" target="#b170">[171]</ref> are proposed to increase receptive fields and gather context information at low computational cost. The strategy became popular among most works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b163">164,</ref><ref type="bibr" target="#b176">177]</ref>. Such convolutions are only suitable for dense networks (as opposed to sparse networks), and even then should only be applied in deeper layers of the network after dilation of the input manifold. In <ref type="bibr" target="#b85">[86]</ref>, a feature aggregation module is introduced by using Atrous Spatial Pyramid Pooling blocks (ASPP) <ref type="bibr" target="#b14">[15]</ref>, which exploits multi-scale features by employing multiple parallel filters with different dilation rates, <ref type="figure" target="#fig_5">Fig. 8c</ref>. A lightweight ASPP is presented in <ref type="bibr" target="#b76">[77]</ref>. Dilated convolutions in the ASPP module can be replaced by Residual dilated blocks <ref type="bibr" target="#b60">[61]</ref> to increase spatial context and improve gradient flow. A Guided Residual Block (GRB) to amplify fused features and a Global Aggregation module to aggregate global context through 3D global pooling are proposed in <ref type="bibr" target="#b18">[19]</ref>. An additional feature aggregation strategy is presented in <ref type="bibr" target="#b176">[177]</ref>, where multi-context aggregation is achieved by a cascade pyramid architecture, <ref type="figure" target="#fig_5">Fig. 8b</ref>. In <ref type="bibr" target="#b21">[22]</ref> multi-scale features are aggregated together following a Primal-Dual optimization algorithm <ref type="bibr" target="#b109">[110]</ref>, which ensures semantically stable predictions and further acts as a regularizer for the learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Position awareness</head><p>Geometric information contained in voxels at different positions has high variability, i.e. Local Geometric Anisotropy.</p><p>In particular, voxels inside an object are homogeneous and likely to belong to the same semantic category as their neighbors. Conversely, voxels at the surface, edges, and vertices of the scene provide richer geometric information due to the higher variance of their surroundings. To deal with this, PALNet <ref type="bibr" target="#b77">[78]</ref> proposes a Position Aware loss (cf. Sec. 4.4.1), which consists of a cross entropy loss with individual voxel weights assigned according to their geometric anisotropy, providing slightly faster convergence and improving results.</p><p>Likewise, AM 2 FNet <ref type="bibr" target="#b15">[16]</ref> supervises contour information by an additional cross entropy loss as a supplementary cue for segmentation.</p><p>In the same line of work, EdgeNet <ref type="bibr" target="#b31">[32]</ref> calculates Canny edges <ref type="bibr" target="#b11">[12]</ref> then fused with an f-TSDF obtained from the depth image. Hence, it increases the gradient along the geometrical edges of the scene. Additionally, detection of RGB edges enables the identification of objects lacking geometrical saliency. The same network is used in <ref type="bibr" target="#b32">[33]</ref> to predict complete scenes from panoramic RGB-D images.</p><p>Similarly, <ref type="bibr" target="#b16">[17]</ref> introduces an explicit and compact geometric embedding from depth information by predicting a 3D sketch containing scene edges from an input TSDF. They show that this feature embedding is resolution-insensitive, which brings high benefit, even from partial low-resolution observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Fusion strategies</head><p>SSC requires outputting both geometry and semantics. Though highly coupled -geometry helping semantics and vice-versa -, there is a natural benefit to use inputs of different natures for example to provide additional texture or geometry insights. We found that about two-thirds of the literature use multi-modal inputs though it appears less trendy in most recent works (see Tab. 2 col 'Input'). For the vast majority of multi-input works, RGB is used alongside various geometrical input <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b179">180]</ref> as it is a natural candidate for semantics. Even without color, the fusing of 2D and 3D modalities is often employed because it enables richer feature estimation. This is because 2D and 3D neighborhoods differ, since 2D data results of planar projection along the optical axis of the sensor. Subsequently, a common strategy consists of fusing geometrical features processed with different 2D / 3D encoding to obtain richer local scene descriptors. In <ref type="bibr" target="#b5">[6]</ref> depth and occupancy are fused while <ref type="bibr" target="#b77">[78]</ref> uses depth along with TSDF-like data. As mentioned earlier (cf. Sec. 4.1), TSDF provides a gradient field easing the network convergence. Finally, applicationoriented fusion is also found such as fusing bird eye view along with geometrical inputs as in <ref type="bibr" target="#b20">[21]</ref> -which is better suited for outdoor SSC.</p><p>We group the type of fusion in three categories, shown in <ref type="figure" target="#fig_6">Fig. 9</ref>. Fusion applied at the input level (Early fusion), at the mid-level features (Middle fusion), or at the late/output level (Late fusion). They are respectively referred to as E, M, and L in column 'Fusion strategies' Tab. 2.</p><p>Early fusion. The most trivial approach is to concatenate input modalities <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b179">180]</ref> before any further processing, see <ref type="figure" target="#fig_6">Fig. 9a</ref>. There are two strategies here: when spatially aligned (e.g. RGB/Depth) inputs can be concatenated channel-wise; alternatively, inputs can be projected  to a shared 3D common space (aka features lifting). For spatially aligned modalities, it is common to use pairs of normals/depth <ref type="bibr" target="#b53">[54]</ref> or RGB/semantics <ref type="bibr" target="#b5">[6]</ref> and to process them with 2D CNNs. The second strategy lifts any 2D inputs to 3D -assuming depth information and accurate inter-sensors calibration -and processes it with 3D networks. This has been done with RGB/depth <ref type="bibr" target="#b50">[51]</ref>, depth/semantics <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">43]</ref>, points/semantics <ref type="bibr" target="#b179">[180]</ref>. Except when using points, this second strategy leads to a sparse tensor since not all 3D cells have features. Noteworthy, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b179">180]</ref> use semantics, which is first estimated either from RGB or depth-like data. A 2D or 3D network processes the concatenated tensor, and while it logically outperforms single-modality input <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54]</ref> there seems to be little benefit to apply early fusion.</p><p>Middle fusion. To exploit all modalities, middle fusion uses expert networks that learn modality-centric features. A straightforward fusion strategy is employed in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b85">86]</ref> where the features are simply concatenated and processed with a U-Net style architecture (cf. <ref type="figure" target="#fig_6">Fig. 9b</ref>, single-stage), which improves over early fusion but still limits the exchange of information between modalities. The information flow is improved in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b88">89]</ref> by fusing modality-centric features in a multi-stage manner (cf. <ref type="figure" target="#fig_6">Fig. 9b</ref>, multi-stage); meaning that low-level features from different modalities are fused together and aggregated with fused mid/high level features gathered similarly. While ultimately the number of fusion stages shall be a function of the input/output size, 3 stages are often used <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77]</ref>, though <ref type="bibr" target="#b88">[89]</ref> claims 4 stages boost performances with similar input/output. The fused mechanism can be a simple summation <ref type="bibr" target="#b76">[77]</ref> or concatenation <ref type="bibr" target="#b75">[76]</ref>, but <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b88">89]</ref> benefit from smarter selective fusion schemes using respectively RefineNet <ref type="bibr" target="#b84">[85]</ref> and Gated Recurrent Fusion. Overall, the literature consensus is that middle fusion is highly efficient for SSC. The ablation studies of Liu et al. <ref type="bibr" target="#b88">[89]</ref> reports that any selective fusion schemes bring at least a 20% performance boost over simple sum/concat/max schemes.</p><p>Late fusion. Few works use late fusion for SSC <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86]</ref>, see <ref type="figure" target="#fig_6">Fig. 9c</ref>. The straightforward strategy in <ref type="bibr" target="#b78">[79]</ref> is to apply fusion -namely, element-wise multiplication -of two SSC branches (a 3D guidance branch, and a semantic completion branch), followed by a softmax. The benefit still appears little (5 to 10%) given the extra computational effort. Similarly, color and geometry branches are concatenated and shallowly convolved before softmax in <ref type="bibr" target="#b85">[86]</ref>, also providing a small benefit (less than 3%). A unique strategy was proposed in the recent S3CNet <ref type="bibr" target="#b20">[21]</ref> where the output of parallel 2D top-view and 3D SSC are fused together in a semantic-wise manner. While it was only evaluated on outdoor scenes -which setup naturally minimizes vertically overlapping semantic labelsablation reports an overall 20% boost.</p><p>Summarizing the different strategies, Middle fusion appears to be the best general SSC practice, though Late fusion was found beneficial in some specific settings. On fused modalities, RGB/geometry fusion boosts performance but at the cost of an additional sensor need, but even using fusion of geometrical input with different encodings is highly beneficial. An interesting insight from <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref> advocates that RGB or geometry can be fused with edge features as they provide additional boundaries guidance for the SSC network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Lightweight designs</head><p>A few techniques for lightweight designs are often applied for SSC with the aim of addressing two separate problems: how to improve the memory or computation efficiency, and how to design meaningful convolutions to improve the information flow. We detail either problem and its solutions below.</p><p>Memory and computation efficiency. Voxel grids are often used as input/output encoding of the 3D data since current datasets provide ground truth in such a format. However, only a tiny portion of the voxels are occupied which makes the naive dense grid inefficient in memory and computation. Memory wise, a few works use compact hierarchical 3D representation inspired from pre-deep learning, like Kd-Tree <ref type="bibr" target="#b6">[7]</ref> and Octree <ref type="bibr" target="#b93">[94]</ref>. Octree-based deep networks are often used for learning object reconstruction <ref type="bibr" target="#b118">[119,</ref><ref type="bibr" target="#b119">120,</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b152">153]</ref> though little applied on real semantic scene completion problem <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b156">157]</ref>. Meanwhile, deep Kd-Networks <ref type="bibr" target="#b70">[71]</ref> proposal seems less appropriate and has not yet been applied to SSC. Computation-wise, <ref type="bibr" target="#b21">[22]</ref> proposed a custom network architecture with adjustable multi-scale branches in which inference and backpropagation can be run in parallel, subsequently enabling faster training and good performance with low-capacity dense 3D CNNs. Alternatively, few SSC or SC works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b173">174]</ref> use sparse networks like SparseConvNet <ref type="bibr" target="#b47">[48]</ref> or Minkowski <ref type="bibr" target="#b22">[23]</ref> which operate only in active locations through a hash table. While sparse convolutions are very memory/computation efficient, they are less suitable for completion, since they deliberately avoid filling empty voxels to prevent dilation of the input domain. To remedy this for the SSC task, dense convolutions are still applied in the decoder, which subsequently reduces sparse networks efficiency. Overall, while Kd-/Octree-networks are highly memory efficient, the complexity of their implementation has restricted a wider application. Contrastingly, sparse networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b47">48]</ref> are more used <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b168">169,</ref><ref type="bibr" target="#b173">174]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient convolutions.</head><p>A key observation is the spatial redundancy of data since neighboring voxels contain similar information. To exploit such redundancy, <ref type="bibr" target="#b173">[174]</ref> proposes Spatial Group Convolutions (SGC) to divide input volume into different sparse tensors along the spatial dimensions which are then convolved with shared sparse networks. A similar strategy is followed by <ref type="bibr" target="#b27">[28]</ref>, dividing the volumetric space into a set of eight interleaved voxel groups and performing an auto-regressive prediction <ref type="bibr" target="#b116">[117]</ref>. Dilated convolutions are also widely used for semantic completion methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b78">[79]</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b176">177]</ref>, since they increase receptive fields at small cost, providing large context, which is crucial for scene understanding as discussed in Sec. 4.3.1. Dilated convolutions with separated kernels are proposed in <ref type="bibr" target="#b176">[177]</ref> by separating the input tensor into subvolumes. This enables to reduce the number of parameters and consider depth profiles in which depth values are continuous only in neighboring regions. DDRNet <ref type="bibr" target="#b76">[77]</ref> also introduces Dimensional Decomposition Residual (DDR) block, decomposing 3D convolutions into three consecutive layers along each dimension, subsequently reducing the network parameters. In <ref type="bibr" target="#b75">[76]</ref>, this concept is extended with the use of anisotropic convolutions, where the kernel size of each 1D convolution is adaptively learned during training to model the dimensional anisotropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Refinement</head><p>Refinement is commonly used in many vision tasks, but little applied in SSC. VD-CRF <ref type="bibr" target="#b175">[176]</ref> extends SSCNet <ref type="bibr" target="#b136">[137]</ref> by applying Conditional Random Field (CRF) to refine output consistency, achieving little over 4% gain. Additionally, S3CNet <ref type="bibr" target="#b20">[21]</ref> presents a 3D spatial propagation network <ref type="bibr" target="#b86">[87]</ref> to refine segmentation results after fusion of 2D semantically completed bird eye view image and 3D grid. Additional partial refinement is applied in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b163">164]</ref> to fuse SSC predictions from different viewpoints, by either softmax applied to overlapping partitions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b163">164]</ref> or an occupancy-based fusion policy <ref type="bibr" target="#b163">[164]</ref>. Though few works address the refinement problem, some notable performance boosts are found in the literature, thus being an encouraging topic to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>We now detail the SSC training process, starting with the SSC losses (Sec. 4.4.1), and subsequently the implemented training strategies (Sec. 4.4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Losses</head><p>We classify the SSC losses found in the literature in 3 broad categories: geometric losses which optimize geometrical accuracy, semantics losses which optimize semantics prediction, and consistency losses which guide the overall completion consistency. Note that other non-SSC losses are often added and that the type of SSC losses are commonly mixed -specifically, geometric+semantics <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b151">152]</ref> or all three types <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b159">160]</ref>. We refer to Tab. 2 for a quick overview of the losses used by each method. In this section, we also refer to? as SSC prediction and y as ground truth, though for clarity we add a subscript notation to distinguish between SSC encoding. For example, y mesh corresponds to the ground truth mesh.</p><p>Geometric losses. These losses penalize the geometrical distance of the output? to ground truth y, in a self-unsupervised manner.</p><p>On occupancy grids outputs (? occ ), Binary Cross-Entropy loss (BCE) is most often used <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b151">152]</ref> to discriminate free voxels from occupied. Assuming a binary class mapping where all non-free semantic classes map to 'occupy'. It writes:</p><formula xml:id="formula_1">L BCE = ? 1 N N i=0? occ i log(y occ i )?(1?? occ i )log(1?y occ i ) ,<label>(1)</label></formula><p>with N the number of voxels. The drawback of such loss is that it provides little guidance to the network due to its sparsity. Smoother guidance can be provided by outputting an implicit surface (? SDF ) through minimization of the predicted signed distance values in? SDF and corresponding SDFencoded mesh (y SDF ) -using 1 or 2 norms.</p><p>On points outputs (? pts ), if SSC is approached as a generative task, the above losses could also be used to penalize distance to a ground truth mesh, though it might be more suitable to apply points-to-points distances, thus assuming a ground truth point cloud (y pts ). To that end, permutation invariant metrics as the Chamfer Distance (CD) <ref type="bibr" target="#b35">[36]</ref> or the Earth Mover's Distance (EMD) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b72">73]</ref> have been employed for object completion tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b171">172]</ref> but have not been explored yet for SSC because of their computational greediness <ref type="bibr" target="#b35">[36]</ref>. We highlight that such losses could provide an additional geometric supervision signal when used in conjunction with semantic losses described below.</p><p>Semantic losses. Such losses are suitable for occupancy grids or points and can accommodate for either C classes (considering only semantics classes of occupied voxels or points) or C + 1 classes (considering all voxels/points and 'free space' being the additional class). Note that only the second case (C + 1 classes) enforce reconstruction, so the first one (C classes) would require additional geometric losses. Cross-Entropy loss (CE) is the preferred loss for SSC <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b173">174,</ref><ref type="bibr" target="#b176">177]</ref>, it models classes as independent thus considering the latter to be equidistant in the semantic space. Formally, supposing (y,?) it writes:</p><formula xml:id="formula_2">L CE = ? 1 C N i=0 N c=0 w c?i,c log e yi,c C c e y i,c ,<label>(2)</label></formula><p>assuming here that y is the one-hot-encoding of the classes (i.e. y i,c = 1 if y i label is c and otherwise y i,c = 0). In practice, (y,?) can be either occupancy grids (y occ ,? occ ) or points (y pts ,? pts ). A rare practice from <ref type="bibr" target="#b159">[160]</ref> is to address classification with BCE (Eq. 1) through the sum of C binary classification problems between each semantic class and the free class. However, such a practice is unusual and arguably beneficial. Recently, PALNet <ref type="bibr" target="#b77">[78]</ref> proposed the Position Aware loss (PA), a weighted cross-entropy accounting for the local semantics entropy to encourage sharper semantics/geometric gradients in the completion (cf. Sec 4.3.2). The loss writes:</p><formula xml:id="formula_3">L PA = ? 1 N N i=0 C c=0 (? + ?W LGAi )? occ i,c log e y occ i,c C c e y occ i,c ,<label>(3)</label></formula><p>with ? and ? being simple base and weight terms, and W LGAi being the Local Geometric Anisotropy of i that scales accordingly to the semantic entropy in its direct vicinity (i.e. W LGA lowers in locally smooth semantics areas). We refer to <ref type="bibr" target="#b77">[78]</ref> for an in-depth explanation. From the latter, L PA leads to a small performance gain of 1-3%. Noteworthy, this loss could easily accommodate point clouds as well.</p><p>Note that geometric or semantics losses can only be computed on known ground truth location, due to the ground truth sparsity. Additionally, because SSC is a highly imbalanced problem (cf. <ref type="figure">Fig. 3</ref>), class-balancing strategy is often used.</p><p>Consistency losses. Different from most semantics losses, these losses <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b121">122]</ref> provide a self-supervised semantic signal. In <ref type="bibr" target="#b16">[17]</ref> the completion consistency (CCY) of predictions from multiple partitioned sparse inputs is enforced via a Kullback-Leibler divergence. Differently, <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b121">122]</ref> enforces spatial semantics consistency (SCY) by minimizing the Jenssen-Shannon divergence of semantic inference between a given spatial point and some given support points. This self-supervision signal is available at any position within the scene. However, the strategy for support points is highly application dependent and while suitable for outdoor scenes which have repetitive semantic patterns, we conjecture it might not scale as efficiently to cluttered indoor scenes.</p><p>Overall, few self-supervised or even unsupervised strategies exist and we believe that such type of new losses <ref type="bibr" target="#b172">[173]</ref> should be encouraged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Training strategies</head><p>The vast majority of SSC works are trained end-to-end for single-scale reconstruction, although others prefer multiscale supervision to output SSC at different resolutions. Few works also employ multi-modal supervision commonly relying on auxiliary 2D semantic segmentation. It is also possible to train n-stage networks with coarse-to-fine strategies, or even train with adversarial learning to enforce realism. Strategies are illustrated in <ref type="figure" target="#fig_8">Fig. 10</ref> (with link color indicating the stage) and reviewed below.</p><p>End-to-end. Most architectures <ref type="figure" target="#fig_8">(Fig. 10a, top left)</ref> are trained end-to-end and output a single scale SSC <ref type="bibr">[19, 22, 32, 33, 43, 54, 76-78, 89, 121, 137, 152, 177]</ref> -often similar to the input size. Training that way is straightforward and often offers minimal memory footprint. Noteworthy, <ref type="bibr" target="#b24">[25]</ref> -which does geometric completion only -gradually increases sparsity during training to ease the completion of large missing chunks. To guide the training, multi-scale SSC outputs can also be supervised, typically from the early layers of a U-Net decoder. A simple, yet efficient multi-scale strategy <ref type="bibr" target="#b123">[124,</ref><ref type="bibr" target="#b173">174]</ref> is to minimize the sum of SSC losses at different resolutions <ref type="figure" target="#fig_8">(Fig. 10a, top right)</ref>, thus also enforcing coarse SSC representations in the network. In <ref type="bibr" target="#b173">[174]</ref>, two different scales are predicted, versus four in <ref type="bibr" target="#b123">[124]</ref> providing down to 1:8 (1 over 8) downscaled SSC. In the latter, authors also report that the decoder can be ablated to provide very fast inference at coarsest resolution (370FPS at 1:8 scale). When available, some </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point-based MLP</head><p>(a) End-to-end, single-scale: <ref type="bibr">[19, 22, 32, 33, 43, 54, 76-79, 89, 121, 137,152,177]</ref>, multi-scale: <ref type="bibr" target="#b123">[124,</ref><ref type="bibr" target="#b173">174]</ref>, multi-modal supervision: <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b179">180]</ref>  works leverage multi-modal supervision relying on intermediate auxiliary tasks, typically 2D or 3D semantics, later used along original input data to infer the final SSC <ref type="figure" target="#fig_8">(Fig. 10a</ref>, bottom), as in <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b179">180]</ref>. The latter could also be trained in a two-stage manner. In general, end-to-end training is conducted from scratch, though some <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b176">177]</ref> report pretraining on the synthetic SUNCG dataset.</p><p>Coarse-to-fine. ScanComplete <ref type="bibr" target="#b27">[28]</ref> also follows a multiscale strategy somehow close to <ref type="bibr" target="#b123">[124,</ref><ref type="bibr" target="#b173">174]</ref>, though training in a coarse-to-fine manner <ref type="figure" target="#fig_8">(Fig. 10b</ref>). In detail, three sequential training are achieved at increasingly higher resolutions, with each stage network taking as input the ad-hoc sparse input and the previous stage SSC prediction (for stage&gt;1). Interestingly, no one explored a continuous curriculum learning setting, which could yield stabler training and performance improvement. Still, <ref type="bibr" target="#b21">[22]</ref> (intentionally omitted <ref type="figure" target="#fig_8">Fig. 10b</ref>) applies a unique coarse-to-fine proposal in a fully end-to-end manner, via parallel backpropagations in all scales. Of simi-lar spirit, <ref type="bibr" target="#b24">[25]</ref> proposes an iteration-based progressive refinement during training for scene completion, but insights of such strategy are not deeply discussed.</p><p>Adversarial. Even SSC ground truth has large missing chunks of data, leading to ambiguous supervision. To address this, <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b158">159,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b163">164]</ref> use adversarial training <ref type="figure" target="#fig_8">(Fig. 10c</ref>), since the discriminator provides an additional supervision signal. This is straightforwardly implemented in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b163">164]</ref>, where the discriminator classifies ground truth from generated SSC (aka real/fake). In <ref type="bibr" target="#b158">[159,</ref><ref type="bibr" target="#b159">160]</ref> of same authors, 2 discriminators are used in a somehow similar fashion to discriminate both the SSC output and the latent depth or semantics features to enforce deep shared representation. Additionally, <ref type="bibr" target="#b16">[17]</ref> employs a Conditional Variational Autoencoder (CVAE) to generate completed border sketches to be fed to the main SSC branch. Despite few works on the matter, adversarial appears a logical choice to improve SSC consistency and provide additional self-supervision. Both <ref type="bibr" target="#b158">[159]</ref> and <ref type="bibr" target="#b163">[164]</ref> report a 10%-15% boost on several datasets.</p><p>Finally, on implementation -where mentioned -only <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b158">159,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b179">180]</ref> train with Adam optimizer, <ref type="bibr" target="#b19">[20]</ref> with a mix of Adam/SGD, and all others use only SGD with momentum 0.9 and 10 ?4 weight decay, except for <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b176">177]</ref> using 5 ? 10 ?4 . The training most often uses standard learning rate scheduler <ref type="bibr">[19, 43, 76-78, 89, 151, 152, 177, 180]</ref> though sophisticated scheduling <ref type="bibr" target="#b31">[32]</ref> or fixed learning rate <ref type="bibr" target="#b78">[79]</ref> are also used. Because of 3D greediness, the common practice is to train with small batch size of 1 <ref type="bibr" target="#b78">[79]</ref>, 2 <ref type="bibr" target="#b76">[77]</ref>, 3 <ref type="bibr" target="#b31">[32]</ref>, 4 <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b176">177,</ref><ref type="bibr" target="#b179">180]</ref>, 8 <ref type="bibr" target="#b150">[151,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b158">159,</ref><ref type="bibr" target="#b159">160]</ref> or 16 <ref type="bibr" target="#b18">[19]</ref> to fit in standard 12GB GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation</head><p>We now provide an in-depth evaluation of the field, reviewing first the common metrics (Sec. 4.5.1), the qualitative and quantitative performance of the literature (Sec. 4.5.2), and the networks' efficiency (Sec. 4.5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Metrics</head><p>Joint Semantics-Geometry. The preferred metric for SSC is the mean Jaccard Index or mean Intersection over Union (mIoU) <ref type="bibr" target="#b34">[35]</ref>, which considers IoU of all semantic classes for prediction, without considering free space. It writes</p><formula xml:id="formula_4">mIoU = 1 C C c=1 TP c TP c + FP c + FN c ,<label>(4)</label></formula><p>where TP c , FP c and FN c are the true positives, false positives and false negatives predictions for class c, respectively. Since ground truth is commonly semi-dense for real-world datasets, evaluation is performed in known space only.</p><p>Geometry only. Because mIoU considers semantic classes, the pure geometrical reconstruction quality is not encompassed. Therefore Intersection over Union (IoU), along with Precision and Recall are commonly used on the binary free/occupy scene representation, obtained by mapping all semantic classes to occupy. Alternatively, any distance metrics from Sec. 4.4.1 (i.e.</p><p>On common practice, we highlight that evaluation on real indoor or outdoor datasets is usually performed differently. This results of the common sensors setup, respectively RGB-D (indoor) and LiDAR (outdoor), providing significantly different density information. Referring to <ref type="figure" target="#fig_0">Fig. 2</ref>, in real indoor <ref type="bibr" target="#b136">[137]</ref> the geometrical IoU is evaluated on input occluded regions while the mIoU is evaluated on input occluded (blue) and observed (red) surfaces. In real outdoor <ref type="bibr" target="#b5">[6]</ref> the IoU and mIoU are commonly evaluated on the entire known space, regardless of whether regions were observed or occluded in the input. Obviously, synthetic datasets can cope with either practice. In the following, we describe the common practices and report semantics metrics (mIoU) along with geometrical ones (Precision, Recall, IoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Performance</head><p>We report the available mIoU and IoU performance on the most popular SSC datasets in Tab. 3, which are all obtained from voxelized ground truth. For non-voxel methods the output is voxelized beforehand. Additionally, detailed classwise performance of top five methods for SemanticKITTI, NYUv2 and SUNCG are presented in Tabs. 4, 5 and 6, respectively. From the performance Tab. 3, the mIoU of the best methods plateaus around 75 ? 85% on synthetic indoor dataset, 52% on real indoor, and 30% on real outdoor. Importantly, note that most indoor datasets performance are evaluated at 1:4 of the original ground truth resolution -that is 60?36?60 -for historical reasons 4 . Additionally, some methods refine parameters on NYUv2 and NYUCAD after SUNCG pre-training and are shown separately in Tab. 7. This makes indoor / outdoor performance comparison tricky. It is interesting to note that IoU -geometrical completion (i.e. ignoring semantics)is way higher than best mIoU. In detail, best IoU are 78% on <ref type="bibr" target="#b3">4</ref> In their seminal work, for memory reason Song et al. <ref type="bibr" target="#b136">[137]</ref> evaluated SSC only at the 1:4 scale. Subsequently, to provide fair comparisons between indoor datasets and methods, most other indoor SSC have been using the same resolution despite the fact that higher resolution ground truth is available. Recent experiments in <ref type="bibr" target="#b16">[17]</ref> advocate that using higher input/output resolution boosts the SSC performance significantly. real indoor, and 57% on real outdoor. Qualitative results of a dozen of methods are shown in <ref type="figure">Fig. 11</ref> for indoor datasets, and <ref type="figure" target="#fig_0">Fig. 12</ref> for outdoor datasets.</p><p>Overall, one may note the synthetic to real best performance gap of indoor datasets, which is approx. 10 ? 35% mIoU and 10 ? 18% IoU. While a difference is expected, once again it highlights that geometry has a smaller synthetic/real domain gap compared to semantics. On a general note also, most methods perform significantly better on IoU than on mIoU, demonstrating the complexity of the semantics scene completion. In fact, the ranking of methods differs depending on the metric. For example, on NYUv2 (indoor) the recently presented SISNet has best mIoU (52.4%) and IoU (78.2%) by a large margin thanks to their semantic instance completion block and iterative SSC architecture. By reliyng solely on SSC, CCPNet <ref type="bibr" target="#b176">[177]</ref> gets best indoor mIoU (41.3%), although achieved through SUNCG pre-training. Conversely, 3DSketch <ref type="bibr" target="#b16">[17]</ref> achieves similar mIoU (41.1%) by training solely on NYUv2 and Chen et al. <ref type="bibr" target="#b18">[19]</ref> is second overall on IoU (73.4%) with the same setup. On Se-manticKITTI (outdoor) S3CNet <ref type="bibr" target="#b20">[21]</ref> has best mIoU (29.5%) and Local-DIFs <ref type="bibr" target="#b120">[121]</ref> best IoU (57.7%). Note also the large difference between best indoor/outdoor metrics. While only a handful of methods <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b173">174]</ref> are evaluated in both setups, they indeed perform significantly worse on outdoor data -though indoor/outdoor performance should be carefully compared given the different resolution. This is partially explained by the higher sparsity in outdoor datasets, visible in 'input' of <ref type="figure" target="#fig_0">Fig. 12</ref>. Another explanation is the higher number of classes in SemanticKITTI versus NYU and the extreme class-imbalance setup given that minor classes are very rarely observed, see <ref type="figure">Fig. 3</ref>. On general qualitative results, either indoor ( <ref type="figure">Fig. 11</ref>) or outdoor ( <ref type="figure" target="#fig_0">Fig. 12</ref>) results show that predictions are accurate in large homogeneous areas (walls/ground, floor, buildings) and most errors occur at object boundaries. This is evident in Tab. 4, where most methods achieve high performance in the largest classes of SemanticKITTI, but struggle with predictions in less represented ones (e.g. bicycle, motorcycle, person). Worth mentioning, S3CNet <ref type="bibr" target="#b20">[21]</ref> achieves considerably larger scores in rare classes (+25%, +37%, +38% respectively), more than twice when compared to next best classed scores. The reason for such behavior is regrettably not deeply explored in their work.</p><p>Inputs. To ease interpretation, col 'Input' in Tab. 3 shows the nature of input used, where 'G' is Geometry of any type (depth, TSDF, points, etc.) -possibly several -and 'T' is Texture (RGB). From Tab. 3 using both geometry and texture (G+T) performs among the best indoor, such as SISNet <ref type="bibr" target="#b10">[11]</ref> using both RGB and TSDF, and 3DSketch <ref type="bibr" target="#b16">[17]</ref> which relies on textural edges and depth. Generally speaking, G+T enables the prediction of non-salient geometric objects (i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indoor Outdoor</head><p>Real-world Synthetic Real-world NYUv2 <ref type="bibr" target="#b132">[133]</ref> NYUCAD <ref type="bibr" target="#b37">[38]</ref> SUNCG <ref type="bibr" target="#b136">[137]</ref> SemanticKITTI <ref type="bibr">[</ref>   paints, windows, doors) as shown in <ref type="figure">Fig. 11a</ref> by the door predicted 3DSketch <ref type="bibr" target="#b16">[17]</ref> and missed by SSCNet <ref type="bibr" target="#b136">[137]</ref>. Noteworthy, among the best mIoU methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b176">177]</ref> all use TSDF-encoding as geometrical input. On outdoor datasets, only TS3D <ref type="bibr" target="#b42">[43]</ref> uses texture without significant improvement. More works are required to evaluate the benefit of RGB modality on outdoor data.</p><formula xml:id="formula_5">- - - - LMSCNet [124] G - - - - - - - - - - - - 77.1 66.2 55.3 17.0 LMSCNet-SS [124] G - - 62.2 ? 28.4 ? - - - - - - - - 81.6 65.1 56.7 17.6 S3CNet [21] G - - - - - - - - - - - - - - 45.6 29.5 JS3C-Net [169] G - - - - - - - - - - - - 71.5 73.5 56.6 23.8 Local-DIFs [121] G - - - - - - - - - - - - - -</formula><p>Architecture and design choices. One may notice the good performance of hybrid networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b179">180]</ref>  <ref type="figure">(Fig. 7d</ref>), which we believe results from richer input signal due to the fusion of multiple modalities. We also argue that multiple neighboring definitions (2D and 3D) provide beneficial complementary signals. For instance, S3CNet combines 2D BEV and 3D f-TSDF for late fusion through postprocessing refinement, achieving the best semantic completion performance on SemanticKITTI <ref type="bibr" target="#b5">[6]</ref> by a considerable margin (+5.7% mIoU). Qualitative results of the approach are shown in <ref type="figure" target="#fig_0">Fig. 12c, 12d</ref>. Similarly, JS3CNet <ref type="bibr" target="#b168">[169]</ref> ranks second in the same dataset (23.8% mIoU and 56.6% IoU) with point-wise semantic labeling through SparseConvNet architecture <ref type="bibr" target="#b47">[48]</ref> and dense semantic completion using a point-voxel interaction module, enabling to better infer small vehicles as shown in circled areas of <ref type="figure" target="#fig_0">Fig. 12e, 12f</ref>. Analogously, PALNet <ref type="bibr" target="#b77">[78]</ref> middle fuses depth image and f-TSDF features, achieving good performance on NYUv2 (34.1% mIoU and 61.3% mIoU) and NYUCAD (46.6% mIoU and 80.8% mIoU) datasets, such performance can also be attributed to its position-aware loss, to be discussed next.</p><p>Contextual awareness (Sec. 4.3.1) seems also to play an important role for the task. This is noticeable with CCP-Net <ref type="bibr" target="#b176">[177]</ref> encouraging results given the use of a single geometric input (see <ref type="figure">Fig. 11d, 11e</ref>). Note however that in addition to its lightweight design, the output of CCPNet is higher in resolution (240 ? 144 ? 240) which was proved to    boost performance <ref type="bibr" target="#b176">[177]</ref>. The performance of SISNet <ref type="bibr" target="#b10">[11]</ref> is also remarkable thanks to instance-wise completion at high resolution and iterative SSC. On position awareness (Sec. 4.3.2) it seems to boost intra-class consistency together with inter-class distinction. For example 3DSKetch <ref type="bibr" target="#b16">[17]</ref> and PALNet <ref type="bibr" target="#b77">[78]</ref>, both use position awareness and achieve high performances in indoor Indoor Real-world Synthetic NYUv2 <ref type="bibr" target="#b132">[133]</ref> NYUCAD <ref type="bibr">[</ref>   <ref type="bibr" target="#b76">[77]</ref> 0.20 27.2 CCPNet <ref type="bibr" target="#b176">[177]</ref> 0.08 6.5 2020 GRFNet <ref type="bibr" target="#b88">[89]</ref> 0.82 713 PALNet <ref type="bibr" target="#b77">[78]</ref> 0.22 78.8 AIC-Net <ref type="bibr" target="#b75">[76]</ref> 0.72 96.77 Chen et al. <ref type="bibr" target="#b18">[19]</ref> 0.07 1.6 a Reported in <ref type="bibr" target="#b176">[177]</ref>.</p><p>(a) <ref type="bibr" target="#b59">60</ref>    <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b132">133,</ref><ref type="bibr" target="#b136">137]</ref> or 256 ? 32 ? 256 for outdoor dataset <ref type="bibr" target="#b5">[6]</ref>.</p><p>door and outdoor networks because they have different output resolutions. Notice the extreme variations between networks, which scale from 1:144 in number of parameters and 1:1260 in FLOPs. Chen et al. <ref type="bibr" target="#b18">[19]</ref> and LMSCNet <ref type="bibr" target="#b123">[124]</ref> are by far the lightest networks with the fewest parameters and lower FLOPs, in indoor and outdoor settings respectively. They also account for the lower number of operations, which canthough not necessarily <ref type="bibr" target="#b91">[92]</ref> -contribute to faster inference times. Furthermore, the use of sparse convolutions <ref type="bibr" target="#b47">[48]</ref> is commonly applied as a strategy to reduce memory overhead in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b168">169,</ref><ref type="bibr" target="#b173">174]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Despite growing interest, there are still major challenges to solve SSC as the best methods still perform poorly on real datasets (see Tabs. <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4)</ref>. In this section, we wish to highlight important remaining issues and provide future research directions.</p><p>Best practices for SSC. Among the various viable choices for SSC, some were proven highly beneficial. For instance, contextual aggregation (Sec. 4.3.1) improves the information flow. In that sense, fusion of low and high level features with different receptive fields provides a contextual signal for the network, benefiting both the identification of finegrained details and the overall scene understanding. Like for semantic segmentation, features aggregation significantly boosts performance, typically with UNet <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b168">169,</ref><ref type="bibr" target="#b173">174]</ref>, Cascade Pyramid <ref type="bibr" target="#b176">[177]</ref> or Feature Aggregation Modules <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b88">89]</ref>. Further geometrical cues often boost SSC, whether if it is multiple geometrical representations (e.g. depth + voxel, Sec. 4.1) or boundaries (e.g. edges, Sec. 4.3.2). Multi-task and auxiliary tasks can also bring a performance boost. This is commonly done in the form of separated semantic segmentation <ref type="bibr" target="#b168">[169]</ref> or instance completion <ref type="bibr" target="#b10">[11]</ref> to improve input to SSC. Sketch supervision <ref type="bibr" target="#b16">[17]</ref> -which comes at virtually no cost -was shown particularly helpful to boost indoor performance but might be less beneficial for outdoor scenarios given larger semantic geometrical variance (i.e. vegetation). On networks, sparse convolutions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b168">169]</ref> can reduce memory needs and therefore enable higher resolution, although they must be combined with dense convolutions. Similar for dilated convolutions that have been used in the wide majority of SSC works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b163">164,</ref><ref type="bibr" target="#b176">177]</ref>. On training, <ref type="bibr" target="#b120">[121]</ref> shows that free space supervision close to the geometry can provide sharper inference, and we believe adversarial training (Sec. 4.4.2) is key to cope with the ground truth ambiguities. Another way to have an additional signal is the use of position aware losses <ref type="bibr" target="#b77">[78]</ref> which provides additional spatial supervision and was shown to bring performance improvements in both indoor <ref type="bibr" target="#b77">[78]</ref> and outdoor scenarios <ref type="bibr" target="#b20">[21]</ref>. On evaluation, we encourage authors to evaluate on both indoor and outdoor datasets which exhibit different challenges. Finally, for real-time applications, more works like <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b123">124]</ref> should account for lightweight and fast inference architectures (Sec. 4.5.3).</p><p>Supervision bias. An important challenge for completion results from the big imbalance ratio between free and occupied space (9:1 in both NYUv2 <ref type="bibr" target="#b132">[133,</ref><ref type="bibr" target="#b136">137]</ref> and SemanticKITTI <ref type="bibr" target="#b5">[6]</ref>) which biases the networks towards free space predictions. To deal with this problem, random undersampling of the major free class is often applied <ref type="bibr" target="#b136">[137]</ref> to reach an acceptable 2:1 ratio. The strategy reportedly improves completion performance (i.e. +4% IoU <ref type="bibr" target="#b136">[137]</ref>) and is widely employed <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b173">174,</ref><ref type="bibr" target="#b176">177]</ref>. Similarly, the loss can be balanced to favor occupy predictions <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b88">89]</ref>. Again, few works like <ref type="bibr" target="#b120">[121]</ref> efficiently benefit from free space information.</p><p>Semantic class balancing. Imbalance is also present in the semantic labels, especially in outdoor datasets, where there is a prevalence of road or vegetation (see <ref type="figure">Fig. 1 and 3)</ref>. Classbalancing can be applied to mitigate imbalanced distribution, usually weighting each class according to the inverse of its frequency <ref type="bibr" target="#b123">[124]</ref>, though prediction of under-represented classes still suffers (e.g. pedestrian or motorcycle in <ref type="bibr" target="#b5">[6]</ref>). This may have a catastrophic impact on robotics applications. An approach worth mentioning is S3CNet <ref type="bibr" target="#b20">[21]</ref>, where NYUCAD <ref type="bibr" target="#b37">[38]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>Depth Visible Surface Ground Truth SSCNet <ref type="bibr" target="#b136">[137]</ref> 3DSKetch <ref type="bibr" target="#b16">[17]</ref> (a) RGB Depth Visible Surface Ground Truth DDRNet <ref type="bibr" target="#b76">[77]</ref> PALNet <ref type="bibr" target="#b77">[78]</ref> 3DSKetch <ref type="bibr" target="#b16">[17]</ref> AIC-Net <ref type="bibr" target="#b75">[76]</ref> SISNet <ref type="bibr" target="#b10">[11]</ref> (b) RGB Depth Visible Surface Ground Truth SSCNet <ref type="bibr" target="#b136">[137]</ref> DDRNet <ref type="bibr" target="#b76">[77]</ref> PALNet <ref type="bibr" target="#b77">[78]</ref> 3DSKetch <ref type="bibr" target="#b16">[17]</ref> (c)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUNCG [137]</head><p>Depth Visible Surface Ground Truth SSCNet <ref type="bibr" target="#b136">[137]</ref> VVNet <ref type="bibr" target="#b53">[54]</ref> ESSCNet <ref type="bibr" target="#b173">[174]</ref> CCPNet <ref type="bibr" target="#b176">[177]</ref> (d)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth</head><p>Visible Surface Ground Truth SSCNet <ref type="bibr" target="#b136">[137]</ref> VVNet <ref type="bibr" target="#b53">[54]</ref> ESSCNet <ref type="bibr" target="#b173">[174]</ref> CCPNet <ref type="bibr" target="#b176">[177]</ref> (e) Sofa <ref type="table">Table  TVs</ref> Furn. Objects Bed Chair Window Wall Floor Ceil. <ref type="figure">Fig. 11</ref>: Performance of indoor Semantic Scene Completion on NYUCAD <ref type="bibr" target="#b37">[38]</ref> and SUNCG <ref type="bibr" target="#b136">[137]</ref>. Methods with RGB modalities (i.e. 3DSketch) enable detection of color salient objects as the highlighted door in row (a). Position awareness also contributes to better reconstruction consistency and inter-class distinction as seen in rows (b), (c) by PALNet <ref type="bibr" target="#b77">[78]</ref> and 3DSketch <ref type="bibr" target="#b16">[17]</ref>. Furthermore, SISNet <ref type="bibr" target="#b10">[11]</ref> overcomes all other methods through their scene-instance-scene loop architecture, seen in row (b). Multi-scale aggregation also improves reconstruction performance as seen on rows (d), (e), where CCPNet <ref type="bibr" target="#b176">[177]</ref> achieves the best performance on SUNCG <ref type="bibr" target="#b136">[137]</ref>. combined weighted cross entropy and position aware loss (cf. Sec. 4.4.1) achieve impressive improvements in underrepresented classes of SemanticKITTI. We believe SSC could benefit from smarter balancing strategies.</p><p>Object motion. As mentioned in Sec. 3.1, real-world ground truth is obtained by the rigid registration of contiguous frames. While this corrects for ego-motion, it doesn't account for scene motion and moving objects produce temporal tubes in the ground truth, as visible in SemanticKITTI <ref type="bibr" target="#b5">[6]</ref>  <ref type="figure">(Fig. 4c</ref>). As such, to maximize performance, the SSC network must additionally predict the motion of any moving objects. To evaluate the influence of such imperfections for SSC, some works reconstruct target scenes by accounting only for a few future scans <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b168">169]</ref>. Results show marginal comple-tion improvement from the application of such a strategy. An alternative proposal <ref type="bibr" target="#b69">[70]</ref>, is to remove dynamic objects from the detection of spatial singularities after frames registration. On the challenging SemanticKITTI <ref type="bibr" target="#b5">[6]</ref>, because there are few insights to classify dynamic objects, all methods tend to predict vehicles as stationary (cf. <ref type="figure" target="#fig_0">Fig. 12</ref>) -producing appealing results but being punished by dataset metrics. This obviously results from the dataset bias, given the abundance of parked vehicles.</p><p>The introduction of larger synthetic datasets <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b125">126]</ref> could be an interesting solution to fight ground truth inaccuracies.</p><p>Datasets extendable for SSC. Because semantic labeling is the most complex and costly, we denote that a large amount of SemanticKITTI <ref type="bibr" target="#b5">[6]</ref> Input</p><p>Ground Truth SSCNet-full <ref type="bibr" target="#b123">[124,</ref><ref type="bibr" target="#b136">137]</ref> LMSCNet <ref type="bibr" target="#b123">[124]</ref> (a)  <ref type="figure" target="#fig_0">Fig. 12</ref>: Performance of outdoor Semantic Scene Completion on SemanticKITTI <ref type="bibr" target="#b5">[6]</ref>. LMSCNet <ref type="bibr" target="#b123">[124]</ref> proposes a lightweight architecture with small performance decrease, rows (a), (b). S3CNet <ref type="bibr" target="#b20">[21]</ref> achieves SoA performance by their sparse bird's eye view and 3D f-TSDF feature encoders, rows (c), (d). Two-stage JS3CNet <ref type="bibr" target="#b168">[169]</ref> performs point-wise semantic segmentation and semantic scene completion sequentially, enabling better completion as seen in rows (e), (f). Finally, Local-DIFs <ref type="bibr" target="#b120">[121]</ref> enables continuous surface prediction, thanks to deep implicit functions, which enable predictions of considerably larger spatial extent, rows (g), (h).  <ref type="bibr" target="#b9">[10]</ref> 2020 Real-world Outdoor Points/RGB ? Lidar- <ref type="bibr">32 32</ref> Dense scenes registration 1000 Toronto-3D <ref type="bibr" target="#b143">[144]</ref> 2020 Real-world Outdosor ? Points Lidar- <ref type="bibr">32 8</ref> Sparse input scene subsampling 4 3D-FRONT <ref type="bibr" target="#b38">[39]</ref> 2020 Synthetic Indoor ? Mesh --Sparse input from virtual RGB-D - ? Synthetically augmented. <ref type="table">Table 9</ref>: SSC-extendable datasets. To promote research on SSC we highlight that existing 3D semantic datasets could be extended for SSC, at the cost of processing work (cf. col. 'Extension'). While some extensions could be obtained with little processing (e.g. Replica <ref type="bibr" target="#b139">[140]</ref>, 3D-Front <ref type="bibr" target="#b38">[39]</ref>), others are significantly more complex (e.g. nuScenes <ref type="bibr" target="#b9">[10]</ref>).</p><p>existing 3D semantics datasets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b139">140,</ref><ref type="bibr" target="#b143">144,</ref><ref type="bibr" target="#b148">149]</ref> could also be extended to SSC at the cost of some processing effort. A selective list of these SSC-extendable datasets is in Tab. 9 and we believe that their use should be encouraged to serve the interest of research on SSC. Interestingly, most need little processing for SSC (e.g. sparse input generation from 3D meshes or point clouds, virtual sensor configurations) <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b139">140,</ref><ref type="bibr" target="#b143">144,</ref><ref type="bibr" target="#b148">149]</ref>, though some require more complex processing (e.g. aggregation of sparse inputs <ref type="bibr" target="#b9">[10]</ref>). We also encourage the use of autonomous driving simulators such as CARLA <ref type="bibr" target="#b30">[31]</ref>, SYNTHIA <ref type="bibr" target="#b125">[126]</ref> for synthetic dataset generation, devoid of dynamic objects and subsequent registration problems. More extensive surveys on RGB-D and Lidar datasets are provided in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper provided a comprehensive survey on contemporary state-of-the-art methods for 3D Semantic Scene Completion. We reviewed, and critically analyzed major aspects of proposed approaches, including important design choices to be considered, and compared their performance in popular SSC datasets. We believe that this survey will support further development in the field, aiming to provide new insights and help inexpert readers to navigate the field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Scene acquisition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>. 5 .</head><label>5</label><figDesc>While voxel grid or point cloud are the most used, other implicit/explicit representations are of interest for applications like rendering, fluid simulation, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(Fig. 5 :</head><label>5</label><figDesc>2 https://futurism.com/tech-suing-facebook-princeton-data 3 https://competitions.codalab.org/competitions/22037 SSC representations. Several 3D representations coexist in the literature. Its choice has a major impact on the method to use, as well as the memory or the computation needs. (Source:<ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b153">154,</ref><ref type="bibr" target="#b159">160]</ref>)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>TSDF variants. Projective TSDF (b) is fast to obtain but view-dependent. TSDF (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Contextual Awareness -DC, Dilated Convolutions. (LW)-ASPP, (Lightweight) Atrous Spatial Pyramid Pooling. CCP, Cascaded Context Pyramid. FAM, Feature Aggregation Module. AIC, Anisotropic Convolutional Module. GA, Global Aggregation. MSFA, Multi-scale Feature Aggregation. PDA, Primal-Dual Algorithm. Fusion Strategies -E, Early. M, Middle. L, Late. Lightweight Design -GrpConv, Group Convolution. DDR, Dimensional Decomposition Residual Block. RAB, Residual Attention Block. MSO, MultiScale Optimization. Losses -Geometric: BCE, Binary Cross Entropy. 1 , L1 norm. Semantic: CE, Cross Entropy. PA, Position Awareness. Consistency: CCY, Completion Consistency. SCY, Spatial Semantics Consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Multi-scale contextual aggregation. While context is indubitably important for SSC, different strategies are used to aggregate features from various spatial/scale contexts. Color blocks stand for convolutions with different dilation rates. F stands for any type of fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Fusion Strategies. To accommodate for multiple input modalities (Mod. 1, Mod. 2), several fusion strategies are found in the literature. Here, F stands for fusion and might be any type of fusion like concat C , sum + , multiply ? , convolutions, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>... ... ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FFig. 10 :</head><label>10</label><figDesc>Training strategies. Most SSC architectures are trained end-to-end (a) outputting single or multi-scale SSC. Additionally, multi-modal supervision training commonly lift semantic features calculated on sparse input to a second stage network. Coarse-to-fine (b), similarly to multi-scale relies on multiple size predictions, but trains in a multi-stage coarse to fine manner. Finally, Adversarial training (c) discriminates between ground truth and predicted scenes. F stands for fusion of any type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Derivates from subset of SUNCG<ref type="bibr" target="#b136">[137]</ref> where missing RGB images were rendered. d Derivates from ScanNet<ref type="bibr" target="#b23">[24]</ref> by fitting CAD models to dense mesh.</figDesc><table><row><cell></cell><cell>d 2020 Real-world  ? Indoor</cell><cell>RGB-D ? Mesh/Voxel</cell><cell>RGB-D</cell><cell>11</cell><cell>1513</cell><cell>45448/11238</cell></row><row><cell>SemanticPOSS [105]</cell><cell>2020 Real-world Outdoor</cell><cell>Points/RGB ? Points</cell><cell>Lidar-40</cell><cell>14</cell><cell>2988</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? Simulated sensor.? Synthetically augmented.a Mesh annotations from [52].b Derivates from NYUv2 [133] by rendering depth images from mesh annotation.c* SSC: Semantic Scene Completion; DE: Depth Estimation; SPE: Sensor Pose Estimation; SS: Semantic Segmentation; OC: Object Classification; SNE: Surface Normal Estimation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Semantic Scene Completion (SSC) methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Net ... ... ... ... ... ... ... .. ... ... ...</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3D Input</cell><cell>-based MLP</cell><cell>3D Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Point cloud</cell><cell>SSC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Obs. Point Encoder</cell><cell>Obs. to Occl. Point Decoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(c) Point-based Nets. [180]</cell></row><row><cell>point-2D</cell><cell>3D Input Point cloud</cell><cell cols="2">Point-based MLP</cell><cell cols="4">Bird-Eye View</cell><cell cols="3">Point-based MLP 2D U-3D Output SSC</cell></row><row><cell>par.-2D-3D</cell><cell>Occupancy fTSDF 3D Input</cell><cell cols="3">3D U-Net</cell><cell></cell><cell></cell><cell cols="2">Bird-Eye View 2D Input</cell><cell cols="2">2D U-Net</cell><cell>F</cell><cell>3D Output SSC</cell></row><row><cell>point-aug.</cell><cell>2D Input</cell><cell cols="2">2D U-Net</cell><cell></cell><cell>2D</cell><cell>FL</cell><cell>3D</cell><cell cols="2">3D Input Point cloud</cell><cell>F</cell><cell>Point-based MLP Obs. Point Obs. to Occl. Encoder Point Decoder 3D Output SSC</cell></row><row><cell>mix-2D-3D</cell><cell>RGB Depth HHA 2D Input</cell><cell>2D Encoder 2D</cell><cell>FL</cell><cell>3D</cell><cell cols="3">Occupancy fTSDF 3D Input</cell><cell>3D Encoder</cell><cell>F</cell><cell>3D U-Net</cell><cell>3D Output SSC</cell></row><row><cell cols="5">(d) Hybrid Nets., point-2D:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>6] 60 ? 36 ? 60 60 ? 36 ? 60 60 ? 36 ? 60 256 ? 32 ? 256 Method Input Prec. Recall IoU mIoU Prec. Recall IoU mIoU Prec. Recall IoU mIoU Prec. Recall IoU mIoU ? 74.5 ? 59.5 ? 29.3 ? 91.1 ? 86.6 ? 79.8 ? 40.9 ? 90.8 91.? 90.8 ? 63.5 ? 38.5 ? 91.3 ? 92.6 ? 82.4 ? 53.2 ? 98.2 ? 96.8 ? 91.4 ? 74.2 ?</figDesc><table><row><cell>2017 SSCNet  ? [137] a</cell><cell>G</cell><cell cols="2">57.0 94.5</cell><cell>55.1</cell><cell>24.7</cell><cell cols="2">75.0 92.3</cell><cell>70.3</cell><cell>-</cell><cell cols="3">76.3 95.2 73.5</cell><cell>46.4</cell><cell cols="3">31.7 83.4 29.8 9.5</cell></row><row><cell>SSCNet-full [124]</cell><cell>G</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">59.6 75.5 50.0 16.1</cell></row><row><cell>2018 Guedes et al. [51]</cell><cell cols="3">G+T 62.5 82.3</cell><cell cols="2">54.3 27.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VVNet  ? [54]</cell><cell>G</cell><cell cols="11">74.8  7 84.0</cell><cell>66.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VD-CRF  ? [176]</cell><cell>G</cell><cell cols="3">? r e f e r t o Ta b. 7</cell><cell></cell><cell cols="3">? r e f e r t o Ta b. 7</cell><cell></cell><cell>-</cell><cell>-</cell><cell>74.5</cell><cell>48.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ESSCNet [174] b</cell><cell>G</cell><cell cols="2">71.9 71.9</cell><cell cols="2">56.2 26.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">92.6 90.4 84.5</cell><cell cols="4">70.5 62.6 55.6 41.8 17.5</cell></row><row><cell>SATNet  ? [86]</cell><cell>G+T</cell><cell cols="3">? r e f e r t o Ta b. 7</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">80.7 * 96.5* 78.5 * 64.3 *</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>2019 DDRNet [77]</cell><cell cols="3">G+T 71.5 80.8</cell><cell cols="2">61.0 30.4</cell><cell cols="2">88.7 88.5</cell><cell cols="2">79.4 42.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TS3D [43] c</cell><cell>G+T</cell><cell>-</cell><cell>-</cell><cell cols="2">60.0 34.1</cell><cell>-</cell><cell>-</cell><cell>76.1</cell><cell>46.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">31.6 84.2 29.8 9.5</cell></row><row><cell>TS3D+DNet [6]</cell><cell>G</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">25.9 88.3 25.0 10.2</cell></row><row><cell cols="2">TS3D+DNet+SATNet [6] G</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">80.5 57.7 50.6 17.7</cell></row><row><cell>EdgeNet  ? [32]</cell><cell cols="3">G+T 76.0 68.3</cell><cell>56.1</cell><cell>27.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">93.1* 90.4 * 84.8* 69.5*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSC-GAN [20]</cell><cell>G</cell><cell cols="2">63.1 87.8</cell><cell cols="2">57.8 22.7</cell><cell cols="2">80.7 91.1</cell><cell cols="2">74.8 42.0</cell><cell cols="2">83.4 92.4</cell><cell cols="2">78.1 55.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ForkNet  ? [160]</cell><cell>G</cell><cell cols="3">? r e f e r t o Ta b. 7</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">86.9 ? 63.4 ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CCPNet  ? [177]</cell><cell cols="14">G 74.2 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AM 2 FNet [16]</cell><cell cols="3">G+T 72.1 80.4</cell><cell cols="2">61.3 31.7</cell><cell cols="2">87.2 91.0</cell><cell cols="2">80.2 44.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>2020 GRFNet [89]</cell><cell cols="3">G+T 68.4 85.4</cell><cell cols="2">61.2 32.9</cell><cell cols="2">87.2 91.0</cell><cell cols="2">80.1 45.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AMFNet [79]</cell><cell cols="3">G+T 67.9 82.3</cell><cell cols="2">59.0 33.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PALNet [78]</cell><cell>G</cell><cell cols="2">68.7 85.0</cell><cell cols="2">61.3 34.1</cell><cell cols="2">87.2 91.7</cell><cell>80.8</cell><cell>46.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3DSketch [17]</cell><cell cols="3">G+T 85.0 81.6</cell><cell>71.3</cell><cell>41.1</cell><cell cols="2">90.6 92.2</cell><cell>84.2</cell><cell>55.2</cell><cell>-</cell><cell>-</cell><cell cols="2">88.2* 76.5*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AIC-Net [76]</cell><cell cols="3">G+T 62.4 91.8</cell><cell cols="2">59.2 33.3</cell><cell cols="2">88.2 90.3</cell><cell cols="2">80.5 45.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang et al. [152]</cell><cell>G</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">92.1 95.5 88.1</cell><cell>74.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IPF-SPCNet [180]</cell><cell cols="3">G+T 70.5 46.7</cell><cell>39.0</cell><cell>35.1</cell><cell cols="2">83.3 72.7</cell><cell cols="2">63.5 50.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Chen et al. [19]</cell><cell>G</cell><cell>-</cell><cell>-</cell><cell>73.4</cell><cell>34.4</cell><cell>-</cell><cell>-</cell><cell cols="2">82.2 44.5</cell><cell>-</cell><cell>-</cell><cell>84.8</cell><cell>63.5</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>SSC performance on the most popular datasets without pretraining. The relatively low best mIoU scores on the challenging real outdoor SemanticKitti<ref type="bibr" target="#b5">[6]</ref> (29.5%) and real indoor NYUv2<ref type="bibr" target="#b132">[133]</ref> (41.1%) shows the complexity of the task. In the 'method' column, we indicate variants with an offset. To better interpret the performance, column 'Input' shows the type of input modality used where 'G' is Geometry (depth, range, points, etc.) and 'T' is Texture (RGB). Note that all indoor datasets commonly report performance for 60 ? 36 ? 60 grids for historical reasons though 4x bigger input is commonly treated, cf. Sec. 4.5.2. Top 5 methods are highlighted in each column from red to white.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Detailed SSC class performance on SemanticKITTI [6] dataset. Best 5 methods are presented and ordered in decreasing mIoU performance from top to bottom. 93.8 53.2 41.9 43.6 66.2 61.4 38.1 29.8 53.9 40.3 52.4 CCPNet ? ? [177] G 25.5 98.5 38.8 27.1 27.3 64.8 58.4 21.5 30.1 38.4 23.8 41.3 3DSketch [17] G+T 43.1 93.6 40.5 24.3 30.0 57.1 49.3 29.2 14.3 42.5 28.6 41.1 ForkNet ? ? [160] G 36.2 93.8 29.2 18.9 17.7 61.6 52.9 23.3 19.5 45.4 20.0 37.1 IPF-SPCNet [180] G+T 32.7 66.0 41.2 17.2 34.7 55.3 47.0 21.7 12.5 38.4 19.2 35.1</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>ceil. (0.74%)</cell><cell>floor (12.44%)</cell><cell>wall (9.67%)</cell><cell>win. (2.12%)</cell><cell>chair (2.03%)</cell><cell>bed (9.17%)</cell><cell>sofa (6.78%)</cell><cell>table (4.14%)</cell><cell>tvs (0.53%)</cell><cell>furn. (36.64%)</cell><cell>objs. (15.74%)</cell><cell>mIoU</cell></row><row><cell>SISNet [11]</cell><cell cols="2">G+T 54.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? Results reported on resolution different to (60 ? 36 ? 60). CCPNet: (240 ? 144 ? 240). ForkNet: (80 ? 48 ? 80).? Pretraining on SUNCG.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Detailed SSC class performance on NYUv2<ref type="bibr" target="#b132">[133]</ref> dataset. Best 5 methods from Tabs. 3 and 7 are presented and ordered in decreasing mIoU performance from top to bottom.[17] G* 97.8 91.9 84.1 72.6 60.8 86.8 81.7 68.7 52.6 75.7 68.2 76.5 Wang et al. [152] G 98.2 92.8 76.3 61.9 62.4 87.5 80.5 66.3 55.2 74.6 67.8 74.8 CCPNet ? [177] G 99.2 89.3 76.2 63.3 58.2 86.1 82.6 65.6 53.2 76.8 65.2 74.2 ESSCNet [174] G 96.6 83.7 74.9 59.0 55.1 83.3 78.0 61.5 47.4 73.5 62.9 70.5 EdgeNet [32] G* 97.2 94.4 78.4 56.1 50.4 80.5 73.8 54.5 49.8 69.5 59.2 69.5 ? Results reported on resolution different to (60 ? 36 ? 60). CCPNet: (240 ? 144 ? 240). * Texture input not used due to absence in SUNCG.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>ceil. (2.68%)</cell><cell>floor (12.27%)</cell><cell>wall (33.55%)</cell><cell>win. (5.79%)</cell><cell>chair (1.80%)</cell><cell>bed (5.95%)</cell><cell>sofa (4.94%)</cell><cell>table (2.90%)</cell><cell>tvs (0.36%)</cell><cell>furn. (15.04%)</cell><cell>objs. (14.73%)</cell><cell>mIoU</cell></row><row><cell>3DSketch  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? Results provided by authors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Detailed SSC class performance on SUNCG [137] dataset. Best 5 methods are presented and ordered in decreasing mIoU performance from top to bottom.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>? 94.3 ? 67.1 ? 41.3 ? 93.4 ? 91.2 ? 85.1 ? 55.0 ? Input: Geometry (depth, range, points, etc.), Texture (RGB). Results reported at a different resolution. CCPNet: (240 ? 144 ? 240).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38]</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">60 ? 36 ? 60</cell><cell></cell><cell cols="2">60 ? 36 ? 60</cell></row><row><cell>Method</cell><cell cols="7">Input Prec. Recall IoU mIoU Prec. Recall IoU mIoU</cell></row><row><cell>2017 SSCNet [137]</cell><cell cols="7">G 59.3 92.9 56.6 30.5 75.0 96.0 73.0</cell><cell>-</cell></row><row><cell>2018 VVNet [54]</cell><cell cols="7">G 69.8 83.1 61.1 32.9 86.4 92.0 80.3</cell><cell>-</cell></row><row><cell cols="2">VD-CRF [176] G</cell><cell>-</cell><cell>-</cell><cell>60.0 31.8</cell><cell>-</cell><cell>-</cell><cell>78.4 43.0</cell></row><row><cell cols="5">SATNet [86] G+T 67.3 85.8 60.6 34.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">2019 EdgeNet [32] G+T 79.1 66.6 56.7 33.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ForkNet [160] G</cell><cell>-</cell><cell cols="2">-63.4 ? 37.1 ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">CCPNet [177] G 78.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>?ForkNet: (80 ? 48 ? 80).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>SSC performance on indoor datasets with SUNCG pre-training. Synthetic data pre-training results on slight performance gains for all methods. scenes with 3DSketch ranking second in mIoU among nonpre-trained methods and third overall (Tabs. 3 and 7), visible in Figs. 11a, 11b, 11c. Similarly, S3CNet dominates performance in SemanticKITTI as already mentioned, which performance is noticeable in Figs. 12c, 12d.An interesting observation is the high density of the completion even regarding the ground truth, visible in Figs. 12b, 12g, 12h. This relationship is studied in<ref type="bibr" target="#b24">[25]</ref>, where sparsity is exploited by removing input data to impulse unknown space completion.Synthetic data pre-training. Pre-training on large SUNCG is a common workaround to improve performance on the smaller NYUv2 and NYUCAD datasets. Comparison between Tabs. 3 and 7 show that the technique always brings performance gains although the gap becomes less important for most recent methods (i.e. +5.8% mIoU for SSCNet vs. +2.8% mIoU for CCPNet). Since SUNCG is no longer legally available, this practice is less common in recent works.</figDesc><table><row><cell>4.5.3 Network efficiency</cell></row><row><cell>In Tab. 8, network parameters and floating-point operations</cell></row><row><cell>(FLOPs) are listed -where possible -with separation of in-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>? 36 ? 60 prediction (indoor)</figDesc><table><row><cell>Method</cell><cell cols="2">Params (M) FLOPs (G)</cell></row><row><cell>2017 SSCNet [137] b</cell><cell>0.93</cell><cell>82.5</cell></row><row><cell>SSCNet-full [137] b</cell><cell>1.09</cell><cell>769.6</cell></row><row><cell>2019 TS3D [43] b</cell><cell>43.77</cell><cell>2016.7</cell></row><row><cell>TS3D+DNet [6] b</cell><cell>51.31</cell><cell>847.1</cell></row><row><cell>TS3D+DNet+SATNet [6] b</cell><cell>50.57</cell><cell>905.2</cell></row><row><cell>2020 LMSCNet [124]</cell><cell>0.35</cell><cell>72.6</cell></row><row><cell>JS3C-Net [169]</cell><cell>3.1</cell><cell>-</cell></row><row><cell>Local-DIFs [121]</cell><cell>9.9</cell><cell>-</cell></row><row><cell cols="2">b Reported in [124].</cell><cell></cell></row><row><cell cols="3">(b) 256 ? 32 ? 256 prediction (outdoor)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Network statistics. Number of parameters and FLOPs are reported per method, grouped by resolution output: 60 ? 36 ? 60 for typical indoor datasets</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">, 2 , EMD or CD) may be used as in<ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref> though less used in real datasets, due to their lower precision when sparsity increases.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep 3D semantic scene extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sahillioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E R</forename><surname>Shabayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cherenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ottersten</surname></persName>
		</author>
		<title level="m">A survey on deep learning advances on different 3d data representations. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint 2D-3D-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Avetisyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dahnert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scan2CAD: Learning CAD model alignment in RGB-D scans</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">SceneCAD: Predicting object alignments and layouts in RGB-D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Avetisyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A dataset for semantic scene understanding of LiDAR sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unstructured point cloud semantic labeling using deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">nuScenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic scene completion via integrating instances and scene in-the-loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 7, 8, 15</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Matterport3D: Learning from RGB-D data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manitsaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D reconstruction of deformable revolving object under heavy hand interaction</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with Deep Convolutional Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atrous Convolution, and Fully Connected CRFs. PAMI</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Am2fnet: Attention-based multiscale &amp; multi-modality fused network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ROBIO</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">3D sketch-aware semantic scene completion via semi-supervised structure prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2020) 5, 6, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multi-view 3D object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Real-time semantic scene completion via feature aggregation and conditioned prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>In: ICIP (2020) 6, 7, 8, 10, 12, 13, 14, 15</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D semantic scene completion from a single depth image using adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">S3CNet: A sparse semantic scene completion network for LiDAR point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Agia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bingbing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL (2020) 6, 7, 8, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning priors for semantic 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cherabier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">4D spatio-temporal ConvNets: Minkowski convolutional neural networks. In: CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">ScanNet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">SG-NN: Sparse generative neural networks for self-supervised scene completion of RGB-D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2020) 2, 4, 5, 6, 9, 12, 13, 14, 15</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">3DMV: Joint 3D-multi-view prediction for 3D semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shape completion using 3D-encoder-predictor CNNs and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">ScanComplete: Large-scale scene completion and semantic segmentation for 3D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Filling holes in complex surfaces using volumetric diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. First International Symposium on 3D Data Processing Visualization and Transmission</title>
		<meeting>First International Symposium on 3D Data Processing Visualization and Transmission</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">3D scene reconstruction from a single viewport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">EdgeNet: Semantic scene completion from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dourado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>ICPR (2020) 5, 6, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semantic scene completion from a single 360-Degree image and depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dourado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>In: VISIGRAPP (2020) 5, 6, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From points to multi-object 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">RGBD datasets: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">3D-FRONT: 3D furnished rooms with layouts and semantics</title>
		<imprint>
			<publisher>ArXiv</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual simultaneous localization and mapping: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fuentes-Pacheco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Ascencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rend?n-Mancha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">VirtualWorlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Are we hungry for 3D LiDAR data for semantic segmentation? a survey of datasets and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Two stream 3D semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semantics for robotic mapping, perception and interaction: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cosgun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Robotics</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The KITTI dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Joint 3D object and layout inference from a single RGB-D image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>GCPR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Mesh R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Synthcity: A large scale synthetic point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boehm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">AtlasNet: A papier-m?ch? approach to learning 3D surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic scene completion combining colour and depth: preliminary experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B S</forename><surname>Guedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Support surface prediction in indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep learning for 3D point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">View-volume network for semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<title level="m">Semantic3D.net: A new large-scale point cloud classification benchmark. ISPRS Annals</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Image-based 3D object reconstruction: State-of-the-art and trends in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Laga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Highresolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning of volumeguided progressive view inpainting for 3D point scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">SceneNet: Understanding real world indoor scenes with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">3D-SIS: 3D semantic instance segmentation of RGB-D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">RevealNet: Seeing behind objects RGB-D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">SceneNN: A scene meshes dataset with annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep neural network for 3D point cloud completion with multistage loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Control And Decision Conference (CCDC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">PF-Net: Point fractal network for 3D point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<title level="m">IM2CAD. In: CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A survey of deep learning-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Access</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Poisson surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SGP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Remove, then revert: Static point cloud map construction using multiresolution range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS. IEEE</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Escape from cells: Deep Kd-networks for the recognition of 3D point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">3D-RCNN: Instance-level 3D object reconstruction via render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">DeformNet: Free-form deformation network for 3D shape reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Shape completion from a single RGBD image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Anisotropic convolutional networks for 3D semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020) 7, 8, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">RGBD based dimensional decomposition residual network for 3D semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2019) 7, 8, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Depth based semantic scene completion with position importance aware loss. Robotics and Automation Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note>RA-L) (2020) 5, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Attention-based multimodal fusion network for semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI (2020) 7, 8, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">PointCNN: convolution on X-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Database-assisted object retrieval for real-time 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deep learning for LiDAR point clouds in autonomous driving: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Deep marching cubes: Learning explicit surface representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Donn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Holistic scene understanding for 3D object detection with RGBD cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">RefineNet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">See and think: Disentangling semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Deep learning on point clouds and its application: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">3D gated recurrent fusion for semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ArXiv (2020) 7, 8, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Marching cubes: A high resolution 3D surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno>arXiv (2020) 2</idno>
		<title level="m">Deep learning for 3d point cloud understanding: A survey</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Geometric modeling using octree encoding. Computer graphics and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meagher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">VV-Net: Voxel VAE net with group convolutions for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Symmetry in 3D geometry: Extraction and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Seeing behind objects for 3D multi-object tracking in RGB-D sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">High accuracy TOF and stereo sensor fusion at interactive rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lenzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">A search-classify approach for cluttered indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Laplacian mesh optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nealen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GRAPHITE &apos;06</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">KinectFusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Symposium on Mixed and Augmented Reality</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">3D point cloud segmentation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robotics, Automation and Mechatronics (RAM)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Total3DUnderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">RfD-Net: Point scene understanding by semantic instance reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Seman-ticPOSS: A point cloud dataset with large quantity of dynamic instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IV</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">DeepSDF: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Examplebased 3D scan completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Giesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SGP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Discovering structural regularity in 3D geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wallner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 2008</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">State-of-the-art in automatic 3D reconstruction of structured indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pintore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ganovelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J F</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pajarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gobbetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CGF</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Diagonal preconditioning for first order primal-dual algorithms in convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A survey of rigid 3D pointcloud registration algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Ambient Computing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A review of point cloud registration algorithms for mobile robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Robotics</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">CoReNet: Coherent 3D scene reconstruction from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bauszat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Parallel multiscale autoregressive density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3D structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">OctNetFusion: Learning depth fusion from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Semantic scene completion using local deep implicit functions on LiDAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emmerichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note>ArXiv (2020) 4, 5, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">SCSSnet: Learning spatially-conditioned scene segmentation on LiDAR point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV (2020) 7, 8, 9</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Completing 3D object shape from one depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">LMSCNet: Lightweight multiscale 3D semantic completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Roldao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note>In: 3DV (2020) 1, 4, 5, 6, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Paris-Lille-3D: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Visual SLAM and structure from motion in dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R U</forename><surname>Saputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">An interactive approach to semantic modeling of indoor scenes with an RGBD camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">VConv-DAE: Deep volumetric shape learning without object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Structure recovery by part assembly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and partaggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Approximate symmetry detection in partial 3D meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sipiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Improved adversarial systems for 3D object generation and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Im2pano3d: Extrapolating 360?structure and semantics beyond the field of view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Least-squares meshes. Proceedings Shape Modeling Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Budge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Briales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gillingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pesqueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Replica dataset: A digital replica of indoor spaces</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Learning 3D shape completion from laser scan data with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Data-driven structural priors for shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Toronto-3D: A large-scale mobile LiDAR dataset for semantic segmentation of urban roadways</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">SEG-Cloud: Semantic segmentation of 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">TopNet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">KPConv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Shape from symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wegbreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">TerraMobilita/iQmulus urban point cloud analysis benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Br?dif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paparoditis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph</title>
		<imprint>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Shape completion enabled robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dechant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">O-CNN: octreebased convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Deep Octree-based CNNs with output-guided skip connections for 3D shape and scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops (2020) 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Adaptive O-CNN: A patchbased deep representation of 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">SGPN: Similarity group proposal network for 3D point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Cascaded refinement network for point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Point cloud completion by learning shape priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Learning 3D semantic reconstruction on octrees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cherabier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">Adversarial semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">ForkNet: Multibranch volumetric semantic completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>In: ICCV (2019) 5, 6, 7, 8, 10, 12, 13, 14</note>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">SoftPoolNet: Shape descriptor for point cloud completion and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Patch-based progressive 3D point set upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Point cloud completion by skip-attention network with hierarchical folding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">SCFusion: Realtime incremental scene reconstruction with semantic completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">SUN3D: A database of big spaces reconstructed using SfM and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">GRNet: Gridding residual network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Linking points with labels in 3D: A review of point cloud semantic segmentation. Geoscience and Remote Sensing Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Linking points with labels in 3d: A review of point cloud semantic segmentation. Geoscience and Remote Sensing Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Sparse single sweep LiDAR point cloud segmentation via learning contextual shape priors from scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI (2021)</title>
		<meeting><address><addrLine>6, 7, 8, 9</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Dense 3D object reconstruction from a single depth view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">PCN: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">A metric for evaluating 3D reconstruction and mapping performance with no ground truthing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">Efficient semantic scene completion network with spatial group convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">A review of deep learningbased semantic segmentation for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Semantic scene completion with dense crf from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Cascaded context pyramid for full-resolution 3D semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note>In: ICCV (2019) 5, 6, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">Detail preserved point cloud completion via separated feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<title level="m" type="main">Beyond point clouds: Scene understanding by reasoning geometry and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Semantic point completion network for 3D semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI (2020) 3, 5, 7, 8, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Learning for active 3D mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petr?cek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Salansk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Svoboda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stotko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>G?rlitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">State of the art on 3D reconstruction with RGB-D cameras</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
