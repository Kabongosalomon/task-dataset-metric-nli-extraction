<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
						</author>
						<title level="a" type="main">Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We propose a simple yet generic representation learning framework, named Source HypOthesis Transfer (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and selfsupervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have achieved remarkable success in a variety of applications across different fields but at the expense of laborious large-scale training data annotation. To avoid expensive data labeling, domain adaptation (DA) methods are developed to fully utilize previously labeled datasets and unlabeled data on hand in a transductive manner, which have obtained promising results in object recognition <ref type="bibr" target="#b40">(Long et al., 2015;</ref><ref type="bibr" target="#b63">Tzeng et al., 2017)</ref>, semantic segmentation <ref type="bibr" target="#b75">(Zhang et al., 2017;</ref><ref type="bibr" target="#b25">Hoffman et al., 2018)</ref>, etc. Over the last decade, increasing efforts have been devoted 1 Department of ECE, National University of Singapore (NUS). Correspondence to: Jian Liang &lt;liangjian92@gmail.com&gt;.</p><p>Copyright 2020 by the author(s).</p><p>to deep domain adaptation, especially under the vanilla unsupervised closed-set setting (Panareda <ref type="bibr" target="#b47">Busto &amp; Gall, 2017)</ref> where two domains share the same label space but the target data are not labeled. One prevailing paradigm is to mitigate the distribution divergence between domains by matching the distribution statistical moments at different orders <ref type="bibr" target="#b72">Zellinger et al., 2017;</ref><ref type="bibr" target="#b49">Peng et al., 2019a)</ref>. For example, the most-favored Maximum Mean Discrepancy (MMD)  measure minimizes a certain distance between weighted sums of all raw moments. Another popular paradigm leverages the idea of adversarial learning <ref type="bibr" target="#b21">(Goodfellow et al., 2014)</ref> and introduces an additional domain classifier to minimize the Proxy A-distance <ref type="bibr" target="#b0">(Ben-David et al., 2010)</ref> across domains.</p><p>Nowadays, data are distributed on different devices and usually contain private information, e.g., those on personal phones or from surveillance cameras. Existing DA methods need to access the source data during learning to adapt, which is not efficient for data transmission and may violate the data privacy policy. In this paper, we address an interesting but challenging unsupervised DA setting with only a trained source model provided as supervision. It differs from vanilla unsupervised DA in that the source model instead of the source data is provided to the unlabeled target domain. Such a setting helps protect the privacy in the source domain (e.g., individual hospital profiles), and transferring a light trained model is much efficient than heavy data transmission. For example, as shown in <ref type="table">Table 1</ref>, the storage size of a trained model is much smaller than that of a compressed dataset. <ref type="table">Table 1</ref>. Storage size. The backbone networks of source models are <ref type="bibr">LeNet (LeCun et al., 1998)</ref> and ResNet-101 <ref type="bibr" target="#b24">(He et al., 2016)</ref>, respectively. ? <ref type="bibr" target="#b25">(Hoffman et al., 2018)</ref>, ? <ref type="bibr" target="#b48">(Peng et al., 2017</ref> In terms of privacy protection, this setting seems somewhat similar to a recently proposed transfer learning setting in Hypothesis Transfer Learning (HTL) <ref type="bibr" target="#b31">(Kuzborskij &amp; Orabona, 2013)</ref>, where the learner does not have direct access to the source domain data and can only operate on the hypotheses arXiv:2002.08546v6 [cs.CV] 1 Jun 2021 induced from the source data. However, our method differs significantly from that work. Conventional HTL always requires labeled data in the target domain <ref type="bibr" target="#b61">(Tommasi et al., 2013)</ref> or multiple hypotheses from different source domains <ref type="bibr" target="#b43">(Mansour et al., 2009)</ref>, which however cannot be applied to unsupervised DA.</p><p>To address such a challenging unsupervised DA setting, we propose a simple yet generic solution called Source HypOthesis Transfer (SHOT). Inspired by prior work <ref type="bibr" target="#b63">(Tzeng et al., 2017)</ref>, SHOT assumes that the same deep neural network model consists of a feature encoding module and a classifier module (hypothesis) across domains. It aims to learn a target-specific feature encoding module to generate target data representations that are well aligned with source data representations, without accessing source data and labels for target data. SHOT is designed based on the following observations. If we have learned source-like representations for target data, then the classification outputs from the source classifier (hypothesis) for target data should be similar to those of source data, i.e., close to one-hot encodings. Thus, SHOT freezes the source hypothesis and fine-tunes the source encoding module by maximizing the mutual information between intermediate feature representations and outputs of the classifier, as information maximization <ref type="bibr" target="#b26">(Hu et al., 2017)</ref> encourages the network to assign disparate one-hot encodings to the target feature representations.</p><p>Even though information maximization forces feature representations to fit the hypothesis well, it may still align target feature representations to the wrong source hypothesis. To avoid this, we propose a novel self-supervised pseudo-labeling method to augment the target representation learning. Considering pseudo labels generated by the source classifier may be inaccurate and noisy for target data, we propose to attain intermediate class-wise prototypes for the target domain itself and further obtain cleaner pseudo labels via supervision from these prototypes to guide the mapping module learning. Such self-supervised labeling fully exploits the global structure of the target domain and would learn feature representations that correctly fit the source hypothesis. Besides, we investigate label smoothing <ref type="bibr" target="#b45">(M?ller et al., 2019)</ref>, weight normalization <ref type="bibr" target="#b57">(Salimans &amp; Kingma, 2016)</ref>, and batch normalization <ref type="bibr" target="#b28">(Ioffe &amp; Szegedy, 2015)</ref>, within the network architecture of the source model, to boost adaptation performance. Empirical evidence shows that both SHOT and its baseline method (source only) benefit from these techniques.</p><p>We apply SHOT to the vanilla closed-set <ref type="bibr" target="#b52">(Saenko et al., 2010)</ref> DA scenario and a variety of other unsupervised DA tasks like the partial-set  and open-set (Panareda <ref type="bibr" target="#b47">Busto &amp; Gall, 2017)</ref> cases. Experiments show that SHOT achieves state-of-the-art results for multiple and various domain adaptation tasks. For instance, on the Office-Home dataset, SHOT advances the best average accuracy from 67.6%  to 71.8% for the closed-set setting and from 71.8% <ref type="bibr" target="#b69">(Xu et al., 2019)</ref> to 79.3% for the partial transfer case, and from 69.5% <ref type="bibr" target="#b38">(Liu et al., 2019)</ref> to 72.8% for the open-set scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Domain Adaptation. Lots of unsupervised DA methods have been developed and successfully used in cross-domain applications like object recognition <ref type="bibr" target="#b36">(Liang et al., 2018;</ref><ref type="bibr">?)</ref>, object detection , semantic segmentation <ref type="bibr" target="#b62">(Tsai et al., 2018)</ref>, and sentiment classification <ref type="bibr" target="#b19">(Glorot et al., 2011)</ref>. Besides the two prevailing paradigms, moment matching and adversarial alignment, as aforementioned, there are several sample-based DA methods that estimate the importance weights of source samples <ref type="bibr" target="#b29">(Jiang &amp; Zhai, 2007;</ref><ref type="bibr" target="#b27">Huang et al., 2007)</ref> or sample-to-sample similarity via optimal transport <ref type="bibr" target="#b2">(Bhushan Damodaran et al., 2018)</ref>. Also, a few works <ref type="bibr" target="#b5">(Bousmalis et al., 2016;</ref><ref type="bibr" target="#b18">Ghifary et al., 2016)</ref> impose data reconstruction as an auxiliary task to ensure feature invariance, and some studies consider batch normalization <ref type="bibr" target="#b8">(Cariucci et al., 2017;</ref><ref type="bibr" target="#b66">Wang et al., 2019)</ref> and adversarial dropout <ref type="bibr" target="#b53">(Saito et al., 2018a;</ref><ref type="bibr" target="#b35">Lee et al., 2019b)</ref> within the network architecture. However, all these methods assume the target user's access to the source domain data, which is unsafe and sometimes unpractical since source data may be private and decentralized (stored on another device). In addition, our work is, as far as we know, the first DA framework to address almost all unsupervised DA scenarios including multi-source <ref type="bibr" target="#b49">(Peng et al., 2019a)</ref> and multi-target <ref type="bibr" target="#b50">(Peng et al., 2019b)</ref>, open-set DA (Panareda <ref type="bibr" target="#b47">Busto &amp; Gall, 2017)</ref>, and partial-set DA .</p><p>Hypothesis Transfer Learning (HTL). HTL <ref type="bibr" target="#b31">(Kuzborskij &amp; Orabona, 2013)</ref> can be seen as a generalization of parameter adaptation <ref type="bibr" target="#b14">(Csurka, 2017)</ref>, which assumes the optimal target hypothesis to be closely related to the source hypothesis. Like the famous fine-tuning strategy <ref type="bibr" target="#b71">(Yosinski et al., 2014)</ref>, HTL mostly acquires at least a small set of labeled target examples per class, limiting its applicability to the semi-supervised DA scenario <ref type="bibr" target="#b70">(Yang et al., 2007;</ref><ref type="bibr" target="#b46">Nelakurthi et al., 2018)</ref>. Besides, a seminal work <ref type="bibr" target="#b43">(Mansour et al., 2009)</ref> infers the weights of different source hypotheses for each unlabeled target datum and utilizes a convex combination of source hypotheses. However, it only fits the multi-source scenario and needs an additive distribution estimation of each source domain. To our best knowledge, <ref type="bibr" target="#b12">(Chidlovskii et al., 2016;</ref><ref type="bibr" target="#b37">Liang et al., 2019)</ref> introduce similar settings as ours to the unsupervised DA problem, but they may not work well enough since they do not incorporate the end-toend feature learning module inside.</p><p>Pseudo Labeling (PL). Pseudo labeling <ref type="bibr" target="#b34">(Lee, 2013)</ref> is originally proposed for semi-supervised learning and gains pop-ularity in other transductive learning problems like DA. The main idea is to label unlabeled data with the maximum predicted probability and perform fine-tuning together with labeled data, which is quite efficient. For DA methods, <ref type="bibr" target="#b74">(Zhang et al., 2018b;</ref><ref type="bibr" target="#b13">Choi et al., 2019)</ref> directly incorporate pseudo labeling as a regularization while <ref type="bibr" target="#b41">(Long et al., 2017;</ref> leverage pseudo labels in the adaptation module to pursue discriminative (conditional) distribution alignment. <ref type="bibr" target="#b76">(Zou et al., 2018)</ref> further designs an integrated framework to alternately solve target pseudo labels and perform model training. In the absence of labeled data, DeepCluster <ref type="bibr" target="#b9">(Caron et al., 2018)</ref>, one of the best self-supervised learning methods, generates pseudo labels via k-means clustering and utilizes them to re-train the current model. Considering the dataset shift, our framework combines advantages of both and develops a self-supervised pseudo labeling strategy to alleviate the harm from noisy pseudo labels.</p><p>Federated Learning (FL). FL <ref type="bibr" target="#b4">(Bonawitz et al., 2019</ref>) is a distributed machine learning approach that trains a model across multiple decentralized edge devices without exchanging their data samples. A common usage of FL consumes a simple aggregate of updates from multiple local users for the learning algorithm at the server-side <ref type="bibr" target="#b3">(Bonawitz et al., 2017;</ref><ref type="bibr" target="#b44">McMahan et al., 2018)</ref>, offering a privacy-preserving mechanism. Recently, <ref type="bibr" target="#b51">(Peng et al., 2020)</ref> introduces the first federated DA setting where knowledge is transferred from the decentralized nodes to a new node without any supervision itself and proposes an adversarial-based solution.</p><p>Specifically, it trains one model per source node and updates the target model with the aggregation of source gradients to reduce domain shift. However, it may fail to address the vanilla DA setting with only one source domain available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We address the unsupervised DA task with only a pre-trained source model and without access to source data. In particular, we consider K-way classification. For a vanilla unsupervised DA task, we are given n s labeled samples</p><formula xml:id="formula_0">{x i s , y i s } ns i=1 from the source domain D s where x i s ? X s , y i s ? Y s , and also n t unlabeled samples {x i t } nt i=1 from the target domain D t where x i t ? X t .</formula><p>The goal of DA is to predict the labels</p><formula xml:id="formula_1">{y i t } nt i=1</formula><p>in the target domain, where y i t ? Y t , and the source task X s ? Y s is assumed to be the same with the target task X t ? Y t . Here SHOT aims to learn a target function</p><formula xml:id="formula_2">f t : X t ? Y t and infer {y i t } nt i=1 , with only {x i t } nt i=1 and the source function f s : X s ? Y s available.</formula><p>We address the above source model transfer task for UDA through three steps. First, we generate the source model from source data. Secondly, we abandon source data and transfer the model (including source hypothesis) to the target domain without accessing source data. Thirdly, we further study how to design better network architectures for both models to improve adaptation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Source Model Generation</head><p>We consider to develop a deep neural network and learn the source model f s : X s ? Y s by minimizing the following standard cross-entropy loss,</p><formula xml:id="formula_3">L src (f s ; X s , Y s ) = ?E (xs,ys)?Xs?Ys K k=1 q k log ? k (f s (x s )),<label>(1)</label></formula><p>where ? k (a) = exp(a k ) i exp(ai) denotes the k-th element in the soft-max output of a K-dimensional vector a, and q is the one-of-K encoding of y s where q k is '1' for the correct class and '0' for the rest. To further increase the discriminability of the source model and facilitate the following target data alignment, we propose to adopt the label smoothing (LS) technique as it encourages examples to lie in tight evenly separated clusters <ref type="bibr" target="#b45">(M?ller et al., 2019)</ref>. With LS, the objective function is changed to</p><formula xml:id="formula_4">L ls src (f s ; X s , Y s ) = ?E (xs,ys)?Xs?Ys K k=1 q ls k log ? k (f s (x s )),<label>(2)</label></formula><p>where q ls k = (1 ? ?)q k + ?/K is the smoothed label and ? is the smoothing parameter which is empirically set to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Source Hypothesis Transfer with Information</head><p>Maximization (SHOT-IM)</p><p>The source model parameterized by a DNN in <ref type="figure">Fig. 2</ref> consists of two modules: the feature encoding module g s : X s ? R d and the classifier module h s :</p><formula xml:id="formula_5">R d ? R K , i.e., f s (x) = h s (g s (x)).</formula><p>Here d is the dimension of the input feature. Previous DA methods align different domains by matching the data distributions in the feature space R d using maximum mean discrepancy (MMD) <ref type="bibr" target="#b40">(Long et al., 2015)</ref> or domain adversarial alignment <ref type="bibr" target="#b17">(Ganin &amp; Lempitsky, 2015)</ref>. However, both strategies assume the source and target domains share the same feature encoder and need to access the source data during adaptation. This is not applicable in the proposed DA setting. In contrast, ADDA <ref type="bibr" target="#b63">(Tzeng et al., 2017)</ref> relaxes the parameter-sharing constraint and proposes a new adversarial framework for DA, which learns different mapping functions for each domain. Also, DIRT-T <ref type="bibr" target="#b59">(Shu et al., 2018)</ref> first trains a parameter-sharing DA framework as initialization and then fine-tunes the whole network by minimizing the cluster assumption violation.</p><p>Both methods indicate that learning a domain-specific feature encoding module is practicable and even works better than the parameter-sharing mechanism.</p><p>We follow this strategy and develop a Source HypOthesis Transfer (SHOT) framework by learning the domainspecific feature encoding module while fixing the source classifier module (hypothesis), as the source hypothesis encodes the distribution information of unseen source data. Specifically, SHOT uses the same classifier module for different domain-specific feature learning modules, namely, h t = h s . It aims to learn the optimal target feature learning module g t : X t ? R d such that the output target features can match the source feature distribution well and can be classified accurately by the source hypothesis directly. It is worth noting that SHOT merely utilizes the source data for just once to generate the source hypothesis, while ADDA and DIRT-T need to access the data from source and target simultaneously when learning the model.</p><p>Essentially, we expect to learn the optimal target encoder g t so that the target data distribution p (g t (x t )) matches the source data distribution p (g s (x s )) well. However, featurelevel alignment does not work at all since it is impossible to estimate the distribution of p (g s (x s )) without access to the source data. We view the challenging problem from another perspective: if the domain gap is mitigated, what kind of outputs should unlabeled target data have? We argue the ideal target outputs should be similar to one-hot encoding but differ from each other. For this purpose, we adopt the information maximization (IM) loss <ref type="bibr" target="#b30">(Krause et al., 2010;</ref><ref type="bibr" target="#b26">Hu et al., 2017)</ref>, to make the target outputs individually certain and globally diverse. In practice, we minimize the following L ent and L div that together constitute the IM loss:</p><formula xml:id="formula_6">L ent (f t ; X t ) = ?E xt?Xt K k=1 ? k (f t (x t )) log ? k (f t (x t )), L div (f t ; X t ) = K k=1p k logp k = D KL (p, 1 K 1 K ) ? log K, (3) where f t (x) = h t (g t (x)) is the K-dimensional output of each target sample, 1 K is a K-dimensional vector with all ones, andp = E xt?Xt [?(f t (x t ))</formula><p>] is the mean output embedding of the whole target domain. IM would work better than conditional entropy minimization <ref type="bibr" target="#b22">(Grandvalet &amp; Bengio, 2005)</ref> widely used in prior DA works <ref type="bibr" target="#b65">(Vu et al., 2019;</ref><ref type="bibr" target="#b56">Saito et al., 2019)</ref>, since IM can circumvent the trivial solution where all unlabeled data have the same one-hot encoding via the fair diversity-promoting objective L div .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Source Hypothesis Transfer Augmented with</head><p>Self-supervised Pseudo-labeling</p><p>As shown in <ref type="figure">Fig. 1</ref>, we show the t-SNE visualizations of features learned by SHOT-IM and the source model only.</p><p>Intuitively, the target feature representations are distributed in a mess for the source model only, and using the IM loss helps align the target data with the unseen source data well. However, the target data may be matched to the wrong source hypothesis to some degree in <ref type="figure">Fig. 1(b)</ref>. We argue that the harmful effects result from the inaccurate network outputs. For instance, a target sample from the 2nd class with the normalized network output [0.4, 0.3, 0.1, 0.1, 0.1] may be forced to have an expected output [1.0, 0.0, 0.0, 0.0, 0.0]. To alleviate such effects, we further apply pseudo-labeling <ref type="bibr" target="#b34">(Lee, 2013)</ref> for each unlabeled data to better supervise the target data encoding training. However, pseudo labels that are conventionally generated by source hypotheses are still noisy due to domain shift. Inspired by DeepCluster <ref type="bibr" target="#b9">(Caron et al., 2018)</ref>, we propose a novel self-supervised pseudo-labeling strategy. First, we attain the centroid for each class in the target domain, similar to weighted k-means clustering,</p><formula xml:id="formula_7">c (0) k = xt?Xt ? k (f t (x t ))? t (x t ) xt?Xt ? k (f t (x t )) ,<label>(4)</label></formula><p>wheref t =? t ? h t denotes the previously learned target hypothesis. These centroids can robustly and more reliably characterize the distribution of different categories within the target domain. Then, we obtain the pseudo labels via the nearest centroid classifier:</p><formula xml:id="formula_8">y t = arg min k D f (? t (x t ), c (0) k ),<label>(5)</label></formula><p>where D f (a, b) measures the cosine distance between a and b. Finally, we compute the target centroids based on the new pseudo labels:</p><formula xml:id="formula_9">c (1) k = xt?Xt 1(? t = k)? t (x t ) xt?Xt 1(? t = k) , y t = arg min k D f (? t (x t ), c (1) k ).<label>(6)</label></formula><p>We term? t as self-supervised pseudo labels since they are generated by the centriods obtained in an unsupervised manner. In practice, we update the centroids and labels in Eq. (6) for multiple rounds. However, experiments verify that updating for once gives sufficiently good pseudo labels.</p><p>To summarize, given the source model f s = g s ? h s and pseudo labels generated as above, SHOT freezes the hypothesis from source h t = h s and learns the feature encoder g t <ref type="figure">Figure 2</ref>. The pipeline of our SHOT framework. The source model consists of the feature learning module and the classifier module (hypothesis). SHOT keeps the hypothesis frozen and utilizes the feature learning module as initialization for target domain learning.</p><p>with the full objective as</p><formula xml:id="formula_10">L(g t ) = L ent (h t ? g t ; X t ) + L div (h t ? g t ; X t ) ? ? E (xt,?t)?Xt??t K k=1 1 [k=?t] log ? k (h t (g t (x t ))),<label>(7)</label></formula><p>where ? &gt; 0 is a balancing hyper-parameter.</p><p>Remark. The proposed SHOT framework can also be easily extended to other UDA tasks like partial-set DA  and open-set DA (Panareda <ref type="bibr" target="#b47">Busto &amp; Gall, 2017)</ref>. More details can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Architecture of Source Model</head><p>Now we study how to train a better source hypothesis for our own problem. We discuss some architecture choices for the neural network model to parameterize both the feature encoding module and hypothesis. First, we need to look back at the expected network outputs for crossentropy loss in Eq. (1). If y s = k, then maximizing f</p><p>s (x s ) = exp(w k gs(xs)) i exp(w i gs(xs)) means minimizing the distance between g s (x s ) and w k , where w k is the k-th weight vector in the last FC layer. Ideally, all the samples from the k-th class would have a feature embedding near to w k . If unlabeled target samples are given the correct pseudo labels, it is easily understandable that source feature embeddings are similar to target ones via the pseudo-labeling term. The intuition behind is quite similar to previous studies <ref type="bibr" target="#b39">(Long et al., 2013;</ref><ref type="bibr" target="#b67">Xie et al., 2018)</ref> where a simplified MMD is exploited for discriminative domain confusion.</p><p>Since the weight norm matters in the inner distance within the soft-max output, we adopt weight normalization (WN) <ref type="bibr" target="#b57">(Salimans &amp; Kingma, 2016)</ref> to keep the norm of each weight vector w i the same in the FC classifier layer. Besides, as indicated in prior studies, batch normalization (BN) <ref type="bibr" target="#b28">(Ioffe &amp; Szegedy, 2015)</ref> can reduce the internal dataset shift since different domains share the same mean (zero) and variance which can be considered as first-order and second-order mo-ments. Based on these considerations, we form the complete framework of SHOT as shown in <ref type="figure">Fig. 2.</ref> In the experiments, we extensively study the effect of each aforementioned architecture design in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>To testify its versatility, we evaluate SHOT in a variety of unsupervised DA scenarios, covering several popular visual benchmarks below. Full code is available at https: //github.com/tim-learn/SHOT.</p><p>Office <ref type="bibr" target="#b52">(Saenko et al., 2010</ref>) is a standard DA benchmark which contains three domains (Amazon (A), DSLR (D), and Webcam (W)) and each domain consists of 31 object classes under the office environment. <ref type="bibr" target="#b20">(Gong et al., 2012)</ref> further extracts 10 shared categories between Office and Caltech-256 (C), and forms a new benchmark named Office-Caltech. Both benchmarks are small-sized.</p><p>Office-Home <ref type="bibr" target="#b64">(Venkateswara et al., 2017</ref>) is a challenging medium-sized benchmark, which consists of four distinct domains (Artistic images (Ar), Clip Art (Cl), Product images (Pr), and Real-World images (Rw)). There are totally 65 everyday objects categories in each domain.</p><p>VisDA-C <ref type="bibr" target="#b48">(Peng et al., 2017)</ref> is a challenging large-scale benchmark that mainly focuses on the 12-class synthesisto-real object recognition task. The source domain contains 152 thousand synthetic images generated by rendering 3D models while the target domain has 55 thousand real object images sampled from Microsoft COCO.</p><p>Digits is a standard DA benchmark that focuses on digit recognition. We follow the protocol of <ref type="bibr" target="#b25">(Hoffman et al., 2018)</ref> and utilize three subsets: SVHN (S), MNIST (M), and USPS (U). Like , we train our model using the training sets of each domain and report the recognition results on the standard test set of the target domain.   <ref type="bibr" target="#b17">(Ganin &amp; Lempitsky, 2015)</ref>, we replace the original FC layer with a bottleneck layer (256 units) and a taskspecific FC classifier layer in <ref type="figure">Fig. 2</ref>. A BN layer is put after FC inside the bottleneck layer and a weight normalization layer is utilized in the last FC layer.</p><p>Network hyper-parameters. We train the whole network through back-propagation, and the newly added layers are trained with learning rate 10 times that of the pre-trained layers (backbone in <ref type="figure">Fig. 2)</ref>. Concretely, we adopt minibatch SGD with momentum 0.9 and weight decay 1e ?3 and learning rate ? 0 = 1e ?2 for the new layers and layers learned from scratch for all experiments except ? 0 = 1e ?3 for VisDA-C. We further adopt the same learning rate scheduler ? = ? 0 ? (1 + 10 ? p) ?0.75 as <ref type="bibr" target="#b17">(Ganin &amp; Lempitsky, 2015;</ref><ref type="bibr" target="#b42">Long et al., 2018)</ref>, where p is the training progress changing from 0 to 1. Besides, we set the batch size to 64 for all the tasks. We utilize ? = 0.3 for all experiments except ? = 0.1 for Digits.</p><p>For Digits, we train the optimal source hypothesis using the test set of the source dataset as validation. For other datasets without train-validation splits, we randomly specify a 0.9/ 0.1 split in the source dataset and generate the optimal source hypothesis based on the validation split. The maximum number of epochs for Digits, Office, Office-Home, VisDA-C and Office-Caltech is empirically set as 30, 100, 50, 10 and 100, respectively. For learning in the target domain, we update the pseudo-labels epoch by epoch, and the maximum number of epochs is empirically set as 15. We randomly run our methods for three times with different random seeds {2019, 2020, 2021} via PyTorch and report the average accuracies. Note that we do not exploit any target augmentation such as the ten-crop ensemble  for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results of Digit Recognition</head><p>For digit recognition, we evaluate SHOT on three closedset adaptation tasks, SVHN?MNIST, USPS?MNIST, and MNIST?USPS. The classification accuracies of SHOT and prior work are reported in <ref type="table" target="#tab_1">Table 2</ref>. Obviously, SHOT obtains the best or the the second-best mean accuracies for each task and outperforms prior work in terms of the average accuracy. Compared with source model only, SHOT-IM always achieves better results, and SHOT performs slightly better than SHOT-IM due to the contribution of self-supervised pseudo-labeling. It is worth noting that SHOT is comparable to the target-supervised results in M?U. This may be because MNIST is much larger than USPS, which alleviates the domain shift well.  <ref type="bibr" target="#b17">(Ganin &amp; Lempitsky, 2015)</ref> 79.7 82.0 68.2 96.9 67.4 99.1 82.2 DAN <ref type="bibr" target="#b40">(Long et al., 2015)</ref> 78.6 80.5 63.6 97.1 62.8 99.6 80.4 CDAN+E  92.9 94.1 71.0 98.6 69.3 100. 87.7 rRevGrad+CAT <ref type="bibr" target="#b15">(Deng et al., 2019)</ref> 90 Next, we evaluate SHOT on a variety of object recognition benchmarks including Office, Office-Home and VisDA-C under the vanilla closed-set DA setting. As shown in <ref type="table" target="#tab_2">Table 3</ref>, SHOT performs the best for two challenging tasks, D?A and W?A, and performs worse than previous state-of-theart method  for other tasks. This may be because SHOT needs a relatively large target domain to learn the hypothesis f t while D and W are small as the target domain. Generally, SHOT obtains competitive performance even with no direct access to the source domain data.</p><p>As expected, on the medium-sized Office-Home dataset, SHOT significantly outperforms previously published stateof-the-art approaches, advancing the average accuracy from 67.6%  to 71.8% in <ref type="table" target="#tab_4">Table 4</ref>. Besides, SHOT performs the best among 10 out of 12 separate tasks. For a large-scale synthesis-to-real VisDA-C dataset in <ref type="table">Table 5</ref>, SHOT still achieves the best per-class accuracy and performs the best among 7 out of 12 tasks. Carefully comparing SHOT with prior work, we find that SHOT performs well for the most challenging class 'truck'. Again, the proposed self-supervised pseudo-labeling strategy works well as SHOT always performs better than SHOT-IM. Ablation Study. We study the advantages of the selfsupervised pseudo-labeling (PL) strategy over naive one <ref type="bibr" target="#b34">(Lee, 2013)</ref> in <ref type="table" target="#tab_5">Table 6</ref>. It is clearly shown that both PL strategies work well and our self-supervised PL is always much better than the naive one. Surprisingly, merely using L ent along with self-supervised PL even produces lower results. But using both L ent and L div , the results become better than the self-supervised PL baseline, which indicates the importance of the diversity-promoting objective L div . We then evaluate the contribution of each component within the network architecture, and show the results in <ref type="figure" target="#fig_2">Fig. 3</ref>. It seems that the accuracies of SHOT-IM partially depend on the quality of source model only. The results of source model only are especially boosted with the help of BN that is also adopted in prior studies <ref type="bibr" target="#b69">(Xu et al., 2019;</ref><ref type="bibr" target="#b66">Wang et al., 2019)</ref>. Using WN and LS may decrease the accuracies of source model only, but we still obtain better results via SHOT-IM. All these components complement to each other, and using each two of them together helps source model only and SHOT-IM achieve the best performance. Finally, SHOT-IM achieves the best performance when all these components are used in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results of Object Recognition (Beyond Vanilla Closed-set)</head><p>We further extend SHOT to two other DA scenarios, multisource <ref type="bibr" target="#b49">(Peng et al., 2019a)</ref> and multi-target (Peng et al., 2019b). For the multi-source setting, we first learn multiple source hypotheses from different source domains and directly transfer them to the target domain one by one. Then the prediction scores of different optimized target hypotheses via SHOT are added up to find the label which has the maximum value. For the multi-target setting, we naively combine these target domains and treat it as the new target domain for SHOT. The results of SHOT and previously published state-of-the-arts are shown in <ref type="table" target="#tab_7">Table 8</ref>. It is easy to find that SHOT achieves the best results for both settings. Note that FADA <ref type="bibr" target="#b51">(Peng et al., 2020)</ref> does not access to the source domain but utilizes the gradient update information like federated learning, but it performs much worse than our SHOT for the multi-source setting. Finally, we evaluate the generalization of SHOT by extending it to two challenging DA tasks, PDA and ODA, and follow the protocols on Office-Home in  and <ref type="bibr" target="#b38">(Liu et al., 2019)</ref>, respectively. For PDA, there are totally 25 classes (the first 25 in alphabetical order) in the target domain and 65 classes in the source domain, while for ODA, the source domain consists of the same 25 classes but the target domain contains 65 classes including unknown samples. More details about how SHOT works for these two scenarios are provided in Appendix B and C, respectively. Results in <ref type="table" target="#tab_6">Table 7</ref> validate the effectiveness of SHOT for these two challenging asymmetric DA tasks.</p><p>Special Case. One may wonder whether SHOT works if we cannot train the source model by ourselves.</p><p>To find the answer, we utilize the most popular off-the-shelf pretrained ImageNet models ResNet-50 <ref type="bibr" target="#b24">(He et al., 2016</ref>) and consider a PDA task (ImageNet ? Caltech) to evaluate the effectiveness of SHOT with the same basic setting as . Obviously, in <ref type="table" target="#tab_8">Table 9</ref>, SHOT still achieves slightly higher accuracy than the state-of-the-art ETN  even without accessing the source data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we address a practical unsupervised DA setting with a simple yet generic representation learning framework named SHOT. SHOT merely needs the well-trained source model and offers the feasibility of unsupervised DA without access to the source data which may be private and decentralized. Specifically, SHOT learns the optimal target-specific feature learning module to fit the source hypothesis by exploiting the information maximization and self-supervised pseudo-labeling. Experiments for both digit and object recognition verify that SHOT achieves competitive and even state-of-the-art performance.</p><p>Acknowledgements: Jiashi Feng was partially supported by AISG-100E-2019-035 and MOE2017-T2-2-151. The authors would like to acknowledge the (partial) support from the Open Project Program (No. KBDat1505) of Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science &amp; Technology. The authors also thank Quanhong Fu and Weihao Yu for their help to improve the technical writing aspect of this paper.</p><p>Algorithm 1 SHOT algorithm for closed-set UDA task.</p><p>Input: source model fs = gs ? hs, target data {x i t } n t i=1 , maximum number of epochs Tm, trade-off parameter ?. Initialization: Freeze the final classifier layer ht = hs, and copy the parameters from gs to gt as initialization. for epoch = 1 to Tm do Obtain self-supervised pseudo labels via Eq. (6) for iter = 1 to n b do # min-batch optimization Sample a batch from target data and get the corresponding pseudo labels. Update the parameters in gt via L(gt) in Eq. <ref type="formula" target="#formula_10">(7)</ref>. end for end for 6. Appendix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. UDA scenarios and Network Architecture</head><p>Concerning the DA scenarios, the closet-set setting <ref type="bibr" target="#b52">(Saenko et al., 2010)</ref> is the most favored, where the source and the target domain share the same label space. Later, the original one-to-one adaptation is extended to the multi-source <ref type="bibr" target="#b49">(Peng et al., 2019a)</ref> and the multi-target <ref type="bibr" target="#b50">(Peng et al., 2019b)</ref> setting, respectively. To be more realistic, open-set DA (Panareda <ref type="bibr" target="#b47">Busto &amp; Gall, 2017)</ref> assumes the target domain has some unseen classes, while partial-set DA  considers a setting where the label space of the source domain subsumes that of the target domain.</p><p>For the digit experiments, we follow  and show the detailed networks in <ref type="figure" target="#fig_4">Fig. 4</ref>.  B. Details of SHOT for Partial-set DA (PDA)</p><p>Looking at the diversity-promoting term L div in Eq. <ref type="formula">(3)</ref>, it encourages the target domain to own a uniform target label distribution. It sounds reasonable for closed-set DA, but not suitable for partial-set DA. In reality, the target domain only contains some classes of the whole classes in the source domain, making the label distribution sparse. Hence, we drop the second term L div for PDA.</p><p>Besides, within the self-supervised pseudo-labeling strategy, we usually need to obtain K centroids. However, for the PDA task, there are some tiny centroids which should be considered as empty like k-means clustering. Particularly, SHOT discards tiny centroids whose size is smaller than T c in Eq. (6) for PDA. Next, we show the average accuracy on Office-Home with regards to the choice of T c in <ref type="figure" target="#fig_5">Fig. 5</ref>. Conversely, the target domain in an open-set DA problem contains some unknown classes besides the classes seen in the source domain. However, it is hard to change the final classifier layer for SHOT and learn it totally in the target domain. Thus we resort to a confidence thresholding strategy to reject unknown samples in the target domain during the learning of SHOT. In effect, we utilize the entropy of network output to compute the uncertainty, and normalize it in the range of [0,1] by dividing log K. For each epoch, we first compute all the uncertainty values and perform a 2-class k-means clustering. The cluster with larger mean uncertainty would be treated as the unknown class and not be considered to update the target centroids and compute the objective in L ent . Next, we show the changes in OS score, OS* score, and unknown accuracy during the process of SHOT in <ref type="figure" target="#fig_6">Fig. 6</ref>. Specifically, the OS score includes the unknown class and measures the per-class accuracy, i.e., OS = 1 K+1 K+1 k=1 Acc k , where K indicates the number of known classes and (K+1)-th class is an unknown class. OS* score only measure the per-class accuracy on the known classes, i.e., OS = 1 K K k=1 Acc k . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Source model only (b) SHOT-IMFigure 1. t-SNE visualizations for a 5-way classification task. Circles in dark colors denote unseen source data and stars in light denote target data. Different colors represent different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Baseline Methods. For vanilla unsupervised DA in digit recognition, we compare SHOT with ADDA<ref type="bibr" target="#b63">(Tzeng et al., 2017)</ref>, ADR<ref type="bibr" target="#b53">(Saito et al., 2018a)</ref>, CDAN, CyCADA<ref type="bibr" target="#b25">(Hoffman et al., 2018)</ref>, CAT<ref type="bibr" target="#b15">(Deng et al., 2019)</ref>, and SWD<ref type="bibr" target="#b33">(Lee et al., 2019a)</ref>. For object recognition, we compare with DANN<ref type="bibr" target="#b17">(Ganin &amp; Lempitsky, 2015)</ref>, DAN<ref type="bibr" target="#b40">(Long et al., 2015)</ref>, ADR, CDAN, CAT, SAFN<ref type="bibr" target="#b69">(Xu et al., 2019)</ref>, BSP<ref type="bibr" target="#b10">(Chen et al., 2019)</ref>, and TransNorm and SWD. For specific partial-set and open-set DA tasks, we compare with IWAN<ref type="bibr" target="#b73">(Zhang et al., 2018a)</ref>, SAN, ETN, SAFN, and ATI-? (Panareda<ref type="bibr" target="#b47">Busto &amp; Gall, 2017)</ref>, OSBP<ref type="bibr" target="#b55">(Saito et al., 2018c)</ref>, OpenMax<ref type="bibr" target="#b1">(Bendale &amp; Boult, 2016)</ref>, STA<ref type="bibr" target="#b38">(Liu et al., 2019)</ref>, respectively. For mutli-source and multi-target DA, we compare with DCTN<ref type="bibr" target="#b68">(Xu et al., 2018)</ref>, MCD(Saito et al.,   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Accuracies (%) on the Ar?Cl task for closed-set UDA.[Weight normalization/ Batch normalization/ Label smoothing]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Network architectures used for digit experiments. (a) for MNIST?USPS and (b) for SVHN?MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Average accuracies (%) of SHOT-IM and SHOT for all partial-set DA tasks on Office-Home. (ResNet-50) C. Details of SHOT for Open-set DA (ODA)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Different scores of SHOT w.r.t. the number of epochs for the open-set DA task Ar ? Cl on Office-Home. (ResNet-50)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>).Storage size (MB) Digits? VisDA-C ?</figDesc><table><row><cell>Source Dataset</cell><cell>33.2</cell><cell>7884.8</cell></row><row><cell>Source Model</cell><cell>0.9</cell><cell>172.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracies (%) on Digits dataset for vanilla closed-set DA. S: SVHN, M:MNIST, U: USPS.</figDesc><table><row><cell>Method (Source?Target)</cell><cell>S?M</cell><cell>U?M</cell><cell cols="2">M?U Avg.</cell></row><row><cell cols="5">Source only (Hoffman et al., 2018) 67.1?0.6 69.6?3.8 82.2?0.8 73.0</cell></row><row><cell>ADDA (Tzeng et al., 2017)</cell><cell cols="4">76.0?1.8 90.1?0.8 89.4?0.2 85.2</cell></row><row><cell>ADR (Saito et al., 2018a)</cell><cell cols="4">95.0?1.9 93.1?1.3 93.2?2.5 93.8</cell></row><row><cell>CDAN+E (Long et al., 2018)</cell><cell>89.2</cell><cell>98.0</cell><cell>95.6</cell><cell>94.3</cell></row><row><cell>CyCADA (Hoffman et al., 2018)</cell><cell cols="4">90.4?0.4 96.5?0.1 95.6?0.4 94.2</cell></row><row><cell cols="5">rRevGrad+CAT (Deng et al., 2019) 98.8?0.0 96.0?0.9 94.0?0.7 96.3</cell></row><row><cell>SWD (Lee et al., 2019a)</cell><cell cols="4">98.9?0.1 97.1?0.1 98.1?0.1 98.0</cell></row><row><cell>Source model only</cell><cell cols="4">70.2?1.2 88.0?2.2 79.7?2.5 79.3</cell></row><row><cell>SHOT-IM (ours)</cell><cell cols="4">99.0?0.1 97.6?0.5 97.7?0.1 98.2</cell></row><row><cell>SHOT (full, ours)</cell><cell cols="4">98.9?0.1 98.0?0.6 97.9?0.2 98.3</cell></row><row><cell>Target-supervised (Oracle)</cell><cell cols="4">99.4?0.1 99.4?0.1 98.0?0.1 98.8</cell></row><row><cell cols="5">2018b), M 3 SDA-? (Peng et al., 2019a), FADA (Peng et al.,</cell></row><row><cell cols="5">2020), DANN, and DADA (Peng et al., 2019b), respec-</cell></row><row><cell cols="5">tively. Note that results are directly cited from published</cell></row><row><cell cols="5">papers if we follow the same setting. Source model only</cell></row><row><cell cols="5">denotes using the entire mode from source for target label</cell></row><row><cell cols="5">prediction. SHOT-IM is a special case of SHOT, where the</cell></row><row><cell cols="5">self-supervised pseudo-labeling is ignored by letting ? = 0.</cell></row></table><note>4.2. Implementation Details Network architecture. For the digit recognition task, we use the same architectures with CDAN (Long et al., 2018), namely, the classical LeNet-5 (LeCun et al., 1998) network is utilized for USPS?MNIST and a variant of LeNet is uti- lized for SVHN?MNIST. Detailed networks are provided in Appendix A. For the object recognition task, we employ the pre-trained ResNet-50 or ResNet-101 (He et al., 2016) models as the backbone module like (Long et al., 2018; Deng et al., 2019; Xu et al., 2019; Peng et al., 2019a). Fol- lowing</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Classification accuracies (%) on small-sized Office dataset for vanilla closed-set DA (ResNet-50).</figDesc><table><row><cell>Method (Source?Target)</cell><cell>A?D A?W D?A D?W W?A W?D Avg.</cell></row><row><cell>ResNet-50 (He et al., 2016)</cell><cell>68.9 68.4 62.5 96.7 60.7 99.3 76.1</cell></row><row><cell>DANN</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b66">Wang et al., 2019)</ref> 94.0 95.7 73.4 98.7 74.2 100. 89.3</figDesc><table><row><cell></cell><cell>.8 94.4 72.2 98.0 70.2 100. 87.6</cell></row><row><cell>SAFN+ENT (Xu et al., 2019)</cell><cell>90.7 90.1 73.0 98.6 70.2 99.8 87.1</cell></row><row><cell>CDAN+BSP (Chen et al., 2019)</cell><cell>93.0 93.3 73.6 98.2 72.6 100. 88.5</cell></row><row><cell>CDAN+TransNorm (Source model only</cell><cell>80.8 76.9 60.3 95.3 63.6 98.7 79.3</cell></row><row><cell>SHOT-IM (ours)</cell><cell>90.6 91.2 72.5 98.3 71.4 99.9 87.3</cell></row><row><cell>SHOT (full, ours)</cell><cell>94.0 90.1 74.7 98.4 74.3 99.9 88.6</cell></row><row><cell cols="2">4.4. Results of Object Recognition (Vanilla Closed-set)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Classification accuracies (%) on medium-sized Office-Home dataset for vanilla closed-set DA (ResNet-50).</figDesc><table><row><cell>Method (Source?Target)</cell><cell cols="13">Ar?Cl Ar?Pr Ar?Re Cl?Ar Cl?Pr Cl?Re Pr?Ar Pr?Cl Pr?Re Re?Ar Re?Cl Re?Pr Avg.</cell></row><row><cell>ResNet-50 (He et al., 2016)</cell><cell>34.9</cell><cell>50.0</cell><cell>58.0</cell><cell>37.4</cell><cell>41.9</cell><cell></cell><cell>46.2</cell><cell>38.5</cell><cell>31.2</cell><cell>60.4</cell><cell>53.9</cell><cell>41.2</cell><cell>59.9 46.1</cell></row><row><cell>DANN (Ganin &amp; Lempitsky, 2015)</cell><cell>45.6</cell><cell>59.3</cell><cell>70.1</cell><cell>47.0</cell><cell>58.5</cell><cell></cell><cell>60.9</cell><cell>46.1</cell><cell>43.7</cell><cell>68.5</cell><cell>63.2</cell><cell>51.8</cell><cell>76.8 57.6</cell></row><row><cell>DAN (Long et al., 2015)</cell><cell>43.6</cell><cell>57.0</cell><cell>67.9</cell><cell>45.8</cell><cell>56.5</cell><cell></cell><cell>60.4</cell><cell>44.0</cell><cell>43.6</cell><cell>67.7</cell><cell>63.1</cell><cell>51.5</cell><cell>74.3 56.3</cell></row><row><cell>CDAN+E (Long et al., 2018)</cell><cell>50.7</cell><cell>70.6</cell><cell>76.0</cell><cell>57.6</cell><cell>70.0</cell><cell></cell><cell>70.0</cell><cell>57.4</cell><cell>50.9</cell><cell>77.3</cell><cell>70.9</cell><cell>56.7</cell><cell>81.6 65.8</cell></row><row><cell>CDAN+BSP (Chen et al., 2019)</cell><cell>52.0</cell><cell>68.6</cell><cell>76.1</cell><cell>58.0</cell><cell>70.3</cell><cell></cell><cell>70.2</cell><cell>58.6</cell><cell>50.2</cell><cell>77.6</cell><cell>72.2</cell><cell>59.3</cell><cell>81.9 66.3</cell></row><row><cell>SAFN (Xu et al., 2019)</cell><cell>52.0</cell><cell>71.7</cell><cell>76.3</cell><cell>64.2</cell><cell>69.9</cell><cell></cell><cell>71.9</cell><cell>63.7</cell><cell>51.4</cell><cell>77.1</cell><cell>70.9</cell><cell>57.1</cell><cell>81.5 67.3</cell></row><row><cell cols="2">CDAN+TransNorm (Wang et al., 2019) 50.2</cell><cell>71.4</cell><cell>77.4</cell><cell>59.3</cell><cell>72.7</cell><cell></cell><cell>73.1</cell><cell>61.0</cell><cell>53.1</cell><cell>79.5</cell><cell>71.9</cell><cell>59.0</cell><cell>82.9 67.6</cell></row><row><cell>Source model only</cell><cell>44.6</cell><cell>67.3</cell><cell>74.8</cell><cell>52.7</cell><cell>62.7</cell><cell></cell><cell>64.8</cell><cell>53.0</cell><cell>40.6</cell><cell>73.2</cell><cell>65.3</cell><cell>45.4</cell><cell>78.0 60.2</cell></row><row><cell>SHOT-IM (ours)</cell><cell>55.4</cell><cell>76.6</cell><cell>80.4</cell><cell>66.9</cell><cell>74.3</cell><cell></cell><cell>75.4</cell><cell>65.6</cell><cell>54.8</cell><cell>80.7</cell><cell>73.7</cell><cell>58.4</cell><cell>83.4 70.5</cell></row><row><cell>SHOT (full, ours)</cell><cell>57.1</cell><cell>78.1</cell><cell>81.5</cell><cell>68.0</cell><cell>78.2</cell><cell></cell><cell>78.1</cell><cell>67.4</cell><cell>54.9</cell><cell>82.2</cell><cell>73.3</cell><cell>58.8</cell><cell>84.3 71.8</cell></row><row><cell cols="14">Table 5. Classification accuracies (%) on large-scale VisDA-C dataset for vanilla closed-set DA (ResNet-101).</cell></row><row><cell>Method (Synthesis ? Real)</cell><cell cols="3">plane bcycl bus</cell><cell cols="10">car horse knife mcycl person plant sktbrd train truck Per-class</cell></row><row><cell>ResNet-101 (He et al., 2016)</cell><cell>55.1</cell><cell cols="5">53.3 61.9 59.1 80.6 17.9</cell><cell>79.7</cell><cell>31.2</cell><cell>81.0</cell><cell>26.5</cell><cell>73.5</cell><cell>8.5</cell><cell>52.4</cell></row><row><cell cols="2">DANN (Ganin &amp; Lempitsky, 2015) 81.9</cell><cell cols="5">77.7 82.8 44.3 81.2 29.5</cell><cell>65.1</cell><cell>28.6</cell><cell>51.9</cell><cell>54.6</cell><cell>82.8</cell><cell>7.8</cell><cell>57.4</cell></row><row><cell>DAN (Long et al., 2015)</cell><cell>87.1</cell><cell cols="5">63.0 76.5 42.0 90.3 42.9</cell><cell>85.9</cell><cell>53.1</cell><cell>49.7</cell><cell>36.3</cell><cell cols="2">85.8 20.7</cell><cell>61.1</cell></row><row><cell>ADR (Saito et al., 2018a)</cell><cell>94.2</cell><cell cols="5">48.5 84.0 72.9 90.1 74.2</cell><cell>92.6</cell><cell>72.5</cell><cell>80.8</cell><cell>61.8</cell><cell cols="2">82.2 28.8</cell><cell>73.5</cell></row><row><cell>CDAN (Long et al., 2018)</cell><cell>85.2</cell><cell cols="5">66.9 83.0 50.8 84.2 74.9</cell><cell>88.1</cell><cell>74.5</cell><cell>83.4</cell><cell>76.0</cell><cell cols="2">81.9 38.0</cell><cell>73.9</cell></row><row><cell>CDAN+BSP (Chen et al., 2019)</cell><cell>92.4</cell><cell cols="5">61.0 81.0 57.5 89.0 80.6</cell><cell>90.1</cell><cell>77.0</cell><cell>84.2</cell><cell>77.9</cell><cell cols="2">82.1 38.4</cell><cell>75.9</cell></row><row><cell>SAFN (Xu et al., 2019)</cell><cell>93.6</cell><cell cols="5">61.3 84.1 70.6 94.1 79.0</cell><cell>91.8</cell><cell>79.6</cell><cell>89.9</cell><cell>55.6</cell><cell cols="2">89.0 24.4</cell><cell>76.1</cell></row><row><cell>SWD (Lee et al., 2019a)</cell><cell>90.8</cell><cell cols="5">82.5 81.7 70.5 91.7 69.5</cell><cell>86.3</cell><cell>77.5</cell><cell>87.4</cell><cell>63.6</cell><cell cols="2">85.6 29.2</cell><cell>76.4</cell></row><row><cell>Source model only</cell><cell>60.9</cell><cell cols="4">21.6 50.9 67.6 65.8</cell><cell>6.3</cell><cell>82.2</cell><cell>23.2</cell><cell>57.3</cell><cell>30.6</cell><cell>84.6</cell><cell>8.0</cell><cell>46.6</cell></row><row><cell>SHOT-IM (ours)</cell><cell>93.7</cell><cell cols="5">86.4 78.7 50.7 91.0 93.5</cell><cell>79.0</cell><cell>78.3</cell><cell>89.2</cell><cell>85.4</cell><cell cols="2">87.9 51.1</cell><cell>80.4</cell></row><row><cell>SHOT (full, ours)</cell><cell>94.3</cell><cell cols="5">88.5 80.1 57.3 93.1 94.9</cell><cell>80.7</cell><cell>80.3</cell><cell>91.5</cell><cell>89.1</cell><cell cols="2">86.3 58.2</cell><cell>82.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Average accuracies on three closed-set UDA datasets.</figDesc><table><row><cell>Methods / Datasets</cell><cell cols="3">Office Office-Home VisDA-C</cell></row><row><cell>Source model only</cell><cell>79.3</cell><cell>60.2</cell><cell>46.6</cell></row><row><cell cols="2">naive pseudo-labeling (PL) (Lee, 2013) 83.0</cell><cell>64.1</cell><cell>76.6</cell></row><row><cell>Self-supervised PL (ours)</cell><cell>87.6</cell><cell>68.9</cell><cell>80.7</cell></row><row><cell>Lent</cell><cell>83.5</cell><cell>55.5</cell><cell>63.3</cell></row><row><cell>Lent + Ldiv</cell><cell>87.3</cell><cell>70.5</cell><cell>80.4</cell></row><row><cell>Lent + Ldiv + naive PL (Lee, 2013)</cell><cell>87.5</cell><cell>70.3</cell><cell>82.9</cell></row><row><cell>Lent + Ldiv + Self-supervised PL</cell><cell>88.6</cell><cell>71.8</cell><cell>82.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>(OS) Classification accuracies (%) on Office-Home dataset for partial-set and open-set DA (ResNet-50).</figDesc><table><row><cell>Partial-set DA (Source?Target)</cell><cell cols="12">Ar?Cl Ar?Pr Ar?Re Cl?Ar Cl?Pr Cl?Re Pr?Ar Pr?Cl Pr?Re Re?Ar Re?Cl Re?Pr Avg.</cell></row><row><cell>ResNet-50 (He et al., 2016)</cell><cell>46.3</cell><cell>67.5</cell><cell>75.9</cell><cell>59.1</cell><cell>59.9</cell><cell>62.7</cell><cell>58.2</cell><cell>41.8</cell><cell>74.9</cell><cell>67.4</cell><cell>48.2</cell><cell>74.2 61.3</cell></row><row><cell>IWAN (Zhang et al., 2018a)</cell><cell>53.9</cell><cell>54.5</cell><cell>78.1</cell><cell>61.3</cell><cell>48.0</cell><cell>63.3</cell><cell>54.2</cell><cell>52.0</cell><cell>81.3</cell><cell>76.5</cell><cell>56.8</cell><cell>82.9 63.6</cell></row><row><cell>SAN (Cao et al., 2018)</cell><cell>44.4</cell><cell>68.7</cell><cell>74.6</cell><cell>67.5</cell><cell>65.0</cell><cell>77.8</cell><cell>59.8</cell><cell>44.7</cell><cell>80.1</cell><cell>72.2</cell><cell>50.2</cell><cell>78.7 65.3</cell></row><row><cell>ETN (Cao et al., 2019)</cell><cell>59.2</cell><cell>77.0</cell><cell>79.5</cell><cell>62.9</cell><cell>65.7</cell><cell>75.0</cell><cell>68.3</cell><cell>55.4</cell><cell>84.4</cell><cell>75.7</cell><cell>57.7</cell><cell>84.5 70.5</cell></row><row><cell>SAFN (Xu et al., 2019)</cell><cell>58.9</cell><cell>76.3</cell><cell>81.4</cell><cell>70.4</cell><cell>73.0</cell><cell>77.8</cell><cell>72.4</cell><cell>55.3</cell><cell>80.4</cell><cell>75.8</cell><cell>60.4</cell><cell>79.9 71.8</cell></row><row><cell>Source model only</cell><cell>45.2</cell><cell>70.4</cell><cell>81.0</cell><cell>56.2</cell><cell>60.8</cell><cell>66.2</cell><cell>60.9</cell><cell>40.1</cell><cell>76.2</cell><cell>70.8</cell><cell>48.5</cell><cell>77.3 62.8</cell></row><row><cell>SHOT-IM (ours)</cell><cell>57.9</cell><cell>83.6</cell><cell>88.8</cell><cell>72.4</cell><cell>74.0</cell><cell>79.0</cell><cell>76.1</cell><cell>60.6</cell><cell>90.1</cell><cell>81.9</cell><cell>68.3</cell><cell>88.5 76.8</cell></row><row><cell>SHOT (full, ours)</cell><cell>64.8</cell><cell>85.2</cell><cell>92.7</cell><cell>76.3</cell><cell>77.6</cell><cell>88.8</cell><cell>79.7</cell><cell>64.3</cell><cell>89.5</cell><cell>80.6</cell><cell>66.4</cell><cell>85.8 79.3</cell></row><row><cell>Open-set DA (Source?Target)</cell><cell cols="12">Ar?Cl Ar?Pr Ar?Re Cl?Ar Cl?Pr Cl?Re Pr?Ar Pr?Cl Pr?Re Re?Ar Re?Cl Re?Pr Avg.</cell></row><row><cell>ResNet (He et al., 2016)</cell><cell>53.4</cell><cell>52.7</cell><cell>51.9</cell><cell>69.3</cell><cell>61.8</cell><cell>74.1</cell><cell>61.4</cell><cell>64.0</cell><cell>70.0</cell><cell>78.7</cell><cell>71.0</cell><cell>74.9 65.3</cell></row><row><cell cols="2">ATI-? (Panareda Busto &amp; Gall, 2017) 55.2</cell><cell>52.6</cell><cell>53.5</cell><cell>69.1</cell><cell>63.5</cell><cell>74.1</cell><cell>61.7</cell><cell>64.5</cell><cell>70.7</cell><cell>79.2</cell><cell>72.9</cell><cell>75.8 66.1</cell></row><row><cell>OSBP (Saito et al., 2018c)</cell><cell>56.7</cell><cell>51.5</cell><cell>49.2</cell><cell>67.5</cell><cell>65.5</cell><cell>74.0</cell><cell>62.5</cell><cell>64.8</cell><cell>69.3</cell><cell>80.6</cell><cell>74.7</cell><cell>71.5 65.7</cell></row><row><cell>OpenMax (Bendale &amp; Boult, 2016)</cell><cell>56.5</cell><cell>52.9</cell><cell>53.7</cell><cell>69.1</cell><cell>64.8</cell><cell>74.5</cell><cell>64.1</cell><cell>64.0</cell><cell>71.2</cell><cell>80.3</cell><cell>73.0</cell><cell>76.9 66.7</cell></row><row><cell>STA (Liu et al., 2019)</cell><cell>58.1</cell><cell>53.1</cell><cell>54.4</cell><cell>71.6</cell><cell>69.3</cell><cell>81.9</cell><cell>63.4</cell><cell>65.2</cell><cell>74.9</cell><cell>85.0</cell><cell>75.8</cell><cell>80.8 69.5</cell></row><row><cell>Source model only</cell><cell>36.3</cell><cell>54.8</cell><cell>69.1</cell><cell>33.8</cell><cell>44.4</cell><cell>49.2</cell><cell>36.8</cell><cell>29.2</cell><cell>56.8</cell><cell>51.4</cell><cell>35.1</cell><cell>62.3 46.6</cell></row><row><cell>SHOT-IM (ours)</cell><cell>62.5</cell><cell>77.8</cell><cell>83.9</cell><cell>60.9</cell><cell>73.4</cell><cell>79.4</cell><cell>64.7</cell><cell>58.7</cell><cell>83.1</cell><cell>69.1</cell><cell>62.0</cell><cell>82.1 71.5</cell></row><row><cell>SHOT (full, ours)</cell><cell>64.5</cell><cell>80.4</cell><cell>84.7</cell><cell>63.1</cell><cell>75.4</cell><cell>81.2</cell><cell>65.3</cell><cell>59.3</cell><cell>83.3</cell><cell>69.6</cell><cell>64.6</cell><cell>82.3 72.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Classification accuracies (%) on Office-Caltech dataset for multi-source and multi-target DA (ResNet-101). [ * R denotes the rest three domains except the single source / target.]</figDesc><table><row><cell>Multi-source (R ?)</cell><cell cols="4">R ?A R ?C R ?D R ?W Avg.</cell></row><row><cell>ResNet-101 (He et al., 2016)</cell><cell>88.7</cell><cell>85.4</cell><cell>98.2</cell><cell>99.1 92.9</cell></row><row><cell>DAN (Long et al., 2015)</cell><cell>91.6</cell><cell>89.2</cell><cell>99.1</cell><cell>99.5 94.8</cell></row><row><cell>DCTN (Xu et al., 2018)</cell><cell>92.7</cell><cell>90.2</cell><cell>99.0</cell><cell>99.4 95.3</cell></row><row><cell>MCD (Saito et al., 2018b)</cell><cell>92.1</cell><cell>91.5</cell><cell>99.1</cell><cell>99.5 95.6</cell></row><row><cell>M 3 SDA-? (Peng et al., 2019a)</cell><cell>94.5</cell><cell>92.2</cell><cell>99.2</cell><cell>99.5 96.4</cell></row><row><cell>FADA (Peng et al., 2020)</cell><cell>84.2</cell><cell>88.7</cell><cell>87.1</cell><cell>88.1 87.1</cell></row><row><cell>Source model only</cell><cell>95.4</cell><cell>93.7</cell><cell>98.9</cell><cell>98.3 96.6</cell></row><row><cell>SHOT-IM (ours)</cell><cell>96.2</cell><cell>96.1</cell><cell>98.5</cell><cell>99.7 97.6</cell></row><row><cell>SHOT (full, ours)</cell><cell>96.4</cell><cell>96.2</cell><cell>98.5</cell><cell>99.7 97.7</cell></row><row><cell>Multi-target (? R)</cell><cell cols="4">A? R C? R D? R W? R Avg.</cell></row><row><cell>ResNet-101 (He et al., 2016)</cell><cell>90.5</cell><cell>94.3</cell><cell>88.7</cell><cell>82.5 89.0</cell></row><row><cell>SE (French et al., 2018)</cell><cell>90.3</cell><cell>94.7</cell><cell>88.5</cell><cell>85.3 89.7</cell></row><row><cell>MCD (Saito et al., 2018b)</cell><cell>91.7</cell><cell>95.3</cell><cell>89.5</cell><cell>84.3 90.2</cell></row><row><cell cols="2">DANN (Ganin &amp; Lempitsky, 2015) 91.5</cell><cell>94.3</cell><cell>90.5</cell><cell>86.3 90.7</cell></row><row><cell>DADA (Peng et al., 2019b)</cell><cell>92.0</cell><cell>95.1</cell><cell>91.3</cell><cell>93.1 92.9</cell></row><row><cell>Source model only</cell><cell>90.7</cell><cell>96.1</cell><cell>90.2</cell><cell>90.9 92.0</cell></row><row><cell>SHOT-IM (ours)</cell><cell>95.7</cell><cell>97.2</cell><cell>96.3</cell><cell>96.1 96.3</cell></row><row><cell>SHOT (full, ours)</cell><cell>96.2</cell><cell>97.3</cell><cell>96.3</cell><cell>96.2 96.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Results of a PDA task (ImageNet ? Caltech). ? utilizes the training set of ImageNet besides pre-trained ResNet-50 model.</figDesc><table><row><cell cols="3">Methods ResNet-50 ETN  ? SHOT-IM (ours) SHOT (full, ours)</cell></row><row><cell>Accuracy 69.7?0.0 83.2?0.2</cell><cell>81.7?0.5</cell><cell>83.3?0.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer MLJ</publisher>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhushan Damodaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Practical secure aggregation for privacypreserving machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kreuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcedone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CCS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grieskamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ingerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konecny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01046</idno>
		<title level="m">Towards federated learning at scale: System design</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to transfer examples for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Cariucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation in the absence of source domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chidlovskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pseudo-labeling curriculum for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comprehensive survey on domain adaptation for visual applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation in Computer Vision Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cluster alignment with a teacher for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Selfensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: a deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sampleproblem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative clustering by regularized information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stability and hypothesis transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kuzborskij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sliced wasserstein discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulbricht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Drop to adapt: Learning discriminative features for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-G</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aggregating randomized clustering-promoting invariant projections for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1027" to="1042" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distant supervised centroid shift: A simple and efficient approach to visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Separate to adapt: Open set domain adaptation via progressive separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Domain adaptation with multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning differentially private recurrent language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">When does label smoothing help? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Source free domain adaptation using an off-the-shelf classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Nelakurthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE BigData</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panareda</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Domain agnostic learning with disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Federated adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Open set domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Information-theoretical learning of discriminative clusters for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning categories from few examples with multi model knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="928" to="941" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Transferable normalization: Towards improving transferability of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Cross-domain video concept detection using adaptive svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-MM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Natschl?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Importance weighted adversarial nets for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
