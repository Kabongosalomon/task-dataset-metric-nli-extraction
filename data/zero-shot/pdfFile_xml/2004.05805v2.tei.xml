<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diversity Helps: Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiexin</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
							<email>liwenbin@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
							<email>gaoy@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diversity Helps: Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot learning aims to learn a new concept when only a few training examples are available, which has been extensively explored in recent years. However, most of the current works heavily rely on a large-scale labeled auxiliary set to train their models in an episodic-training paradigm. Such a kind of supervised setting basically limits the widespread use of few-shot learning algorithms. Instead, in this paper, we develop a novel framework called Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation (ULDA), which pays attention to the distribution diversity inside each constructed pretext few-shot task when using data augmentation. Importantly, we highlight the value and importance of the distribution diversity in the augmentation-based pretext few-shot tasks, which can effectively alleviate the overfitting problem and make the few-shot model learn more robust feature representations. In ULDA, we systemically investigate the effects of different augmentation techniques and propose to strengthen the distribution diversity (or difference) between the query set and support set in each few-shot task, by augmenting these two sets diversely (i.e., distribution shifting). In this way, even incorporated with simple augmentation techniques (e.g., random crop, color jittering, or rotation), our ULDA can produce a significant improvement. In the experiments, few-shot models learned by ULDA can achieve superior generalization performance and obtain state-of-the-art results in a variety of established few-shot learning tasks on Omniglot and miniImageNet. The source code is available in https://github.com/WonderSeven/ULDA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability of learning from limited labeled examples is a hallmark of human intelligence, yet it remains a challenge for modern machine learning systems. This problem recently has attracted significant attention from the machine learning community, which is formalized as few-shot learning (FSL). To solve this problem, a large-scale auxiliary set is generally required to learn transferable knowledge to boost the learning of the target few-shot tasks. Specifically, one kind of FSL methods usually resort to using metric losses to enhance the discriminability of the representation learning, such that a simple nearest neighbor or linear classifier is able to achieve satisfactory classification results <ref type="bibr" target="#b23">(Snell, Swersky, and Zemel 2017;</ref><ref type="bibr" target="#b26">Vinyals et al. 2016)</ref>. Another kind of * contributed equally  <ref type="figure">1</ref>: The train and test accuracy curves on the 5-way 1-shot tasks, corresponding to the four combinations of different augmentation methods (i.e., TA and AA) in <ref type="table" target="#tab_0">Table 1</ref>. As seen, the diverse combinations (i.e., the red and yellow lines) enjoy a smaller risk of overfitting (i.e., lower train accuracy and higher test accuracy) than the identical combinations (i.e., the green and blue lines).</p><p>FSL methods incorporates the concept of meta-learning and aims to enhance the ability of quickly updating with a few labeled examples <ref type="bibr" target="#b7">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b22">Ravi and Larochelle 2017;</ref><ref type="bibr" target="#b20">Munkhdalai and Yu 2017)</ref>. Alternatively, some FSL methods address this problem by generating more examples from the provided ones <ref type="bibr">(Gao et al. 2018;</ref><ref type="bibr">Chen et al. 2019b,c)</ref>. Although the aforementioned FSL methods can achieve promising results, most of these methods are fully supervised, which means that they are heavily relying on a largescale fully labeled auxiliary set (e.g., a subset from Ima-geNet in previous works <ref type="bibr" target="#b23">(Snell, Swersky, and Zemel 2017;</ref><ref type="bibr" target="#b7">Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b22">Ravi and Larochelle 2017)</ref>). Through this fully labeled auxiliary set, plenty of supervised few-shot tasks (episodes) can be constructed for model training (i.e., episodic-training mechanism <ref type="bibr" target="#b26">(Vinyals et al. 2016)</ref>). However, in many real-world applications, such a fully supervised condition is relatively severe. It greatly hinder the widespread use of these FSL methods for real applications.</p><p>Because data labeling for a large-scale dataset is normally time-consuming, laborious, and even very expensive for some domain-professional areas like biomedical data analysis. In contrast, large unlabeled data is easily accessible to many real problems. This gives rise to a more challenging problem, called unsupervised few-shot learning, which tries to learn few-shot models by using an unlabeled auxiliary set.</p><p>As for unsupervised few-shot learning, only a few works have been proposed. For example, CACTUs <ref type="bibr" target="#b13">(Hsu, Levine, and Finn 2019)</ref>, a two-stage method, firstly uses a clustering algorithm to obtain pseudo labels, and then trains a model under the common supervised few-shot setting with these pseudo labels. Different from CACTUs, both AAL <ref type="bibr" target="#b1">(Antoniou and Storkey 2019)</ref> and UMTRA <ref type="bibr" target="#b14">(Khodadadeh, Boloni, and Shah 2019)</ref> take each instance as one class and randomly sample multiple examples to construct a support set. Next, they generate a pseudo query set according to the support set by leveraging data augmentation techniques. In this paper, we are more interested in this data augmentation based direction, because it can not only achieve promising results but also can be easily learned in an end-to-end manner. However, we find that the existing data augmentation based methods (i.e., AAL and UMTRA) are sensitive to the selection of augmentation techniques and usually do not contain sufficient regularity for model learning. What's more, they are easily suffering from the overfitting problem during training, because they choose the same data augmentation technique for both the query set and support set. This will make the distributions between the augmented query set and support set too similar. In other words, they construct too many "easy pretext few-shot tasks" for the downstream few-shot training by only using one single data augmentation technique. In some cases, this technique equals to directly copy original samples several times. We argue that such excessive distribution similarity between the query and support set (i.e., easy pretext few-shot tasks) is the main point of leading to overfitting issue in unsupervised few-shot model training.</p><p>To tackle the above overfitting problem, we claim that strengthening the distribution diversity (or difference) between the augmented query set and support set (i.e., hard pretext few-shot tasks) can significantly alleviate the overfitting problem during the model training and make the learned model have a much better generalization ability. To simply verify this point, we perform a preliminary experiment (see <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure">Figure 1</ref>). We observe that there is a high risk of overfitting when using same augmentation technique. Also, when using different (or diverse) augmentation techniques, the classification performance can be significantly improved over using same augmentation technique. Therefore, in this paper, we introduce a novel framework named Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation (ULDA) following the above statement. To be specific, our ULDA augments the query set and support set in diverse ways, aiming to make a significant distribution shift between these two sets. The main contributions of our work could be summarized into the following three folds: 1. We argue that the distribution diversity between the augmented query set and support set is a key point in data augmentation based unsupervised few-shot learning, for the first time in the literature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We briefly review the related work about supervised and unsupervised few-shot learning, respectively. Few-shot learning (FSL). FSL aims to learn a new concept on very limited training examples, which has promising practical application value. A vast number of methods has been proposed in recent years. These methods can be roughly categorized into three classes, i.e., metric-based, optimization-based, and hallucination-based methods.</p><p>The metric-based methods aim to learn discriminative feature representations by using deep metric learning, with the help of intra-class and inter-class constraints <ref type="bibr" target="#b26">(Vinyals et al. 2016;</ref><ref type="bibr" target="#b23">Snell, Swersky, and Zemel 2017;</ref><ref type="bibr" target="#b25">Sung et al. 2018;</ref><ref type="bibr" target="#b19">Li et al. 2019)</ref>. They employ various metric losses (e.g., pairwise loss, triplet loss) to enhance the discriminability of the learned features. The optimization-based methods strive for enhancing the flexibility of the learned model such that it can be readily updated with a few labeled examples <ref type="bibr" target="#b22">(Ravi and Larochelle 2017;</ref><ref type="bibr" target="#b7">Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b18">Lee et al. 2019;</ref><ref type="bibr" target="#b2">Chen et al. 2019a)</ref>. Alternatively, the hallucinationbased methods attempt to address the data scarcity problem by directly generating more new examples <ref type="bibr" target="#b29">(Zhang, Zhang, and Koniusz 2019;</ref><ref type="bibr" target="#b0">Alfassy et al. 2019;</ref><ref type="bibr">Chen et al. 2019c,b,d)</ref>.</p><p>Most methods train their models under the episodictraining paradigm <ref type="bibr" target="#b26">(Vinyals et al. 2016)</ref>. They organize a large labeled auxiliary dataset into plenty of mimetic fewshot tasks where each task contains a support set and a query set. The support set is used to acquire task-specific information and the query set is used to evaluate the generalization performance of the model. Based on episodic-training, the model expects to learn transferable representations or knowledge, with which, it can generalize to new unseen tasks.</p><p>Unsupervised few-shot learning. Currently, a few works propose unsupervised few-shot learning to tackle the huge requirement of a large labeled auxiliary set in supervised few-shot learning. Hsu et al. <ref type="bibr" target="#b13">(Hsu, Levine, and Finn 2019)</ref> propose CACTUs which uses a clustering algorithm to obtain pseudo labels and then constructs few-shot tasks with these pseudo labels. Differently, Khodadadeh et al. <ref type="bibr" target="#b14">(Khodadadeh, Boloni, and Shah 2019)</ref> and <ref type="bibr" target="#b1">Antoniou et al. (Antoniou and Storkey 2019)</ref> both propose to randomly sample multiple examples to construct the support set and generate a pseudo query set via data augmentation based on the support set.</p><p>Our work belongs to the data augmentation based methods. The main difference is that the existing methods (Khodadadeh, Boloni, and Shah 2019; Antoniou and Storkey 2019)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Sampling</head><p>Query Set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random</head><p>Label Assignment <ref type="figure">Figure 2</ref>: The framework of the proposed ULDA which starts from an unlabeled auxiliary dataset. First, randomly select N examples and assign N random labels to them. After that, the proposed distribution shift-based augmentation module is used to construct a pretext few-shot task (consists of an augmented query set and an augmented support set). Specifically, the query set and support set are augmented by the augmentation operators A Q ? A Q and A S ? A S , respectively. Finally, the constructed pretext few-shot task is adopted to train the few-shot learning model in a supervised way. easily suffers from the overfitting problem, while our proposed ULDA can significantly alleviate this problem. This is because there is usually a large distribution similarity between the query set and support set in the existing methods, while our ULDA strengthens a distribution shift between the augmented query set and support set. Note that similar operations seemingly have appeared in other research fields, such as FixMatch <ref type="bibr" target="#b24">(Sohn et al. 2020</ref>) in semi-supervised learning and SimCLR <ref type="bibr" target="#b2">(Chen et al. 2020)</ref> in unsupervised representation learning. However, we highlight that we do not need to use weak augmentations to assign a higher degree of confidence for unlabeled samples like FixMatch did, and we draw a different observation (diversity helps) from SimCLR (combination is better). In fact, we believe that our observation/perspective is unique in the specific field of unsupervised few-shot learning, which could not be directly borrowed to other fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>The goal of unsupervised few-shot learning is to first train a model on a large-scale unlabeled auxiliary set D train , and then apply this trained model on a novel labeled test set D test , which is composed of a set of few-shot tasks. Note that, according to the setting of FSL, there are only a few labeled examples (e.g., 1 or 5 examples) in each class for each few-shot task in D test . To effectively leverage the unlabeled auxiliary set D train for model training, following the episodic-training mechanism <ref type="bibr" target="#b26">(Vinyals et al. 2016)</ref>, we still try to generate a series of pretext N -way K-shot tasks (episodes) from D train by using an data augmentation framework. In particular, each pretext few-shot task is composed of a pseudo support set (for training) and a pseudo query set (for validation). The pseudo support set consists of N classes and K examples per class (e.g., K=1 in our paper), termed as {(x i , y i )} N ?K i=1 , while the query set {(x 1 ,? 2 ), ..., (x M ,? M )} contains M generated examples augmented based on the pseudo support set. At each iteration, the model is trained by one episode (task) to minimize the classification loss on query set according to support set. After tens of thousands of episodes training, the model is expected to reach convergence and perform well on novel few-shot tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Proposed ULDA Framework</head><p>To detail the proposed Distribution Shift-based Data Augmentation (ULDA) framework (see <ref type="figure">Figure 2</ref>), we first layout the pretext few-shot task construction procedure in unsupervised few-shot learning. Next, we detail the two key modules in ULDA: (1) distribution shift-based data augmentation module, (2) metric-based few-shot learning module. Task Construction for Unsupervised FSL. We randomly sample a mini-batch of N data-points {x 1 , ..., x N } from the unlabeled auxiliary set D train as the initial support samples and construct one pretext few-shot task on augmented examples derived from this initial support set. Specifically, we take each data-point as one class and assign random labels for these data-points X = {(x 1 , 1), ..., (x N , N )}, which is a common strategy in unsupervised learning in the litera- ture <ref type="bibr" target="#b27">(Wu et al. 2018;</ref><ref type="bibr" target="#b10">He et al. 2019</ref>).</p><p>During the augmentation on the support set, for the ith initial support image x i (i.e., the i-th support class), we perform the augmentation operator A S i from A S (A S i ? A S ) on this sample to obtain an augmented support image A S i (x i ). Also, for the augmentation on the query set, we randomly select M augmentation operators A Q 1 , A Q 2 , ..., A Q M ? A Q to augment each initial support image (i.e., each support class) to obtain M augmented query images. So each constructed pretext few-shot task T z consists of an augmented support set S and an augmented query set Q (i.e.,T z = (S, Q)):</p><formula xml:id="formula_0">S = (A S i (xi), i)|i = 1, ..., N , Q = (A Q j (xi), i)|i = 1, ..., N , j = 1, ..., M ,<label>(1)</label></formula><p>where A S i (x i ) means to perform the sampled operator A S i on the i-th initial support image x i in the initial support set. A Q j (x i ) means to perform the sampled operator A Q j on the i-th initial support image x i from the initial support set.</p><p>In this work, we emphasize that maintaining a diversity between A S and A Q (i.e., A S ? A Q = ? and A Q ? A S = ?) benefits the performance. This will be thoroughly discussed in the following section. We summarize the main sampling strategy of ULDA in Algorithm 1. Distribution Shift-based Augmentation Module. Data augmentation technique plays a key role in aforementioned task construction procedure. However, in traditional methods <ref type="bibr" target="#b14">(Khodadadeh, Boloni, and Shah 2019;</ref><ref type="bibr" target="#b1">Antoniou and Storkey 2019)</ref>, the generated tasks do not contain sufficient regularity for model learning as the generated examples are particularly suspect to visual similarity with the original images. To alleviate this problem, we propose to increase the distribution diversity between the augmented support set and query set with a novel distribution shift-based data augmentation module, which employs diverse data augmentation operators to generate the support set and query set.</p><p>To systematically study the impact of diverse data augmentation, we consider to use both the commonly-used data augmentations and recently proposed augmentations. Random crop and color jittering are widely used together in few-shot learning, we bind them as traditional augmentation (TA for short). Typically, for rotation, each image is converted among Sample N data-points x 1 , ..., x N from U.</p><formula xml:id="formula_1">four directions in R = {0 ? , 90 ? , 180 ? , 270 ? }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Randomly assign labels to sampled data-points:</p><formula xml:id="formula_2">X={(x 1 , 1), ..., (x N , N )}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Generate support set S by using operator sampled from A S to augment each sample in X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Generate query set Q by using M operator sampled from A Q to augment each sample in X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><formula xml:id="formula_3">T z ? (S, Q) 7: return T z | Z z=1</formula><p>et al. 2019) is also investigated for its promising performance in UMTRA <ref type="bibr" target="#b14">(Khodadadeh, Boloni, and Shah 2019)</ref>.</p><p>In addition to the above existing augmentation techniques, we also propose a new Distribution Shift-based Task Internal Mixing (DSTIM) augmentation strategy, which is composed of two new augmentation operators, i.e., TIM sub and TIM add . Specifically, we visualize all the augmentation operators used in this work in <ref type="figure" target="#fig_0">Figure 3</ref>. To understand the efficacy of each individual augmentation operator and the difference of different combinations of augmentation operators, we conduct a serial of experiments detailed in our supplementary material. DSTIM. Inspired by the recent works of generating new examples near the boundary of a classifier in <ref type="bibr" target="#b28">(Zhang et al. 2018;</ref><ref type="bibr" target="#b21">Qiao et al. 2019)</ref>, we originally propose a task-level augmentation technique which is termed as Distribution Shiftbased Task Internal Mixing (DSTIM). DSTIM is a simple yet effective method consisting of two augmentation operators TIM sub and TIM add , both of which perform convex combination differently between all images in the operated data set. To be specific, for each instance (x i , y i ) in support (or query) set, we randomly select another instance (x j , y j ) from the same set. TIM add synthesizes a new example (x add ,?) as follows:x</p><formula xml:id="formula_4">add = ? ? xi + (1 ? ?) ? xj,? = yi,<label>(2)</label></formula><p>where ? = max(?, 1 ? ?), ? ? Beta(?, ?), so ? ? [0.5, 1.0]. This means TIM add can extends the distribution of the synthesized example to the margin of the selected two examples. In contrast, for TIM sub , the synthesized examplex sub can be obtained as below:</p><formula xml:id="formula_5">x sub = ? ? xi ? (1.5 ? ?) ? xj,? = yi,<label>(3)</label></formula><p>where ? = 0.5 + max(?, 1 ? ?), ? ? Beta(?, ?), so ? ? [1.0, 1.5]. TIM sub can generate a new instance by performing subtraction between two images. And this operation can extend to get away from other examples. Combining with these two operators, we can extend the distribution of raw examples to two opposite directions which thus strengthen the distribution shift between the two operated sets. Moreover, as we keep the value of ? between 0.5 and 1.5, this will leads to the synthetic label y i rather than y j , so it is an identity-preserved augmentation. In this work, we use TIM sub to augment images in the support set and TIM add for the query set. Metric-based FSL Module. Metric-based few-shot learning algorithms are a kind of simple and effective methods to address the few-shot problems, which aim to enhance the discriminability of learned feature representations via deep metric learning. The main component of these algorithms is a feature extractor f (?; ?), which is a convolutional neural network (CNN) with parameters ?. Given an episode (fewshot task) T z , the feature extractor will map each image x i in T z into a d-dimensional feature, i.e., f (x i ; ?). In the learned feature space, the images in query set are forced to a labeled image in support set when they share similar semantic information <ref type="bibr" target="#b25">(Sung et al. 2018;</ref><ref type="bibr" target="#b19">Li et al. 2019)</ref>. Normally, Euclidean distance or cosine distance is employed to measure the distance or similarity between two examples. As the feature extractor plays a key role in the final classification results, the diversity of the augmented examples is crucial to exhibit the feature extractor to extract discriminative features. Crucially, our proposed ULDA framework can just satisfy this purpose, by increasing the distribution diversity between the augmented support set and query set. Therefore, to construct an effective unsupervised few-shot learning model, we tailor our ULDA into a representative existing metric-based few-shot learning algorithm, ProtoNets <ref type="bibr" target="#b23">(Snell, Swersky, and Zemel 2017)</ref>, and name this new model as ULDA-ProtoNets. Obviously, our ULDA framework is universal and extensible, which can be simply tailored to other existing few-shot models. This part will be further discussed in Section 3.3.</p><p>Given a N-way K-shot episode T z , ProtoNets computes the "prototype" via averaging features for each class in the support set with the feature extractor f (?; ?):</p><formula xml:id="formula_6">pi = 1 K x?S i f A S (x); ? ,<label>(4)</label></formula><p>where S i = {x|(x, y) ? S, y = i} and A S ? A S . These "prototypes" are used to build a simple nearest neighbor classifier. Then, given a new image x q from query set, the classifier outputs a normalized classification score computed with Euclidean distance for each class i:</p><formula xml:id="formula_7">C i f (xq; ?) = f A Q (xq); ? ? pi 2 N j=1 f A Q (xq); ? ? pj 2 ,<label>(5)</label></formula><p>where A Q ? A Q . So, the image x q will be classified to its closest prototype. The few-shot loss function L few for updating the parameter ? is formalized as:</p><formula xml:id="formula_8">L few = Tz ?T (xq ,yq ?Q) ? log C yq f (xq; ?) .<label>(6)</label></formula><p>Note that, the distance between f (A Q (x q ); ?) and its corresponding prototype will not change if we keep A S = A Q . And this makes no sense to secure the discriminability of the feature extractor. Besides, as we use rotation as an augmentation technique, we can also incorporate with a self-supervised loss L self to predict the rotation angle.</p><formula xml:id="formula_9">L self = Tz ?T y self ?R (xq ,yq ?Q) ?log C y self (f (xq; ?, W ) ,<label>(7)</label></formula><p>where W is the parameters of an additional classifier for predicting the rotation angle for each query image x q from R = {0 ? , 90 ? , 180 ? , 270 ? }. Specifically, this classifier is implemented by one fully connected layer. Therefore, the overall loss function can be formulated as:</p><formula xml:id="formula_10">L = L few + ?L self ,<label>(8)</label></formula><p>where ? is a balancing hyper-parameter.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extension to Optimization-based FSL</head><p>Different from the metric-based FSL algorithms, the optimization-based FSL algorithms strive for enhancing the flexibility of a few-shot model such that it can be readily updated using a few labeled examples. Most of these algorithms are generally based on meta learning. See Section 2 for more details. To further verify the effectiveness and scalability of our proposed ULDA framework, we extend ULDA to a recently proposed optimization-based FSL algorithm, i.e., MetaOptNet <ref type="bibr" target="#b18">(Lee et al. 2019)</ref>, and name this new method as ULDA-MetaOptNet in the following parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we detail the experimental settings and compare our ULDA with the state-of-the-art approaches on two challenging datasets, i.e., Omniglot <ref type="bibr" target="#b17">(Lake et al. 2011</ref>) and miniImageNet <ref type="bibr" target="#b26">(Vinyals et al. 2016)</ref>, which are widely used in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Datasets. The Omniglot dataset comprises 1623 characters from 50 different alphabets. Each character contains 20 instances written by different persons. We follow the experiment protocol described by <ref type="bibr" target="#b1">(Antoniou and Storkey 2019)</ref>: classes 1-1150, 1150-1200 and 1200-1623 are used for training, validation and test, respectively. The miniImageNet is the most popular benchmark in the field of few-shot learning, which was introduced in <ref type="bibr" target="#b26">(Vinyals et al. 2016)</ref>. It is composed of 100 classes selected from ImageNet <ref type="bibr" target="#b16">(Krizhevsky, Sutskever, and Hinton 2012)</ref>, and each class contains 600 images with the size of 84 ? 84. We follow the data splits proposed by <ref type="bibr" target="#b22">(Ravi and Larochelle 2017)</ref>, which splits the total 100 classes into 64 classes for training, 16 classes for validation and 20 classes for test, respectively. Backbone network. We employ a four-layer convolutional neural network as the feature extractor backbone, which is widely adopted in the few-shot learning literature <ref type="bibr" target="#b23">(Snell, Swersky, and Zemel 2017;</ref><ref type="bibr" target="#b7">Finn, Abbeel, and Levine 2017)</ref>. Each layer comprises a 64 filters (3 ? 3 kernel) convolutional layer, a batch normalization layer, a ReLU layer and a 2 ? 2 max-pooling layer. Moreover, different ResNet <ref type="bibr" target="#b11">(He et al. 2016)</ref> architectures are also employed to validate the expansibility of our framework. Training strategy. We conduct N -way K-shot classification tasks on the aforementioned datasets. We randomly sample and construct 10,000 pretext few-shot tasks in each epoch and train our networks for a total of 60 epochs. For miniImageNet, we employ AutoAugment <ref type="bibr" target="#b6">(Cubuk et al. 2019)</ref> to augment the support set and traditional augmentation together with rotation to augment the query set. For Omniglot, we use AutoAugment for support set and random crop for query set. Note that, self-supervised loss is not employed in Omniglot. All backbone networks are optimized by Adam <ref type="bibr" target="#b15">(Kingma and Ba 2015)</ref>. The initial learning rate is set as 0.001 and multiplied by 0.06, 0.012, 0.0024 after 20, 40, and 50 epochs, respectively. We conduct all the experiments on GTX 2080Ti. For a fair comparison, the hyper parameters in all of these methods are kept to be the same. Parameter setup. We set ? = 1 in Eq. (8). In Eq. (2), we empirically set ?=0.8 for TIM sub and in Eq.  <ref type="figure">Figure 4</ref>: The train and test accuracy curves on the 5-way 1-shot tasks. As seen, the diverse combinations especially our proposed ULDA (the red lines) enjoys a smaller risk of overfitting and a higher test accuracy.</p><p>TIM add . Our model is robust to different values of ? according to our experiments (see more details in our supplementary material). Thus, we set it in a slightly different manner following our distribution-diversity argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised Few-shot Learning Results</head><p>To verify the effectiveness of our approach for unsupervised few-shot learning, we compare our framework with the stateof-the-art (SOTA) methods in various settings. Moreover, to make our results more convincing, we randomly sample 1,000 episodes from the test set for evaluation. Also, we take the top-1 mean accuracy as evaluation criterion and repeat this process five times. Besides, the 95% confidence intervals are also reported.</p><p>Results on Omniglot. The comparative results between a variety of baseline and recently proposed methods on Omniglot are presented in <ref type="table" target="#tab_2">Table 2</ref>. ULDA shows currently the best results across different tasks. Compared with previous best results, our ULDA-ProtoNets gains 2.6%, 0.18%, 3.8% and 1.96% under 5-way 1-shot, 5-way 5-shot, 20-way 1shot and 20-way 5-shot settings, respectively. Similarly, our ULDA-MetaOptNet can also achieve very competitive results especially in the 5-way 1-shot and 20-way 1-shot settings.</p><p>Results on miniImageNet. The experimental results on miniImageNet are summarized in <ref type="table" target="#tab_3">Table 3</ref>. Our ULDA achieves the state-of-the-art results on both 5-way 1-shot, 5-way 5-shot and 5-way 20-shot settings and achieves competitive results on 5-way 50-shot settings. Besides, the results of ULDA are very close to the results of supervised few-shot learning approaches with a labeled auxiliary set, i.e., ProtoNets and MAML. Note that, when using the same few-shot learning algorithm (i.e., ProtoNets), our ULDA framework outperforms all other methods across different classification tasks. Compared with CACTUs-ProtoNets, our ULDA-ProtoNets gains 1.45%, 2.82%, 2.77%, 2.88% performance boost under 5-way 1-shot, 5-shot, 20-shot and 50shot settings, respectively. The reason is that CACTUS uses clustering algorithms to obtain the pseudo labels before constructing few-shot tasks, but the quality of these pseudo labels will limit the final results. In contrast, our ULDA does not have this limitation. When compared with AAL, which is  the closest work to ours, our ULDA can still achieve 2.96% and 15.89% performance boost for 5-way 1-shot and 5-way 5-shot, respectively. Results on tieredImageNet. We turn to tieredImageNet, a more challenging dataset, which contains more complex classes and examples than miniImageNet. Since the recent unsupervised few-shot leaning methods (i.e., CACTUs, UM-TRA) did not report their results on this dataset, we only compare our methods with the baseline method training from scratch. The results are illustrated in <ref type="table" target="#tab_4">Table 4</ref>. Our ULDA performs much better than learning from scratch and slightly weaker than the supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study on miniImageNet</head><p>The Overfitting Problem In these series of experiments, we study the overfitting problem of different diverse augmentation combinations during the model learning procedure. The results on miniImageNet under 5-way 1-shot are shown in <ref type="figure">Figure 4</ref>. Here, ULDA is the proposed framework in this paper, which employs AutoAugment to generate support set and combines traditional augmentation with rotation to generate query set. Moreover, DSTIM is also employed here. All results are averaged among 1,000 tasks. As expected, diverse augmentation can efficiently alleviate the over-fitting problem. Moreover, when incorporated with our proposed  augmentation method DSTIM, the distribution difference between query set and support set can be further enlarged, i.e.,the generated pretext few-shot tasks enjoy more challenges, which can effectively alleviate the overfitting problem in unsupervised learning manner. As seen in <ref type="figure">Figure 4</ref>, our proposed ULDA obtains a lower train accuracy curves but meanwhile a relative higher test accuracy curves. Composite Augmentation vs. Diverse Augmentation. Another way to alleviate the overfitting problem in unsupervised FSL is that we can compose different augmentation operators together (i.e., a larger augmentation operator set A = A S ? A Q ) to increase the whole diversity of the generated samples as introduced in <ref type="bibr" target="#b2">(Chen et al. 2020</ref>), but we still adopt the same A to augment both the query and support set. We call this composite augmentation. Differently, our ULDA employs a diverse augmentation, i.e., augmenting the query set and support set separately. To figure out the difference between these two augmentation ways, we conduct a serial of experiments on miniImageNet (see <ref type="figure" target="#fig_3">Figure 5</ref>). When we employ more complex operators, both the diverse augmentation and composite augmentation boost the performance. Notably, the diverse augmentation always performs better than the composite augmentation. It shows that the former can gain more distribution shift, which is more beneficial for alleviating the overfitting problem. Comparisons with different backbones. We further perform a series of experiments on ResNets <ref type="bibr" target="#b11">(He et al. 2016)</ref> with different depths. Note that the settings are kept almost the same as the above experiments expect the learning rate. We set the learning rate to 0.1 following <ref type="bibr" target="#b18">(Lee et al. 2019)</ref>. The results are reported in <ref type="table" target="#tab_6">Table 5</ref>. As seen, our ULDA can achieve much higher results with much deeper networks, e.g., ResNet12 and ResNet18. For example, when using ResNet12 as the backbone, our ULDA-ProtoNets can even further gain 2.1% improvements over a Conv64F-based version, which is also significantly better than other SOTA methods. However, the performance of our ULDA-ProtoNets begins to drop with ResNet18/ResNet34/ResNet50, which indicates that these models suffer from a new overfitting risk. We may need to further increase the diversity between the constructed tasks. We leave this as our future work. Effectiveness of distribution shift-based augmentation module. Despite the promising results achieved by our entire framework, we also expect to know how it works, especially the relationship between the distribution shift in generated two sets and the final results. With this purpose, we employ the aforementioned augmentation techniques (i.e., random crop, color jittering, rotation, AutoAugment and our proposed DSTIM) and combine them in various ways to produce these two sets with different distribution shift. Besides, we use Kullback-Leibler divergence (KL divergence) and Fr?chet Inception Distance (FID) <ref type="bibr" target="#b12">(Heusel et al. 2017)</ref> to evaluate the distribution difference. The results are illustrated in <ref type="figure" target="#fig_4">Figure  6</ref>. We can draw the conclusion from these results that the models tend to perform much better when trained on pretext few-shot tasks that have large distribution difference. In order to intuitively show the effect of our framework, we also visualize the augmentation effect in feature space in <ref type="figure" target="#fig_5">Figure 7</ref>. As seen, when augmenting support set and query set with the same augmentation techniques, the generated query set gathers tightly around support set, and these tend to exist heavy overlap in these augmented data-points. However, with our ULDA, the generated examples share more diversity and more distribution difference between the support and query set. We will analyze this issue in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present an unsupervised few-shot learning framework that aims to increase the diversity of generated few-shot tasks based on data augmentation. We argue that when strengthening the distribution shift between the support set and query set in each few-shot task with different augmentation techniques can increase the generalization ability for model training. A serial of experiments have been conducted to demonstrate the correctness of our finding. We also incorporate our framework with two representative few-shot learning algorithms, i.e., ProtoNets and MetaOptNet, and achieve the state-of-the-art results across a variety of few-shot learning tasks established on Omniglot and miniImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The Comparison of Different Diverse Augmentation Combinations</head><p>To verify the diverse augmentation which we claim on, we do a serial of experiments. Here, we employ the aforementioned augmentation techniques (i.e., TA, AA, R and our proposed DSTIM) and combine them in various ways to produce different distribution shift when constructing query and support set. Besides, we use Kullback-Leibler divergence (KL divergence) and Fr?chet Inception Distance (FID) to measure the distribution difference. Also, the results in <ref type="table" target="#tab_7">Table A</ref>.1 are the detailed values of <ref type="figure" target="#fig_3">Figure 5</ref> in our original paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Feature Representation</head><p>In order to intuitively show the effect of our framework, we also visualize the augmentation efficacy in feature space in <ref type="figure">Figure B</ref>.1. We find that, when augmenting support set and query set using the same augmentation technique, the generated query set gathers tightly around support set, and this case tends to exist heavy overlap in these augmented datapoints. However, by using our approach, the generated examples share more diversity and more distribution difference between the support set and query set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C The Value of ? in DSTIM</head><p>Our default setting is ? = 0.8 for TIM sub and ? = 0.6 for TIM add . The performance remains stable with using different values of ?. The results are shown in <ref type="table" target="#tab_7">Table C</ref>.1.  <ref type="figure">Figure B</ref>.1: Visualization of feature transformations in generated support images and query images. Same color means generated from the same data-point. The generated images own more diversity and there exist little overlap between generated support images and query images via our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Illustrators of the employed augmentation techniques in this work. Top: Original images, Bottom: augmented images, transformed by an augmentation operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The learned AutoAugment (AA for short) method proposed in (Cubuk Algorithm 1 The main sampling strategy in ULDA require: N : class-count, M : meta-test size, Z: episodic number require: U: unlabeled auxiliary set require: A S , A Q : two sets of different augmentation operators 1: for z = 1, ..., Z do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(3), ?=0.6 for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparison between composite augmentation and diverse augmentation on miniImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The performance changing with the value of distribution divergence on miniImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7</head><label>7</label><figDesc>: t-SNE plots in feature space. (a) common augmentation, (b) our ULDA. Zoom in for best visual effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The results of N -way K-shot tasks on miniImageNet by using different augmentation methods on ProtoNet to construct the query and support sets. TA and AA indicate traditional augmentation and AutoAugment, respectively.</figDesc><table><row><cell cols="4">Support Query 5-way 1-shot 5-way, 5-shot</cell></row><row><cell>TA</cell><cell>TA</cell><cell>32.58</cell><cell>44.40</cell></row><row><cell>AA</cell><cell>AA</cell><cell>31.53</cell><cell>41.83</cell></row><row><cell>TA</cell><cell>AA</cell><cell>34.07</cell><cell>47.31</cell></row><row><cell>AA</cell><cell>TA</cell><cell>35.37</cell><cell>49.16</cell></row><row><cell>Figure</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2. We propose a Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation (ULDA) framework and a new simple augmentation method named Distribution Shift-based Task Internal Mixing (DSTIM) to strengthen the distribution diversity when constructing the pretext few-shot training tasks.</figDesc><table><row><cell>3. Extensive experiments on both Omniglot and</cell></row><row><cell>miniImageNet datasets demonstrate the superiority</cell></row><row><cell>of our proposed ULDA and DSTIM.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Unsupervised few-shot classification results (%) under N -way K-shot (i.e., (N, K)) setting on Omniglot.</figDesc><table><row><cell>Algorithms</cell><cell>Clustering</cell><cell>(5, 1)</cell><cell>(5, 5)</cell><cell>(20, 1)</cell><cell>(20, 5)</cell></row><row><cell>Training from scratch</cell><cell>N/A</cell><cell>52.50?0.84</cell><cell>74.78?0.69</cell><cell>24.91?0.33</cell><cell>47.62?0.44</cell></row><row><cell>knn-nearest neighbors</cell><cell>DeepCluster</cell><cell>49.55?1.27</cell><cell>68.06?0.71</cell><cell>27.37?0.33</cell><cell>46.70?0.36</cell></row><row><cell>linear classifier</cell><cell>DeepCluster</cell><cell>48.28?1.25</cell><cell>68.72?0.66</cell><cell>27.80?0.61</cell><cell>45.82?0.37</cell></row><row><cell>MLP with dropout</cell><cell>DeepCluster</cell><cell>40.54?0.79</cell><cell>62.56?0.79</cell><cell>19.92?0.32</cell><cell>40.71?0.40</cell></row><row><cell>cluster matching</cell><cell>DeepCluster</cell><cell>43.96?0.80</cell><cell>58.62?0.78</cell><cell>21.54?0.32</cell><cell>31.06?0.37</cell></row><row><cell>AAL-ProtoNes (Antoniou and Storkey 2019)</cell><cell>N/A</cell><cell>84.66?0.70</cell><cell>88.41?0.27</cell><cell>68.79?1.03</cell><cell>74.05?0.46</cell></row><row><cell>AAL-MAML++ (Antoniou and Storkey 2019)</cell><cell>N/A</cell><cell>88.40?0.75</cell><cell>97.96?0.32</cell><cell>70.21?0.27</cell><cell>88.32?1.22</cell></row><row><cell cols="2">CACTUs-ProtoNets (Hsu, Levine, and Finn 2019) ACAI</cell><cell>68.12?0.84</cell><cell>83.58?0.61</cell><cell>47.75?0.43</cell><cell>66.27?0.37</cell></row><row><cell>CACTUs-MAML (Hsu, Levine, and Finn 2019)</cell><cell>ACAI</cell><cell>68.84?0.80</cell><cell>87.78?0.50</cell><cell>48.09?0.41</cell><cell>73.36?0.34</cell></row><row><cell>UMTRA (Khodadadeh, Boloni, and Shah 2019)</cell><cell>N/A</cell><cell>83.80??</cell><cell>95.43??</cell><cell>74.25??</cell><cell>92.12??</cell></row><row><cell>ULDA-ProtoNets(ours)</cell><cell>N/A</cell><cell>91.00?0.42</cell><cell>98.14?0.15</cell><cell>78.05?0.31</cell><cell>94.08?0.13</cell></row><row><cell>ULDA-MetaOptNet(ours)</cell><cell>N/A</cell><cell>90.51?0.45</cell><cell>97.60?0.17</cell><cell>76.32?0.32</cell><cell>92.48?0.15</cell></row><row><cell></cell><cell cols="2">Supervised (Upper Bound)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProtoNets</cell><cell>N/A</cell><cell>98.35?0.22</cell><cell>99.58?0.09</cell><cell>95.31?0.18</cell><cell>98.81?0.07</cell></row><row><cell>MAML</cell><cell>N/A</cell><cell>94.46?0.77</cell><cell>98.83?0.12</cell><cell>84.60?0.32</cell><cell>96.29?0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Unsupervised few-shot classification results (%) under N -way K-shot (i.e., (N, K)) setting on miniImageNet. "-" means the results are not reported in their source papers.</figDesc><table><row><cell>Algorithms</cell><cell>Clustering</cell><cell>(5, 1)</cell><cell>(5, 5)</cell><cell>(5, 20)</cell><cell>(5, 50)</cell></row><row><cell>Training from scratch</cell><cell>N/A</cell><cell>27.59?0.59</cell><cell>38.48?0.66</cell><cell>51.53?0.72</cell><cell>59.63?0.74</cell></row><row><cell>knn-nearest neighbors</cell><cell>DeepCluster</cell><cell>28.90?1.25</cell><cell>42.25?0.67</cell><cell>56.44?0.43</cell><cell>63.90?0.38</cell></row><row><cell>linear classifier</cell><cell>DeepCluster</cell><cell>29.44?1.22</cell><cell>39.79?0.64</cell><cell>56.19?0.43</cell><cell>65.28?0.34</cell></row><row><cell>MLP with dropout</cell><cell>DeepCluster</cell><cell>29.03?0.61</cell><cell>39.67?0.69</cell><cell>52.71?0.62</cell><cell>60.95?0.63</cell></row><row><cell>cluster matching</cell><cell>DeepCluster</cell><cell>22.20?0.50</cell><cell>23.50?0.52</cell><cell>24.97?0.54</cell><cell>26.87?0.55</cell></row><row><cell>AAL-ProtoNes (Antoniou and Storkey 2019)</cell><cell>N/A</cell><cell>37.67?0.39</cell><cell>40.29?0.68</cell><cell>-</cell><cell>-</cell></row><row><cell>AAL-MAML++ (Antoniou and Storkey 2019)</cell><cell>N/A</cell><cell>34.57?0.74</cell><cell>49.18?0.47</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CACTUs-ProtoNets (Hsu, Levine, and Finn 2019) DeepCluster</cell><cell>39.18?0.71</cell><cell>53.36?0.70</cell><cell>61.54?0.68</cell><cell>63.55?0.64</cell></row><row><cell>CACTUs-MAML (Hsu, Levine, and Finn 2019)</cell><cell>DeepCluster</cell><cell>39.90?0.74</cell><cell>53.97?0.70</cell><cell>63.84?0.70</cell><cell>69.64?0.63</cell></row><row><cell>UMTRA (Khodadadeh, Boloni, and Shah 2019)</cell><cell>N/A</cell><cell>39.93??</cell><cell>50.73??</cell><cell>61.11??</cell><cell>67.15??</cell></row><row><cell>ULDA-ProtoNets(ours)</cell><cell>N/A</cell><cell>40.63?0.61</cell><cell>56.18?0.59</cell><cell>64.31?0.51</cell><cell>66.43?0.47</cell></row><row><cell>ULDA-MetaOptNet(ours)</cell><cell>N/A</cell><cell>40.71?0.62</cell><cell>54.49?0.58</cell><cell>63.58?0.51</cell><cell>67.65?0.48</cell></row><row><cell></cell><cell cols="2">Supervised (Upper Bound)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProtoNets</cell><cell>N/A</cell><cell>46.56?0.76</cell><cell>62.29?0.71</cell><cell>70.05?0.65</cell><cell>72.04?0.60</cell></row><row><cell>MAML</cell><cell>N/A</cell><cell>46.81?0.77</cell><cell>62.13?0.72</cell><cell>71.03?0.69</cell><cell>75.54?0.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Unsupervised few-shot classification results in % of N -way K-shot (N, K) learning methods on tieredImageNet.</figDesc><table><row><cell>Algorithms</cell><cell>Clustering</cell><cell>(5, 1)</cell><cell>(5, 5)</cell><cell>(5, 20)</cell><cell>(5, 50)</cell></row><row><cell>Training from scratch</cell><cell>N/A</cell><cell>26.27?1.02</cell><cell>34.91?0.63</cell><cell>38.14?0.58</cell><cell>38.67?0.44</cell></row><row><cell>ULDA-ProtoNets(ours)</cell><cell>N/A</cell><cell>41.60?0.64</cell><cell>56.28?0.62</cell><cell>64.07?0.55</cell><cell>66.00?0.54</cell></row><row><cell>ULDA-MetaOptNet(ours)</cell><cell>N/A</cell><cell>41.77?0.65</cell><cell>56.78?0.63</cell><cell>67.21?0.56</cell><cell>71.39?0.53</cell></row><row><cell></cell><cell cols="2">Supervised (Upper Bound)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProtoNets</cell><cell>N/A</cell><cell>46.66?0.63</cell><cell>66.01?0.60</cell><cell>77.62?0.46</cell><cell>81.70?0.44</cell></row><row><cell>MetaOptNet</cell><cell>N/A</cell><cell>47.32?0.64</cell><cell>66.16?0.58</cell><cell>77.68?0.47</cell><cell>80.61?0.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>ProtoNets 42.73?0.62 42.05?0.56 40.48?0.57 39.48?0.56</figDesc><table><row><cell>: 5-way 1-shot accuracy (%) on miniImageNet with</cell></row><row><cell>different network architectures.</cell></row><row><cell>ResNet12 ResNet18 ResNet34 ResNet50</cell></row><row><cell>ULDA-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A</head><label>A</label><figDesc>.1: The comparison with different augmentation methods on miniImageNet. The results in % of N -way K-shot (N , K) are reported. 181.14 39.42?0.57 53.87?0.58 AA+TIM sub R+TIM add 4.45 185.27 39.52?0.58 54.26?0.57 AA+TIM sub R+TA+TIM add 5.33 202.42 39.64?0.60 54.37?0.58</figDesc><table><row><cell>A S</cell><cell>A Q</cell><cell cols="2">KL FID</cell><cell>(5, 1)</cell><cell>(5, 5)</cell></row><row><cell>TA</cell><cell>TA</cell><cell cols="4">0.00 16.07 32.58?0.49 44.40?0.49</cell></row><row><cell>AA</cell><cell>AA</cell><cell cols="4">0.15 19.52 31.53?0.49 41.83?0.53</cell></row><row><cell>TA</cell><cell>AA</cell><cell>0.41</cell><cell>-</cell><cell cols="2">34.07?0.51 47.31?0.52</cell></row><row><cell>AA</cell><cell>TA</cell><cell cols="4">0.46 133.97 35.37?0.53 49.16?0.52</cell></row><row><cell>AA</cell><cell>R</cell><cell cols="4">0.73 183.06 39.18?0.58 53.30?0.58</cell></row><row><cell>AA</cell><cell>R+TA</cell><cell cols="4">2.66 172.22 39.28?0.59 53.55?0.58</cell></row><row><cell>AA</cell><cell>R+TIM add</cell><cell>2.93</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table C .</head><label>C</label><figDesc>1: The comparison with different augmentation methods on miniImageNet. The results in % of N -way K-shot (N , K) are reported.? for TIM sub ? for TIM add</figDesc><table><row><cell></cell><cell></cell><cell>(5, 1)</cell><cell>(5, 5)</cell></row><row><cell>0.6</cell><cell>0.6</cell><cell>40.08 ?0.59</cell><cell>54.33 ?0.56</cell></row><row><cell>0.6</cell><cell>0.8</cell><cell>40.06 ?0.61</cell><cell>54.47 ?0.58</cell></row><row><cell>0.8</cell><cell>0.6</cell><cell cols="2">40.63 ?0.61 55.41 ?0.57</cell></row><row><cell>0.8</cell><cell>0.8</cell><cell>39.92 ?0.60</cell><cell>54.76 ?0.56</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">LaSO: Label-Set Operations networks for multi-label few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alfassy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Assume, Augment and Learn: Unsupervised Few-Shot Meta-Learning via Random Labels and Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A Closer Look at Few-shot Classification</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image Block Augmentation for One-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image Deformation Meta-Networks for One-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-level Semantic Feature Augmentation for One-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<title level="m">Momentum contrast for unsupervised visual representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<title level="m">Unsupervised Learning via Meta-Learning. ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised Meta-Learning for Few-Shot Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the cognitive science society</title>
		<meeting>the annual meeting of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meta-Learning with Differentiable Convex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Meta networks. In ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transductive Episodic-Wise Adaptive Metric for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimization as a Model for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<title level="m">Prototypical Networks for Few-shot Learning. NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<title level="m">FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to Compare: Relation Network for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Z</forename><surname>Amd Tao Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Matching Networks for One Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mixup: Beyond Empirical Risk Minization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="975" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Few-Shot Learning via Saliency-guided Hallucination of Samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
