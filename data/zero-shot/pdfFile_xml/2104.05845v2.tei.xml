<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Goal-Step Inference using wikiHow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Yang</surname></persName>
							<email>yueyang1@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artemis</forename><surname>Panagopoulou</surname></persName>
							<email>artemisp@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lyu</surname></persName>
							<email>lyuqing@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
							<email>myatskar@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Goal-Step Inference using wikiHow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset harvested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-theart multimodal models. Moreover, the multimodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accuracy by 15 -20%. Our task will facilitate multimodal reasoning about procedural events.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, there has been growing attention on the representation of complex events, with renewed interest in script learning and commonsense reasoning <ref type="bibr">(Park and Motahari Nezhad, 2018;</ref><ref type="bibr">Mujtaba and Mahapatra, 2019;</ref><ref type="bibr">Li et al., 2020)</ref>. One aspect of event representation is the relationship between high-level goals and the steps involved <ref type="bibr">(Zhang et al., 2020b,a)</ref>. For example, given a goal (e.g. "change a tire"), an intelligent system should be able to infer what steps need to be taken to accomplish the goal (e.g. "place the jack under the car", "raise the jack"). In most work, events are represented as text <ref type="bibr">(Zellers et al., 2018;</ref><ref type="bibr" target="#b1">Coucke et al., 2018;</ref><ref type="bibr">Zhang et al., 2019)</ref>, while they could have different modalities in the real world.</p><p>Learning goal-step relations in a multimodal fashion is an interesting challenge since it requires reasoning beyond image captioning. We contend that multimodal event representation learning will have interesting implications for tasks such as schema learning <ref type="bibr">(Li et al., 2020</ref><ref type="bibr">(Li et al., , 2021</ref> to mitigate reporting bias (Gordon and Van Durme, 2013) since steps are often not explicitly mentioned in a body of text. For instance, if a robot is asked to "get a slice of cake," it has to know that it should "take the cake out of the box", "cut a slice", "put it on a plate", and then "take the plate to the user". Such steps are commonsense to people and thus rarely specified explicitly, making them hard to infer from textual data. However, with multimodal learning, we could obtain such details from visual signals. This multimodal goal-step relation could also be used for vision-enabled dialog systems 1 to recognize what task a user is completing and provide helpful recommendations. We propose a new task called Visual Goal-Step Inference (VGSI): given a textual goal and multiple images representing candidate events, a model must choose one image which constitutes a reasonable step towards the given goal. This means that a model should correctly recognize not only the specific action illustrated in an image (e.g., "turning on the oven", in <ref type="figure" target="#fig_0">Figure 1</ref>), but also the intent of the action ("baking fish").</p><p>We collect data from wikiHow articles, where most steps are illustrated with images. Our VGSI training set is constructed using three sampling strategies to select negative image candidates as distractors. In the format of multiple-choice and image retrieval, we evaluate four state-of-the-art multimodal models: <ref type="bibr">DeViSE (Frome et al., 2013)</ref>, Similarity Networks <ref type="bibr">(Wang et al., 2018)</ref>, Triplet Networks <ref type="bibr">(Hoffer and Ailon, 2015)</ref>, and LXMERT (Tan and Bansal, 2019) to human performance. It is observed that SOTA models designed for captionbased multimodal tasks <ref type="bibr">(Karpathy et al., 2014;</ref><ref type="bibr">Johnson et al., 2016</ref>) struggle on VGSI, exhibiting a 40% gap in accuracy from human performance when using a challenging sampling strategy.</p><p>One limitation of wikiHow is that most images are drawings rather than photos (which are more typically used in computer vision research). The knowledge learned from wikiHow is nevertheless useful when applied to real photos. We demonstrate this by pre-training a triplet-network on our wikiHow VGSI task and then conducting transfer learning on out-of-domain datasets. Our experiments show that pre-trained models can effectively transfer the goal-step knowledge to task-oriented video datasets, such as <ref type="bibr">COIN (Tang et al., 2019)</ref> and <ref type="bibr">Howto100m (Miech et al., 2019)</ref>. In addition, we design an aggregation model on top of SOTA models which treats wikiHow as a knowledge base that further increases the transfer learning performance (see Appendix C).</p><p>We make three key contributions: (1) We pro- pose the VGSI task, which is more challenging than traditional caption-based image-text matching tasks and requires the model to have an intermediate reasoning process about goal-step relations.</p><p>(2) To study the VGSI task, we collect a multimodal dataset from wikiHow which contains over 770k images.</p><p>(3) Through transfer learning, we show that the knowledge learned from our dataset can be readily applied to out-of-domain datasets, with an accuracy improvement of 15-20% on VGSI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">wikiHow as Multimodal Resource</head><p>We use wikiHow as the data source for VGSI because it has been successfully adopted in prior work for procedural learning <ref type="bibr">(Zhou et al., 2019)</ref> and intent detection <ref type="bibr">(Zhang et al., 2020a)</ref> in the language domain. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, each wikiHow article contains a high-level goal and one or more different methods 2 to achieve it. Each method then includes a series of specific steps, typically accompanied with corresponding images. The format of wikiHow articles provides a hierarchical multimodal relationship between images and sentences. We can obtain three types of textimage pairs from wikiHow, in increasing specificity: goal-image, method-image, and step-image. However, these text-image pairs are not enough information for a system to succeed on VGSI; it also needs the appropriate background knowledge. For the example in <ref type="figure" target="#fig_1">Figure 2</ref>, the system needs to know that "Trick-or-Treating" and "candies" are Halloween traditions and that a "mask" is required during "COVID-19".</p><p>In total, as shown in <ref type="table">Table 1</ref>, the corpus consists of 53,189 wikiHow articles across various categories of everyday tasks, 155,265 methods, and 772,294 steps with corresponding images 3 3 Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Given a high-level goal G-defined as a sequence of words-and an image I ? R 3?h?w -with the dimension of 3 color channels, the width, and the height-the model outputs the matching score:</p><formula xml:id="formula_0">match(G, I) = F (X G , X I )<label>(1)</label></formula><p>in which, X G ? R d G and X I ? R d I are the feature representations of the goal and the image, respectively. F is the scoring function that models the interactions between the two representations. At inference time, the model will choose the candidate with the highest matching score as the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Models</head><p>DeViSE takes in the pre-trained embedding vectors from the two modalities and maps the source vector onto the span of the target vector. DeViSE is trained only on the positive pairs (G, I) and projects an image embedding onto the same dimension as the goal with L2 normalization. Then it computes the cosine similarity of the two normalized vectors as the matching score. Similarity Network Each branch of the similarity network maps one modality to the joint span and executes point-wise multiplication to construct a joint vector. The last layer is fully-connected with softmax activation and outputs an array of size two to denote the weights of each class for binary classification. We compute the matching score by taking the dot product [1, ?1] with the output. Triplet Network requires the input to be the format of a triplet (G, I pos , I neg ). Three branches in the network map the three embeddings to the same joint span, such that the branches of positive and negative images share the same weight. The network learns the cross-modality by minimizing the positive pair distance and maximizing the negative pair distance. We choose cosine distance as the metric which is also used as the matching score.</p><p>LXMERT is a multimodal encoder that aims to ground text to images through attention layers. The image input is represented as a sequence of objects and the sentence input is a sequence of words. LXMERT utilizes two single-modality transformer encoders (language and object encoders) and a cross-modality transformer encoder to achieve the attention mechanism and capture the relationship between the two modalities. Same as the similarity network, LXMERT is trained as a binary classifier.  <ref type="table">Table 2</ref>: Accuracy of SOTA models on the wikiHow VGSI test set with different sampling strategies (sample size is shown in parentheses).</p><p>Figure 3: Accuracy of human (circles) and model (triangles) on the modified wikiHow VGSI test set with different textual input (e.g., in <ref type="figure" target="#fig_0">Fig 1,</ref> the goal prompt will be replaced by method -"Baking the Fish." or step -"Preheat the oven.").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multiple-Choice Sampling</head><p>We formulate the task as a 4-way multiple choice question, which is easy for evaluating the imagetext matching performance and is feasible for human annotation. Specifically, a model is given a textual goal &amp; four images to predict the most reasonable step towards the goal. We utilize three sampling strategies to obtain negative candidates: Random Strategy We randomly pick three different articles and select one image by chance from each article as the negative sample. Similarity Strategy We greedily select the most similar images based on the feature vectors and use FAISS <ref type="bibr">(Johnson et al., 2019)</ref> to retrieve the top-3 most similar images from three different articles. Category Strategy The three negative samples are randomly selected from articles within the same wikiHow category as the prompt goal.</p><p>In addition to the multiple-choice format, we also evaluate VGSI in a more realistic goal-image retrieval format (see Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human Annotation</head><p>Considering that VGSI is a novel task, we also evaluate how difficult it is for humans. All of our six human annotators are graduate students with good English proficiency. For each annotation test, we selected 100 samples from the testing set. A pair of annotators completed each test and their scores were averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>We report both model and human accuracy for the multiple-choice task. For the retrieval task, we adopt recall at k (recall@k) and median rank (Med r) to measure the performance (see Appendix B). <ref type="table">Table 2</ref> shows the performance of the models and humans on the wikiHow dataset. The Triplet Network with BERT embeddings has the best performance. However, there is still a big gap between human and model performance, indicating that VGSI is challenging for even SOTA models. LXMERT performs badly using similarity and category strategies presumably because it heavily depends on grounding objects, and negative samples generated by these two strategies could share similar objects but refer to different goals. <ref type="figure">Figure 3</ref> demonstrates that both humans and models perform better with lower-level texts as prompt, which reflects that our VGSI task is more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">In-Domain Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Transfer Learning</head><p>To robustly show the potential of wikiHow as a multimodal transfer learning resource, we compare it with two existing caption-based datasets, Flickr30K <ref type="bibr">(Plummer et al., 2015)</ref> and <ref type="bibr">MSCOCO (Vinyals et al., 2016)</ref>, which are used as pre-training alternatives. We use the official train/val split for each dataset and pre-train two models separately on Flickr and MSCOCO using the same multiplechoice sampling strategies as VGSI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Target Datasets &amp; Keyframe Extraction</head><p>Our transfer targets include COIN and Howto100m, both large-scale datasets of instructional videos. Each video depicts the process of accomplishing a high-level goal, mostly everyday tasks. Since these two datasets are video-based while our task is image-based, we apply a key frame extraction heuristic to get critical frames from videos. We   then consider the key frames as steps, thus converting the datasets into the VGSI format. Howto100m: We randomly select 1,000 goals and one video for each goal. To extract key frames, we apply k-means clustering in the feature space of the frames of each video and select the closest frame to each cluster center. We further filter these frames by manually removing unrelated frames such as the introduction, transition animations, repetitive frames, etc. We finally obtain 869 goals 4 with 24.7 frames for each goal. COIN: We randomly select 900 videos (5 videos per goal) to construct the test set, and use the remaining 9,709 videos for training. Since COIN has annotations of textual steps and their corresponding video segment, we randomly select one frame within each video segment as a VGSI candidate, resulting in 230.1 frames per goal. Then we use these frames to construct the multiple-choice examples with the same three sampling strategies. We also compare using wiki-How against using COIN and Howto100m as pretraining data to perform transfer learning to each other since both are instructional video datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Transfer Learning Performance</head><p>We use two different transfer learning setups for COIN 5 and Howto100m. For COIN, we formulate the test as a K-shot learning task where K is the number of VGSI training examples for each goal. The 180 goals for testing are seen during training to simulate the scenario where we have some instances of a task. For Howto100m, we split the 869 goals into 8:2 for training and testing, where the test goals are unseen during training.</p><p>Tables 3 and 4 both indicate that pre-training on wikiHow can improve VGSI performance on outof-domain datasets. Especially for the Howto100m results, the model pre-trained on wikiHow without fine-tuning outperforms even those pre-trained on other caption-based datasets that were fine-tuned on wikiHow. This is strong evidence that wikiHow can serve as a useful knowledge resource since the learned multimodal representation can be directly applied to other datasets.</p><p>To further validate whether the advantages of pre-training on wikiHow persist with the increasing number of fine-tuning examples, we report the performance with K ? {0, 5, 10, 15, 20, 25} for COIN and training examples ranging from 50 to 9,249 (full) for Howto100m. Shown in <ref type="figure" target="#fig_2">Figure 4</ref> &amp; 5, the model pre-trained on wikiHow consistently 5 The small number of goals in COIN leads to an extreme imbalance between video frames and texts, which makes it hard for training. Thus there is no train/test split on goals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose the novel Visual Goal-</p><p>Step Inference task (VGSI), a multimodal challenge for reasoning over procedural events. We construct a dataset from wikiHow and show that SOTA multimodal models struggle on it. Based on the transfer learning results on Howto100m and COIN, we validate that the knowledge harvested from our dataset could transfer to other domains. The multimodal representation learned from VGSI has strong potential to be useful for NLP applications such as multimodal dialog systems.  <ref type="figure">G)</ref>. Then, the model projects the image embedding onto the same dimension as the goal and we apply L2 normalization to obtain the unit vectors:X</p><formula xml:id="formula_1">I = L 2 N (X I W I?G ) X G = L 2 N (X G )<label>(2)</label></formula><p>where, L 2 N stands for L2 normalization and W I?G ? R d I ?d G is the weight. Then the DeViSE model uses a similarity function (here we choose cosine distance) to compute the distance betweenX I andX G as the loss:</p><formula xml:id="formula_2">L DeV iSE = cos(X I ,X G ) match(G, I) DeV iSE = 1 ? cos(X I ,X G )<label>(3)</label></formula><p>In which cos means the cosine distance. For De-ViSE, the matching score is the cosine similarity between the two unit vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Similarity Network</head><p>A Similarity Network is one type of two-branch networks for matching an image and text. It is a supervised model which takes in (G i , I i , y i ), and y i ? {0, 1} is the binary label that indicates whether G i and I i are related or not. Each branch of the network maps one modality to the cross-modality span and executes pointwise multiplication to construct a joint vector:</p><formula xml:id="formula_3">X I = L 2 N (X I W I?J ) X G = L 2 N (X G W G?J ) X J =X I X G<label>(4)</label></formula><p>in which, W I?J ? R d I ?d J and W G?J ? R d G ?d J are the weights and represents an element-wise product.</p><p>The similarity network can be viewed as a binary classifier, and therefore we could use binary crossentropy (BCE) as the loss function:</p><formula xml:id="formula_4">L sim = ? ? N i y i ? log p(y i ) + (1 ? y i ) ? log(1 ? p(y i ))<label>(5)</label></formula><p>The last layer of the similarity network is a fullyconnected layer with a softmax activation function, and the output is an array of size two, in which the elements denote the weight for each class. We compute the matching score by multiplying +1 (matched) and ?1 (unmatched) on these two elements:</p><formula xml:id="formula_5">? = softmax(fc(X J )) match(G,I) sim = 1 ? ?[0] + (?1) ? ?[1]<label>(6)</label></formula><p>where fc stands for fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Triplet Network</head><p>A Triplet Network requires the input to be in the format of a triplet (G, I pos , I neg ). There will be three branches in the network which map the three embeddings to the same joint span:</p><formula xml:id="formula_6">X G = L 2 N (X G W G?J ) X Ipos = L 2 N (X Ipos W I?J ) X Ineg = L 2 N (X Ineg W I?J )<label>(7)</label></formula><p>in which, W G?J ? R d G ?d J and W I?J ? R d I ?d J are weights, and the branches of positive and negative images share the same weight. The network learns the cross-modality by minimizing the distance between positive pairs and maximizing the distance between negative pairs. We choose cosine distance as the distance function which will also be used to compute the matching score:</p><formula xml:id="formula_7">L trip =max(0, cos(X G ,X Ipos ) ? cos(X G ,X Ineg ) + m) match(G, I) trip = cos(X G ,X I )<label>(8)</label></formula><p>Where m is the margin, which is set to 0.2 in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.4 LXMERT</head><p>LXMERT (Tan and Bansal, 2019) is a multimodal encoder that aims to ground text to images. It takes as an input image I and a related sentence G = {w 1 , w 2 , . . . , w n }. The image objects are embedded using a feature extractor <ref type="bibr" target="#b0">(Anderson et al., 2018)</ref> pre-trained on ImageNet <ref type="bibr">(Deng et al., 2009)</ref>. Given I the detector finds m objects {o 1 , o 2 , . . . , o m } where: o i = {p i , f i }, s.t. p i is its bounding box and f i is its 2048-dimensional region of interest (RoI).</p><p>LXMERT learns a position-aware embedding as follows:</p><formula xml:id="formula_8">f i = L 2 N (W F f i + b F ) (9) p i = L 2 N (W P p i + b P ) (10) v i = (f i + p i )/2<label>(11)</label></formula><p>The text tokens are extracted using a tokenizer <ref type="bibr">(Wu et al., 2016)</ref> and converted to index-aware embeddings s.t. w i and i are projected onto embedding spaces w i , u i , to get a common embedding.</p><formula xml:id="formula_9">h i = L 2 N (w i + u i )<label>(12)</label></formula><p>Those inputs are then passed through a language encoder E G , an object relationship encoder E I , and a cross-modality transformer en-</p><formula xml:id="formula_10">coder E J . Let X I = {v 1 , v 2 , . . . , v n } and X G = {h 1 , h 2 , . . . , h n }.X G = E G (X G ) X I = E I (X I ) X G J , X G I = E J (X G ,X I )<label>(13)</label></formula><p>Then the cross-modality output X J is extracted from the output embedding X G J that corresponds to the special token [CLS] appended to each input text.</p><p>Similarly to A.1.2, we use BCE loss.</p><formula xml:id="formula_11">L lxmert = ? ? N i y i ? log p(y i ) + (1 ? y i ) ? log(1 ? p(y i ))<label>(14)</label></formula><p>and compute the matching score:</p><formula xml:id="formula_12">? =softmax(fc 2 fc 1 (X J )) match(G, I) lxmert = 1 ? ?[0] + (?1) ? ?[1]<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Vision</head><p>We select InceptionV3 <ref type="bibr">(Szegedy et al., 2015)</ref> as the feature extractor for the image. We have tried VGG19 and Resnet50, but InceptionV3 turns out to have the best performance. We use the second last hidden layer of InceptionV3 to obtain a vector of (2048, ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Language</head><p>We use a pre-trained BERT sentence transformer (Reimers and Gurevych, 2019) with bert-base-uncased as our base model. Then, we use max-pooling to get the feature vector with a dimension of (768, ).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyper Parameters</head><p>See <ref type="table" target="#tab_7">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Training Details</head><p>The training of DeViSE, Similarity Network and Triplet Network were on a single NVIDIA RTX 2080 for 200 epochs with early stopping. The training took less than 10 hours. We used a pre-trained LXMERT model with 9 language layers, 5 cross-encoder layers, 5 vision encoder layers, and a 2 layer linear classification head, with GELU() <ref type="bibr">(Hendrycks and Gimpel, 2016)</ref> and ReLU() activation, with a Sigmoid final layer and with normalization in the first layer.</p><p>We fine-tune the model for 10 epochs while allowing the gradient to flow through the LXMERT pre-trained layers. We use a binary cross-entropy loss from the PyTorch library and an Adam (Kingma and Ba, 2014) optimizer. Note that we deal with imbalanced datasets by repeating the positive samples and shuffling the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Goal-Image Retrieval Task B.1 Sampling</head><p>Goal-Image Retrieval is a more practical format that gives a high-level goal and a pool of images and aims to rank these images based on their similarity with the goal query.</p><p>In this experiment, we randomly select 1,000 high-level goals from the testing set of multiplechoice tasks and choose 5 images for each goal, thus building a pool of 5,000 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Evaluation Metrics</head><p>We perform recall at k (recall@k, higher the better) and median rank (Med r, lower the better) to measure the retrial performance. For the 5k image pool, k ? {10, 25, 50, 100}, while for the 1k image pool, k ? {1, 5, 10, 25}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 In-Domain Performance</head><p>As shown in <ref type="table" target="#tab_9">Table 6</ref>, the triplet network with BERT as the text embedding has the best performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Query on Different Prompts</head><p>As can be seen from <ref type="table" target="#tab_10">Table 7</ref>, the model has higher performance when using the detailed step description as a prompt. Through qualitative analysis (see <ref type="figure" target="#fig_6">Figure 8</ref>) on some samples, we discovered that some method descriptions are very general, and short abstract keywords are even more refined than the goal description. To quantify this finding, we calculate the average length of tokens (remove stop words) and the vocabulary size of the three types of prompts. Apparently, the step description is more fruitful than the method and goal with higher token length and vocab size. The method described has a lower average length of tokens, which is in line with our observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Transfer Performance on Retrieval</head><p>We also evaluate the transfer performance on a retrieval task. For COIN, we choose 5-6 images for each video from the 180 goals and construct a pool of 1,000 images. For Howto100m, we randomly select 5-6 images of each of the videos in the testing set and also form a pool of 1K images. <ref type="table" target="#tab_12">Table 8</ref> and 9 indicates the model pre-trained on wikiHow outperforms the other dataset in the retrieval task and the aggregation model could further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Step-Aggregation Model</head><p>We have seen that SOTA models do not perform well in VGSI because of the implicit visionlanguage relation. So we develop a step aggregation model that takes advantage of the existing goal-step knowledge from wikiHow. The main idea is as follows: given an unseen textual goal, we use k-nearest neighbors to find the most related   article title from wikiHow, then extract the n steps from this article as S = {s 1 , s 2 , ..., s n }. Instead of directly using the given goal to match the images (goal score -Score g ), we could use the sequence of steps to improve the matching (step score -Score s ). Then use linear interpolation to summarize these two scores as our final matching score.</p><p>Score g = match(G, I) Score s = max i=1:n (match(s i , I))</p><p>Score f inal = ? ? Score g + (1 ? ?) ? Score s</p><p>where, ? adjusts the step and goal scores weights, we choose ? = 0.5. The main idea of the model is to break down the high-level goal into intermediate steps via schema. Then we use the induced sequence of steps as the new query to improve the matching performance. For example in <ref type="figure" target="#fig_4">Figure 6</ref>, when we want to match the goal "Install License Plate" with two images, the model makes a wrong choice because the negative sample (the right one) also involves the "install" action. However, we could fetch the intermediate steps from wikiHow and use these steps to match the images. The left image (the correct choice) has a higher Step-Image similarity score than the right one. Therefore, the model could improve its performance with the help of this step information. As we can see from the example steps, they contain some useful entities such as "screw", "bracket", "bumper", etc., which are closely related   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Model R@1 R@5 R@10 R@25 Med r COIN wikiHow 1.4 7.6 12.6 23.8 102 wikiHow agg 1.9(+35.7%) 7.8(+2.6%) 13.6(+7.9%) 25.9(+8.8%) 97(-4.9%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Howto100m</head><p>wikiHow 2.0 7.9 14.7 26.7 84 wikiHow agg 2.1(+5.0%) 8.3(+5.1%) 15.8(+7.5%) 27.7(+3.7%) 80(-4.8%) to the visual information in the image but do not show up in the goal sentence.</p><p>We apply the aggregation model on both multiple-choice and retrieval VGSI tasks. As shown in <ref type="table" target="#tab_15">Table 10</ref> and 11, with the assistance of the aggregation model, the accuracy of multiplechoice increased by 0.2% -2%, and the median rank of retrieval decreased by 5%. Since our approach to utilize these steps is very simple, but still achieve a marginal improvement. We hope to see more advanced models to realize the full potential of wikiHow steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Qualitative Examples</head><p>See <ref type="figure" target="#fig_5">Figure 7</ref>, 8, 9.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example Visual Goal-Step Inference Task: given a text goal (bake fish), select the image (C) that represents a step towards that goal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Hierarchical multimodality of wikiHow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Few-shot performance on COIN (similarity sampling) with different pre-training datasets vs. the number of examples per goal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Transfer performance on Howto100m (similarity sampling) with different pre-training datasets vs. the number of training examples.outperforms those pre-trained on the other datasets by significant margins with the increase of finetuning examples. The curve of wikiHow does not converge with the other curves even with the maximum number of training examples, which reflects that wikiHow could be a reliable pre-training data source for both low-and rich-resource scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The architecture of the Step-Aggregation Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative Examples Using Different Sampling Strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative Examples Using Different Query Prompts. (Yellow bounding box is the goal's prediction, blue bounding box denotes the method's prediction, red bounding box denotes the step's prediction, green checkmark represents the ground truth.)Figure 9: Qualitative Examples of Transfer Learning on Howto100m. (The first row shows the multiple-choice examples of Howto100m video frames, the yellow bounding box is the prediction of the model without pre-training on wikiHow, blue bounding box denotes the prediction of the pre-trained model, and green checkmark represents the ground truth. The second row shows the related images and descriptions we found in wikiHow.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Transfer performance (4-way multiple choice accuracy) on COIN. PT stands for pre-training, FT for fine-tuning. FT results are obtained by fine-tuning the model on 5 examples of the COIN training set (i.e., 5shot). Red numbers indicate the best zero-shot performance. Blue numbers are the best fine-tuned results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Sampling Strategy</cell></row><row><cell cols="5">PT-Data FT? Random Similarity Category</cell></row><row><cell>-</cell><cell></cell><cell>.6005</cell><cell>.6096</cell><cell>.4434</cell></row><row><cell>Flickr30K</cell><cell></cell><cell>.4837 .6207</cell><cell>.5398 .6408</cell><cell>.3856 .4740</cell></row><row><cell>MSCOCO</cell><cell></cell><cell>.5099 .6340</cell><cell>.5715 .6640</cell><cell>.3958 .4794</cell></row><row><cell>COIN</cell><cell></cell><cell>.5067 .6170</cell><cell>.5161 .6343</cell><cell>.3978 .4638</cell></row><row><cell>wikiHow</cell><cell></cell><cell>.6556 .6855</cell><cell>.6754 .7249</cell><cell>.4750 .5143</cell></row><row><cell>Human</cell><cell>-</cell><cell>.8300</cell><cell>.7858</cell><cell>.7550</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Transfer performance (4-way multiple choice accuracy) on Howto100m. FT results are obtained by fine-tuning the model on the full training set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Manling Li, Qi Zeng, Ying Lin, Kyunghyun Cho, Heng Ji, Jonathan May, Nathanael Chambers, and Clare Voss. 2020. Connecting the dots: Event graph schema induction with path language modeling. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 684-695. Yin Li, Jing Huang, and Svetlana Lazebnik. 2018. Learning two-branch neural networks for image-text matching tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2):394-407.</figDesc><table><row><cell>A Model Implementation Details A.1 Architecture and Loss Function A.1.1 DeViSE The Deep-Visual Semantic Embedding (DeViSE) Liwei Wang, Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V model takes in the pre-trained embedding vectors Le, Mohammad Norouzi, Wolfgang Macherey, from two modalities and maps the source vector Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine onto the span of the target vector representation. translation system: Bridging the gap between hu-First, the DeViSE model is only trained on the re-Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin man and machine translation. arXiv preprint lated (positive) pairs (G, I), and we map the image arXiv:1609.08144. to the goal (I ?</cell><cell>Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embed-ding by watching hundred million narrated video clips. In Proceedings of the IEEE international con-ference on computer vision, pages 2630-2640.</cell></row><row><cell>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hier-archical image database. In 2009 IEEE conference Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceed-ings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). on computer vision and pattern recognition, pages 248-255. Ieee. Chenwei Zhang, Yaliang Li, Nan Du, Wei Fan, and</cell><cell>Dena Mujtaba and Nihar Mahapatra. 2019. Recent trends in natural language understanding for proce-dural knowledge. In 2019 International Conference on Computational Science and Computational Intel-ligence (CSCI), pages 420-424. IEEE.</cell></row><row><cell>Philip Yu. 2019. Joint slot filling and intent detec-</cell><cell></cell></row><row><cell>tion via capsule neural networks. In Proceedings of</cell><cell>Hogun Park and Hamid Reza Motahari Nezhad. 2018.</cell></row><row><cell>the 57th Annual Meeting of the Association for Com-</cell><cell>Learning procedures from text: Codifying how-to</cell></row><row><cell>putational Linguistics, pages 5259-5267, Florence,</cell><cell>procedures in deep neural networks. In Companion</cell></row><row><cell>Italy. Association for Computational Linguistics.</cell><cell>Proceedings of the The Web Conference 2018, pages</cell></row><row><cell></cell><cell>351-358.</cell></row><row><cell>Li Zhang, Qing Lyu, and Chris Callison-Burch. 2020a.</cell><cell></cell></row><row><cell>Intent detection with WikiHow. In Proceedings of</cell><cell></cell></row><row><cell>the 1st Conference of the Asia-Pacific Chapter of the</cell><cell></cell></row><row><cell>Association for Computational Linguistics and the</cell><cell></cell></row><row><cell>10th International Joint Conference on Natural Lan-</cell><cell></cell></row><row><cell>guage Processing, pages 328-333, Suzhou, China.</cell><cell></cell></row><row><cell>Association for Computational Linguistics.</cell><cell></cell></row><row><cell>arXiv preprint</cell><cell></cell></row><row><cell>arXiv:1606.08415. Elad Hoffer and Nir Ailon. 2015. Deep metric learning using triplet network. In International Workshop on Similarity-Based Pattern Recognition, pages 84-92. Springer. Li Zhang, Qing Lyu, and Chris Callison-Burch. 2020b. Reasoning about goals, steps, and temporal ordering with WikiHow. In Proceedings of the 2020 Con-ference on Empirical Methods in Natural Language Processing (EMNLP), pages 4630-4639, Online. As-sociation for Computational Linguistics.</cell><cell>Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</cell></row><row><cell>Jeff Johnson, Matthijs Douze, and Herv? J?gou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data. Yilun Zhou, Julie Shah, and Steven Schockaert. 2019. Learning household task knowledge from WikiHow descriptions. In Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5), pages 50-56,</cell><cell>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2015. Re-thinking the inception architecture for computer vi-sion. CoRR, abs/1512.00567.</cell></row><row><cell>Justin Johnson, Andrej Karpathy, and Li Fei-Fei. 2016. Densecap: Fully convolutional localization recognition, pages 4565-4574. the IEEE conference on computer vision and pattern networks for dense captioning. In Proceedings of Macau, China. Association for Computational Lin-guistics.</cell><cell>ing. on Empirical Methods in Natural Language Process-formers. In Proceedings of the 2019 Conference Hao Tan and Mohit Bansal. 2019. Lxmert: Learning cross-modality encoder representations from trans-</cell></row><row><cell>Andrej Karpathy, Armand Joulin, and Li Fei-Fei. 2014. Deep fragment embeddings for bidirectional image sentence mapping. arXiv preprint arXiv:1406.5679.</cell><cell>Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. 2019. Coin: A large-scale dataset for comprehen-</cell></row><row><cell>Diederik P Kingma and Jimmy Ba. 2014. Adam: A</cell><cell>sive instructional video analysis. In Proceedings of</cell></row><row><cell>method for stochastic optimization. arXiv preprint</cell><cell>the IEEE/CVF Conference on Computer Vision and</cell></row><row><cell>arXiv:1412.6980.</cell><cell>Pattern Recognition (CVPR).</cell></row><row><cell></cell><cell>Oriol Vinyals, Alexander Toshev, Samy Bengio, and</cell></row><row><cell></cell><cell>Dumitru Erhan. 2016. Show and tell: Lessons</cell></row><row><cell></cell><cell>learned from the 2015 mscoco image captioning</cell></row><row><cell></cell><cell>challenge. IEEE transactions on pattern analysis</cell></row><row><cell></cell><cell>and machine intelligence, 39(4):652-663.</cell></row></table><note>Andrea Frome, Greg S Corrado, Jon Shlens, Samy Ben- gio, Jeff Dean, Marc'Aurelio Ranzato, and Tomas Mikolov. 2013. Devise: A deep visual-semantic em- bedding model. In Advances in neural information processing systems, pages 2121-2129. Jonathan Gordon and Benjamin Van Durme. 2013. Re- porting bias and knowledge acquisition. In Proceed- ings of the 2013 workshop on Automated knowledge base construction, pages 25-30. Dan Hendrycks and Kevin Gimpel. 2016. Gaus- sian error linear units (gelus).Manling Li, Sha Li, Zhenhailong Wang, Lifu Huang, Kyunghyun Cho, Heng Ji, Jiawei Han, and Clare Voss. 2021. Future is not one-dimensional: Graph modeling based complex event schema induction for event prediction. arXiv preprint arXiv:2104.06344.Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image- to-sentence models. In Proceedings of the IEEE In- ternational Conference on Computer Vision (ICCV).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyper Parameters of All Models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>In-Domain Retrieval results with Different Models.</figDesc><table><row><cell>Prompt</cell><cell>Token Length</cell><cell>Vocab Size</cell><cell cols="5">1K Testing Images R@1 R@5 R@10 R@25 Med r</cell></row><row><cell>Goal</cell><cell>3.34</cell><cell>19,299</cell><cell>4.6</cell><cell>14.6</cell><cell>22.8</cell><cell>36.3</cell><cell>49</cell></row><row><cell>Method</cell><cell>3.11</cell><cell>24,180</cell><cell>2.5</cell><cell>11.4</cell><cell>18.9</cell><cell>33.3</cell><cell>57</cell></row><row><cell>Step</cell><cell>4.67</cell><cell>49,999</cell><cell>6.1</cell><cell>20.1</cell><cell>31.4</cell><cell>48.7</cell><cell>26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Query on Different Prompts</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Zero-shot Retrieval on COIN</figDesc><table><row><cell>1K Test Images</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Retrieval Performance on Howto100m</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>.7657(+0.2%) .6942(+1.3%) .5764(+1.9%) .6947(+1.3%) .7392(+2.0%) .5245(+2.0%)</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Random</cell><cell>Sampling Strategy Similarity</cell><cell>Category</cell></row><row><cell cols="2">wikiHow wikiHow wikiHow Howto100m COIN wikiHow</cell><cell>.7639 .6855</cell><cell>.6854 .7249</cell><cell>.5659 .5143</cell></row></table><note>aggagg</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Apply Step-Aggregation model on multiple-choice VGSI (agg stands for aggregation model).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Apply Step-Aggregation model on retrieval VGSI.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Like the Alexa Prize Taskbot Challenge. arXiv:2104.05845v2 [cs.CV] 10 Sep 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In some articles, they use parts instead of methods. 3 Both datasets and code are available here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Some goals have no valid frames remaining after the annotation, and are therefore removed altogether.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is based upon work supported in part by the DARPA KAIROS Program (contract FA8750-19-2-1004), the DARPA LwLL Program (contract FA8750-19-2-0201), and the IARPA BET-TER Program (contract 2019-19051600004). Approved for Public Release, Distribution Unlimited. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, IARPA, or the U.S. Government.</p><p>We thank Chenyu Liu for annotations. We also thank Simmi Mourya, Keren Fuentes, Carl Vondrick, Zsolt Kira, Mohit Bansal, Lara Martin, and anonymous reviewers for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?odore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10190</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
