<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Permutation-Invariant Relational Network for Multi-person 3D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ugrinovic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IRI CSIC-UPC 08028</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Ruiz</surname></persName>
							<email>adriaruiz@seedtag.com</email>
							<affiliation key="aff1">
								<address>
									<postCode>08014</postCode>
									<settlement>Seedtag, Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Agudo</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">IRI CSIC-UPC 08028</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sanfeliu</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">IRI CSIC-UPC 08028</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">IRI CSIC-UPC 08028</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Permutation-Invariant Relational Network for Multi-person 3D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recovery of multi-person 3D poses from a single RGB image is a severely ill-conditioned problem due to the inherent 2D-3D depth ambiguity, inter-person occlusions, and body truncations. To tackle these issues, recent works have shown promising results by simultaneously reasoning for different people. However, in most cases this is done by only considering pairwise person interactions, hindering thus a holistic scene representation able to capture long range interactions. This is addressed by approaches that jointly process all people in the scene, although they require defining one of the individuals as a reference and a pre-defined person ordering, being sensitive to this choice. In this paper, we overcome both these limitations, and we propose an approach for multi-person 3D pose estimation that captures long range interactions independently of the input order. For this purpose we build a residual-like permutation-invariant network that successfully refines potentially corrupted initial 3D poses estimated by an off-the-shelf detector. The residual function is learned via Set Transformer [13] blocks, that model the interactions among all initial poses, no matter their ordering nor number. A thorough evaluation demonstrates that our approach is able to boost the performance of the initially estimated 3D poses by large margins, achieving state-of-the-art results on standardized benchmarks. Additionally, the proposed module works in a computationally efficient manner and can be potentially used as a drop-in complement for any 3D pose detector in multi-people scenes.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating 3D human pose from RGB images is a long-standing problem in computer vision, with broad applications in, e.g., action recognition, 3D content production, the game industry and humanrobot interaction. While recent approaches have shown impressive results, most of them focus on a single person <ref type="bibr">[14; 22; 44; 26; 27; 31; 41; 19; 43]</ref>. The problem of multi-person 3D pose estimation introduces additional challenges to the single person setup, mostly due to inter-person occlusions. In order to tackle it, sequential <ref type="bibr">[38; 2]</ref> and multi-camera systems <ref type="bibr">[4; 16; 33; 37]</ref> have been exploited. In this paper, however, we aim to address the most constricting version of the problem by considering multi-people 3D pose estimation from one single view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>Refined (and Initial) Bird-view (Refined and Initial) <ref type="figure">Figure 1</ref>: Given a set of potentially noisy input 3D poses, we leverage on the Set Transformer architecture <ref type="bibr" target="#b12">[13]</ref> to compute a holistic encoding of all poses. This encoding which can take an arbitrarily large number of poses in any order, helps to predict a residual for each pose and refine the initial estimates. The approach is robust to large errors on the initial poses. Note how our refinement corrects the scale and translation of person P2. Our model also shows improvements in the root-relative pose (see main text).</p><p>Only a reduced number of works have proposed strategies to better exploit multi-person relations. For instance, <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b8">[9]</ref> use depth-aware ordinal losses to better estimate the pose of each person in relation to others. Nevertheless, these methods still reason about multiple people in a local neighborhood and in a strictly pairwise manner. They do not consider the fact that human interactions can occur at large ranges and among several people in the scene. For instance, in sports, a team formation is correlated as a whole and not at local regions of the pitch. Leveraging on a self-attention layer, Pi-Net <ref type="bibr" target="#b6">[7]</ref> introduced a mechanism to simultaneously reason for all people in the image. However, this approach is sensitive to permutations in the input order as it also uses an RNN which are known to have this short-comming <ref type="bibr" target="#b34">[35]</ref>. Fieraru et al. <ref type="bibr" target="#b5">[6]</ref> capture people interactions with specialized losses for a variable number of people. However, they use a parametric human body model and rely on direct supervision for modeling closed contact human interactions. Moreover, at the time of this writing, there does not exist any publicly available dataset with human-human interactions annotations; these being difficult to produce.</p><p>In order to overcome the limitations of previous approaches, we propose a novel scheme to model people interactions in a holistic and permutation-invariant fashion. In our approach, we assume access to an initial set of potentially noisy 3D poses estimated by an off-the-shelf algorithm <ref type="bibr" target="#b24">[25]</ref>. These poses are treated as elements of a set and fed into a Set Transformer-based architecture <ref type="bibr" target="#b12">[13]</ref>. This allows us to simultaneously process all of input poses, and compute a global encoding not sensitive to the input order. This encoding contains interaction information and is then used to estimate a correction residual for the initial 3D poses. Note, that we do not train with direct interaction supervision. Instead, our model learns interactions between person's poses implicitly. <ref type="figure">Figure 1</ref> shows how our model refines the initial pose estimations and yields to more correct ones, even under strong occlusions. This way, capturing the interactions between input poses, the model can correct the relative pose, translation, and scale of the people in the scene.</p><p>A thorough evaluation on MuPoTS-3D <ref type="bibr" target="#b22">[23]</ref>, Panoptic <ref type="bibr" target="#b9">[10]</ref> and NBA2K <ref type="bibr" target="#b44">[45]</ref> multi-people datasets shows that the proposed approach consistently improves over Pi-Net <ref type="bibr" target="#b6">[7]</ref> and other state-of-the-art (SOTA) methods <ref type="bibr">[36; 42]</ref>. This happens even when initial 3D poses estimated by the off-the-shelf <ref type="bibr" target="#b24">[25]</ref> network are noisy, corrupted by occlusions or contain truncated body joints.</p><p>Interestingly, the proposed approach runs efficiently and can be potentially used as a post-processing module on top of any pose estimation method with negligible computation overhead. In summary, our key contributions are the following: (1) We introduce a novel approach to capture the relationship among the 3D poses of multiple people. (2) Our model does not depend on the input order (i.e., it is permutation-invariant) and can handle an arbitrarily large number of people. (3) The proposed module is computationally efficient and could be used along with any 3D pose detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this paper, we focus on single-view multi-person 3D human pose estimation. In recent years there has been a growing attention on this topic. Typically, there exist two types of approaches: top-down <ref type="bibr">[28; 29; 25; 15; 3; 38; 9]</ref> and bottom-up <ref type="bibr">[22; 23; 21; 39; 24; 42; 32; 40]</ref>. Although, very recently, a new trend to integrate both approaches has emerged <ref type="bibr">[36; 2; 11]</ref>. In this work, we use a top-down approach <ref type="bibr" target="#b24">[25]</ref> exploiting contextual information, specifically, human-human interactions.  <ref type="figure">Figure 2</ref>: Overview of our approach. Given an input image (a), we first estimate the 3D keypoints as the initialization (b). Then, we input these initial estimations in the form of a set (hence the keys) to our interactionbased permutation-invariant model. We obtain the interaction-based embedding (f ) and concatenate it with another embedding for each person. This person embedding is calculated as a projection from the input space to the same dimension as f via a feed-forward layer, denoted FF. We use an MLP network to get the corrections of the initial estimations (b) and compute the final estimations (d) by adding these corrections to the initial poses. We show the poses of the people with bounding boxes in the image just for clarity. Our model inputs all the poses in the scene.</p><p>The idea of using human-human interaction information to improve 3D human pose estimation of multiple people was first proposed by <ref type="bibr" target="#b0">[1]</ref>. However, it was not until recently that the community shifted its attention to exploit this information. Recently, Jiang et al. <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b35">[36]</ref> exploit depthorder relationships between people. <ref type="bibr" target="#b1">[2]</ref> propose a pose discriminator to capture two-person natural interactions. <ref type="bibr" target="#b4">[5]</ref> proposed to exploit close contact information between two persons. Finally, closest to our approach, Guo et al. <ref type="bibr" target="#b6">[7]</ref> and Fieraru et al. <ref type="bibr" target="#b5">[6]</ref> propose pose interacting networks to exploit human-to-human interaction information.</p><p>In this paper, we aim to capture the interaction of people present in a scene in a permutationinvariant manner, meaning that the order we input each person to our model should not matter. Ideally, we should be able to treat all the poses as elements of a set. <ref type="bibr" target="#b29">[30]</ref> proposes a relational network that sum-pools all pairwise interactions of elements in a given set. However, we want to model higher-order interactions as the pose of a person may affect (or be affected by) not one but multiple other persons directly or indirectly. At the same time, a group of people could affect a person's pose. <ref type="bibr" target="#b17">[18]</ref> uses a Transformer <ref type="bibr" target="#b33">[34]</ref> to model high-order interactions between objects. However, they use mean-pooling to obtain aggregated features where interaction information may be loss. More suited to our problem, we choose <ref type="bibr" target="#b12">[13]</ref> as our base architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The key idea of our approach is to implicitly capture the interaction information between all human body poses in a scene and use it to refine an initial set of 3D pose estimations. We represent this information with an interaction embedding. People in a scene are usually involved in a specific interaction and this constrains the range of possible poses. Thus, we argue that learning pose correlations can improve initial noisy estimations. Inspired by the effectiveness of Transformers <ref type="bibr" target="#b33">[34]</ref> to capture correlations, we use the Set Transformer <ref type="bibr" target="#b12">[13]</ref>, a model unaffected by the order of its inputs. In short, we obtain an information-rich and permutation-invariant interaction embedding that captures human-to-human interactions and use it to refine each person's pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relational Network for Multi-person 3D Pose Estimation</head><p>Let I ? R H?W ?3 be an input RGB image with N people interacting in the same scene and p 1:N to be the set of 3D joints corresponding to each person where p n ? R J?3 with J number of estimated joints and n a number in the range {1, ..., N }. These joints are obtained from an initial estimation using an off-the-shelf 3D pose estimator, such as <ref type="bibr" target="#b24">[25]</ref>. All N poses are assumed to be represented in absolute camera coordinates.</p><p>Given the previous definitions, we aim to improve each initial pose estimation p n taking into account the pose of all the people present in the scene. These initial estimations could be inaccurate as they are sensitive to inter-person occlusions, self-occlusions, scale/depth ambiguity, and truncation. Although the latter does not originate from a lack of interaction information, we shall see that our model also deals with these cases. This is because, aside from modeling interaction information, our model also captures the error distribution of the initial estimation method.</p><p>The mapping between these initial estimations and the refined poses takes the form q 1:N = ?(p 1:N ), where q 1:N refers to the set of poses {q 1 , ..., q n } refined by exploiting the interactions, and q n ? R J?3 . The function ? materializes as a neural-network capable of extracting interaction information between the input poses p 1:N and refining them.</p><p>For this purpose, we split the problem into two parts: First, we obtain an embedding (interaction embedding) able to capture the interactions of the scene; second, we use this to refine the initial poses. The interaction embedding f is a d-dimensional vector where d is a hyperparameter of our model. The embedding f , obtained via Set Transformer blocks, aims to globally capture interactions between people from the initial estimations p 1:N . Specifically,</p><formula xml:id="formula_0">f = G(p 1:N ),<label>(1)</label></formula><p>where G is a neural network composed of the Set Transformer elements described in the next section. Ideally, we are looking for a function G capable of capturing the embedding regardless the input order and the number of people in the scene. If we explicitly take into account the interaction embedding, we can express the relation between the initial and refined estimations as:</p><formula xml:id="formula_1">q 1:N = ?(f , p 1:N ),<label>(2)</label></formula><p>where ? is our full model described in more detail in Sec. 3.3 and depicted in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computing interaction embeddings with Set Transformers</head><p>As stated before, our interaction embedding f should comply with two key requirements to model the interaction information: (1) being independent of the order of the input person's body joints and (2) being able to process input scenes containing any number of people. Requirement (1) comes from the fact that we do not want the input order of our model to affect the pose refinement. Only the information regarding interaction between people's body poses should affect the refinement. Both requirements are not easily satisfied by classical feed-forward neural networks, and recursive neural networks (RNNs) are sensitive to the input order <ref type="bibr" target="#b34">[35]</ref>. Thus, to get the desired interaction embedding, we base our model in an attention-based permutation-invariant neural network. For this purpose, we take components from the Set Transformer <ref type="bibr" target="#b12">[13]</ref>. Choosing an attention-based architecture for our model and working with sets as inputs allows us to: compute a variable number of input poses, disregard the input order, and naturally encode interaction between these elements. In this manner, we are able to attend to any person's initial pose and obtain a rich permutation-invariant embedding that captures the interactions in the scene. We then use this feature to guide the pose refinement process.</p><p>In our context, we treat the initial pose estimations p 1:N as a set ((b) in <ref type="figure">Fig. 2)</ref>. Thus, our model attends to each person's joints and first generates an embedding for each person using a Set Attention Block (SAB). Later, these individual embeddings are aggregated in a learned fashion using a Pooling by Multihead Attention (PMA) operation, providing us the interaction embedding f . For a formal definition of the SAB and PMA modules, we refer the reader to <ref type="bibr" target="#b12">[13]</ref>.</p><p>The SAB module used here emerges as an adaptation of the encoder block of the Transformer <ref type="bibr" target="#b33">[34]</ref>.</p><p>To build the SAB, dropout and the positional encoding are discarded. This module uses self-attention to concurrently encode the input set. This allows to capture pairwise and higher-order relationships among instances during the encoding process. The output of the SAB contains information about pairwise interactions among the elements of the input set X and can be stacked K times by more of the same modules to capture higher than pairwise interactions. In our context, X is composed by the set of initial pose estimations p 1:N , thus, X = {p 1 , ..., p n }.</p><p>To obtain a permutation-invariant feature, we use the Pooling by Multihead Attention (PMA) operation which aggregates the features obtained by the SAB. This constitutes a key step to make f permutationinvariant. The PMA operation aggregates the features by applying multi-head attention on a learnable set of k seed vectors S ? R k?d . In our case, k = 1, as we only have one embedding to represent the whole scene. At the output of the PMA block, we find the desired interaction embedding. Under the definitions provided before, we define our embedding in the following manner:</p><formula xml:id="formula_2">f = PMA 1 (SAB(X)) .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pose refinement via interaction information</head><p>Once we have the feature f , we use them to refine the initial 3D human pose estimations. For this, we employ an MLP that takes as input f concatenated with a projection of each person's joints into a d-dimensional vector. This projection is done via a feed-forward layer, denoted FF. Finally, the MLP outputs a vector (?) containing all the correction values needed to improve each of the initial joints locations in 3D space. We define correction vector as:</p><formula xml:id="formula_3">? 1:N = ?(f , p 1:N ),<label>(4)</label></formula><p>where our network ? is based on the SAB/PMA modules and the MLP in charge of decoding the interaction embedding. The set-processing modules (SAB and PMA) generate the interaction embedding, as considered in Eq. ( <ref type="formula" target="#formula_0">(1)</ref>). Adding this correction vector to our initial estimations, we can now compute the refined joints by the following relation:</p><formula xml:id="formula_4">q 1:N = p 1:N + ? 1:N .<label>(5)</label></formula><p>To guide the learning process, we optimize the whole network parameters by minimizing an L 2 loss over the final refined 3D joints and the ground truth joints:</p><formula xml:id="formula_5">L(p 1:N ) = 1 N N i=1 ||q i ? p i GT || 2 ,<label>(6)</label></formula><p>where p GT denotes the 3D human pose ground truth. For more details regarding our final architecture, please refer to the supplementary material and the code of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present implementation details, the evaluation of our approach in comparison with other relevant SOTA, and an ablation study focusing on various types of interactions. Finally, we present an analysis of the refinement process and the computational complexity of our model. Our model has the advantage of being highly computationally efficient, lightweight and fast to train (see Sec 4.6). We experiment on three datasets: MuPoTS-3D <ref type="bibr" target="#b22">[23]</ref>, Panoptic <ref type="bibr" target="#b9">[10]</ref> and NBA2K <ref type="bibr" target="#b44">[45]</ref>. Additionally, we include qualitative results on COCO <ref type="bibr" target="#b16">[17]</ref>. We also use standard metrics for evaluation such as MPJPE [8] -which measures the accuracy of the 3D root-relative pose-and 3DPCK <ref type="bibr" target="#b19">[20]</ref> with a threshold of 15cm, as it is standard in the literature <ref type="bibr">[7; 42; 15]</ref>. Complementary to 3DPCK (from now on PCK), we use AUC (area under the curve) as a more complete metric. Additionally, PCK abs is used to evaluate absolute camera-centered 3D human poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>We optimize our model parameters using ADAM <ref type="bibr" target="#b11">[12]</ref> with learning rate of 0.0001 in a single GTX 1080 Ti. To estimate the initial 3D human poses we use <ref type="bibr" target="#b24">[25]</ref>. Although, any other initialization approach could be used. Regarding training data, although generally used for this task, we discard the use of MuCo-3DHP. Given its synthetic nature it does not contain real interactions between people. Instead, we use MuPoTS-3D as both train and test set which does contain real interactions. For fair comparison, we resource to k-fold cross-validation dividing the dataset into 10 folds. For evaluating on the Panoptic studio <ref type="bibr" target="#b9">[10]</ref> dataset, we follow the evaluation protocol presented in <ref type="bibr">[38; 39]</ref>. Finally, please notice that even though the NBA2K dataset is synthetic, it captures plausible interaction between players as opposed to MuCo-3DHP. For more details regarding data and implementation, please refer to the supplementary material.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with state-of-the-art methods</head><p>We present a direct comparison with the method closest to ours, PI-Net <ref type="bibr" target="#b6">[7]</ref>, and show other SOTA methods that also deal with multi-person 3D pose estimation <ref type="bibr">[36; 15; 42; 39]</ref>. The quantitative results for MuPoTS-3D dataset are reported in <ref type="table" target="#tab_0">Table 1</ref>. Here, we show results of the initialization method RootNet <ref type="bibr" target="#b24">[25]</ref> used by both PI-Net <ref type="bibr" target="#b6">[7]</ref> and our model. We present, two rows referencing PI-Net <ref type="bibr" target="#b6">[7]</ref>. The first one, shows the results when training the model with MuCo-3DHP <ref type="bibr" target="#b22">[23]</ref> dataset, as reported in their work. For a fair comparison, we fine-tune the model with the MuPoTS-3D <ref type="bibr" target="#b22">[23]</ref> dataset and perform the same cross validation. Note that we train and test with MuPoTS-3D due to the reasons explained in Sec. 4.1. As it can be seen, our model shows a 3.0% improvement over this method when estimating the root-relative pose, 2.2% for AUC for all people and 2.1% for only matched people. Also, worthy of notice, we remarkably outperform RootNet <ref type="bibr" target="#b24">[25]</ref> in all metrics. Comparing to the rest of the methods, our model outperforms the best PCK metric from HDNet by 3.3%, the best AUC from HMOR by 2.6%, and the best PCK abs from HMOR <ref type="bibr" target="#b35">[36]</ref> by 0.3% . For qualitative comparisons on this dataset, please refer to <ref type="figure">Fig. 5</ref>.</p><p>The results for the CMU Panoptic dataset are shown in <ref type="table" target="#tab_1">Table 2</ref>. We evaluate our method under MPJPE after root alignment following previous works <ref type="bibr">[38; 39]</ref>. The dataset presents a challenging scenario as the majority of images contain several people at a time in a closed environment, severely affected by occlusion and truncation. Our method successfully reduces the interference of occlusions and truncation and improves by a large amount the initial estimations (fine-tuned RootNet <ref type="bibr" target="#b24">[25]</ref>). To show how our model is able to deal with truncation, results of the metric calculated over all joints in the dataset are presented in <ref type="table" target="#tab_1">Table 2</ref>, including those that are out of the image and are not visible. Most of these non-visible joint constitute cases of either truncation or occlusion. Our method improves over 30 mm. in average over the initial method. Note that we also have a significant improvement (14.1 mm) over the initial method if we account only for visible joints which is the standard practice. With regards to the SOTA in this dataset (HMOR <ref type="bibr" target="#b35">[36]</ref>), we outperform the method by an overall of 5.3 mm and have a consistent improvement over all the actions. For qualitative comparisons on the Panoptic <ref type="bibr" target="#b9">[10]</ref> dataset, please refer to <ref type="figure">Fig. 5</ref> and the supplementary material.</p><p>For evaluating on the NBA2K dataset, we use the MPJPE without and with Procrustes Alignment (MPJPE-PA) as shown in <ref type="table" target="#tab_2">Table 3</ref>. See how both methods that use interaction information from the scene (Pi-Net and ours) are able to improve the results over the initial estimations (RootNet <ref type="bibr" target="#b24">[25]</ref>). Furthermore, our method outperforms the baseline (PI-Net <ref type="bibr" target="#b6">[7]</ref>) which we fine-tune with this dataset.  <ref type="table">Table 4</ref>: Importance of different levels of interaction in our model. For each dataset we have 4 different models: the initialization method (RootNet <ref type="bibr" target="#b24">[25]</ref>), a no interaction baseline, a baseline with interaction of all ungrouped joints in the scene (scene interaction), and interaction between joints grouped by persons (people interaction). The model that captures interactions between people makes better pose refinements. See text for more details.  Moreover, our method shows to be superior than the baseline at capturing interactions and refining the 3D pose of multiple people (see <ref type="figure">Fig. 5</ref>). This are particularly interesting results as the dataset contains several people in each scene with high levels of interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Interaction vs. no interaction</head><p>Having shown the effectiveness of our method at refining 3D poses, we continue with a careful analysis of our interaction component. <ref type="table">Table 4</ref> shows how the level at which we enforce the interaction to be learned affects the performance in comparison to the initial estimations. We define three different levels of interaction: (1) no interaction, (2) scene interaction, and (3) people interaction. The latter corresponds to our final method. For all the cases, we use the same architecture. However, we change the interaction levels by changing what we input to our method. To eliminate learning interactions (no interaction), we input each person's pose individually as a unique and different set and not together. In this manner, it is impossible for the model to build an interaction-based embedding. At most, the model remains restricted to capture self-joint interactions.</p><p>To enforce learning what we refer to as scene interaction, we make each joint in the scene a set by itself. Having each joint as set element instead of the whole person's pose, we enforce a representation that can learn interactions between joints but without knowing which joint corresponds to which person. Thus, loosing the sense of person as an "entity". The results from <ref type="table">Table 4</ref> show that our method based on people's interaction clearly outperforms other degrees of interaction consistently over both datasets. We report the results on the MuPoTS-3D and Panoptic datasets and use the MPJPE metric only for simplicity. <ref type="figure" target="#fig_1">Figure 3</ref> gives us insight on what happens with the refinement on each level of interaction. Here, we present two images containing three persons each: high interaction between individuals (top image), low interaction (bottom image). Additionally, for each image we show three matrices, each regarding an interaction type presented above. Each matrix depicts the effect of the pose refinement when perturbing one joint over all of the joints of every person in the scene. This also includes the person whose joint has been perturbed. Each column represents the joint being perturbed and each row represents the affected joints. The perturbation applied to the joints is a displacement in the positive directions of x, y and z in the 3D space by 10 cm. The magnitude in each element in the matrix represents the maximum absolute value of the change in any of the 3D space coordinate direction in meters. With this setup, we can study how one person's joint affects other person's joints as well as its own. Here, we notice some key observations. (1) for the cases in which we enforce to learn interactions (people and scene), the effect of one person's joint over other person's joints (effect of interaction) is higher for the image on the top (high interaction) than for the one in the bottom (lower interaction). <ref type="bibr" target="#b1">(2)</ref> in the case of people interaction, it can be clearly seen that one person's joint affects in greater degree the pose of its own body in contrast to other people's body. This is expected, as the model here has the notion of which joints correspond to which person. This does not happen in the scene interaction case. Also, (3) we can confirm that in the case of no-interaction, one person's pose has no effect over others. The reader is referred to the supplemental material for additional examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effects of the refinement over initial estimations</head><p>We show the effect of our model in refining the initial estimations. Our method can improve both absolute and root-relative poses while more effectively dealing with inter-person occlusions and truncations. This is achieved because our interaction embedding enables the model to reason directly in the 3D space, whereas other methods can only reason from 2D image cues. Furthermore, our loss encourages the model to learn a refinement for both the absolute and the root-relative poses. See <ref type="figure" target="#fig_2">Fig. 4</ref>. The three middle columns depict the initial estimation, the refined poses and the ground truth from a slightly rotated camera view along with a bird-view, respectively. From these views, we can appreciate the interaction between person 1 (P 1 ) and person 3 (P 3 ). The initial estimation does not take into account this interaction and, therefore, makes the mistake of overlapping the two bodies.</p><p>Our model yields to more realistic estimations by exploiting these interactions. The rightmost picture shows the initial, refined and ground truth root-relative poses for P <ref type="bibr" target="#b0">1</ref>  example, the hands and ankle joints are closer to the ground truth than the initial estimations. The same happens with the hip joints. Additionally, <ref type="figure">Fig. 1</ref> shows the input image, the ground truth and the refined poses overlapped with the initial estimations (in transparency) both in a tilted camera view and a bird-view. Here, we can also appreciate an interaction between the people that are about to hug (P 2 and P 3 ). Better seen in the bird-view, our refinement locates both persons in a more coherent way, whereas, the initial estimation places them further apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head><p>Qualitative results on the MuPoTS-3D, Panoptic, NBA2K ana COCO datasets are shown in <ref type="figure">Fig. 5</ref>. Row 1 fist column shows a case where people are closely interacting with each other (holding hands). Our model corrects the persons poses so their hands are closer together. Row 1 second column shows how our method corrects cases of severe truncation. Row 2 shows results on the NBA2K <ref type="bibr" target="#b44">[45]</ref> dataset and the last row shows as well results on images-in-the-wild from COCO <ref type="bibr" target="#b16">[17]</ref> dataset. NBA2K <ref type="bibr" target="#b44">[45]</ref> presents several interactions in each scene. We show how our method improves over the SOTA <ref type="bibr" target="#b24">[25]</ref>, especially in cases were people need to be grouped closely together. Our method captures the interactions between the players and can determine which players should be grouped together. In each case, with dotted red circles, we show either a correctly located group of players (refined poses) or incorrectly placed players (initial poses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Computational complexity</head><p>Our model is fast and has very few parameters. Also, we need less data to train to make the refinements in direct comparison to <ref type="bibr" target="#b6">[7]</ref> and indirectly compared to <ref type="bibr">[25; 36; 42]</ref>. We use 90K model parameters, need 5.1M FLOPS to make an inference which takes 2ms and we required roughly 15K training samples. In comparison to <ref type="bibr" target="#b24">[25]</ref>, which takes 250.7ms to make an inference, using our model as a refinement step constitutes a negligible overhead. For a more comprehensible comparison table, please refer to our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we have proposed a novel algorithm to tackle the problem of multi-person 3D pose estimation from one single image. Building on the Set Transformer paradigm, we have introduced a holistic encoding of the entire scene, given an initial set of potentially noisy input 3D body poses. This encoding captures multi-person relationships, does not depend on the input order and can represent an arbitrarily large number of inputs. We use it to refine the initial poses in a residual manner. A thorough evaluation shows that our approach provides state-of-the-art results on several benchmarks. Additionally, the proposed module is computationally efficient and can be used as a post-processing step for any 3D pose detector in multi-people scenes to improve its accuracy and make it more robust to truncation and occlusions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Interaction analysis. We show the effect of one joint over all other joints in the scene. Joints are grouped by person. We present 17 joints for each person. Each person's number in the matrices corresponds to the number shown in the bounding box in the images. The magnitude of each matrix element represents the maximum displacement in 3D space measured in meters of the joints in each row caused by the corresponding joint in each column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Effects of the pose refinement. From left to right: Input image, initial 3D pose estimations, refined poses, ground truth, and detail of the update on person P1's joints. For each estimation we include a bird-view so that absolute translation is better appreciated. The last column shows the root-relative pose improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison on the MuPoTS-3D<ref type="bibr" target="#b22">[23]</ref> dataset. *Results shown for these methods are merely referential as they are not re-trained with the same data as ours. Note that both PI-Net and our method use<ref type="bibr" target="#b24">[25]</ref> for initialization.</figDesc><table><row><cell>Metric</cell><cell></cell><cell>All people</cell><cell></cell><cell></cell><cell>Matched people</cell><cell></cell><cell>Datasets training</cell></row><row><cell></cell><cell cols="6">3DPCK 3DPCK abs AUC rel 3DPCK 3DPCK abs AUC rel</cell><cell></cell></row><row><cell>Initial (RootNet [25])</cell><cell>81.2</cell><cell>31.4</cell><cell>39.5</cell><cell>82.5</cell><cell>32.0</cell><cell>40.2</cell><cell>MuCo + COCO</cell></row><row><cell>PI-Net [7] (MuCo)</cell><cell>82.5</cell><cell>-</cell><cell>-</cell><cell>83.9</cell><cell>-</cell><cell>-</cell><cell>MuCo</cell></row><row><cell>PI-Net [7] (fine-tunned)</cell><cell>82.8</cell><cell>-</cell><cell>43.9</cell><cell>84.3</cell><cell>-</cell><cell>44.7</cell><cell>MuPots Cross Validation</cell></row><row><cell>Ours</cell><cell>85.8</cell><cell>44.1</cell><cell>46.1</cell><cell>87.3</cell><cell>45.0</cell><cell>46.9</cell><cell>MuPots Cross Validation</cell></row><row><cell>HMOR [36]*</cell><cell>82.0</cell><cell>43.8</cell><cell>43.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>MuCo + COCO +</cell></row><row><cell>HDNet [15]*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.7</cell><cell>35.2</cell><cell>-</cell><cell>MuCo</cell></row><row><cell>SMAP [42]*</cell><cell>73.5</cell><cell>35.4</cell><cell>-</cell><cell>80.5</cell><cell>38.7</cell><cell>42.7</cell><cell>MuCo + COCO</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on the Panoptic<ref type="bibr" target="#b9">[10]</ref> dataset. RootNet<ref type="bibr" target="#b24">[25]</ref> model was fine-tuned with CMU Panoptic data to provide a better initialization. The reported metric is MPJPE relative to the root joint and results are reported in mm. *The average of<ref type="bibr" target="#b38">[39]</ref> and<ref type="bibr" target="#b35">[36]</ref> are recalculated following the standard practice in<ref type="bibr" target="#b37">[38]</ref> and<ref type="bibr" target="#b41">[42]</ref> (i.e. average over activities) for a more direct comparison.</figDesc><table><row><cell>Method</cell><cell cols="5">Haggling Mafia Ultim. Pizza Mean ?</cell></row><row><cell>RootNet [25] (w/ all joints)</cell><cell>83.3</cell><cell cols="3">107.9 106.0 118.4</cell><cell>103.9</cell></row><row><cell>Ours (w/ all joints)</cell><cell>59.4</cell><cell>68.9</cell><cell>67.2</cell><cell>86.4</cell><cell>70.5</cell></row><row><cell>Zanfir et al. *[39]</cell><cell>72.4</cell><cell>78.8</cell><cell>66.8</cell><cell>94.3</cell><cell>78.1</cell></row><row><cell>RootNet [25]</cell><cell>52.1</cell><cell>65.3</cell><cell>58.0</cell><cell>80.4</cell><cell>63.9</cell></row><row><cell>SMAP [42]</cell><cell>63.1</cell><cell>60.3</cell><cell>56.6</cell><cell>67.1</cell><cell>61.8</cell></row><row><cell>HMOR* [36]</cell><cell>50.9</cell><cell>50.5</cell><cell>50.7</cell><cell>68.2</cell><cell>55.1</cell></row><row><cell>Ours</cell><cell>42.0</cell><cell>50.3</cell><cell>47.3</cell><cell>59.4</cell><cell>49.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on the NBA2K<ref type="bibr" target="#b44">[45]</ref> dataset. We use the MPJPE metric. *This method has been fine-tuned with the same dataset and uses the same initial method (RootNet<ref type="bibr" target="#b24">[25]</ref>) for fair comparison.</figDesc><table><row><cell>Method</cell><cell>MPJPE [mm]</cell><cell></cell><cell cols="2">MPJPE-PA [mm]</cell><cell></cell></row><row><cell></cell><cell cols="2">Cory Glen Oscar Tomas Mean ?</cell><cell cols="3">Cory Glen Oscar Tomas Mean ?</cell></row><row><cell>RootNet [25]</cell><cell>154.3 167.7 159.3 136.2</cell><cell>154.1</cell><cell cols="2">115.8 137.5 122.4 103.9</cell><cell>119.7</cell></row><row><cell cols="2">Pi-Net* (fine-tunned) [7] 136.6 155.3 140.2 119.8</cell><cell>137.8</cell><cell>109.7 129.2 111.5</cell><cell>96.2</cell><cell>111.5</cell></row><row><cell>Ours</cell><cell>130.0 142.0 134.7 121.7</cell><cell>131.9</cell><cell>99.6 111.5 104.4</cell><cell>95.8</cell><cell>102.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. See how our estimations correct the initial joint positions taking them closer to the ground truth (highlighted in red arrows). Qualitative results. We show results on the MuPoTS-3D<ref type="bibr" target="#b22">[23]</ref>, Panoptic<ref type="bibr" target="#b9">[10]</ref>, NBA2K<ref type="bibr" target="#b44">[45]</ref> and COCO<ref type="bibr" target="#b16">[17]</ref> datasets. Here he show cases of close interactions (first two rows), severe truncation (first row, second column), and our model on images in-the-wild (last row).</figDesc><table><row><cell>Input image</cell><cell>SOTA [25]</cell><cell>Ours</cell><cell>Input image</cell><cell>SOTA [25]</cell><cell>Ours</cell></row><row><cell>Input image</cell><cell>Ours</cell><cell>Input image</cell><cell>Ours</cell><cell>Input image</cell><cell>Ours</cell></row><row><cell>Figure 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>For</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human context: Modeling human-human interactions for monocular 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Articulated Motion and Deformable Objects</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="260" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monocular 3d multi-person pose estimation by integrating top-down and bottom-up networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="7649" to="7659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiperson 3d human pose estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<idno type="DOI">10.1109/3DV.2019.00052</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="405" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Three-dimensional reconstruction of human interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Remips: Physically consistent 3d reconstruction of multiple interacting people under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fieraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021-12" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="pre" to="proceedings" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pi-net: Pose interacting network for multi-person monocular 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2796" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human3.6m: Large Scale Datasets and Predictive Methods for 3d Human Sensing in Natural Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2013.248</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khirodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13349</idno>
		<title level="m">Occluded human mesh recovery</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Y. Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hdnet: Human depth estimation for multi-person camera-space localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="633" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-view multi-person 3d pose estimation with plane sweep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11886" to="11895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attend and interact: Higherorder object interactions for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6790" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09813</idno>
		<title level="m">Monocular 3d Human Pose Estimation In The Wild Using Improved CNN Supervision</title>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Singleshot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Xnect: Real-time multi-person 3d motion capture with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multiperson pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3433" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular, one-stage, regression of multiple 3d people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="197" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hmor: Hierarchical multi-person ordinal relations for monocular multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="242" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph-based 3d multi-person pose estimation using multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11148" to="11157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes -the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<biblScope unit="page">8410</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Body meshes as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Smap: Single-shot multiperson absolute 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Reconstructing nba players</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
