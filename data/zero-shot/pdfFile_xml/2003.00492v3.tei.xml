<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks with Adaptive Sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yan</surname></persName>
							<email>xuyan1@link.</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<email>lizhen@</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
							<email>shuguangcui@cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks with Adaptive Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Raw point clouds data inevitably contains outliers or noise through acquisition from 3D sensors or reconstruction algorithms. In this paper, we present a novel endto-end network for robust point clouds processing, named PointASNL, which can deal with point clouds with noise effectively. The key component in our approach is the adaptive sampling (AS) module. It first re-weights the neighbors around the initial sampled points from farthest point sampling (FPS), and then adaptively adjusts the sampled points beyond the entire point cloud. Our AS module can not only benefit the feature learning of point clouds, but also ease the biased effect of outliers. To further capture the neighbor and long-range dependencies of the sampled point, we proposed a local-nonlocal (L-NL) module inspired by the nonlocal operation. Such L-NL module enables the learning process insensitive to noise. Extensive experiments verify the robustness and superiority of our approach in point clouds processing tasks regardless of synthesis data, indoor data, and outdoor data with or without noise. Specifically, PointASNL achieves state-of-theart robust performance for classification and segmentation tasks on all datasets, and significantly outperforms previous methods on real-world outdoor SemanticKITTI dataset with considerate noise. Our code is released through https: //github.com/yanx27/PointASNL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the popularity of 3D sensors, it's relatively easy for us to obtain more raw 3D data, e.g., RGB-D data, LiDAR data, and MEMS data <ref type="bibr" target="#b43">[44]</ref>. Considering point clouds as the fundamental representative of 3D data, the understanding of point clouds has attracted extensive attention for various applications, e.g., autonomous driving <ref type="bibr" target="#b28">[29]</ref>, robotics <ref type="bibr" target="#b36">[37]</ref>, and place recognition <ref type="bibr" target="#b22">[23]</ref>. Here, a point cloud has two components: the points P ? R N ?3 and the features F ? R N ?D . Unlike 2D images, the sparsity and disorder proprieties make robust point clouds processing a challenging task. Furthermore, the raw data obtained from those 3D sensors or reconstruction algorithms inevitably contain outliers or noise in real-world situations.</p><p>In this work, we present a novel end-to-end network for robust point clouds processing, named PointASNL, which can deal with point clouds with noise or outliers effectively. Our proposed PointASNL mainly consists of two general modules: adaptive sampling (AS) module and localnonlocal (L-NL) module. The AS module is used to adjust the coordinates and features of the sampled points, whereas the L-NL module is used to capture the neighbor and longrange dependencies of the sampled points.</p><p>Unlike the cases in 2D images, traditional convolution operations cannot directly work on unstructured point cloud data. Thus, most of the current methods usually use sampling approaches to select points from the original point clouds for conducting local feature learning. Among these sampling algorithms, farthest point sampling (FPS) <ref type="bibr" target="#b24">[25]</ref>, Poisson disk sampling (PDS) <ref type="bibr" target="#b10">[11]</ref>, and Gumbel subset sampling (GSS) <ref type="bibr" target="#b47">[48]</ref> are proposed in previous works. However, as the most representative one, FPS is rooted in Euclidean distance, which is task-dependent and outliers sen-sitive. PDS, a predefined uniformly sampling method, also cannot solve the problem above in a data-driven way. GSS only performs sampling from a high-dimension embedding space and ignores the spatial distribution of points. Furthermore, the shared key issue in these approaches is that the sampled points are limited to a subset of the original point clouds. Therefore, as shown in the left part of <ref type="figure" target="#fig_0">Fig. 1</ref>, suppose an outlier point is sampled, it will influence the downstream process inevitably.</p><p>To overcome the issues mentioned above, we propose a differentiable adaptive sampling (AS) module to adjust the coordinates of the initial sampled points (e.g., from FPS) via a data-driven way. Such coordinate adjusting facilitates to fit the intrinsic geometry submanifold and further shifts to correct points beyond original point clouds without the influence of outliers. Thus, the AS module can not only benefit point feature learning, but also improve the model robustness to noise.</p><p>To further enhance the performance as well enables the learning process insensitive to noise, we proposed a localnonlocal (L-NL) module for capturing neighbor and longrange dependencies of the sampled points. The underlying reason is that, currently, most appealing methods for feature learning is to query a local group around the each sampled point, and then they construct the graph-based learning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b13">14]</ref> or define convolution-like operations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b33">34]</ref> (we denote them as Point Local Cell). Nonetheless, such point local cell only considers local information interaction in the neighbor area and then acquires the global context through a hierarchical structure, which usually leads to bottom-up feature learning. Inspired by the success of the Nonlocal network <ref type="bibr" target="#b40">[41]</ref>, we innovatively design this L-NL module, in which the key component is the Point Nonlocal Cell. In particular, the point nonlocal cell allows the computation of the response of a sampled point as a weighted sum of the influences of the entire point clouds, instead of just within a limited neighbor range. With the learned long-dependency correlation, the L-NL module can provide more precise information for robust point clouds processing. As shown in the right part of <ref type="figure" target="#fig_0">Fig. 1</ref>, although the sampled points within the lower engine are covered with noise, our L-NL module can still learn the features from the other engine with a different noise distribution.</p><p>Our main contribution can be summarized as follows: 1) We propose an end-to-end model for robust point clouds processing, PointASNL, which can effectively ease the influence of outliers or noise; 2) With the proposed adaptive sampling (AS) module, PointASNL can adaptively adjust the coordinates of the initial sampled points, making them more suitable for feature learning with intrinsic geometry and more robust for noisy outliers; and 3) We further design a point nonlocal cell in the proposed local-nonlocal (L-NL) module, which enhances the feature learning in point local cells. Extensive experiments on classification and segmentation tasks verify the robustness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Volumetric-based and Projection-based Methods. Considering the sparsity of point clouds and memory consumption, it is not very effective to directly voxelized point clouds and then use 3D convolution for feature learning. Various subsequent improvement methods have been proposed, e.g., efficient spatial-temporal convolution MinkowskiNet <ref type="bibr" target="#b4">[5]</ref>, computational effective Submanifold sparse convolution <ref type="bibr" target="#b6">[7]</ref>, and Oc-tree based neural networks O-CNN <ref type="bibr" target="#b38">[39]</ref> and OctNet <ref type="bibr" target="#b26">[27]</ref>. Such methods greatly improve the computational efficiency, thus leading to the entire point clouds as input without sampling and superior capacity. There are also other grid-based methods using traditional convolution operations, e.g., projecting 3D data to multi-view 2D images <ref type="bibr" target="#b31">[32]</ref> and lattice space <ref type="bibr" target="#b30">[31]</ref>. Yet, the convolution operation of these methods lacks the ability to capture nonlocally geometric features. Point-based Learning Methods. PointNet <ref type="bibr" target="#b23">[24]</ref> is the pioneering work directly on sparse and unstructured point clouds, which summarizes global information by using pointwise multi-layer perceptions (MLPs) followed by the max-pooling operation. PointNet++ <ref type="bibr" target="#b24">[25]</ref> further applies a hierarchical structure with k-NN grouping followed by max-pooling to capture regional information. Since it aggregates local features simply to the largest activation, regional information is not yet fully utilized. Recently, much effort has been made for effective local feature aggregation. PointCNN <ref type="bibr" target="#b19">[20]</ref> transforms neighboring points to the canonical order, which enables traditional convolution to play a normal role. Point2Sequence <ref type="bibr" target="#b20">[21]</ref> uses the attention mechanism to aggregate the information of different local regions. Methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref> directly use the relationship between neighborhoods and local centers to learn a dynamic weight for convolution, where ECC <ref type="bibr" target="#b29">[30]</ref> and RS-CNN <ref type="bibr" target="#b21">[22]</ref> use ad-hoc defined 6-D and 10-D vectors as edge relationship, PCCN <ref type="bibr" target="#b39">[40]</ref> and PointConv <ref type="bibr" target="#b43">[44]</ref> project the relative position of two points to a convolution weight. A-CNN <ref type="bibr" target="#b15">[16]</ref> uses ring convolution to encode features that have different distances from the local center points, and PointWeb <ref type="bibr" target="#b50">[51]</ref> further connects every point pairs in a local region to obtain more representative region features. Still, these methods only focus on local feature aggregation and acquire global context from local features through a hierarchical structure. On the other hand, there are various works for learning the global context from the local features. A-SCN <ref type="bibr" target="#b45">[46]</ref> uses a global attention mechanism to aggregate global features but lacks the support of local information, which does not achieve good results. DGCNN <ref type="bibr" target="#b41">[42]</ref> proposes the EdgeConv module to generate edge features and search neighbors in   features space. LPD-Net <ref type="bibr" target="#b22">[23]</ref> further extends DGCNN on both spatial neighbors and features neighbors aggregation. Nonetheless, the neighbors in the feature space are not representative of the global features, and spatial receptive fields of the network gradually become confused without a hierarchical structure.</p><p>Outlier Removal and Sampling Strategy. Outliers and noise usually exist in raw point clouds data. Previous robust statistics methods <ref type="bibr" target="#b0">[1]</ref> for outlier removal suffer from non-trivial parameter tuning or require additional information <ref type="bibr" target="#b42">[43]</ref>. Various data-driven methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref> are proposed for outlier removal, which first discard some outliers and then projects noisy points to clean surfaces. Yet, such methods cannot inherently merge the robust point cloud feature learning with outlier removal in a joint learning manner. On the other hand, deep learning based point cloud processing methods usually sample points to decrease computational consumption. Though, most sampling methods are limited by noise sensitivity and not driven by data <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11]</ref>, or without the consideration of spatial distribution <ref type="bibr" target="#b47">[48]</ref>. SO-Net <ref type="bibr" target="#b18">[19]</ref> uses an unsupervised neural network, say selforganizing map (SOM), to utilize spatial distribution of point clouds. It then employs PointNet++ <ref type="bibr" target="#b24">[25]</ref> to multiple smaller sampled 'nodes'. However, SO-Net does not belong to online adaptive sampling. Under the assumption of local label consistency, some works use the geometric centers of voxel grids to uniformly represent sampled points <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30]</ref>, which ignores the difference of point distribution influence. Still, these methods are extremely sensitive to noise and cannot learn the spatial distribution of sampled points at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>In this paper, we propose two modules in PointASNL, namely adaptive sampling (AS) module in Sec. 3.1 and local-nonlocal (L-NL) module in Sec. 3.2. In Sec.3.3, we combine AS and L-NL modules in a hierarchical manner to form our proposed PointASNL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Adaptive Sampling (AS) Module</head><p>Farthest point sampling (FPS) is widely used in many point cloud framework, as it can generate a relatively uniform sampled points. Therefore, their neighbors can cover all input point clouds as much as possible. Nevertheless, there are two main issues in FPS: (1) It is very sensitive to the outlier points, making it highly unstable for dealing with real-world point clouds data. (2) Sampled points from FPS must be a subset of original point clouds, which makes it challenging to infer the original geometric information if occlusion and missing errors occur during acquisition.</p><p>To overcome the above-mentioned issues, we first use FPS to gain the relatively uniform points as original sampled points. Then our proposed AS module adaptively learns shifts for each sampled point. Compared with the similar process widely used in mesh generation <ref type="bibr" target="#b37">[38]</ref>, the downsampling operation must be taken into account both in spatial and feature space when the number of points is reduced. For the AS module, let P s ? R Ns?3 as the sampled N s points from N input points of certain layer, x i from P s and f i from F s ? R Ns?D l as a sampled point and its features. We first search neighbors of sampled points as groups via k-NN query, then use general self-attention mechanism <ref type="bibr" target="#b34">[35]</ref> for group features updating.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 2</ref> (a), we update group features by using attention within all group members. For x i,1 , ..., x i,K ? N (x i ) and their corresponding features f i,1 , ..., f i,K , where N (x i ) is K nearest neighbors of sampled point x i , feature updating of group member x i,k can be written as</p><formula xml:id="formula_0">f i,k = A(R(x i,k , x i,j )?(x i,j ), ?x i,j ? N (x i )),<label>(1)</label></formula><p>where a pairwise function R computes a high level relationship between group members x i,k , x i,j ? N (x i ). The unary function ? change the each group feature f i,j from dimension D l to another hidden dimension D and A is a aggregation function.</p><p>For less computation, we consider ? in the form of a linear transformation of point features ?(x i,j ) = W ? f i,j , and relationship function R is dot-product similarity of two points as follows,</p><formula xml:id="formula_1">R(x i,k , x i,j ) = Softmax(?(f i,k ) T ?(f i,j )/ ? D ),<label>(2)</label></formula><p>where ? and ? are independent two linear transformations and can be easily implemented by independent 1D convolution Conv :</p><formula xml:id="formula_2">R D l ? R D ,</formula><p>where D l and D are input and output channel, respectively. After that, point-wise MLPs, i.e., ? p and ? f with softmax activation function on K group members are used to obtain the corresponding intensity of each point in a group, which can be represented as normalized weights for each coordinate axis and features channel.</p><formula xml:id="formula_3">F p = {? p (f i,k )} K k=1 , W p = Softmax(F p ), F f = {? f (f i,k )} K k=1 , W f = Softmax(F f ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">F p , F f , W p , W f ? R K?1 are outputs of point-wise</formula><p>MLPs and normalized weights after softmax function. Finally, a adaptive shifting on both K neighbors' coordinates from X ? R K?3 and their features from F ? R K?D are implemented by the weighted sum operation. We obtain a new coordinate of the sampled point x * i and its features f * i by following operations,</p><formula xml:id="formula_5">x * i = W T p X, X = {x i,k } K k=1 , f * i = W T f F, F = {f i,k } K k=1 .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local-Nonlocal (L-NL) Module</head><p>Within our L-NL module, there are two cells: point local (PL) cell and point nonlocal (PNL) cell. Specifically, the PL cell can be any appealing algorithms (e.g., PointNet++ <ref type="bibr" target="#b24">[25]</ref>, PointConv <ref type="bibr" target="#b43">[44]</ref>), and the PNL cell innovatively considers the correlations between sampled points and the entire point cloud in multi-scale. Consequently, the contextual learning of the point cloud is enhanced by combining the local and global information (See <ref type="figure" target="#fig_2">Fig. 2(b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Point Local Cell</head><p>The local features mining of point clouds often exploits the local-to-global strategy <ref type="bibr" target="#b24">[25]</ref>, which aggregates local features in each group and gradually increases the receptive field by hierarchical architectures. We adopt such methods in point local (PL) cell. Similar to the previous definition for a local sampled point x i , corresponding feature f i and neighborhoods N (x i ), a generalized local aggregation function used in PL can be formulated as </p><formula xml:id="formula_6">f l i = A(L(f n ), ?x n ? N (x i )),<label>(5)</label></formula><p>where f l i is updated features of local center x i , which is updated by local feature transformation function L and aggregation function A. For PointNet++ <ref type="bibr" target="#b24">[25]</ref>, L is multi-layer perceptions (MLPs) and A is max-pooling. Recently, more and more works directly design convolution operators on the local regions, which mainly change L to be a learnable weighted multiply obtained by neighbor relationships. Considering the efficiency and effectiveness of the operation in a compromise, we implement the convolution operation by adaptively projecting the relative position of two points to a convolution weight <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref>, and aggregate local features,</p><formula xml:id="formula_7">L(f n ) := g(x n ? x i )f n ,<label>(6)</label></formula><p>where g is chosen as MLPs: R 3 ? R D l ?D mid , which transfers 3 dimension relative position to D l ? D mid transformation matrix. D l represents the channel of the input features in certain layer and D mid is the channel of the updated features by PL cell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Point Nonlocal Cell</head><p>Inspired by nonlocal neural networks <ref type="bibr" target="#b40">[41]</ref> in 2D images for long-range dependencies learning, we design a specific point nonlocal (PNL) cell for global context aggregation ( <ref type="figure" target="#fig_3">Fig. 3</ref>). There are two main differences between our point nonlocal cell and component proposed in <ref type="bibr" target="#b40">[41]</ref>: <ref type="bibr" target="#b0">(1)</ref> We use our sampled points as query points to calculate similarity with entire points in certain layers (say, key points P k ). Furthermore, our query points are not limited within a subset of input point clouds, as each sampled point adaptively updates its coordinate and features by the AS module (Sec. 3.1). (2) Our output channel is gradually increased with the down-sampling operation in each layer, which avoids information loss in the down-sampling encoder. Specifically, similar with Eq. 1, given query point x i and key point from P k , the nonlocal operation N L is defined as:</p><formula xml:id="formula_8">N L(x i , P k ) := A(R(f i , f j )?(f j ), ?x j ? P k ), (7)</formula><p>where P k ? R N ?3 stands for the entire N key points in a certain layer. Finally, a single nonlinear convolution layer ? fuse the global context and adjust the channel of each point to the same dimension with the output of PL D l+1 (Eq. 5). Hence, for a sampled point x i , its updated feature is computed by PNL with function</p><formula xml:id="formula_9">f nl i = ?(N L(x i , P k )).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Local-Nonlocal (L-NL) Fusion</head><p>By combining PL and PNL, we construct a local-nonlocal module to encode local and global features simultaneously. As shown in <ref type="figure" target="#fig_2">Fig. 2 (b)</ref>, it uses query points and key points as inputs, and exploit k-NN grouping for neighborhoods searching for each query point. Then, the group coordinates and features of each local region are sent through PL for local context encoding. For PNL, it uses whole key points to integrate global information for each query point via an attention mechanism. Finally, for each updated point, a channel-wise sum with a nonlinear convolution ? is used to fuse local and global information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">PointASNL</head><p>By combining the two components proposed in Sec.3.1 and Sec 3.2 in each layer, we can implement a hierarchical architecture for both classification and segmentation tasks.</p><p>For the classification, we designed a three-layer network and down-sample input points at two levels. In particular, the first two layers sample 512 and 124 points. The third layer concatenate global features of former two layers with max pooling, where new features are processed by fully connected layers, dropout, and softmax layer, respectively. The batch normalization layers and the ReLU function are used in each layer. Furthermore, skip connections <ref type="bibr" target="#b9">[10]</ref> are used in the first two layers.</p><p>For the segmentation (see <ref type="figure" target="#fig_4">Fig. 4</ref>), each encoder layer is similar with the setting in classification, but network has a deeper structure (1024-256-64-16). In the decoder part, we use 3-nearest interpolation <ref type="bibr" target="#b24">[25]</ref> to get the up-sampled features and also use the L-NL Block for better feature learning. Furthermore, skip connections are used to pass the features between intermediate layers of the encoder and the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We evaluate our PointASNL on various tasks, including synthetic dataset, large-scale indoor and outdoor scene seg- mentation dataset. In all experiments, we implement the models with Tensorflow on one GTX 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification</head><p>We evaluate our model on synthetic dataset ModelNet10 and ModelNet40 <ref type="bibr" target="#b44">[45]</ref> for classification, where ModelNet40 is composed of 9843 train models and 2468 test models in 40 classes and ModelNet10 is a subset of ModelNet40 that consists of 10 classes with 3991 training and 908 testing objects.</p><p>Shape Classification. Training and testing data in classification are provided by <ref type="bibr" target="#b23">[24]</ref>. For training, we select 1024 points as the input. The augmentation strategy includes the following components: random anisotropic scaling in range [?0.8, 1.25], translation in the range [?0.1, 0.1], and random dropout 20% points. For testing, similar to <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, we apply voting test using random scaling and then average the predictions. In Tab. 1, our method outperforms almost all state-of-the-art methods in 1024 input points except RS-CNN. Note that RS-CNN <ref type="bibr" target="#b21">[22]</ref> can achieve 93.6% from 92.9% on uniformly sampling with tricky voting strategy (the best of 300 repeated tests), which is different from normal random sampling and once voting setting.</p><p>Shape Classification with Noise. Most of the methods can achieve decent performance on synthetic datasets, as they have stable distribution and do not contain any noise. Though, such a good performance often leads to a lack of robustness of the model. To further verify the robustness of our model, we did the experiments like KC-Net <ref type="bibr" target="#b27">[28]</ref> to replace a certain number of randomly picked points with random noise ranging [?1.0, 1.0] during testing. The comparisons with PointNet <ref type="bibr" target="#b23">[24]</ref>, PointConv <ref type="bibr" target="#b43">[44]</ref> and KC-Net <ref type="bibr" target="#b27">[28]</ref> are shown in <ref type="figure">Fig. 5 (b)</ref>. As shown in this figure, our model is very robust to noise, especially after adding the AS module. It can be seen from (c) and (d) that the adaptive sampling guarantees the proper shape of the sampled point clouds, making the model more robust. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Segmentation</head><p>Indoor Scene Segmentation. <ref type="bibr" target="#b0">1</ref> Unlike classification on synthetic datasets <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b48">49]</ref>, indoor 3D scene segmentation is a more difficult task, because it is real-world point clouds and contains lots of outliers and noise. We use Stanford 3D Large-Scale Indoor Spaces (S3DIS) <ref type="bibr" target="#b1">[2]</ref> and ScanNet v2 (ScanNet) <ref type="bibr" target="#b5">[6]</ref> datasets to evaluate our model. S3DIS dataset is sampled from 3 different buildings, which includes 6 large-scale indoor areas with 271 rooms. Each point in this dataset has a semantic label that belongs to one of the 13 categories. We compare mean per-class IoU (mIoU) on both 6-fold cross-validation over all six areas and Area 5 (see supplementary material). ScanNet dataset contains 1513 scanned indoor point clouds for training and 100 test scans with all semantic labels unavailable. Each point has been labeled with one of the 21 categories. We submitted our results to the official evaluation server to compare against other state-of-the-art methods on the benchmark.</p><p>During the training process, we generate training data by randomly sample 1.5m ? 1.5m ? 3m cubes with 8192 points from the indoor rooms. 0.1m padding of sampled cubes is used to increase the stability of the cube edge prediction, which is not considered in the loss calculation. On both datasets, we use points position and RGB information </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>S3DIS ScanNet methods use unspecific number of points as input TangentConv <ref type="bibr" target="#b32">[33]</ref> 52.8 40.9 SPGraph <ref type="bibr" target="#b16">[17]</ref> 62.1 -KPConv <ref type="bibr" target="#b33">[34]</ref> 70.6 68.4 methods use fixed number of points as input PointNet++ <ref type="bibr" target="#b24">[25]</ref> 53.4 33.9 DGCNN <ref type="bibr" target="#b41">[42]</ref> 56.1 -RSNet <ref type="bibr" target="#b12">[13]</ref> 56.5 -PAT <ref type="bibr" target="#b47">[48]</ref> 64.3 -PointCNN <ref type="bibr" target="#b19">[20]</ref> 65 as features. We did not use the relative position in Point-Net <ref type="bibr" target="#b23">[24]</ref> as a feature to train the model in S3DIS, because our model already learns relative position information well. During the evaluation process, we use a sliding window over the entire rooms with 0.5m stride to complement 5 voting test. In Tab. 2, we compare our PointASNL with other stateof-the-art methods under the same training and testing strategy (randomly chopping cubes with a fixed number of points), e.g., PointNet++ <ref type="bibr" target="#b24">[25]</ref>, PointCNN <ref type="bibr" target="#b19">[20]</ref>, Point-Conv <ref type="bibr" target="#b43">[44]</ref>, PointWeb <ref type="bibr" target="#b50">[51]</ref> and HPEIN <ref type="bibr" target="#b13">[14]</ref>. We also list results of another kind of methods (using points of unfixed number or entire scene as input), e.g., TangentConv <ref type="bibr" target="#b32">[33]</ref>    and KPconv <ref type="bibr" target="#b33">[34]</ref>. All methods use only point clouds as input without voxelization. As shown in Tab. 2, PointASNL outperforms all methods with same the training strategy in both S3DIS and ScanNet. In particular, our result is 8% higher than previous state-ofthe-art PointConv <ref type="bibr" target="#b43">[44]</ref> on the ScanNet with the same experiment setting, in which the convolution design is similar to our PL cell. Nevertheless, without proper sampling and global information support, it cannot achieve such results with the same network architecture.</p><p>On the other hand, training using more points as input can obtain more information. Instead of learning from randomly selected cubes with fixed number, KP-Conv <ref type="bibr" target="#b33">[34]</ref> performs grid sampling based on the assumption of local label consistency so that larger shape of the point cloud can be included as input.</p><p>The qualitative results are visualized in <ref type="figure">Fig. 6</ref>. Our method can correctly segments objects even in complex scenes. Outdoor Scene Segmentation. Compared with its indoor counterpart, an outdoor point cloud covers a wider area and has a relatively sparser point distribution with noise. For this reason, it is more challenging to inference from outdoor scenes.</p><p>We evaluated our model on SemanticKITTI <ref type="bibr" target="#b3">[4]</ref>, which is a large-scale outdoor scene dataset, including 43,552 scans captured in the wild. The dataset consists of 22 sequences (00 to 10 as the training set, and 11 to 21 as the test set), each of which contains a series of sequential laser-scans. Each individual scan is a point clouds generated with a commonly used automotive LiDAR. The whole sequence can be generated by aggregating multiple consecutive scans.</p><p>In our experiments, we only evaluated our model under a single scan semantic segmentation. In the single scan experiment <ref type="bibr" target="#b3">[4]</ref>, sequential relations among scans in the same sequence are not considered. The total number of 19 classes is used for training and evaluation. Specifically, the input data generated from the scan is a list of coordinates of the three-dimensional points along with their remissions.</p><p>During training and testing, we use a similar sliding windows based strategy as indoor segmentation. Since point clouds in the outdoor scene are more sparse, we set the size of the cube with 10m ? 10m ? 6m and 1m padding. In Tab. 3, we compare PointASNL with other state-of-theart methods. Our approach outperforms others by a large margin. Supplementary material shows that our method achieves the best result in 13 of 19 categories. Furthermore, <ref type="figure" target="#fig_7">Fig. 7</ref> illustrates our qualitative visualization of two samples, even if the scene is covered with a lot of noise  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To further illustrate the effectiveness of proposed AS and L-NL module, we designed an ablation study on both the shape classification and the semantic segmentation. The results of the ablation study are summarized in Tab. 4.</p><p>We set two baselines: A and B. Model A only encodes global features by PNL, and model B only encodes local features. The baseline model A gets a low accuracy of 90.1% and 45.7% IoU on segmentation, and model B gets 92.0% and 56.1%, respectively. When we combine local and global information (models C), there is a notable improvement in both classification and segmentation. Finally, when we add the AS module, the model will have a significant improvement in the segmentation task (93.2% and 63.5% in model D).</p><p>Furthermore, our proposed components L-NL module and AS module can directly improve the performance of other architecture. When we use PointNet++ <ref type="bibr" target="#b24">[25]</ref> in our PL cell (model F), it will reduce the error of classification and segmentation tasks by 23.1% and 12.6%, respectively, with its original model (model E). It should be noted that the AS module does not increase the accuracy of the classification task, even reduced the accuracy of classification when adding on PointNet++ (model F). This is because the synthetic dataset does not have a lot of noise like scene segmentation, for some simpler local aggregation (e.g., max pool), it may make them unable to adapt the uneven point cloud distribution after using AS. Furthermore, we also use DGCNN <ref type="bibr" target="#b41">[42]</ref> as our local aggregation baseline (model H), and fused architecture (model I and J) can largely improve the performance on two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Robustness for Sparser Point Clouds</head><p>To further verify the robustness of the PointASNL model, we take sparser points (i.e., 1024, 512, 256, 128 and 64) as the input to various models trained with 1024 points. Then we compare our method with PointNet <ref type="bibr" target="#b23">[24]</ref>, Point-Net++ <ref type="bibr" target="#b24">[25]</ref>, SO-Net <ref type="bibr" target="#b18">[19]</ref> and the recent state-of-the-art RS-CNN <ref type="bibr" target="#b21">[22]</ref>. We follow these methods to apply random input dropout during the training.</p><p>As can be seen from the <ref type="figure" target="#fig_8">Fig .8 (c)</ref>, PNL can greatly improve the robustness of our model with different density inputs. In particular, when the input contains only 64 points, PNL can even help to improve the accuracy of our model, from 73.9% to 85.2%, which largely exceeds the current state-of-the-art RS-CNN <ref type="bibr" target="#b21">[22]</ref> (about 75%). The experimental results fully demonstrate that the use of local and global learning methods can greatly improve the robustness of the model. As shown in <ref type="figure" target="#fig_8">Fig .8 (a)</ref> and (b), when the input points reduce to 64, even humans can hardly recognize the airplane, but our model can classify it correctly. Such superior robustness makes our proposed PointASNL model suitable for raw noisy point clouds with limited sampling points, especially for large scale outdoor scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented the adaptive sampling (AS) and the local-nonlocal (L-NL) module to construct the architecture of PointASNL for robust 3D point cloud processing. By combining local neighbors and global context interaction, we improve traditional methods dramatically on several benchmarks. Furthermore, adaptive sampling is a differentiable sampling strategy to fine-tune the spatial distribution of sampled points, largely improve the robustness of the network. Experiments with our state-of-the-art results on competitive datasets and further analysis illustrate the effectiveness and rationality of our PointASNL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>In this supplementary material, we first provide more additional experiments to further verify the superiority of our model in Section B. Besides, we show the our network architecture details in Section C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Part Segmentation</head><p>Due to space limitation, we illustrate the part segmentation experiments using man-made synthetic dataset ShapeNet <ref type="bibr" target="#b48">[49]</ref>, which contains 16,881 shapes from 16 classes and 50 parts. We use the data provided by <ref type="bibr" target="#b24">[25]</ref> and adopt the same training and test strategy, i.e., randomly pick 2048 points as the input and concatenate the one-hot encoding of the object label to the last layer.</p><p>The quantitative comparisons with the state-of-the-art point-based methods are summarized in Tab. 5. Note that we only compare with methods use 2048 points. When compared with the state-of-the-arts, PointASNL achieves comparable result, which is only slightly lower than RS-CNN <ref type="bibr" target="#b21">[22]</ref> using different sampling and voting strategy (as the same reason for classification task).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Selection of Adaptive Sampling</head><p>Two variable conditions, i.e., the sampling strategy for initial sampled points and deformation method, are investigated for this issue. Tab. 1 summarizes the results. For the initial sampling points, we chose two strategies, i.e., FPS and random sampling (RS). Also for local coordinate points and feature updates, we compare the effects of using the weight learning by group feature (GF) and simple average of all neighbors coordinates and features. Note that the number of neighbors is set to be equal for a fair comparison. <ref type="figure" target="#fig_0">Figure 1</ref>. Selected results of part segmentation.  <ref type="figure" target="#fig_2">Figure 2</ref>. Visualization of local-global learning. For each sampled point (red), we search its local neighbors (blue) and the K points with the highest global response value (green), where K is equal to the number of local neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction Ground Truth Error</head><p>As Tab. 1 shows,, if we just use RS sample the initial points and then average their coordinates and features (model A), we will get very low accuracy of 87.9%. However, if we use FPS instead of RS (model B), it can increase to 91.5%. Furthermore, model C and D illustrate the weight learning using group features can largely increase the inference ability of our model. However, if we use RS as sampling strategy, it will cause some accuracy loss while we add the group features learning. This shows that AS module can only finely adjust the distribution of the sampled point cloud instead of 'creating' the missing information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Visualization of L-NL Module</head><p>We further demonstrate the local-global learning of PointASNL in <ref type="figure" target="#fig_2">Fig. 2</ref>. In the first layer of the network, PNL can find global points that have similar characteristics with sampled points (e.g., edge and normal vectors). In the second layer, these global highly responsive points have the same semantics information with sampled points, even when sampled points are at the junction of the two different semantics. This is why global features can help sampled points to better aggregate local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Visualization of Adaptive Sampling</head><p>When the input point cloud has a lot of noise, adaptive sampling has the ability to ensure the distribution of the  <ref type="figure">Fig. 6</ref> to prove the robustness of the AS module. As can be seen from <ref type="figure">Fig. 6</ref>, AS module can effectively reduce noise in the sample points and maintain the shape of the sampled manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Further Improvement of PointASNL</head><p>The result in manuscript only conducts a fair comparison (same model structure and training strategy) against appealing recent methods under the same setting of Point-Net++ <ref type="bibr" target="#b24">[25]</ref>. However, our PointASNL can still achieve further improvement if we use other data pre-processing or deeper structure.</p><p>As shown in Tab. 2, our PointASNL can still improve its performance if we use grid sampling pre-processing, more input points and deeper structure. As for the structure of deeper PointASNL, we add an additional point local cell at the end of each layer. Furthermore, by conducting ensemble learning with model from different training epochs, we can finally achieve 66.6% on ScanNet benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. Concrete Results</head><p>In this section, we give our detailed results on the S3DIS (Tab.6 and Tab.7) and SemanticKITTI (Tab.8) dataset as a benchmark for future work. ScanNet <ref type="bibr" target="#b5">[6]</ref> is an online benchmark, the class scores can be found on its website. Furthermore, we provide more visualization results to illustrate the performance of our model in complicated scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Layer Setting</head><p>For each encoder layer, it can be written as the following form: Abstraction(npoint, nsample, as neighbor, mlp), where npoint is the number of sampled points of layer. nsample and as neighbor are number of group neighbors in point local cell and AS module, and they share the same k-NN query. mlp is a list for MLP construction in our layers and used in both PL and PNL. Tab. 3 shows the configuration of PointASNL on both classification and segmenttaion tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Loss Function</head><p>Like other previous works, we use cross entropy (CE) loss in classification and part segmentation, and consider the number of each category as weights in semantic segmentation. Furthermore, in order to avoid the sampled points being too close to each other in some local areas after the AS module transformation, we also use Repulsion Loss <ref type="bibr" target="#b49">[50]</ref> to restrict the deformation of sampled point clouds. In particular, we only use this loss in the first layer since it has the highest point density. The Repulsion loss does not bring any performance improvement, but the training procedure is significantly accelerated. Altogether, we train the PointASNL in an end-to-end manner by minimizing the following joint loss function:</p><formula xml:id="formula_10">L(?) = L CE + ?L Rep + ?||?|| 2 , L Rep = N i=0 i ?N (xi) w(||x i ? x i ||),<label>(9)</label></formula><p>where ? indicates the parameters in our network, ? = 0.01 balances the CE loss and Repulsion loss, and ? denotes the multiplier of the weight decay. For Repulsion loss, it penalizes the sampled point x i only when it is too close to its neighboring points x i ? N (x i ). w(r) = e r 2 /h 2 is a fastdecaying weight function and N is the number of sampled points.</p><p>The Repulsion loss also ensures that each sample point itself has a larger weight in the AS module in a relatively constant density, which makes them cannot move too far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Model Speeds</head><p>Tab. 4 shows the statistics of our models on different datasets. Since our L-NL module only uses sampled points as query points instead of the whole point cloud, the AS module and NL cell can be both efficient and effective with the bottleneck structures (only around 30% extra time).    <ref type="figure" target="#fig_3">Figure 3</ref>. More examples on S3DIS datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction Ground Truth Input Prediction Ground Truth Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head><p>Ground Truth Input </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>PointASNL for robust point clouds processing. The adaptive sampling module adaptively adjusts the sampled point from point clouds with noise. Besides, the local-nonlocal module not only combines the local features in Euclidean space, but also considers the long-range dependency in feature space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Part (a) shows adaptive sampling (AS) module, which firstly updates features of grouping point by reasoning group relationship, then normalized weighs re-weight initial sampled points to achieve new sampled points. Part (b) illustrates the construction of localnonlocal (L-NL) module, which consists of point local cell and point nonlocal cell. Ns stands for sampled point number, N stands for point number of entire point clouds, D l , D mid , and D l+1 stand for channel numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Inner structure of point nonlocal (PNL) cell. For the notations Ns, N, D l , D mid please refer to the caption of Fig. 2, D is the intermediate channel numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Architecture of our PointASNL for point cloud semantic segmentation. The L-NL modules are used in both encoder and decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>(a) Point cloud with some points being replaced with random noise. (b) Classification results of different models with noisy points, where PL, PNL, AS mean point local cell, point nonlocal cell and adaptive sampling, respectively. (c) Farthest point sampling on noisy data. (d) Adaptive sampling on noisy data, which maintain the distribution of the point cloud. Examples of indoor semantic segmentation on S3DIS and ScanNet datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Example of outdoor SemanticKITTI datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>(a) A sample of input point cloud. (b) A sample of input point cloud after randomly select 64 points (c) Results of testing with sparser points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>1 0.4 0.0 58.3 58.2 0.0 0.0 0.0 0.0 71.1 9.9 19.3 0.0 0.0 0.0 23.1 5.6 0.0 PointNet++ [25] 20.1 72.0 41.8 18.7 5.6 62.3 53.7 0.9 1.9 0.2 0.2 46.5 13.8 30.0 0.9 1.0 0.0 16.9 6.0 8.9 TangentConv [33] 40.9 83.9 63.9 33.4 15.4 83.4 90.8 15.2 2.7 16.5 12.1 79.5 49.3 58.1 23.0 28.4 8.1 49.0 35.8 28.5 PointASNL 46.8 87.4 74.3 24.3 1.8 83.1 87.9 39.0 0.0 25.1 29.2 84.1 52.2 70.6 34.2 57.6 0.0 43.9 57.8 36.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 .Figure 5 .Figure 6 .</head><label>456</label><figDesc>More examples on ScanNet datasets. More examples on SemanticKITTI datasets. Visualized results of AS module. (a) Sampled points via farthest point sampling (FPS). (b) Sampled points ajusted by AS module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Local-Nonlocal Module Conv Point Non- Local Cell Point Local Cell k-NN Grouping Key Points N?(3+Dl) Query Points Ns?(3+Dl) Aggregated Points Ns?(3+Dl+1)</head><label></label><figDesc></figDesc><table><row><cell>Sampled</cell><cell></cell><cell></cell></row><row><cell>Point</cell><cell cols="2">Ns?K?(3+Dl)</cell><cell>Ns?(3+Dmid)</cell></row><row><cell>Neighbors</cell><cell></cell><cell></cell></row><row><cell>Updated</cell><cell></cell><cell></cell></row><row><cell>Features</cell><cell></cell><cell></cell></row><row><cell>High-level</cell><cell>MLPs with Softmax</cell><cell></cell></row><row><cell>Relation</cell><cell></cell><cell></cell><cell>Ns?(3+Dmid)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ns?(3+Dmid)</cell></row><row><cell>Adaptive Shifting</cell><cell>Weighted Sum</cell><cell></cell></row><row><cell></cell><cell>3D coordinates</cell><cell cols="2">Features</cell><cell>Element-wise Sum</cell></row><row><cell></cell><cell></cell><cell cols="2">(b) Local-NonLocal Module</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Overall accuracy on ModelNet10 (M10) and ModelNet40 (M40) datasets. "pnt" stands for coordinates of point and "nor" stands for normal vector.</figDesc><table><row><cell>Method</cell><cell>input</cell><cell cols="3">#points M10 M40</cell></row><row><cell>O-CNN [39]</cell><cell>pnt, nor</cell><cell>-</cell><cell>-</cell><cell>90.6</cell></row><row><cell>SO-Net [19]</cell><cell>pnt, nor</cell><cell>2k</cell><cell cols="2">94.1 90.9</cell></row><row><cell>Kd-Net [15]</cell><cell>pnt</cell><cell>32k</cell><cell cols="2">94.0 91.8</cell></row><row><cell>PointNet++ [25]</cell><cell>pnt, nor</cell><cell>5k</cell><cell>-</cell><cell>91.9</cell></row><row><cell>SpiderCNN [47]</cell><cell>pnt, nor</cell><cell>5k</cell><cell>-</cell><cell>92.4</cell></row><row><cell>KPConv [34]</cell><cell>pnt</cell><cell>7k</cell><cell>-</cell><cell>92.9</cell></row><row><cell>SO-Net [19]</cell><cell>pnt, nor</cell><cell>5k</cell><cell cols="2">95.7 93.4</cell></row><row><cell>Pointwise CNN [12]</cell><cell>pnt</cell><cell>1k</cell><cell>-</cell><cell>86.1</cell></row><row><cell>ECC [30]</cell><cell>graphs</cell><cell>1k</cell><cell cols="2">90.8 87.4</cell></row><row><cell>PointNet [24]</cell><cell>pnt</cell><cell>1k</cell><cell>-</cell><cell>89.2</cell></row><row><cell>PAT [48]</cell><cell>pnt, nor</cell><cell>1k</cell><cell>-</cell><cell>91.7</cell></row><row><cell>Spec-GCN [36]</cell><cell>pnt</cell><cell>1k</cell><cell>-</cell><cell>91.8</cell></row><row><cell>PointGrid [18]</cell><cell>pnt</cell><cell>1k</cell><cell>-</cell><cell>92.0</cell></row><row><cell>PointCNN [20]</cell><cell>pnt</cell><cell>1k</cell><cell>-</cell><cell>92.2</cell></row><row><cell>DGCNN [42]</cell><cell>pnt</cell><cell>1k</cell><cell>-</cell><cell>92.2</cell></row><row><cell>PCNN [3]</cell><cell>pnt</cell><cell>1k</cell><cell cols="2">94.9 92.3</cell></row><row><cell>PointConv [44]</cell><cell>pnt, nor</cell><cell>1k</cell><cell>-</cell><cell>92.5</cell></row><row><cell>A-CNN [16]</cell><cell>pnt, nor</cell><cell>1k</cell><cell cols="2">95.5 92.6</cell></row><row><cell>Point2Sequence [21]</cell><cell>pnt</cell><cell>1k</cell><cell cols="2">95.3 92.6</cell></row><row><cell>RS-CNN [22]</cell><cell>pnt</cell><cell>1k</cell><cell>-</cell><cell>93.6</cell></row><row><cell>PointASNL</cell><cell>pnt</cell><cell>1k</cell><cell cols="2">95.7 92.9</cell></row><row><cell>PointASNL</cell><cell>pnt, nor</cell><cell>1k</cell><cell cols="2">95.9 93.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Segmentation results on indoor S3DIS and ScanNet datasets in mean per-class IoU (mIoU,%).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Semantic Segmentation results on SemanticKITTI, where pnt stands for coordinates of point and SPGraph<ref type="bibr" target="#b16">[17]</ref> uses all of points as input.</figDesc><table><row><cell>Method</cell><cell>input</cell><cell>mIoU(%)</cell></row><row><cell>PointNet [24]</cell><cell>50k pnt</cell><cell>14.6</cell></row><row><cell>SPGraph [17]</cell><cell>-</cell><cell>17.4</cell></row><row><cell>SPLATNet [31]</cell><cell>50k pnt</cell><cell>18.4</cell></row><row><cell>PointNet++ [25]</cell><cell>45k pnt</cell><cell>20.1</cell></row><row><cell>TangentConv [33]</cell><cell>120k pnt</cell><cell>40.9</cell></row><row><cell>PointASNL</cell><cell>8k pnt</cell><cell>46.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on ModelNet40 and ScanNet v2 validation set. PL, PNL and AS mean point local cell, point nonlocal cell and adaptive sampling.</figDesc><table><row><cell>Model</cell><cell>Ablation</cell><cell cols="2">ModelNet40 ScanNet</cell></row><row><cell>A</cell><cell>PNL only</cell><cell>90.1</cell><cell>45.7</cell></row><row><cell>B</cell><cell>PL only</cell><cell>92.0</cell><cell>56.1</cell></row><row><cell>C</cell><cell>PL+PNL</cell><cell>93.2</cell><cell>60.8</cell></row><row><cell>D</cell><cell>PL+PNL+AS</cell><cell>93.2</cell><cell>63.5</cell></row><row><cell>E</cell><cell>PointNet2 [24]</cell><cell>90.9</cell><cell>48.9</cell></row><row><cell>F</cell><cell>PointNet2+PNL</cell><cell>93.0</cell><cell>54.6</cell></row><row><cell>G</cell><cell>PointNet2+PNL+AS</cell><cell>92.8</cell><cell>55.4</cell></row><row><cell>H</cell><cell>DGCNN [42]</cell><cell>92.2</cell><cell>52.7</cell></row><row><cell>I</cell><cell>DGCNN+PNL</cell><cell>92.9</cell><cell>56.7</cell></row><row><cell>J</cell><cell>DGCNN+PNL+AS</cell><cell>93.1</cell><cell>58.3</cell></row><row><cell cols="4">caused by unmanned collection, our model can still predict</cell></row><row><cell>perfectly.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>The results (%) of four selection strategies on adaptive sampling. For a fair comparison, the number of neighbors is set to be equal in each layer between the two models.</figDesc><table><row><cell cols="2">Model RS FPS Average GF ModelNet40</cell></row><row><cell>A</cell><cell>87.9</cell></row><row><cell>B</cell><cell>91.5</cell></row><row><cell>C</cell><cell>92.3</cell></row><row><cell>D</cell><cell>93.2</cell></row><row><cell>Layer 1</cell><cell></cell></row><row><cell>Layer 2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 .</head><label>2</label><figDesc>The results (mIoU, (%)) on ScanNet v2 validation set with other model setting.</figDesc><table><row><cell>Model</cell><cell>input</cell><cell>grid sample deeper mIoU</cell></row><row><cell>A</cell><cell>8192 pnt</cell><cell>63.5</cell></row><row><cell>B</cell><cell>8192 pnt</cell><cell>64.5</cell></row><row><cell>C</cell><cell>10240 pnt</cell><cell>64.8</cell></row><row><cell>D</cell><cell>10240 pnt</cell><cell>66.4</cell></row><row><cell cols="3">sample point manifold. We give some examples of com-</cell></row><row><cell cols="2">parative visualization in</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 .</head><label>3</label><figDesc>Network Configurations.</figDesc><table><row><cell cols="4">Layer npoint nsample as neighbor</cell><cell>mlp</cell></row><row><cell>Task</cell><cell></cell><cell></cell><cell>Classification</cell><cell></cell></row><row><cell>1</cell><cell>512</cell><cell>32</cell><cell>12</cell><cell>[64,64,128]</cell></row><row><cell>2</cell><cell>128</cell><cell>64</cell><cell>12</cell><cell>[128,128,256]</cell></row><row><cell>3</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>[256,512,1024]</cell></row><row><cell>Task</cell><cell></cell><cell></cell><cell>Segmentation</cell><cell></cell></row><row><cell>1</cell><cell>1024</cell><cell>32</cell><cell>8</cell><cell>[32,32,64]</cell></row><row><cell>2</cell><cell>256</cell><cell>32</cell><cell>4</cell><cell>[64,64,128]</cell></row><row><cell>3</cell><cell>64</cell><cell>32</cell><cell>0</cell><cell>[128,128,256]</cell></row><row><cell>4</cell><cell>36</cell><cell>32</cell><cell>0</cell><cell>[256,256,512]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>The running time on ModelNet40 and ScanNet datasets. Part segmentation performance with part-avaraged IoU on ShapeNetPart.</figDesc><table><row><cell>Dataset</cell><cell>process</cell><cell>input</cell><cell>time (s/sample)</cell></row><row><cell cols="3">ModelNet40 Training 1024 pnt</cell><cell>0.00046</cell></row><row><cell>ScanNet</cell><cell cols="2">Training 8192 pnt</cell><cell>0.17611</cell></row><row><cell cols="3">ModelNet40 Inference 1024 pnt</cell><cell>0.00024</cell></row><row><cell>ScanNet</cell><cell cols="2">Inference 8192 pnt</cell><cell>0.11363</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Semantic segmentation results on the SemanticKITTI. 61.6 35.7 15.8 1.4 41.4 46.3 0.1 1.3 0.3 0.8 31.0 4.6 17.6 0.2 0.2 0.0 12.9 2.4 3.7 SPGraph [17] 17.4 45.0 28.5 0.6 0.6 64.3 49.3 0.1 0.2 0.2 0.8 48.9 27.2 24.6 0.3 2.7 0.1 20.8 15.9 0.8 SPLATNet [31] 18.4 64.6 39.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell><cell>road</cell><cell>sidewalk</cell><cell>parking</cell><cell>other-ground</cell><cell>building</cell><cell>car</cell><cell>truck</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>other-vehicle</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>fence</cell><cell>pole</cell><cell>traffic sign</cell></row><row><cell>PointNet [24]</cell><cell>14.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Supplementary material shows that with more sampled points and deeper structure, our PointASNL can still achieve further improvement to 66.6% on ScanNet benchmark.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Outlier analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="237" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE/CVF International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">4d spatiotemporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flexconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><forename type="middle">Pa</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pcpnet learning local shape properties from raw point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanir</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="75" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pere-Pau</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?lvar</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A-cnn: Annularly convolutional neural networks on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Komarichev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointgrid: A deep network for 3d shape understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9204" to="9214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Bin Fan, Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lpd-net: 3d point cloud learning for large-scale place recognition and environment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanzhe</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2831" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointcleannet: Learning to denoise and remove outliers from dense point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Julie</forename><surname>Rakotosaona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><surname>Vittorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barbera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vision</title>
		<meeting>IEEE Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08889</idno>
		<title level="m">Kpconv: Flexible and deformable convolution for point clouds</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="15607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph cnn for learning on point clouds</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Point cloud noise and outlier removal for image-based 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modeling point clouds with selfattention and gumbel subset sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3323" to="3332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pu-net: Point cloud upsampling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Method pIoU areo bag cap car chair ear guitar knife lamp laptop motor mug pistol rocket skate table phone board #shapes 2690</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Semantic segmentation results on S3DIS dataset evaluated on Area 5</title>
		<imprint/>
	</monogr>
	<note>Table 6</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Method OA mAcc mIoU ceiling floor wall beam column window door table chair sofa bookcase board clutter PointNet</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Semantic segmentation results on the S3DIS dataset with 6-fold cross validation. Method OA mAcc mIoU ceiling floor wall beam column window door table chair sofa bookcase board clutter</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
