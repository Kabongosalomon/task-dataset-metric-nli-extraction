<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bilinear Scoring Function Search for Knowledge Graph Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Bilinear Scoring Function Search for Knowledge Graph Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Automated machine learning</term>
					<term>Knowledge graph</term>
					<term>Neural architecture search</term>
					<term>Graph embedding !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning embeddings for entities and relations in knowledge graph (KG) have benefited many downstream tasks. In recent years, scoring functions, the crux of KG learning, have been human designed to measure the plausibility of triples and capture different kinds of relations in KGs. However, as relations exhibit intricate patterns that are hard to infer before training, none of them consistently perform the best on benchmark tasks. In this paper, inspired by the recent success of automated machine learning (AutoML), we search bilinear scoring functions for different KG tasks through the AutoML techniques. However, it is non-trivial to explore domain-specific information here. We first set up a search space for AutoBLM by analyzing existing scoring functions. Then, we propose a progressive algorithm (AutoBLM) and an evolutionary algorithm (AutoBLM+), which are further accelerated by filter and predictor to deal with the domain-specific properties for KG learning. Finally, we perform extensive experiments on benchmarks in KG completion, multi-hop query, and entity classification tasks. Empirical results show that the searched scoring functions are KG dependent, new to the literature, and outperform the existing scoring functions. AutoBLM+ is better than AutoBLM as the evolutionary algorithm can flexibly explore better structures in the same budget.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE knowledge graph (KG) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> is a graph in which the nodes represent entities, the edges are the relations between entities, and the facts are represented by triples of the form (head entity, relation, tail entity) (or (h, r, t) in short). The KG has been found useful in a lot of data mining and machine learning applications and tasks, including question answering <ref type="bibr" target="#b3">[4]</ref>, product recommendation <ref type="bibr" target="#b4">[5]</ref>, knowledge graph completion <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, multi-hop query <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and entity classification <ref type="bibr" target="#b8">[9]</ref>.</p><p>In a KG, plausibility of a fact (h, r, t) is given by f (h, r, t), where f is the scoring function. Existing f 's are customdesigned by human experts, and can be categorized into the following three families: (i) translational distance models (TDMs) <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref>, which model the relation embeddings as translations from the head entity embedding to the tail entity embedding; (ii) bilinear model (BLMs) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b18">[19]</ref>, which model the interaction between entities and relations by a bilinear product between the entity and relation embeddings; and (iii) neural network models (NNMs) <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b23">[24]</ref>, which use neural networks to capture the interaction. The scoring function can significantly impact KG learning performance <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Most TDMs are less expressive and have poor empirical performance <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b25">[26]</ref>. NNMs are powerful but have large numbers of parameters and may overfit the training triples. In comparison, BLMs ? Y. <ref type="bibr">Zhang</ref>  are more advantageous in that they are easily customized to be expressive, have linear complexities w.r.t. the numbers of entities/relations/dimensions, and have state-of-the-art performance <ref type="bibr" target="#b17">[18]</ref>. While a number of BLMs have been proposed, the best BLM is often dataset-specific. Recently, automated machine learning (AutoML) <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> has demonstrated its power in many machine learning tasks such as hyperparameter optimization (HPO) <ref type="bibr" target="#b28">[29]</ref> and neural architecture search (NAS) <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>. The models discovered have better performance than those designed by humans, and the amount of human effort required is significantly reduced. Inspired by its success, we propose in this paper the use of AutoML for the design of KGdependent scoring functions. To achieve this, one has to pay careful consideration to the three main components in an AutoML algorithm: (i) search space, which identifies important properties of the learning models to search; (ii) search algorithm, which ensures that finding a good model in this space is efficient; and (iii) evaluation method, which offers feedbacks to the search algorithm.</p><p>In this paper, we make the following contributions in achieving these goals: <ref type="bibr">?</ref> We design a search space of scoring functions, which includes all the existing BLMs. We further analyze properties of this search space, and provide conditions for a candidate scoring function to be expressive, degenerate, and equivalent to another. ? To explore the above search space properties and reduce the computation cost in evaluation, we design a filter to remove degenerate and equivalent structures, and a performance predictor with specifically-designed symmetryrelated features (SRF) to select promising structures. <ref type="bibr">?</ref> We customize a progressive algorithm (AutoBLM) and an evolutionary algorithm (AutoBLM+) that, together arXiv:2107.00184v2 [cs.AI] 4 Mar 2022</p><p>with the filter and performance predictor, allow flexible exploration of new BLMs. Extensive experiments are performed on the tasks of KG completion, multi-hop query and entity classification. The results demonstrate that the models obtained by AutoBLM and AutoBLM+ outperform the start-of-the-art humandesigned scoring functions. In addition, we show that the customized progressive and evolutionary algorithms are much less expensive than popular search algorithms (random search, Bayesian optimization and reinforcement learning) in finding a good scoring function.</p><p>Differences with the Conference Version. Compared to the preliminary version published in ICDE 2020 <ref type="bibr" target="#b32">[33]</ref>, we have made the following important extensions: 1) Theory. We add new analysis to the designed search space based on bilinear models. We theoretically prove when the candidates in the search space can be expressive (Section 3.2), degenerate (Section 3.4.1) and equivalent structures (Section 3.4.2). 2) Algorithm. We extend the search algorithm with the evolutionary algorithm (Section 4.4), i.e., AutoBLM+. The evolutionary strategy in Algorithm 4 can explore better in the search space, and can also leverage the filter and predictor to deal with the domain-specific properties. 3) Tasks. We extend AutoBLM and AutoBLM+ to two new tasks, namely, multi-hop query (Section 5.2) and entity classification in (Section 5.3). We show that the search problem can be well adopted to these new scenarios, and achieve good empirical performance. 4) Ablation Study. We conduct more experiments on the performance (Section 5. Notations. In this paper, vectors are denoted by lowercase boldface, and matrix by uppercase boldface. The important notations are listed in <ref type="table" target="#tab_1">Table 1</ref>. </p><formula xml:id="formula_0">:= d i=1 a i b i c i = a diag(b) c v 1 1 -norm of vector v Re(v)</formula><p>real part of complex vector v ? C d v conjugate of complex vector v ? C d 2 BACKGROUND AND RELATED WORKS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Scoring Functions for Knowledge Graph (KG)</head><p>A knowledge graph (KG) can be represented by a thirdorder tensor G ? R |E|?|R|?|E| , in which G hrt = 1 if the corresponding triple (h, r, t) exists in the KG, and 0 otherwise. The scoring function f (h, r, t) : E ? R ? E ? R measures plausibility of the triple (h, r, t). As introduced in ancestorOf, isPartOf f (t, r, h) = ?f (h, r, t) general asymmetry locatedIn, profession f (t, r, h) = f (h, r, t)</p><p>inverse hypernym, hyponym f (t, r, h) = f (h, r , t) Section 1, it is desirable for a scoring function to be able to represent any of the symmetric, anti-symmetric, general asymmetric and inverse KG relations in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Definition 1 (Expressiveness <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>). A scoring function is fully expressive if for any KG G and the corresponding tensor G ? R |E|?|R|?|E| , one can find an instantiation f of the scoring function such that f (h, r, t) = G hrt , ?h, t ? E, r ? R.</p><p>Not all scoring functions are fully expressive. For example, consider a KG with two people A, B, and a relation "OlderThan". Obviously, we can have either (A, OlderThan, B) or (B, OlderThan, A), but not both. The scoring function f (h, r, t) = h, r, t = d i=1 h i r i t i , where h, r, t are ddimensional embeddings of h, r and t, respectively, cannot be fully expressive since f (h, r, t) = f (t, r, h).</p><p>On the other hand, while expressiveness indicates the ability of f to fit a given KG, it may not generalize well when inference on different KGs. As real-world KGs can be very sparse <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, a scoring function with a large amount of trainable parameters may overfit the training triples. Hence, it is also desirable that the scoring function has only a manageable number of parameters.</p><p>In the following, we review the three main types of scoring functions, namely, translational distance model (TDM), neural network model (NNM), and biLinear model (BLM). As will be seen, many TDMs (such as TransE <ref type="bibr" target="#b9">[10]</ref> and TransH <ref type="bibr" target="#b10">[11]</ref>) cannot model the symmetric relations well <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Neural network models, though fully expressive, have large numbers of parameters. This not only prevents the model from generalizing well on unobserved triples in a sparse KG, but also increases the training and inference costs <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>. In comparison, BLMs (except DistMult) can model all relation pattens in <ref type="table" target="#tab_2">Table 2</ref> and are fully expressive. Besides, these models (except RESCAL and TuckER) have moderate complexities (with the number of parameters linear in |E|, |R| and d). Therefore, we consider BLM as a better choice, and it will be our focus in this paper.</p><p>Translational Distance Model (TDM). Inspired by analogy results in word embeddings <ref type="bibr" target="#b36">[37]</ref>, scoring functions in TDM take the relation r as a translation from h to t. The most representative TDM is TransE <ref type="bibr" target="#b9">[10]</ref>, with f (h, r, t) = ? t ? (h + r) <ref type="bibr" target="#b0">1</ref> . In order to handle one-to-many, many-to-one and many-to-many relations, TransH <ref type="bibr" target="#b10">[11]</ref> and TransR <ref type="bibr" target="#b11">[12]</ref> introduce additional vectors/matrices to map the entities to a relation-specific hyperplane. The more recent RotatE <ref type="bibr" target="#b12">[13]</ref> treats the relations as rotations in a complex-valued space: f (h, r, t) = ? t ? h ? r 1 , where h, r, t ? C d and ? is the Hermitian product <ref type="bibr" target="#b13">[14]</ref>. As discussed in <ref type="bibr" target="#b33">[34]</ref>, most TDMs are not fully expressive. For example, TransE and TransH cannot model symmetric relations.</p><p>Neural Network Model (NNM). NNMs take the entity  <ref type="bibr" target="#b14">[15]</ref> Re(h ? r ?t)</p><formula xml:id="formula_1">? ? O (|E|d + |R|d) Analogy [16] ? ,r,t + Re(h ? r ?t) ? ? O (|E|d + |R|d) SimplE [17]/CP [18] ? ,r, t + h, r,t ? ? O (|E|d + |R|d) QuatE [19] h r t ? ? O (|E|d + |R|d) TuckER [35] W ? 1 h ? 2 r ? 3 t ? ? O |E|d + |R|d + d 3</formula><p>and relation embeddings as input, and output a probability for the triple (h, r, t) using a neural network. Earlier works are based on multilayer perceptrons <ref type="bibr" target="#b19">[20]</ref> and neural tensor networks <ref type="bibr" target="#b37">[38]</ref>. More recently, ConvE <ref type="bibr" target="#b20">[21]</ref> uses the convolutional network to capture interactions among embedding dimensions. By sampling relational paths <ref type="bibr" target="#b38">[39]</ref> from the KG, RSN <ref type="bibr" target="#b21">[22]</ref> and Interstellar <ref type="bibr" target="#b39">[40]</ref> use the recurrent network <ref type="bibr" target="#b40">[41]</ref> to recurrently combine the head entity and relation with a step-wise scoring function. As the KG is a graph, R-GCN <ref type="bibr" target="#b22">[23]</ref> and CompGCN <ref type="bibr" target="#b23">[24]</ref> use the graph convolution network <ref type="bibr" target="#b8">[9]</ref> to aggregate entity-relation compositions layer by layer. Representations at the final layer are then used to compute the scores. Because of the use of an additional neural network, NNM requires more parameters and has larger model complexity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BiLinear Model (BLM</head><formula xml:id="formula_2">, t ? C d with f (h, r, t) = Re h diag(r)t = Re (h ? r ?t),</formula><p>where ? is the Hermitian product in complex space <ref type="bibr" target="#b13">[14]</ref>. HolE <ref type="bibr" target="#b14">[15]</ref> uses the circular correlation instead of the dot product, but is shown to be equivalent to ComplEx <ref type="bibr" target="#b41">[42]</ref>. Analogy <ref type="bibr" target="#b15">[16]</ref> decomposes the head embedding h into a real part? ? Rd and a complex part h ? C d . Relation embedding r (resp. tail embedding t) is similarly decomposed into a real partr (resp.t) and a complex part r (resp. t). f is then written as: f (h, r, t) = ? ,r,t + Re h ? r ?t , which can be regarded as a combination of DistMult and ComplEx. To simultaneously model the forward triplet (h, r, t) and its inverse (t, r , h), SimplE <ref type="bibr" target="#b16">[17]</ref> / CP <ref type="bibr" target="#b17">[18]</ref> similarly splits the embeddings to a forward part (?,r,t ? R d ) and a backward part (h, r, t ? R d ): f (h, r, t) = ? ,r, t + t, r,? . To allow more interactions among embedding dimensions, the recent QuatE <ref type="bibr" target="#b18">[19]</ref> uses embeddings in the hypercomplex space (h, r, t ? H d ) to model f (h, r, t) = h r t where is the Hamilton product. By using the Tucker decomposition <ref type="bibr" target="#b42">[43]</ref>, TuckER <ref type="bibr" target="#b34">[35]</ref> proposes a generalized bilinear model and introduces more parameters in the core tensor W ? R d?d?d : f (h, r, t) = W ? 1 h ? 2 r ? 3 t, where ? i is the tensor product along the ith mode. A summary of these BLMs is in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Common Learning Tasks in KG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">KG Completion</head><p>KG is naturally incomplete <ref type="bibr" target="#b0">[1]</ref>, and KG completion is a representative task in KG learning <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Scores on the observed triples are maximized, while those on the non-observed triplets are minimized. After training, new triples can be added to the KG by entity prediction with either a missing head entity (?, r, t) or a missing tail entity (h, r, ?) <ref type="bibr" target="#b2">[3]</ref>. For each kind of query, we enumerate all the entities e ? E and compute the corresponding scores f (e, r, t) or f (h, r, e). Entities with larger scores are more likely to be true facts. Most of the models in Section 2.1 can be directly used for KG completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Multi-hop Query</head><p>In KG completion, we predict queries (h, r, ?) with length one, i.e., 1-hop query. In practice, there can be multi-hop queries with lengths larger than one <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b38">[39]</ref>. For example, one may want to predict "who is the sister of Tony's mother". To solve this problem, we need to solve the length-2 query problem (?, sister ? mother, T ony) with the relation composition operator ?.</p><p>Given the KG G = {E, R, S}, let ? r , corresponding to the relation r ? R, be a binary function E ?E ? {T rue, F alse}. The multi-hop query is defined as follows.</p><p>Definition 2 (Multi-hop query <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>). The multi-hop query (e 0 , r 1 ? r 2 ? ? ? ? ? r L , e ? ) with length L &gt; 1 is defined as ?e 1 . . . e L?1 , e ? : ? r1 (e 0 , e 1 )?? r2 (e 1 , e 2 )?. . .?? r L (e L?1 , e ? ) where ? is the conjunction operation, e 0 is the starting entity, e ? is the entity to predict, and e 1 . . . e L?1 are intermediate entities that connect the conjunctions.</p><p>Similar to KG completion, plausibility of a query (e 0 , r 1 ? r 2 ? ? ? ? ? r L , e L ) is measured by a scoring function <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b38">[39]</ref>:</p><formula xml:id="formula_3">f (e 0 , r 1 ? r 2 ? ? ? ? ? r L , e L ) = e 0 R (r1) ? ? ? R (r L ) e L ,<label>(1)</label></formula><p>where R (ri) is a relation-specific matrix of the ith relation.</p><p>The key is on how to model the composition of relations in the embedding space. Based on TransE <ref type="bibr" target="#b9">[10]</ref>, TransE-Comp <ref type="bibr" target="#b38">[39]</ref> models the composition operator as addition, and defines the scoring function as f (e 0 , r 1 ? ? ? ? ? r L , e L ) = ? e L ? (e 0 + r 1 + ? ? ? + r L ) 1 . Diag-Comp <ref type="bibr" target="#b38">[39]</ref> uses the multiplication operator in DistMult <ref type="bibr" target="#b6">[7]</ref> to define f (e 0 , r 1 ? ? ? ? ? r L , e L ) = e 0 D r1 ? ? ? D r L e L , where D ri = diag(r i ). Following RESCAL <ref type="bibr" target="#b5">[6]</ref>, GQE <ref type="bibr" target="#b7">[8]</ref> performs the composition with a product of relational matrices {R (r1) , . . . , R (r L ) }, as:</p><p>f (e 0 , r 1 ? ? ? ? ? r L , e L ) = e 0 R (r1) ? ? ? R (r L ) e L . More recently, Query2box <ref type="bibr" target="#b3">[4]</ref> models the composition of relations as a projection of box embeddings and defines an entity-to-box distance to measure the score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Entity Classification</head><p>Entity classification aims at predicting the labels of the unlabeled entities. Since the labeled entities are few, a common approach is to use a graph convolutional network (GCN) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b43">[44]</ref> to aggregate neighborhood information. The GCN operates on the local neighborhoods of each entity and aggregates the representations layer-by-layer as:</p><formula xml:id="formula_4">e +1 i = ? W 0 e i + j:(i,r,j)?Stra W e j ,</formula><p>where S tra contains all the training triples, ? is the activation function, e i , e j are the layer-representations of i and the neighboring entities j, respectively, and W 0 , W ? R d?d are weighting matrices sharing across different entities in the th layer.</p><p>GCN does not encode relations in edges. To alleviate this problem, R-GCN <ref type="bibr" target="#b22">[23]</ref> and CompGCN <ref type="bibr" target="#b23">[24]</ref> encode relation r and entity j together by a composition function ?:</p><formula xml:id="formula_5">e +1 i = ? W 0 e i + (r,j):(i,r,j)?Stra W ?(e j , r ) ,</formula><p>where r is the representation of relation r at the th layer. The composition function ?(e j , r ) can significantly impact performance <ref type="bibr" target="#b23">[24]</ref>. R-GCN uses the composition operator in RESCAL <ref type="bibr" target="#b5">[6]</ref>, and defines ?(e j , r ) = R (r) e j , where R (r) is a relation-specific weighting matrix in the th layer. CompGCN, following TransE <ref type="bibr" target="#b9">[10]</ref>, DistMult <ref type="bibr" target="#b6">[7]</ref> and HolE <ref type="bibr" target="#b14">[15]</ref>, defines three operators: subtraction ?(e j , r ) = e j ? r , multiplication ?(e j , r ) = e j ? r where ? is the element-wise product, and circular correlation ?(e j , r ) =</p><formula xml:id="formula_6">e j r where [a b] k = d i=1 a i b k+i?1 mod d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Automated Machine Learning (AutoML)</head><p>Recently, automated machine learning (AutoML) <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> has demonstrated its advantages in the design of better machine learning models. AutoML is often formulated as a bi-level optimization problem <ref type="bibr" target="#b44">[45]</ref>, in which model parameters are updated from the training data in the inner loop, while hyper-parameters are tuned from the validation data in the outer loop. There are three important components in AutoML <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b45">[46]</ref>: 1) Search space: This identifies important properties of the learning models to search. The search space should be large enough to cover most manually-designed models, while specific enough to ensure that the search will not be too expensive. 2) Search algorithm: A search algorithm is used to search for good solutions in the designed space. Unlike convex optimization problems, there is no universally efficient optimization tool. 3) Evaluation: Since the search aims at improving performance, evaluation is needed to offer feedbacks to the search algorithm. The evaluation procedure should be fast and the signal should be accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Neural Architecture Search (NAS)</head><p>Recently, a variety of NAS algorithms have been developed to facilitate efficient search of deep networks <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>. They can generally be divided into model-based approach and sample-based approach <ref type="bibr" target="#b26">[27]</ref>. The modelbased approach builds a surrogate model for all candidates in the search space, and selects candidates with promising performance using methods such as Bayesian optimization <ref type="bibr" target="#b28">[29]</ref>, reinforcement learning <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b46">[47]</ref>, and gradient descent <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b47">[48]</ref>. It requires evaluating a large number of architectures for training the surrogate model or requires a differentiable objective w.r.t. the architecture. The samplebased approach is more flexible and explores new structures in the search space by using heuristics such as progressive algorithm <ref type="bibr" target="#b48">[49]</ref> and evolutionary algorithm <ref type="bibr" target="#b49">[50]</ref>. As for evaluation, parameter-sharing <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref> allows faster architecture evaluation by combining architectures in the whole search space with the same set of parameters. However, the obtained results can be sensitive to initialization, which hinders reproducibility. On the other hand, stand-alone methods <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> train and evaluate the different models separately. They are slower but more reliable. To improve its efficiency, a predictor can be used to select promising architectures <ref type="bibr" target="#b48">[49]</ref> before it is fully trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AUTOMATED BILINEAR MODEL</head><p>In the last decade, KG learning has been improving with new scoring function designs. However, as different KGs may have different properties, it is unclear how a proper scoring function can be designed for a particular KG. This raises the question: Can we automatically design a scoring function for a given KG? To address this question, we first provide a unified view of BLMs, and then formulate the design of scoring function as an AutoML problem: AutoBLM ("automated bilinear model").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Unified View of BLM</head><p>Recall from Section 2.1 that a BLM may operate in the real/complex/hypercomplex space. To write the different BLMs in the same form, we first unify them to the same representation space. The idea is to partition each of the embeddings h, r, t to K = 4 equal-sized chunks, as h = [h 1 , . . . , h 4 ], r = [r 1 , . . . , r 4 ] and t = [t 1 , . . . , t 4 ]. The BLM is then written in terms of { h i , r j , t k } i,j,k?{1,...,4} .</p><p>? DistMult <ref type="bibr" target="#b6">[7]</ref>, which uses f (h, r, t) = h, r, t . We simply split h ? R d (and analogously r and t) into 4 parts as</p><formula xml:id="formula_7">{h 1 , h 2 , h 3 , h 4 }, where h i ? R d/4 for i = 1, 2, 3, 4. Obviously, h, r, t = h 1 , r 1 , t 1 + h 2 , r 2 , t 2 + h 3 , r 3 , t 3 + h 4 , r 4 , t 4 .</formula><p>? SimplE <ref type="bibr" target="#b16">[17]</ref> / CP <ref type="bibr" target="#b17">[18]</ref>, which uses f (h, r, t) = ? ,r, t + t, r,? . We split? ? R d (and analogouslyr andt) into 2 parts as {h 1 , h 2 } (where h 1 , h 2 ? R d/2 ), and similarly h as {h 3 , h 4 } (and analogously r and t). Then,</p><formula xml:id="formula_8">? ,r, t + t, r,? = h 1 , r 1 , t 3 + h 2 , r 2 , t 4 + h 3 , r 3 , t 1 + h 4 , r 4 , t 2 . -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( (a) DistMult. . ( . ( . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( (b) SimplE. -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( (c) ComplEx. -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( (d) Analogy. . ( . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . '</formula><p>. ( (e) QuatE. <ref type="figure">Fig. 1</ref>. The forms of R (r) for representative BLMs (best viewed in color). Different colors correspond to different parts of [r 1 , r 2 , r 3 , r 4 ] (red for r 1 , blue for r 2 , yellow for r 3 , gray for r 4 ). Solid lines mean positive values, while dashed lines mean negative values. The empty parts have value zero.</p><p>? ComplEx <ref type="bibr" target="#b13">[14]</ref> / HolE <ref type="bibr" target="#b14">[15]</ref>, which uses f (h, r, t) = Re(h ? r ?t), where h, r,t are complex-valued. Recall that any</p><formula xml:id="formula_9">complex vector v ? C d is of the form v r + iv i , where v r ? R d is the real part and v i ? R d is the imaginary part.</formula><p>Thus,</p><formula xml:id="formula_10">Re(h ? r ?t) = h r , r r , t r + h i , r r , t i + h r , r i , t i ? h i , r i , t r . (2)</formula><p>We split h r ? R d (and analogously r r and t r ) into</p><formula xml:id="formula_11">2 parts {h 1 , h 2 } (where h 1 , h 2 ? R d/2 )</formula><p>, and similarly h i = {h 3 , h 4 } (and analogously r i and t i ). Then,</p><formula xml:id="formula_12">Re(h ? r ?t) = h 1 ,r 1 ,t 1 + h 2 ,r 2 ,t 2 + h 3 ,r 1 ,t 3 + h 4 ,r 2 ,t 4 + h 1 ,r 3 ,t 3 + h 2 ,r 4 ,t 4 ? h 3 ,r 3 ,t 1 ? h 4 ,r 4 ,t 2 .</formula><p>? Analogy <ref type="bibr" target="#b15">[16]</ref>, which uses f (h, r, t) = ? ,r,t +Re h?r? t . We split? ? R d (and analogouslyr andt) into 2 parts</p><formula xml:id="formula_13">{h 1 , h 2 } (where h 1 , h 2 ? R d/2 )</formula><p>, and similarly h ? C d/2 (and analogously r and t) into 2 parts</p><formula xml:id="formula_14">{h 3 , h 4 } (where h 3 , h 4 ? R d/2 ). Then, ? ,r,t + Re h ? r ?t = h 1 , r 1 , t 1 + h 2 , r 2 , t 2 + h 3 , r 3 , t 3 + h 3 , r 4 , t 4 + h 4 , r 3 , t 4 ? h 4 , r 4 , t 3 .</formula><p>? QuatE <ref type="bibr" target="#b18">[19]</ref>, which uses f (h, r, t) = h r t. Recall that</p><formula xml:id="formula_15">any hypercomplex vector v ? H d is of the form v 1 + iv 2 + jv 3 + kv 4 , where v 1 , v 2 , v 3 , v 4 ? R d . Thus, h r t = h 1 , r 1 , t 1 ? h 1 , r 2 , t 2 ? h 1 , r 3 , t 3 ? h 1 , r 4 , t 4 + h 2 , r 2 , t 1 + h 2 , r 1 , t 2 + h 2 , r 4 , t 3 ? h 2 , r 3 , t 4 + h 3 , r 3 , t 1 ? h 3 , r 4 , t 2 + h 3 , r 1 , t 3 + h 3 , r 2 , t 4 + h 4 , r 4 , t 1 + h 4 , r 3 , t 2 ? h 4 , r 2 , t 3 + h 4 , r 1 , t 4 .</formula><p>As h i , r k , t j = h i diag(r k ) t j , all the above BLMs can be written in the form of a bilinear function</p><formula xml:id="formula_16">h R (r) t,<label>(3)</label></formula><formula xml:id="formula_17">where 1 h = [h 1 , . . . , h 4 ] , t = [t 1 ;</formula><p>. . . ; t 4 ] ? R d , and R (r) ? R d?d is a matrix with 4 ? 4 blocks, each block being either 0, ?diag(r 1 ) , . . . , or ?diag(r 4 ). <ref type="figure">Figure 1</ref> shows graphically the R (r) for the BLMs considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unified Bilinear Model</head><p>Using the above unified representation, the design of BLM becomes designing R (r) in <ref type="formula" target="#formula_16">(3)</ref>.</p><formula xml:id="formula_18">Definition 3 (Unified BiLinear Model). The desired scoring function is of the form f A (h, r, t) = K i,j=1 sign(A ij ) h i , r |Aij | , t j ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_19">A ? {0, ?1, . . . , ?K} K?K (5)</formula><p>is called the structure matrix. Here, we define r 0 ? 0, and sign(0) = 0.</p><p>It can be easily seen that this covers all the BLMs in Section 3.1 when K = 4. Let g K (A, r) be a matrix with K ? K blocks, with its (i, j)-th block:</p><formula xml:id="formula_20">[g K (A, r)] ij = sign(A ij ) ? diag(r |Aij | ).<label>(6)</label></formula><p>The form in (4) can be written more compactly as</p><formula xml:id="formula_21">f A (h, r, t) = h g K (A, r)t.<label>(7)</label></formula><p>A graphical illustration is shown in <ref type="figure" target="#fig_3">Figure 2</ref>.</p><formula xml:id="formula_22">1 2 # 0 ? ? [&amp; ! ] "" [&amp; ! ] "# ? [&amp; ! ] "! [&amp; ! ] #" [&amp; ! ] ## ? [&amp; ! ] #! ? ? ? ? [&amp; ! ] !" [&amp; ! ] !# ? [&amp; ! ] !! ( " ( # ? ( ! ) " $ ) # $ ? ) ! $ ? ? ? ? ? ? ? ? ? , %&amp; ? ? ? ? ? -" -# ? -! % * ( , ) ? Fig. 2. A graphical illustration of the proposed form of f A (h, r, t).</formula><p>The following Proposition gives a necessary and sufficient condition for the BLM with scoring function in <ref type="bibr" target="#b6">(7)</ref> to be fully expressive. The proof is in Appendix A.1.</p><formula xml:id="formula_23">Proposition 1. Let C ? {r ? R K | r = 0, r[i] ? {0, ?1, . . . , ?K}, i = 1, . . . , K}.<label>(8)</label></formula><p>Given an A in (5), the bilinear model with scoring function <ref type="formula" target="#formula_21">(7)</ref> is fully expressive if 1) ?r ? C such that g K (A,r) is symmetric (i.e., g K (A,r) = g K (A,r)), and 2) ?? ? C such that g K (A,?) is skew-symmtric (i.e., g K (A,?) = ?g K (A,?)). <ref type="table">Table 4</ref> shows examples ofr and? for the existing BLMs (ComplEx, HolE, Analogy, SimplE, CP, and QuatE), thus justifying that they are fully expressive. <ref type="bibr">TABLE 4</ref> Exampler (resp.?) for the two conditions in Proposition 1.</p><formula xml:id="formula_24">modelr? ComplEx / HolE [1, 2, 0, 0] [0, 0, 3, 4] Analogy [1, 2, 3, 0] [0, 0, 0, 4] SimplE / CP [1, 2, 1, 2] [1, 2, ?1, ?2] QuatE [1, 0, 0, 0] [0, 2, 3, 4]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Searching for BLMs</head><p>Using the family of unified BLMs in Definition 3 as the search space A for structure matrix A, the search for a good data-specific BLM can be formulated as the following AutoML problem.</p><p>Definition 4 (Bilinear Model Search (AutoBLM)). Let F (P ; A) be a KG embedding model (where P includes the entity embedding matrix E and relation embedding matrix R, and A is the structure matrix) and M (F, S) be the performance measurement of F on triples S (the higher the better). The AutoBLM problem is formulated as:</p><formula xml:id="formula_25">A * ? Arg max A?A M (F (P * ; A), S val ) (9) s.t. P * = arg max P M (F (P ; A), S tra ) ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_26">A = {A = [A ij ] ? R K?K | A ij ? {0, ?1, . . . , ?K} ?i, j = 1, . . . , K},<label>(11)</label></formula><p>contains all the possible choices of A, S tra is the training set, and S val is the validation set.</p><p>As a bi-level optimization problem, we first train the model to obtain P * (converged model parameters) on the training set S tra by <ref type="bibr" target="#b9">(10)</ref>, and then search for a better A (and consequently a better relation matrix g K (A, r)) based on its performance M on the validation set S val in <ref type="bibr" target="#b8">(9)</ref>. Note that the objectives in (9) and (10) are non-convex, and the search space is large (with (2K + 1) K 2 candidates, as can be seen from <ref type="formula">(5)</ref>). Thus, solving (10) can be expensive and challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Degenerate and Equivalent Structures</head><p>In this section, we introduce properties specific to the proposed search space A. A careful exploitation of these would be key to an efficient search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Degenerate Structures</head><p>Obviously, not all structure matrices in (5) are equally good. For example, if all the nonzero blocks in g K (A, r) are in the first column, f A will be zero for all head embeddings with h 1 = 0. These structures should be avoided.</p><p>Definition 5 (Degenerate structure). Matrix A is degenerate if (i) there exists h = 0 such that h g K (A, r)t = 0, ?r, t; or (ii) there exists r = 0 such that h g K (A, r)t = 0, ?h, t.</p><p>With a degenerate A, the triple (h, r, t) is always nonplausible for every nonzero head embedding h or relation</p><formula xml:id="formula_27">-" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( (a) Analogy. -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . '</formula><p>. ( (b) Permuting rows and columns.  <ref type="figure" target="#fig_1">Figure 3</ref>(a)) and three example equivalent structures based on the conditions in Proposition 3. <ref type="figure" target="#fig_1">Figure 3</ref>(b) permutes the index <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> of rows and columns in A to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>; <ref type="figure" target="#fig_1">Figure 3</ref>(c) permutes the values <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> in A to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>; <ref type="figure" target="#fig_1">Figure 3</ref>(d) flips the signs of values <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> </p><formula xml:id="formula_28">-" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( (c) Permuting values. -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( -" &amp; -# &amp; -' &amp; -( &amp; . " . # . ' . ( (d) Flipping signs.</formula><formula xml:id="formula_29">in A to [?1, 2, ?3, 4].</formula><p>embedding r, which limits expressiveness of the scoring function. The following Proposition shows that it is easy to check whether A is degenerate. Its proof is in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 2. A is not degenerate if and only if rank(</head><formula xml:id="formula_30">A) = K and {1, . . . , K} ? {|A ij | : i, j = 1, . . . , K}.</formula><p>Since K is very small (which is equal to 4 here), the above conditions are inexpensive to check. Hence, we can efficiently filter out degenerate A's and avoid wasting time in training and evaluating these structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Equivalence</head><p>In general, two different A's can have the same performance (as measured by F in Definition 4). This is captured in the following notion of equivalence. If a group of A's are equivalent, we only need to evaluate one of them. Definition 6 (Equivalence). Let P * = arg max P M (F (P ; A), S) and P * = arg max P M (F (P ; A ), S).</p><formula xml:id="formula_31">If A = A but M F (P * ; A), S = M F (P * ; A ), S for all S, then A is equivalent to A (denoted A ? A ).</formula><p>The following Proposition shows several conditions for two structures to be equivalent. Its proof is in Appendix A.3. Examples are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. (iii) Flipping signs: There exists a sign vector</p><formula xml:id="formula_32">Proposition 3. Given an A in (5), construct ? A ? R K?K 2 such that [? A ] |Aij |,(i?1)K+j = sign(A ij ) if |A ij | ? {1,</formula><formula xml:id="formula_33">s ? {?1} K such that [? A ] i,? = s i ? [? A ] i,? , ?i = 1, . . . , K.</formula><p>There are K! possible permutation matrices for conditions (i) and (ii), and 2 K possible sign vectors for condition (iii). Hence, one has to check a total of (K!) 2 2 K combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SEARCH ALGORITHM</head><p>In this section, we design efficient algorithms to search for the structure matrix A in <ref type="bibr" target="#b4">(5)</ref>. As discussed in Section 2.3.1, the model-based approach requires a proper surrogate model for such a complex space. Thus, we will focus on the sample-based approach, particularly on the progressive algorithm and evolutionary algorithm. To search efficiently, one needs to (i) ensure that each new A is neither degenerate nor equivalent to an already-explored structure; and (ii) the scoring function f A (h, r, t) obtained from the new A is likely to have high performance. These can be achieved by designing an efficient filter (Section 4.1) and performance predictor (Section 4.2). Then, we introduce two search algorithms: progressive search (Section 4.3) and evolutionary algorithm (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Filtering Degenerate and Equivalent Structures</head><p>Algorithm 1 shows the filtering procedure. First, step 2 removes degenerate structure matrices by using the conditions in Proposition 2.</p><p>Step 3 then generates a set of (K!) 2 2 K structures that are equivalent to A (Proposition 3). A is filtered out if any of its equivalent structures appears in the set H containing structure matrices that have already been explored. As K is small, this filtering cost is very low compared with the cost of model training in <ref type="bibr" target="#b9">(10)</ref>. Algorithm 1 Filtering degenerate and equivalent structure matrices. The output is "False" if the input structure matrix A is to be filtered out. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Predictor</head><p>After collecting N structures in H, we construct a predictor P to estimate the goodness of each A. As mentioned in Section 2.3, search efficiency depends heavily on how to evaluate the candidate models.</p><p>A highly efficient approach is parameter sharing, as is popularly used in one-shot neural architecture search (NAS) <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b46">[47]</ref>. However, parameter sharing can be problematic when used to predict the performance of scoring functions. Consider the following two A's:</p><formula xml:id="formula_34">(i) A 1 is a 4 ? 4 matrix of all +1's, and so f A1 (h, r, t) = 4 i,j=1 h i , r 1 , t j , and (ii) A 2 is a 4 ? 4 matrix of all ?1's, and so f A2 (h, r, t) = ? 4 i,j=1 h i , r 1 , t j = ?f A1 (h, r, t)</formula><p>. When parameter sharing is used, it is likely that the performance predictor will output different scores for A 1 and A 2 . However, from Proposition 3, by setting s = [?1, ?1, ?1, ?1] in condition (iii), we have A 1 ? A 2 and thus they indeed have the same performance. This problem will also be empirically demonstrated in Section 5.1.8. Hence, instead, we train and evaluate the models separately as in the stand-alone NAS evaluation <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Recall from Section 2.1 that it is desirable for the scoring function to be fully expressive. Proposition 1 shows that this requires looking for ar ? C such that g K (A,r) is symmetric and a? ? C such that g K (A,?) is skew-symmetric. This motivates us to examine each of the (2K + 1) K ? 1 r's in C (defined in <ref type="bibr" target="#b7">(8)</ref>) and see whether it leads to a symmetric or skew-symmetric g K (A, r). However, directly using all these (2K + 1) K ? 1 choices as features to a performance predictor can be computationally expensive. Instead, empirically we find that the following two features can be used to group the scoring functions: (i) number of zeros in r: |{i ? {1, . . . , K} : r i = 0}|; and (ii) number of nonzero absolute values in r: |{j &gt; 0 : r i = j or r i = ?j, i ? {1, . . . , K}}|. The possible choices is reduced to K(K + 1)/2 (groups of scoring functions). We keep two symmetry-related feature (SRF) as ? and ?. If g K (A, r) is symmetric (resp. skew-symmetric) for any r in C, the entry in ? (resp. ?) corresponding to r is set to 1. The construction process is also shown in Algorithm 2. Finally, the SRF vector is composed with vec(?) and vec(?), which vectorize the values in ? and ?, and fed as input to a two-layer MLP for performance prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Progressive Algorithm</head><p>To explore the search space A in <ref type="bibr" target="#b10">(11)</ref>, the simplest approach is by direct sampling. However, it can be expensive as the space is large. Note from (4) that the complexity of f A (h, r, t) is controlled by the number of nonzero elements in A. Inspired by <ref type="bibr" target="#b48">[49]</ref>, we propose in this section a progressive algorithm that starts with A's having only a few nonzero elements, and then gradually expands the search space by allowing more nonzeros.</p><p>The procedure, which is called AutoBLM, is in Algorithm 3. Let A (b) be an A with b nonzero elements, and Input: I: number of top structures; N : number of generated structures; P : number of structures selected by P; b0: number of nonzero elements in initial structures; filter Q and performance predictor P.</p><formula xml:id="formula_35">1: initialization: b := b0, create a candidate set H b = ?; 2: for each A (b) ? {A (b) } do 3: if Q(A (b) , H b ) from Algorithm 1 is true then H b ? H b ? {A (b) }; 4: if |H b | = I, break loop; 5: end for 6: train and evaluate all A (b) 's in H b ; 7: add A (b) 's to T b and record the performance in Y b ; 8: update predictor P with records in (T b , Y b ). 9: repeat 10: b := b + 1;</formula><p>11:</p><formula xml:id="formula_36">H b = ?;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>repeat <ref type="bibr">13:</ref> randomly select a top-</p><formula xml:id="formula_37">I structure A b?1 ? T b?1 ; 14: randomly generate i b , j b , k b ? {1, . . . , K}, s b ? {?1}, and form A (b) with f A (b) ? f A b?1 + s b hi b , r k b , tj b ; 15: if Q(A (b) , H b ? T b ) from Algorithm 1 is true then H b ? H b ? {A (b) }; 16: until H b = N 17: select top-P A (b) 's in H b based on the predictor P;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18:</head><p>train embeddings and evaluate the performance ofA (b) 's; <ref type="bibr">19:</ref> add A (b) 's in T b and record the performance in Y b ; <ref type="bibr">20:</ref> update the predictor (the following commented out) <ref type="bibr">21:</ref> until budget is exhausted or b = K 2 ; 22: select the top-I structures in T based on performance in Y to form the set I. 23: return I.</p><formula xml:id="formula_38">P with (T = T b 0 ? ? ? ? ? T b , Y = Y b 0 ? ? ? ? ? Y b );</formula><p>the corresponding BLM be f A (b) . In step 1, we initialize b to some b 0 and create an empty candidate set H b . As A's with fewer than K nonzero elements are degenerate (Proposition 2), we use b 0 = K. We first sample positions of b 0 nonzero elements, and then randomly assign them values in {?1, ?2, . . . , ?K}. The other entries are set to zero.</p><p>Steps 2-5 filter away degenerate and equivalent structures. The number of nonzero elements b is then increased by 1 (step 10). For each such b, steps 12-16 greedily select a top-performing structure (evaluated based on the mean reciprocal rank (MRR) <ref type="bibr" target="#b2">[3]</ref> performance on S val ) in T b?1 , and generate N candidates. All the candidates are checked by the filter Q (Section 4.1) to avoid degenerate or equivalent solutions. Next, the predictor P in Section 4.2 selects the top-P A (b) 's, which are then trained and evaluated in step 18. The training data for P is collected with the recorded structures and performance in (T , Y) at step 20. Finally, the top-I structures in T evaluated by the corresponding performance in Y are returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evolutionary Algorithm</head><p>While progressive search can be efficient, it may not fully explore the search space and can lead to sub-optimal solutions <ref type="bibr" target="#b50">[51]</ref>. The progressive search can only generate structures from fewer non-zero elements to more ones. Thus, it can not visit and adjust the structures with fewer non-zero elements when b is increased. To address these problems, we consider in this section the use of evolutionary algorithms <ref type="bibr" target="#b51">[52]</ref>. if |I| = I, break loop; 5: end for <ref type="bibr">6:</ref> train and evaluate all A's in I; 7: add A's to T and record the performance in Y; 8: repeat <ref type="bibr">9:</ref> update predictor P with records in (T , Y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>repeat <ref type="bibr" target="#b10">11</ref>:</p><formula xml:id="formula_39">H = ?;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>mutation: sample A ? I and mutate to Anew; or <ref type="bibr">13:</ref> crossover: sample A (a) , A (b) ? I, and use crossover to generate Anew;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>if Q(Anew, H ? T ) is true by Algorithm 1, then H ? H ? {Anew};</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>until |H| = N ; <ref type="bibr">16:</ref> select top-P structures A in H based on the the predictor P;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>for each top-P structure A do <ref type="bibr">18:</ref> train embeddings and evaluate the performance of A;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>survive: update I with A if A is better than the worst structure in I;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20:</head><p>end for <ref type="bibr">21:</ref> add A's in T and record the performance in Y; 22: until budget is exhausted; 23: return I.</p><p>The procedure, which is called AutoBLM+, is in Algorithm 4. As in Algorithm 3, we start with structures having b 0 = K nonzero elements. Steps 1-6 initializes a set I of I non-degenerate and non-equivalent structures. The main difference with Algorithm 3 is in steps 8-15, in which new structures are generated by mutation and crossover. For a given structure A, mutation changes the value of each entry to another one in {0, ?1, . . . , ?K} with a small probability p m . For crossover, given two structures A (a) and A (b) , each entry of the new structure has equal probabilities to be selected from the corresponding entries in A (a) or A (b) . After mutation or crossover, we check if the newly generated A new has to be filtered out. After N structures are collected, we use the performance predictor P in Section 4.2 to select the top-P structures. These are then trained and evaluated for actual performance. Finally, structures in I with performance worse than the newly evaluated ones are replaced (step 19).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, experiments are performed on a number of KG tasks. Algorithm 5 shows the general procedure for each task. First, we find a good hyper-parameter setting to train and evaluate different structures (steps 2-6). Based on the observation that the performance ranking of scoring functions is consistent across different d's (details are in Appendix C), we set d to a smaller value <ref type="bibr" target="#b63">(64)</ref> to reduce model training time. The search algorithm is then used to obtain the set I of top-I structures (step 8). Finally, the hyper-parameters are fine-tuned with a larger d, and the best structure selected (steps <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. Experiments are run on a RTX 2080Ti GPU with 11GB memory. All algorithms are implemented in python <ref type="bibr" target="#b52">[53]</ref>. train SimplE with HPi = {?i, ?i, mi, d}, and evaluate the validation MRR; 5: end for <ref type="bibr">6:</ref> select the best hyper-parameter settingHP ? {HPi} 10 i=1 ; 7: // stage 2: search scoring function 8: using hyper-parameter settingHP , obtain the set I of top-I structures from Algorithm 3 or Algorithm 4; 9: // stage 3: fine-tune the obtained scoring function 10: for j = 1, . . . , 50 (j = 1, . . . , 10 for YAGO3-10) do <ref type="bibr">11:</ref> randomly select a structure Aj ? I;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>randomly select ?j ? [0, 1], ?j ? [10 ?5 , 10 ?1 ], mj ? {256, 512, 1024}, and dj ? {256, 512, 1024, 2048};</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>train the KG learning model with structure Aj and hyper-parameter setting HP j = {?j, ?j, mj, dj} 14: end for <ref type="bibr">15:</ref> select the best structure {A * , HP * } ? {Aj, HP j } 50 j=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Knowledge Graph (KG) Completion</head><p>In this section, we perform experiments on KG completion as introduced in Section 2.2.1. we use the full multi-class logloss <ref type="bibr" target="#b17">[18]</ref>, which is more robust and has better performance than negative sampling <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Setup</head><p>Datasets. Experiments are performed on the following popular benchmark datasets: WN18, FB15k, WN18RR, FB15k237, YAGO3-10, ogbl-biokg and ogbl-wikikg2 ( <ref type="table" target="#tab_6">Table 5</ref>). WN18 and FB15k are introduced in <ref type="bibr" target="#b9">[10]</ref>. WN18 is a subset of the lexical database WordNet <ref type="bibr" target="#b53">[54]</ref>, while FB15k is a subset of the Freebase KG <ref type="bibr" target="#b54">[55]</ref> for human knowledge. WN18RR <ref type="bibr" target="#b20">[21]</ref> and FB15k237 <ref type="bibr" target="#b55">[56]</ref> are obtained by removing the near-duplicates and inverse-duplicate relations from WN18 and FB15k. YAGO3-10 is created by <ref type="bibr" target="#b20">[21]</ref>, and is a subset of the semantic KG YAGO <ref type="bibr" target="#b56">[57]</ref>, which unifies WordNet and Wikipedia. The ogbl-biokg and ogbl-wikikg2 datasets are from the open graph benchmark (OGB) <ref type="bibr" target="#b57">[58]</ref>, which contains realistic and large-scale datasets for graph learning. The ogbl-biokg dataset is a biological KG describing interactions among proteins, drugs, side effects and functions. The ogbl-wikikg2 dataset is extracted from the Wikidata knowledge base <ref type="bibr" target="#b58">[59]</ref> describing relations among entities in Wikipedia. Baselines. For AutoBLM and AutoBLM+, we select the structure for evaluation from the set returned by Algorithm 3 or 4 based on the MRR performance on the validation set. For WN18, FB15k, WN18RR, FB15k237, YAGO3-10, AutoBLM and AutoBLM+ are compared with the following popular human-designed KG embedding models 3 : (i) TDM, including TransH <ref type="bibr" target="#b10">[11]</ref>, RotatE <ref type="bibr" target="#b12">[13]</ref> and PairE <ref type="bibr" target="#b60">[61]</ref>; (ii) NNM, including ConvE <ref type="bibr" target="#b20">[21]</ref>, RSN <ref type="bibr" target="#b21">[22]</ref> and CompGCN <ref type="bibr" target="#b23">[24]</ref>; (iii) BLM, including TuckER <ref type="bibr" target="#b34">[35]</ref>, Quat <ref type="bibr" target="#b18">[19]</ref>, DistMult <ref type="bibr" target="#b6">[7]</ref>, ComplEx <ref type="bibr" target="#b13">[14]</ref>, HolE <ref type="bibr" target="#b14">[15]</ref>, Analogy <ref type="bibr" target="#b15">[16]</ref> SimplE <ref type="bibr" target="#b16">[17]</ref>, and CP <ref type="bibr" target="#b17">[18]</ref>. We do not compare with NASE <ref type="bibr" target="#b61">[62]</ref> as its code is not publicly available.</p><p>For ogbl-biokg and ogbl-wikikg2 <ref type="bibr" target="#b57">[58]</ref>, we compare with the models reported in the OGB leaderboard <ref type="bibr" target="#b3">4</ref> , namely, TransE <ref type="bibr" target="#b9">[10]</ref>, RotatE, PairE, DistMult, and ComplEx.</p><p>Performance Measures. The learned f A (h, r, t) is evaluated in the context of link prediction. Following <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, for each triple (h, r, t), we first take (?, r, t) as the query and obtain the filtered rank on the head</p><formula xml:id="formula_40">rank h = e ? E : f (e, r, t) ? f (h, r, t) ? (e, r, t) / ? S tra ? S val ? S tst + 1,<label>(12)</label></formula><p>where S tra , S val , S tst are the training, validation, and test sets, respectively. Next we take (h, r, ?) as the query and obtain the filtered rank on the tail</p><formula xml:id="formula_41">rank t = e ? E : f (h, r, e) ? f (h, r, t) ? (h, r, e) / ? S tra ? S val ? S tst +1. (13)</formula><p>The following metrics are computed from both the head and tail ranks on all triples: (i) Mean reciprocal ranking (MRR):</p><formula xml:id="formula_42">MRR = 1 2|S| (h,r,t)?S 1 rank h + 1 rank t ;</formula><p>and (ii) H@k: ratio of ranks no larger than k, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H@k = 1 2|S|</head><p>(h,r,t)?S</p><formula xml:id="formula_43">I(rank h ? k) + I(rank t ? k) ,</formula><p>where I(a) = 1 if a is true, otherwise 0. The larger the MRR or H@k, the better is the embedding. Other metrics for the completion task <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref> can also be adopted here.</p><p>For ogbl-biokg and ogbl-wikikg2 <ref type="bibr" target="#b57">[58]</ref>, we only use the MRR as the H@k is not reported by the baselines in the OGB leaderboard.  Testing performance of MRR, H@1 and H@10 on KG completion. The best model is highlighted in bold and the second best is underlined. "-" means that results are not reported in those papers or their code on that data/metric is not available. CompGCN uses the entire KG in each iteration and so runs out of memory on the larger data sets of WN18, FB15k and YAGO3-10. mutation is selected, the value of each entry has a mutation probability of p m = 2/K 2 . A budget is used to terminate the algorithm. This is set to 256 structures on WN18, FB15k, WN18RR, FB15k-237, 128 on YAGO3-10, 64 on ogbl-biokg, and 32 on ogbl-wikikg2. We follow <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref> to use Adagrad <ref type="bibr" target="#b64">[65]</ref> as optimizer. The Adagrad hyper-parameters are selected from the following ranges: learning rate ? in [0, 1], 2 -penalty ? in [10 ?5 , 10 ?1 ], batch size m in {256, 512, 1024}, and dimension d in {64, 256, 512, 1024, 2048}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Results on WN18, FB15k, WN18RR, FB15k237, YAGO3-10</head><p>Performance. <ref type="table" target="#tab_8">Table 6</ref> shows the testing results on WN18, FB15k, WN18RR, FB15k237, and YAGO3-10. As can be seen, there is no clear winner among the baselines. On the other hand, AutoBLM performs consistently well. It outperforms the baselines on FB15k, WN18RR, FB15k237 and YAGO3-10, and is the first runner-up on WN18. AutoBLM+ further improves AutoBLM on FB15k, WN18RR, FB15k237 and YAGO3-10.</p><p>Learning curves. <ref type="figure" target="#fig_5">Figure 4</ref> shows the learning curves of representative models in each type of scoring functions, including: RotatE in TDM; ConvE and CompGCN in NNM; and DistMult, SimplE/CP, ComplEx/HolE, Analogy, QuatE and the proposed AutoBLM/AutoBLM+ in BLM. As can be seen, NNMs are much slower and inferior than BLMs. On the other hand, AutoBLM+ has better performance and comparable time as the other BLMs.</p><p>Data-dependent BLM structure. <ref type="figure" target="#fig_7">Figure 5</ref> shows the BLMs obtained by AutoBLM and AutoBLM+. As can be seen, they are different from the human-designed BLMs in <ref type="figure">Figure 1</ref> and are also different from each other. To demonstrate that these data-dependent structures also have different accuracies on the same dataset, we take the BLM obtained by AutoBLM (or AutoBLM+) on a source dataset and then evaluate it on a different target dataset. <ref type="table" target="#tab_10">Table 7</ref> shows the testing MRRs obtained (the trends for H@1 and H@10 are similar). As can be seen, the different BLMs perform differently on the same dataset, again confirming the need for data-dependent structures.   <ref type="table" target="#tab_11">Table 8</ref> shows the testing MRRs of the baselines (as reported in the OGB leaderboard), the BLMs obtained by AutoBLM and AutoBLM+. As can be seen, AutoBLM and AutoBLM+ achieve significant gains on the testing MRR on both datasets, even though fewer model parameters are needed for AutoBLM+. The searched structures are provided in <ref type="figure" target="#fig_1">Figure 13</ref> in Appendix D.    <ref type="figure" target="#fig_9">Figure 6</ref> shows the mean validation MRR of the top I = 8 structures w.r.t. clock time during the search process. As can be seen, AutoBLM (no Filter, no Predictor, b 0 = 1) and AutoBLM+ (no Filter, no Predictor, b 0 = 1) outperform the rest at the later stages. They have poor initial performance as they start with structures having few nonzero elements, which can be degenerate. This will be further demonstrated in the next section.</p><formula xml:id="formula_44">) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( (a) WN18. ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( (b) FB15k. ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( (c) WN18RR. ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( (d) FB15k237. ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( ) " $ ) # $ ) ' $ ) ( $ ( " ( # ( ' ( ( (e) YAGO3-10.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Results on ogbl-biokg and ogbl-wikikg2</head><formula xml:id="formula_45">, b 0 = 1) with initial b 0 = 1; (v) AutoBLM+ (no Filter, no Predictor, b 0 = 1) with initial b 0 = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Ablation Study 2: Effectiveness of the Filter</head><p>Structures with more nonzero elements are more likely to satisfy the two conditions in Proposition 2, and thus less likely to be degenerate. Hence, the filter is expected to be particularly useful when there are few nonzero elements in the structure. In this experiment, we demonstrate this by comparing AutoBLM/AutoBLM+ with and without the filter. The performance predictor is always enabled.  <ref type="figure" target="#fig_10">Figure 7</ref> shows the mean validation MRR of the top I = 8 structures w.r.t. clock time. As expected, when the filter is not used, using a larger b 0 will be more likely to have non-degenerate structures and thus better performance, especially at the initial stages. When the filter is used, the performance of both b 0 settings are improved. In particular, with b 0 = 4, the initial search space is simpler and leads to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.6">Ablation Study 3: Performance Predictor</head><p>In this experiment, we compare the following Auto-BLM/AutoBLM+ variants: (i) AutoBLM (no-predictor) and AutoBLM+ (no-predictor), which simply randomly select P structures for evaluation (in step 17 of Algorithm 3 and step 16 of Algorithm 4, respectively); (ii) AutoBLM (Predictor+SRF) and AutoBLM+ (Predictor+SRF), using the proposed SRF (in Section 4.2) as input features to the performance predictor; and (iii) AutoBLM (Predictor+1hot) and AutoBLM+ (Predictor+1hot), which map each of the K 2 entries in A (with values in {0, ?1, . . . , ?K}) to a simple (2K + 1)-dimensional one-hot vector, and then use these as features to the performance predictor. The resultant feature vector is thus K 2 (2K + 1)-dimensional, which is much longer than the K(K + 1)-dimensional SRF representation.  As can be seen, the use of performance predictor improves the results over AutoBLM (no-Predictor) and AutoBLM+ (no-Predictor). The SRF features also perform better than the one-hot features, as the one-hot features are higher-dimensional and more difficult to learn. Besides, we observe that AutoBLM+ performs better than AutoBLM, as it can more flexibly explore the search space. Thus, in the remaining ablation studies, we will only focus on AutoBLM+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.7">Ablation Study 4: Varying K</head><p>As K increases, the search space, which has a size of (2K + 1) K 2 <ref type="figure" target="#fig_1">(Section 3.3)</ref>, increases dramatically. Moreover, the SRF also needs to enumerate a lot more (K 2K+1 ) vectors in C. In this experiment, we demonstrate the dependence on K by running AutoBLM+ with K = 3, 4, 5. To ensure that d is divisible by K, we set d = 60. <ref type="figure" target="#fig_13">Figure 9</ref> shows the top-8 mean MRR performance on the validation set of the searched models versus clock time. As can be seen, the best performance attained by different K's are similar. However, K = 5 runs slower.  <ref type="table" target="#tab_12">Table 9</ref> shows the running time of the filter, performance predictor (with SRF features), training and evaluation in Algorithm 4 with different K's. As can be seen, the costs of filter and performance predictor increase a lot with K, while the model training and evaluation time are relatively stable for different K's. For the stand-alone approach, the 100 A's are separately trained and evaluated. <ref type="figure" target="#fig_14">Figure 10</ref> shows the MRR estimated by parametersharing versus the true MRR obtained by individual model training. As can be seen, structures that have high estimated MRRs (by parameter sharing) do not truly have high MRRs. Indeed, the Spearman's rank correlation coefficient 5 between the two sets of MRRs is negative (?0.2686 on WN18RR and ?0.2451 on FB15k237). This demonstrates that the one-shot approach, though faster, cannot find good structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-Hop Query</head><p>In this section, we perform experiment on multi-hop query as introduced in Section 2.2.2. The entity and relation embeddings are optimized by maximizing the scores on positive queries and minimizing the scores on negative queries, which are generated by replacing e L with an incorrect entity. On evaluation, we rank the scores of queries (e 0 , r 1 ? r 2 ? ? ? ? ? r L , e L ) of all e L ? E to obtain the ranking of ground truth entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Setup</head><p>Following <ref type="bibr" target="#b3">[4]</ref>, we use the FB15k and FB15k237 datasets in <ref type="table" target="#tab_6">Table 5</ref>. Evaluation is based on two-hop (2p) and threehop (3p) queries. Interested readers are referred to <ref type="bibr" target="#b3">[4]</ref> for a more detailed description on query generation. For FB15k, there are 273,710 queries in the training set, 8,000 non-overlapping queries in the validation and testing sets. For FB15k237, there are 143,689 training queries, and 5,000 queries for validation and testing. The setting of the search algorithms' hyper-parameters are the same as in Section 5.1. For the learning hyper-parameters, we search the dimension d ? {32, 64}, and the other hyper-parameters are the same as those in Section 5.1. We use the MRR performance on the validation set to search for structures as well as hyperparameters. For performance evaluation, we follow <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and use the testing Hit@3 and MRR.</p><p>We compare with the following baselines: (i) TransE-Comp <ref type="bibr" target="#b38">[39]</ref> (based on TransE); (ii) Diag-Comp <ref type="bibr" target="#b38">[39]</ref> (based on DistMult); (iii) GQE <ref type="bibr" target="#b7">[8]</ref>, which uses a d ? d trainable matrix R (r) for composition, and can be regarded as a composition based on RESCAL <ref type="bibr" target="#b5">[6]</ref>; and (iv) Q2B <ref type="bibr" target="#b3">[4]</ref>, which is a recently proposed box embedding method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results</head><p>Results are shown in <ref type="table" target="#tab_1">Table 10</ref>. As can be seen, among the baselines, TransE-Comp, Diag-Comp and GQE are inferior to Q2B. This shows that the general scoring functions cannot be directly applied to model the complex interactions 5. https://en.wikipedia.org/wiki/Spearman%27s rank correlation coefficient in multi-hop queries. On the other hand, AutoBLM and AutoBLM+ have better performance as they can adapt to the different tasks with different matrices g K <ref type="figure">(A, r)</ref>. The obtained structures can be found in Appendix D. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Entity Classification</head><p>In this section, we perform experiment on entity classification as introduced in Section 2.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Setup</head><p>After aggregation for L layers, representation e L at the last layer is transformed by a multi-layer perception (MLP) to  <ref type="table" target="#tab_1">(Table 11)</ref>: AIFB, an affiliation graph; MUTAG, a bioinformatics graph; and BGS, a geological graph. More details can be found in <ref type="bibr" target="#b67">[68]</ref>. All entities do not have attributes. The entities' and relations' trainable embeddings are used as input to the GCN. The following five models are compared: (i) GCN <ref type="bibr" target="#b8">[9]</ref>, with ?(e j , r ) = e j , does not leverage relations of the edges; (ii) R-GCN <ref type="bibr" target="#b22">[23]</ref>, with ?(e j , r ) = R (r) e j ; (iii) CompGCN <ref type="bibr" target="#b23">[24]</ref> with ?(e j , r ) = e j (-/*/ ) r , in which the operator (subtraction/multiplication/circular correlation as discussed in Section 2.2.3) is chosen based on 5-fold crossvalidation; (iv) AutoBLM; and (v) AutoBLM+. oth AutoBLM and AutoBLM+ use the searched structure A to form ?(e j , r ) = g K (A, r )e j .</p><formula xml:id="formula_46">e o i = M LP (e L i ) ? R C ,</formula><p>Setting of the hyper-parameters are the same as in Section 5.1. As for the learning hyper-parameters, we search the embedding dimension d from {12, 20, 32, 48}, learning rate from [10 ?5 , 10 ?1 ] with Adam as the optimizer <ref type="bibr" target="#b68">[69]</ref>. For the GCN structure, the hidden size is the same as the embedding dimension, the dropout rate for each layer is from [0, 0.5]. We search for 50 hyper-parameter settings for each dataset based on the 5-fold classification accuracy.</p><p>For performance evaluation, we use the testing accuracy. Each model runs 5 times, and then the average testing accuracy reported. <ref type="table" target="#tab_1">Table 12</ref> shows the average testing accuracies. Among the baselines, R-GCN is slightly better than CompGCN on the AIFB dataset, but worse on the other two sparser datasets. By searching the composition operators, AutoBLM and AutoBLM+ outperform all the baseline methods. AutoBLM+ is better than AutoBLM since it can find better structures with the same budget by the evolutionary algorithm. The structures obtained are in Appendix D. <ref type="bibr">TABLE 12</ref> Classification accuracies (in %) on entity classification task. Values marked "*" are copied from <ref type="bibr" target="#b23">[24]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose AutoBLM and AutoBLM+, the algorithms to automatically design and discover better scoring functions for KG learning. By analyzing the limitations of existing scoring functions, we setup the problem as searching relational matrix for BLMs. In AutoBLM, we use a progressive search algorithm which is enhanced by a filter and a predictor with domain-specific knowledge, to search in such a space. Due to the limitation of progressive search, we further design an evolutionary algorithm, enhanced by the same filter and predictor, called AutoBLM+. AutoBLM and AutoBLM+ can efficiently design scoring functions that outperform existing ones on tasks including KG completion, multi-hop query and entity classification from the large search space. Comparing AutoBLM with AutoBLM+, AutoBLM+ can design better scoring functions with the same budget. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOFS</head><p>Denote N = |E|. For vectors and matrix, we use the uppercase italic bold letters, such as G, E, to denote matrices, use uppercase normal bold letters, such as G, to denote tensors, lowercase bold letters, such as r to denote vectors, and normal characters to indicate scalers, such as K. i, j are generally used to indicate index. E i , r i , i = 1 . . . K, still a matrix or vector, are the i-th block of K-chunk split, while r i , i = 1 . . . d is a scalar in the i-th dimension of vector r. E i,: indicates the i-th row of matrix E, and E :,i indicates the i-th column. For g K (A, r) ? R d?d , we use [g K (A, r)] i,j with i, j = 1 . . . K to indicate the (i, j)-th block, while {g K (A, r)} i,j with i, j = 1 . . . d to indicate the element in the i-th row and j-th column of g K (A, r). x means the smallest integer that is equal or larger than x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proposition 1</head><p>Here, we first show some useful lemmas in Appendix A.1.1, then we prove</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Auxiliary Lemmas</head><p>Recall that</p><formula xml:id="formula_47">C ? {r ? R K | r = 0, r i ? {0, ?1, . . . , ?K}, i = 1, . . . , K}.</formula><p>We firstly show that any symmetric matrix can be factorized as a bilinear form if ?r ? C, g K (A,r) = g K (A,r) in Lemma 1; any skew-symmetric matrix can be factorized as a bilinear form if ?? ? C, g K (A,?) = ?g K (A,?) in Lemma 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1.</head><p>If ?r ? C, g K (A,r) = g K (A,r), there exist E ? R KN ?N and? ? R KN such that any symmetric matrix G =? g K (A,?)?.</p><p>Proof. For any real symmetric matrix? ? R N ?N , it can be decomposed as <ref type="bibr" target="#b69">[70]</ref></p><formula xml:id="formula_48">? = P ?P ,<label>(14)</label></formula><p>where P ? R N ?N is an orthogonal matrix and ? ? R N ?N is a diagonal matrix with N elements. If ?r ? C, g K (A,r) = g K (A,r), we discuss with two cases: 1) there exist some non-zero elements in the diagonal; 2) all the diagonal elements are zero.</p><p>To begin with, we evenly split the relation embedding into K parts as? = [? 1 ;? 2 ; . . . ;? K ] ? R KN , and the entity embedding matrix into K blocks as? = [? 1 ;? 2 ; . . . ;? K ] ? R KN ?N . Then for the two cases: 1) If there exist non-zero elements in the diagonal, i.e. ?i ? {1 . . . K} : {g K (A,r)} i,i = 0, we denote any one of the index in the diagonal as (a 1 , a 1 ) with a 1 ? {1 . . . K}. Then, we have {g K (A,r)} a1,a1 = sign(A a1,a1 ) ?r[|A a1,a1 |] = 0, and sign(A a1,a1 ) = 0. Next, we assign? with</p><formula xml:id="formula_49">E i = P i = a 1 0 otherwise .<label>(15)</label></formula><p>And? with</p><formula xml:id="formula_50">r i = sign(A a1,a1 ) ? vec(?) i = |A a1,a1 | 0 otherwise ,<label>(16)</label></formula><p>where vec(?) returns the diagonal elements in the diagonal matrix ? and</p><formula xml:id="formula_51">[g K (A,?)] a1,a1 =sign(A a1,a1 ) ? diag sign(A a1,a1 ) ? vec(?) = sign(A a1,a1 ) 2 ? ? = ?.<label>(17)</label></formula><p>Based on <ref type="bibr" target="#b13">(14)</ref>, <ref type="bibr" target="#b14">(15)</ref> and <ref type="formula" target="#formula_3">(17)</ref>, we hav?</p><formula xml:id="formula_52">E g K (A,?)? = K,K i,j? i [g K (A,?)] i,j?j =0 +? a1 [g K (A,?)] a1,a1?a1 , =0 + P ?P =?.</formula><p>2) If all the diagonal elements are zero, there must be some non-zero elements in the non-diagonal indices, i.e.</p><formula xml:id="formula_53">?(i, j) ? {1 . . . K} ? {1 . . . K} : {g K (A,r)} i,j = 0 ? i = j.</formula><p>We denote any one of the index in the non-diagonal indices as (a 2 , a 3 ) with a 2 , a 3 ? {1 . . . K} and a 2 = a 3 . Then we have</p><formula xml:id="formula_54">{g K (A,r)} a2,a3 = {g K (A,r)} a3,a2 = sign(A a2,a3 ) ?r[|A a2,a3 |] = 0,</formula><p>and sign(A a2,a3 ) = 0. Similarly, we assign? with</p><formula xml:id="formula_55">E i = P i = a 2 or a 3 0 otherwise .<label>(18)</label></formula><p>And? with</p><formula xml:id="formula_56">r i = ? ? ? ? ? sign(Aa 2 ,a 3 ) 2 ? vec(?) i = |A a2,a3 | sign(Aa 3 ,a 2 ) 2 ? vec(?) i = |A a3,a2 | 0 otherwise ,<label>(19)</label></formula><p>which leads to</p><formula xml:id="formula_57">[g K (A,?)] a2,a3 = [g K (A,?)] a3,a2 =sign(A a2,a3 ) ? diag sign(A a2,a3 ) 2 ? vec(?) = 1 2 ?.<label>(20)</label></formula><p>Based on <ref type="bibr" target="#b13">(14)</ref>, <ref type="bibr" target="#b17">(18)</ref> and <ref type="formula" target="#formula_57">(20)</ref>, we hav?</p><formula xml:id="formula_58">E g K (A,?)? = K,K i,j? i [g K (A,?)] i,j?j =0 +? a2 [g K (A,?)] a2,a3?a3 +? a3 [g K (A,?)] a3,a2?a2 , =0 + P ( 1 2 ?)P + P ( 1 2 ?)P =?.</formula><p>Hence, there exist? ? R KN ?N and? ? R KN such that any symmetric matrix? =? g K (A,?)?. Proof. First, if ?? ? C, g K (A,?) = ?g K (A,?), all the elements in the diagonal should be zero, i.e. {g K (A,?)} i,j = 0, with i = j. Therefore, there must be some nonzero elements in the non-diagonal indices, i.e., ?(i, j) ? {1 . . . K} 2 : {g K (A,?)} i,j = 0 ? i = j. We denote any one of the index in the non-diagonal indices as</p><formula xml:id="formula_59">(b 1 , b 2 ) with b 1 , b 2 ? {1 . . . K} and b 1 = b 2 . Then we have {g K (A,?)} b1,b2 = ?{g K (A,?)} b2,b1 = sign(A b1,b2 ) ??[|A b1,b2 |] = 0,</formula><p>and sign(A b1,b2 ) = 0. Next, we assign? with</p><formula xml:id="formula_60">E i = ? ? ? ? ? I i = b 1 G i = b 2 0 otherwise .<label>(21)</label></formula><p>And? withr</p><formula xml:id="formula_61">i = ? ? ? ? ? sign(A b 1 ,b 2 ) 2 ? 1 i = |A b1,b2 | sign(A b 2 ,b 1 ) 2 ? 1 i = |A b2,b1 | 0 otherwise ,<label>(22)</label></formula><p>which leads to</p><formula xml:id="formula_62">[g K (A,r)] b1,b2 = ?[g K (A,r)] b2,b1 =sign(A b1,b2 ) ? diag sign(A b1,b2 ) 2 ? 1 = 1 2 I.<label>(23)</label></formula><p>SinceG is skew-symmetric, we hav?</p><formula xml:id="formula_63">G = ?G.<label>(24)</label></formula><p>Based on (21), <ref type="bibr" target="#b22">(23)</ref> and <ref type="formula" target="#formula_18">(24)</ref> we hav?</p><formula xml:id="formula_64">E g K (A,r)? = K,K i,j? i [g K (A,r)] i,j?j =0 +? b1 [g K (A,r)] b1,b2?b2 +? b2 [g K (A,r)] b2,b1?b1 , =0 + I ( 1 2 I)G +G (? 1 2 I)I =0 + 1 2G ? 1 2 (?G) =G.</formula><p>Hence, there exist? ? R KN ?N andr ? R KN such that any symmetric matrixG =? g K (A,r)?.</p><p>Based on Lemma 1 and 2, we prove the following lemma for any real-valued square matrices. Lemma 3. If 1) ?r ? C, g K (A,r) = g K (A,r)), and 2) ?? ? C, g K (A,?) = ?g K (A,?)), then there exist E ? R 2KN ?N and r ? R 2KN that any matrix G ? R N ?N can be written as</p><formula xml:id="formula_65">G ht = f (h, r, t) = h g K (A, r)t,</formula><p>where h = E :,h , t = E :,t .</p><p>Proof. In Lemma 1 and Lemma 2, we prove that any symmetric matrix can be factorized as? =? g K (A,?)? with? ? R KN ?N and? ? R KN and any skew-symmetric matrix can be factorized asG =? g K (A,r)? ? R N ?N with? ? R KN ?N andr ? R KN . In this part, we show any square matrix G can be split as the sum of a particular? and a particularG and it can be factorized in the bilinear form with the composition of?,? and?,r.</p><p>We firstly composite?,? in the proof of Lemma 1 and E,r in the proof of Lemma 2 into E ? R 2KN ?N and r ? R 2KN . The basic idea is to add the symmetric part into odd rows and skew-symmetric part into even rows. Specifically, we define the rows of E as</p><formula xml:id="formula_66">E i,: = ? i+1 2 ,: i mod 2 = 1 E i 2 ,: i mod 2 = 0 .<label>(25)</label></formula><p>And the relation embedding r is element-wise set as</p><formula xml:id="formula_67">r[i] = ?[ i+1 2 , :] i mod 2 = 1 r[ i 2 , :] i mod 2 = 0 .<label>(26)</label></formula><p>Based on the form of r we can have g K (A, r) ? R 2KN ?2KN where each element is formed by corresponding element in g K (A,?) or g K (A,r) as . <ref type="formula" target="#formula_21">(27)</ref> Refer to the notation introduced at the beginning of Appendix A, {g K (A, r)} i,j represents the (i, j)-th element in the matrix, while [g K (A, r)] i,j represents the (i, j)-th block in previous parts. Note that such a construction of g K (A, r) will not violate the structure matrix A. The construction of (25), <ref type="bibr" target="#b25">(26)</ref> are graphically illustrated in the left part of <ref type="figure" target="#fig_18">Figure 11</ref>, which leads to the construction of <ref type="bibr" target="#b26">(27)</ref> in the right part.</p><formula xml:id="formula_68">{g K (A, r)} i,j = ? ? ? ? ? {g K (A,?)} i+1 2 ,<label>j+1</label></formula><formula xml:id="formula_69">? 1 % ? 1 ? ? ? ? ? ? ? 3 &amp; (5, 7) ? ? ? ? ? ? ? 1 3 &amp; (5, 7) ? ? ? ? ? ? ? ? ? ? 7 ? ? ? ? ? ? ? ? 7 ! " !,: # " !,: ! " $,: # " $,:&amp; [1]</formula><p># " *+,:</p><p>! " *+,: Given any matrix G ? R N ?N , it can be split into a symmetric part? and a skew-symmetric partG, i.e.,</p><formula xml:id="formula_70">) ! (+,.) ) ! (+,. ) &amp;[1] &amp;[2] &amp;[2] "[$%] "[$%]</formula><formula xml:id="formula_71">G = G + G 2 ? + G ? G 2 G .<label>(28)</label></formula><p>Based on <ref type="bibr" target="#b24">(25)</ref>, <ref type="bibr" target="#b25">(26)</ref>, <ref type="bibr" target="#b26">(27)</ref>, and <ref type="formula" target="#formula_23">(28)</ref> </p><formula xml:id="formula_72">E i,h {g K (A, r)} i,j E j,t<label>(31)</label></formula><formula xml:id="formula_73">+ 2KN,2KN (i,j) mod 2=(1,0) E i,h {g K (A, r)} i,j E j,t<label>(32)</label></formula><formula xml:id="formula_74">+ 2KN,2KN (i,j) mod 2=(0,1) E i,h {g K (A, r)} i,j E j,t<label>(33)</label></formula><formula xml:id="formula_75">+ 2KN,2KN (i,j) mod 2=(0,0) E i,h {g K (A, r)} i,j E j,t<label>(34)</label></formula><formula xml:id="formula_76">= 2KN,2KN i,j E i,h {g K (A, r)} i,j E j,t =E :,h g K (A, r)E :,t = h g K (A, r)t = f (h, r, t),</formula><p>with h = E :,h , t = E :,t and r in <ref type="bibr" target="#b25">(26)</ref>. <ref type="formula" target="#formula_16">(32)</ref> and <ref type="formula" target="#formula_16">(33)</ref> are 0 since {g K (A, r)} i,j is zero when i and j are not simultaneously even or odd. From <ref type="bibr" target="#b28">(29)</ref> to <ref type="formula" target="#formula_3">(31)</ref>, we let m = i+1 /2 and n = j+1 /2 to get the odd part in <ref type="formula" target="#formula_66">(25)</ref> and <ref type="bibr" target="#b26">(27)</ref>. From <ref type="bibr" target="#b29">(30)</ref> to <ref type="formula" target="#formula_16">(34)</ref>, we let m = i /2 and n = j /2 to obtain the even part in <ref type="formula" target="#formula_66">(25)</ref> and <ref type="bibr" target="#b26">(27)</ref>.</p><p>Finally, we show a Lemma which bridges 3 order tensor G with bilinear scoring function of form <ref type="bibr" target="#b6">(7)</ref>. Given any KG with tensor form G ? R |E|?|R|?|E| , we denote G r ? R E?E as the r-th slice in the lateral of G, i.e. G r = G ?,r,? , corresponding to relation r. Lemma 4. Given any KG with tensor form G ? R |E|?|R|?|E| , and structure matrix A. If all the G r 's can be independently expressed by a unique entity embedding matrice? E r ? R 2KN ?N and relation embedding? ? R 2KN , i.e. ?h, t = 1 . . . N, [G r ] h,t =? r g K (A,? r )? r , wit? h r = [? r ] ?,h ,? r = [? r ] ?,t , then there exist entity embedding E ? R 2KN |R|?N and relation embedding R ? R 2KN |R|?|R| such that ?h, r, t, G hrt = f A (h, r, t) = h g K (A, r)t.</p><p>Proof. We show that computing G hrt can be independently expressed by [G r ] ht for each relation r. Specifically, we define the rows of entity embedding in E as</p><formula xml:id="formula_77">E i,: = [? i mod |R| ] i |R| ,:<label>(35)</label></formula><p>And each element in the relation embedding r as</p><formula xml:id="formula_78">r[i] = ?[r] i |R| i mod R = r 0 otherwise, ,<label>(36)</label></formula><p>which leads to the element in g K (A, r) as The construction of (35), <ref type="bibr" target="#b35">(36)</ref> and <ref type="formula" target="#formula_16">(37)</ref> can be graphically illustrated in <ref type="figure" target="#fig_3">Figure 12</ref>. Under these definitions, we can get that each element G hrt can be expressed with</p><formula xml:id="formula_79">G hrt = [G r ] h,t = 2K|E| ir=1 2K|E| jr=1 [E r ] ir,h {g K (A, r r )} ir,jr [E r ] jr,t (38) = |R|?2K|E| i=1 |R|?2K|E| j=1 E ih {g K (A, r)} ij E jt (39) = h g K (A, r)t</formula><p>The step to get (38) depends on Lemma 3. Eq. (38) to (39) depends on <ref type="bibr" target="#b34">(35)</ref> and <ref type="bibr" target="#b36">(37)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Proof of Proposition 1</head><p>Proof. Based on Lemma 3 and Lemma 4, if 1) ?r ? C such that g K (A,r) is symmetric, and 2) ?? ? C such that g K (A,?) is skew-symmetric, with C ? {r ? R K | r = 0, r i ? {0, ?1, . . . , ?K}, i = 1, . . . , K},, then given any KG with the tensor form G, there exist entity embedding E ? R 2K|E||R|?|E| and relation embedding R ? R 2K|E||R|?|R| such that for all h, r, t, we have</p><formula xml:id="formula_80">G hrt = f A (h, r, t) = h g K (A, r)t,</formula><p>with h = E :,h , t = E :,t , r = E :,r . Thus, scoring function <ref type="formula" target="#formula_21">(7)</ref> is fully expressive once condition 1) and 2) hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proposition 2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Auxiliary Lemmas</head><p>First, we introduce two lemmas from matrix theory about the rank of Kronecker product and the solution of equation group.</p><p>Lemma 5 <ref type="bibr">( [70]</ref>). Denote ? as the Kronecker product, given two matrices X, Y , we have rank(X ? Y ) = rank(X) ? rank(Y ). Note that the definition of degenerate structure A is that ?h = 0, h g K (A, r)t = 0, ?r, t or ?r = 0, h g K (A, r)t = 0, ?h, t. To proof that A is not degenerate if and only if rank(A) = K and {1, . . . , K} ? {|A ij | : i, j = 1 . . . K}, we prove its converse-negative proposition in Lemma 7. Lemma 7. ?h = 0, h g K (A, r)t = 0, ?r, t or ?r = 0, h g K (A, r)t = 0, ?h, t. if and only if rank(A) &lt; K or ?a ? {1, . . . , K}, a / ? {|A ij | :, i, j = 1 . . . K}.</p><p>This can be decomposed into two separate parts in Lemma 8 and 9. Lemma 8. ?h = 0, h g K (A, r)t = 0, ?r, t if and only if rank(A) &lt; K.</p><p>Proof. To begin with, we show the relationship between the rank of g K (A, r) and the rank of A. If we assign r = 1 ? R d , then the (i, j)-th block will be [g K (A, 1)] ij = sign(A ij ) ? I with the identity matrix I ? R d K ? d K . Using Kronecker product, we can write g K (A, 1) as a Kronecker product,</p><formula xml:id="formula_81">g K (A, 1) = sign(A) ? I,<label>(40)</label></formula><p>where ? here represents the Kronercker product and sign(A) is a K ? K matrix formed by the signs of elements in A. Then, based on Lemma 5, we have</p><formula xml:id="formula_82">rank g K (A, 1) = d /K ? rank(A),<label>(41)</label></formula><p>and ?r ? R d , rank g K (A, r) ? d /K ? rank(A).</p><p>? First, we show the sufficient condition, i.e., if rank(A) &lt; K, we have ?h = 0, h g K (A, r)t = 0, ?r, t.</p><p>Since A is not full rank, we have rank g K (A, r) &lt; d based on <ref type="bibr" target="#b40">(41)</ref>. Then based on Lemma 6, for all r ? R d there exists h = 0 that</p><formula xml:id="formula_83">g K (A, r) h = 0.</formula><p>This leads to h g K (A, r) = 0. Thus, if rank(A) &lt; K, ?h = 0, ?r, t, h g K (A, r)t = 0. ? Then we show the necessary condition, i.e., if ?h = 0, h g K (A, r)t = 0, ?r, t, we have rank(A) &lt; K. We assign r = 1, and a set of t with (1, 0, . . . , 0), (0, 1, . . . , 0), . . . , (0, 0, . . . , 1). Then, this will lead to the following equation group h {g K (A, 1)} ?,1 = 0, h {g K (A, 1)} ?,2 = 0, ? ? ? , h {g K (A, 1)} ?,d = 0.</p><p>We proof the necessary condition by contraction here. Assume rank(A) = K, then rank g K (A, 1) = rank g K (A, 1) = d based on <ref type="bibr" target="#b39">(40)</ref>. Then, based on Lemma 6, there is no h = 0 such that the above equation group is satisfied. Thus, the assumption that rank(A) = K is wrong. Therefore, if ?h = 0, h g K (A, r)t = 0, ?r, t, we have rank(A) &lt; K.</p><p>Based on the proof of sufficient and necessary conditions, we have ?h = 0, h g K (A, r)t = 0, ?r, t if and only if rank(A) &lt; K. Lemma 9. ?r = 0, h g K (A, r)t = 0, ?h, t if and only if ?a ? {1, . . . , K}, a / ? {|A ij | :, i, j = 1 . . . K}.</p><p>Proof. ? First, we show the sufficient condition, i.e., if ?a ? {1, . . . , K}, a / ? {|A ij | :, i, j = 1 . . . K}, we have ?r = 0, h g K (A, r)t = 0, ?h, t.</p><p>Given the K-chunk representation of r, if ? a ? {1, . . . , K}, a / ? {|A ij | :, i, j = 1 . . . K}, we assign</p><formula xml:id="formula_84">r i = 1 i = a 0 i = a .<label>(42)</label></formula><p>Then, r |Aij | = 0 is always true since |A ij | = a. This leads to g K (A, r) = 0 with [g K (A, r)] ij = sign(A ij ) ? diag (r |Aij | ) = 0. As a result, ?h, t, h g K (A, r)t = 0. Therefore, if ?a ? {1, . . . , K}, a / ? {|A ij | :, i, j = 1 . . . K}, we have ?r = 0, h g K (A, r)t = 0, ?h, t. ? Then, we show the necessary condition, i.e., if ?r = 0, h g K (A, r)t = 0, ?h, t, we have ?a ? {1, . . . , K}, a / ? {|A ij | :, i, j = 1 . . . K}. We can enumerate h, t as the set of unit vectors with one dimension as 1 and the remaining to be 0. Then from h g K (A, r)t = 0 we derive g K (A, r) = 0 since any element is 0. Specially, we have that each block in g K (A, r) is</p><formula xml:id="formula_85">[g K (A, r)] ij = sign(A ij ) ? diag (r |Aij | ) = 0.</formula><p>If the number of unique values of set {A ij } is K, we will have r = 0. This is in contrary to the fact that r = 0. Thus there must ?a ? {1, . . . , K}, a / ? {|A ij | :, i, j = 1 . . . K}. Thus, we have ?r = 0, h g K (A, r)t = 0, ?h, t if and only if ?a ? {1, . . . , K}, a / ? {|A ij | :, i, j = 1 . . . K}.</p><p>By combining Lemma 8 and Lemma 9, we can show Lemma 7 that ?h = 0, h g K (A, r)t = 0, ?r, t or ?r = 0, h g K (A, r)t = 0, ?h, t. if and only if rank(A) &lt; K or ?a ? {1, . . . , K}, a / ? {|A ij | :, i, j = 1 . . . K}. Since the original statement is equal to the converse-negative proposition, Proposition 2 is proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Proof of Proposition 2</head><p>Proof. Proposition 2, is equivalent to the statement that A is degenerate if and only if rank(A) &lt; K and ?a ? {1, . . . , K}, a / ? {|A ij | :, i, j = 1 . . . K}. From Definition 5, A is degenerate means 1) ?h = 0, h g K (A, r)t = 0, ?r, t; or 2) ?r = 0, h g K (A, r)t = 0, ?h, t. Here, Lemma 8 proves 1) and Lemma 9 proves 2). Thus, we can conclude that A is non-degenerate, if and only if rank(A) = K and {1, . . . , K} ? {|A ij | : i, j = 1 . . . K}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proposition 3</head><p>We denote the K-chunk representation of the embeddings as E = [E 1 ; . . . ; E K ] with E k ? R d K ?|E| , k = 1 . . . K and R = [R 1 ; . . . ; R K ] with R k ? R d K ?|E| , k = 1 . . . K. Besides, given the permutation matrix ?, we denote ?(i) = j and ? ?1 (j) = i if ? ij = 1 for i, j = 1 . . . K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Auxiliary Lemmas</head><p>Lemma 10. If below two conditions hold, then A ? A .</p><p>? given the optimal embedding E * and R * for f A (h, r, t) there exist E and R such that f A (h, r, t) = f A (h, r, t) always hold; ? given the optimal embedding E * and R * for f A (h, r, t), there exist E and R such that f A (h, r, t) = f A (h, r, t) always hold.</p><p>(d) ogbl-wikikg2 (Au-toBLM+) <ref type="figure" target="#fig_1">Fig. 13</ref>. A graphical illustration of g 4 (A, r) identified by AutoBLM and AutoBLM+ on the large-scale KG completion task with ogbl-biokg and ogbl-wikikg2 datasets. FB15k-n2</p><p>FB15k-n3</p><p>Fb237-n2 Fb237-n3 S2R</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FB15k-n2</head><p>FB15k-n3</p><p>Fb237-n2 Fb237-n3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2R</head><p>FB15k-n2 FB15k-n3</p><p>Fb237-n2 Fb237-n3 S2R</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FB15k-n2</head><p>FB15k-n3 </p><formula xml:id="formula_86">= K i=1 K j=1 sign(A ij ) h i , r |Aij | , t j , = K i=1 K j=1 s |A ij | ? sign(A ij ) ? h * i , s |A ij | ? r * |A ij | , t * j , = K i=1 K j=1 s 2 |A ij | ? sign(A ij ) ? h * i , r * |A ij | , t * j , = K i=1 K j=1 sign(A ij ) ? h * i , r * |A ij | , t * j , = f A (h, r, t).</formula><p>Finally, based on Lemma 10, we have A ? A .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B RELATION DISTRIBUTION IN DIFFERENT DATASETS</head><p>Following <ref type="bibr" target="#b70">[71]</ref>, if more than half of the training triples of a relation r have inverse triples (i.e., |{(t, r, h) ? S tra : (h, r, t) ? S tra }| &gt; 1 2 |{(h, r, t) ? S tra }|), r is considered as symmetric. If there exists no inverse triplet (i.e., |{(t, r, h) ? S tra : (h, r, t) ? S tra }| = 0), r is anti-symmetric. Relations that are neither symmetric nor anti-symmetric are general asymmetric. Finally, a relation r belongs to the inverse type if ?r ? R : |{(t, r , h) ? S tra : (h, r, t) ? S tra }| &gt; 1 2 |{(h, r, t) ? S tra }|.</p><p>As can be seen, the four datasets have very different distributions and thus properties. As demonstrated in neural architecture search <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>, different datasets need different neural architectures. The architectures discovered have better performance than those designed by humans. Hence, the scoring functions should also be data-dependent, as demonstrated empirically in Section 5.1.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C CONSISTENT PERFORMANCE UNDER DIFFERENT DI-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MENSIONS</head><p>In this section, we perform the KG completion experiment in Section 5.1. First, we collect the first 100 structures A's (with d = 64) of AutoBLM+ in Section 5.1 and measure the corresponding validation MRR performance in step 18 of Algorithm 4. We then increase the embedding dimensionality to 1024, retrain and re-evaluate these structures. <ref type="figure" target="#fig_9">Figure 16</ref> compares the validation MRRs obtained with d = 64 and d = 1024 on the WN18RR and FB15k-237 data sets. As can be seen, the two sets of MRRs are correlated, especially for the top performed ones. The Spearman's rank correlation coefficient on WN18RR is 0.4255 and on FB15k-237 is 0.7054.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D MODELS OBTAINED BY AUTOBLM AND AUTO-BLM+</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1.2 and 5.1.3) and analysis (Section 5.1.4) of the new search algorithm, analysis on the influence of K (Section 5.1.7), and the problem of parameter sharing (Section 5.1.8) to analyze the design schemes in the search space and search algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of R (r) for Analogy (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>A: a K ? K structure matrix, H: a set of structures. 1: initialization: Q(A, H) = True. 2: if det(A) = 0 or {1, . . . , K} ? {|Aij| : i, j = 1, . . . , K}, then Q(A, H) = False. 3: generate a set of equivalent structures {A : A ? A} by enumerating permutation matrices P 's and sign vectors s's. 4: for A in {A : A ? A} do 5: if A ? H, then Q(A, H) = False, and exit the loop. 6: end for 7: return Q(A, H).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2</head><label>2</label><figDesc>Construction of the symmetry-related feature (SRF) vectors. Input: structure matrix A. 1: initialization: ?, ? := 0. 2: for r ? C do 3: if r = 0 then 4: x = {i : ri = 0} ; 5: y = {j &gt; 0 : ri = j or ri = ?j} ; // for symmetric case 6: if gK (A, r)?gK (A, r) = 0 then ? (x,y) = 1; // for skew-symmetric case 7: if gK (A, r)+gK (A, r) = 0 then ? (x,y) = 1; 8: end if 9: end for 10: return [vec(?); vec(?)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 3</head><label>3</label><figDesc>Progressive search algorithm (AutoBLM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 4</head><label>4</label><figDesc>Evolutionary search algorithm. (AutoBLM+).Input: I: number of top structures; N : number of generated structures; P : number of structures selected by P; b0: number of nonzero elements in initial structures; filter Q, and performance predictor P. 1: initialization: I = ?; 2: for each A ? {A (b 0 ) } do3:    if Q(A, I) from Algorithm 1 is true then I ? I ? {A}; 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Hyper- parameters .</head><label>parameters</label><figDesc>The search algorithms have the following hyper-parameters: (i) N : number of candidates generated after filtering; (ii) P : number of scoring functions selected by the predictor; (iii) I: number of top structures selected in Algorithm 3 (step 13), or the number of structures survived in I in Algorithm 4; and (iv) b 0 : number of nonzero elements in the initial set. Unless otherwise specified, we use N = 128, P = 8, I = 8 and b 0 = K. For the evolutionary algorithm, the mutation and crossover operations are selected with equal probabilities. When 4. https://ogb.stanford.edu/docs/leader linkprop/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Graphical illustration of the BLMs obtained by AutoBLM (top) and AutoBLM+ (bottom) on the KG completion task (Section 5.1.2). Different colors correspond to different parts of [r 1 (red), r 2 (blue), r 3 (yellow), r 4 (gray)]. Solid lines mean positive values, while dashed lines mean negative values. The empty parts have value zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>For a fair comparison, we do not use the filter and performance predictor in the proposed AutoBLM and AutoBLM+ here. All structures selected by each of the above algorithms are trained and evaluated with the same hyperparameter settings in step 6 of Algorithm 5. Each algorithm evaluates a total of 256 structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of different search algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Comparison of the effect of filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Effectiveness of the performance predictor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8</head><label>8</label><figDesc>shows the mean validation MRR of the top I = 8 structures w.r.t. clock time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 .</head><label>9</label><figDesc>Comparison of different K values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 .</head><label>10</label><figDesc>MRRs of structures as estimated by the parameter-sharing approach and stand-alone approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>where d is the intermediate layer dimension, and is the number of classes. The parameters, including embeddings of entities, relations, W 0 , W 's and the MLP, are optimized by minimizing the cross-entropy loss on the labeled entities: L = ? i?B C c=1 y ic ln e o ic , where B is the set of labeled entities, y ic ? {0, 1} indicates whether the ith entity belongs to class c, and e o ic is the cth dimension of e o i . Three graph datasets are used</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Lemma 2 .</head><label>2</label><figDesc>If ?? ? C, g K (A,?) = ?g K (A,?), there exist E ? R KN ?N andr ? R KN such that any skew-symmetric matrixG =? g K (A,r)? ? R N ?N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>2 , i mod 2 = 1 2 , j 2 , i mod 2 = 0 and j mod 2</head><label>2212222</label><figDesc>and j mod 2 = 1 {g K (A,r)} i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 11 .</head><label>11</label><figDesc>Graphical illustration of the composed embeddings. The blue parts are from?,? or g K (A,?) and the red parts are from?,r or g K (A,r). The white spaces in g K (A, r) are zero values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 12 .</head><label>12</label><figDesc>{g K (A, r)} i,j = {g K (A,? r )} i |R| , j |R| i mod |R| = r and j mod |R| = r 0 otherwise . Graphical illustration of the composed embeddings. Different colors represent components from different embeddings. White colors mean the zero values and gray colors represent spaces with mixed colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Lemma 6 (</head><label>6</label><figDesc><ref type="bibr" target="#b69">[70]</ref>). Given A ? R d?d , there is no non-zero solution x = 0 ? R d for the equation group Ax = 0, if and only if rank(A) = d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 15 .</head><label>15</label><figDesc>FB15k237-3p. Fig. 14. A graphical illustration of g 4 (A, r) identified by AutoBLM (top) and AutoBLM+ (bottom) on the multi-hop query task. A graphical illustration of g 4 (A, r) identified by AutoBLM (top) and AutoBLM+ (bottom) on the entity classification task.E * , R k = s k ? R * k , k = 1 . . . K.In this way, we always have f A (h, r, t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 16 .</head><label>16</label><figDesc>Validation MRRs of the structures with d = 64 and d = 1024.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figures 13 ,</head><label>13</label><figDesc>14 and 15  show the structures obtained by AutoBLM and AutoBLM+ on the tasks of KG completion (Section 5.1) for the ogbl-biokg and ogbl-wikikg2 datasets, multi-hop query (Section 5.2) and entity classification (Section 5.3), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>was with 4Paradigm Inc. Beijing, China. E-mail: zhangy-ongqi@4paradigm.com</figDesc><table /><note>? Q. Yao was with Department of Electronic Engineering, Tsinghua Univer- sity and 4Paradigm Inc. Beijing, China. E-mail: qyaoaa@connect.ust.hk? J.T. Kwok was with Department of Computer Science, Hong Kong University of Science and Technology. Hong Kong, China. E-mail: jamesk@cse.ust.hk The code is public available at https://github.com/AutoML-Research/AutoSF. Corresponding author: Quanming Yao.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Notations used in the paper. E, R, S set of entities, relations, triples |E|, |R|, |S| number of entities, relations, triples (h, r, t) triple of head entity, relation and tail entity h, r, t embeddings of h, r, and t f (h, r, t)scoring function for triple (h, r, t)R d , C d , H d d-dimensional real/complex/hypercomplex space R (r) ? R d?dsquare matrix based on relation embedding r a, b, c triple product</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Popular properties in KG relations. property examples in WN18/FB15k constraint on f symmetry isSimilarTo, spouseOf f (t, r, h) = f (h, r, t) anti-symmetry</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>The representative BLM scoring functions. For each scoring function we show the definitions, expressiveness in Definition 1, the ability to model all common relation patterns inTable 2("RP" for short), and the number of parameters.</figDesc><table><row><cell>scoring function</cell><cell>definition</cell><cell>expressiveness</cell><cell>RP</cell><cell># parameters</cell></row><row><cell>RESCAL [6]</cell><cell>h R (r) t</cell><cell>?</cell><cell>?</cell><cell>O |E|d + |R|d 2</cell></row><row><cell>DistMult [7]</cell><cell>h, r, t</cell><cell>?</cell><cell>?</cell><cell>O(|E|d + |R|d)</cell></row><row><cell>ComplEx [14]/HolE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. . . , K}, and 0 otherwise. 2 Two structure matrices A and A are equivalent if any one of the following conditions is satisfied.</figDesc><table /><note>(i) Permuting rows and columns: There exists a permutation matrix ? ? {0, 1} K?K such that A = ? A?. (ii) Permuting values: There exists a permutation matrix ? ? {0, 1} K?K such that ? A = ?? A ;2. Intuitively, in ? A , the indexes of nonzero values in its |A ij |-th row indicate positions of elements in A whose absolute values are |A ij |.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Algorithm 5</head><label>5</label><figDesc>Experimental procedure for each KG task. Here, HP denotes the hyper-parameters {?, ?, m, d}. : // stage 1: configure hyper-parameters for scoring function search. 2: for i = 1, . . . , 10 do 3: fix d = 64, randomly select ?i?[0, 1], ?i?[10 ?5 , 10 ?1 ] and mi ? {256, 512, 1024};</figDesc><table><row><cell>4:</cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Statistics of the KG completion datasets.</figDesc><table><row><cell>number of samples</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 Testing</head><label>7</label><figDesc>MRR on applying the BLMs obtained from a source dataset (row) to a target dataset (column). Bold numbers indicate the best performance each dataset for the models searched by AutoBLM and AutoBLM+ respectively.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">WN18 FB15k WN18RR FB15k237 YAGO3-10</cell></row><row><cell></cell><cell>WN18</cell><cell>0.952 0.841</cell><cell>0.473</cell><cell>0.349</cell><cell>0.561</cell></row><row><cell></cell><cell>FB15k</cell><cell>0.950 0.853</cell><cell>0.470</cell><cell>0.350</cell><cell>0.563</cell></row><row><cell>AutoBLM</cell><cell cols="2">WN18RR 0.951 0.833</cell><cell>0.490</cell><cell>0.345</cell><cell>0.568</cell></row><row><cell></cell><cell cols="2">FB15k237 0.894 0.781</cell><cell>0.462</cell><cell>0.360</cell><cell>0.565</cell></row><row><cell></cell><cell cols="2">YAGO3-10 0.885 0.835</cell><cell>0.466</cell><cell>0.352</cell><cell>0.571</cell></row><row><cell></cell><cell>WN18</cell><cell>0.952 0.848</cell><cell>0.482</cell><cell>0.350</cell><cell>0.564</cell></row><row><cell></cell><cell>FB15k</cell><cell>0.951 0.861</cell><cell>0.479</cell><cell>0.352</cell><cell>0.563</cell></row><row><cell>AutoBLM+</cell><cell cols="2">WN18RR 0.947 0.841</cell><cell>0.492</cell><cell>0.347</cell><cell>0.551</cell></row><row><cell></cell><cell cols="2">FB15k237 0.860 0.821</cell><cell>0.463</cell><cell>0.364</cell><cell>0.546</cell></row><row><cell></cell><cell cols="2">YAGO3-10 0.951 0.833</cell><cell>0.469</cell><cell>0.345</cell><cell>0.577</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8</head><label>8</label><figDesc>Testing MRR and number of parameters on ogbl-biokg and ogbl-wikikg2. The best performance is indicated in boldface.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>66] and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gaussian mixture model (GMM);</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(iii) Reinforce, which generates the K 2 elements in A by</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>using a LSTM [41] recurrently as in NAS-Net [30]. The</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LSTM is optimized with REINFORCE [67];</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(iv) AutoBLM (no Filter, no Predictor</cell></row><row><cell></cell><cell cols="2">ogbl-biokg</cell><cell cols="2">ogbl-wikikg2</cell></row><row><cell>model</cell><cell>MRR</cell><cell># params</cell><cell>MRR</cell><cell># params</cell></row><row><cell>TransE</cell><cell>0.745</cell><cell>188M</cell><cell>0.426</cell><cell>1251M</cell></row><row><cell>RotatE</cell><cell>0.799</cell><cell>188M</cell><cell>0.433</cell><cell>1250M</cell></row><row><cell>PairE</cell><cell>0.816</cell><cell>188M</cell><cell>0.521</cell><cell>500M</cell></row><row><cell>DistMult</cell><cell>0.804</cell><cell>188M</cell><cell>0.373</cell><cell>1250M</cell></row><row><cell>ComplEx</cell><cell>0.810</cell><cell>188M</cell><cell>0.403</cell><cell>1250M</cell></row><row><cell>AutoBLM</cell><cell>0.828</cell><cell>188M</cell><cell>0.532</cell><cell>500M</cell></row><row><cell>AutoBLM+</cell><cell>0.831</cell><cell>94M</cell><cell>0.546</cell><cell>500M</cell></row></table><note>5.1.4 Ablation Study 1: Search Algorithm Selection First, we study the following search algorithm choices. (i) Random, which samples each element of A indepen- dently and uniformly from {0, ?1, . . . , ?K}; (ii) Bayes, which selects each element of A from {0, ?1, . . . , ?K} by performing hyperparameter optimization using the Tree Parzen estimator [</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 9</head><label>9</label><figDesc>Running time (in minutes) of different components in Algorithm 4. Ablation Study 5: Analysis of Parameter Sharing As mentioned in Section 4.2, parameter sharing may not reliably predict the model performance. To demonstrate this, we empirically compare the parameter-sharing approach, which shares parameter P = {E, R} (where E ? R d?|E| is the entity embedding matrix and R ? R d?|R| is the relation embedding matrix in Section 2.1) and the standalone approach, which trains each model separately. For parameter sharing, we randomly sample a A in each training iteration from the set of top candidate structures (H</figDesc><table><row><cell>dataset</cell><cell cols="5">K filter performance predictor train evaluate</cell></row><row><cell></cell><cell cols="2">3 0.04</cell><cell>1</cell><cell>1217</cell><cell>152</cell></row><row><cell>WN18RR</cell><cell>4</cell><cell>1.4</cell><cell>23</cell><cell>1231</cell><cell>156</cell></row><row><cell></cell><cell>5</cell><cell>90</cell><cell>276</cell><cell>1252</cell><cell>161</cell></row><row><cell></cell><cell cols="2">3 0.04</cell><cell>1</cell><cell>714</cell><cell>178</cell></row><row><cell>FB15k237</cell><cell>4</cell><cell>1.5</cell><cell>22</cell><cell>721</cell><cell>181</cell></row><row><cell></cell><cell>5</cell><cell>91</cell><cell>283</cell><cell>728</cell><cell>186</cell></row><row><cell>5.1.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>b in Algorithm 3 or H in Algorithm 4), and then update parameter P . After one training epoch, the sampled structures are evaluated. After 500 training epochs, the top- 100 A's are output.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 10</head><label>10</label><figDesc>Testing performance of H@3 and MRR on multi-hop query task. Results of 's are copied from<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table><row><cell></cell><cell>FB15K</cell><cell></cell><cell>FB15K237</cell></row><row><cell></cell><cell>2p</cell><cell>3p</cell><cell>2p</cell><cell>3p</cell></row><row><cell></cell><cell cols="4">H@3 MRR H@3 MRR H@3 MRR H@3 MRR</cell></row><row><cell cols="5">TransE-Comp [39] 27.3 .264 15.8 .153 19.4 .177 14.0 .134</cell></row><row><cell cols="5">Diag-Comp [39] 32.2 .309 27.5 .266 19.1 .187 15.5 .147</cell></row><row><cell>GQE [8]</cell><cell cols="4">34.6 .320 25.0 .222 21.3 .193 15.5 .145</cell></row><row><cell>Q2B [4]</cell><cell cols="4">41.3 .373 30.3 .274 24.0 .225 18.6 .173</cell></row><row><cell>AutoBLM</cell><cell cols="4">41.5 .402 29.1 .283 23.6 .232 18.2 .180</cell></row><row><cell>AutoBLM+</cell><cell cols="4">43.2 .415 30.7 .293 24.9 .248 19.9 .196</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 11</head><label>11</label><figDesc>Data sets used in entity classification. Sparsity is computed as #edges/(#entity 2 ? #relation).</figDesc><table><row><cell cols="7">dataset #entity #relation #edges #train #test #classes sparsity</cell></row><row><cell>AIFB</cell><cell>8,285</cell><cell>45</cell><cell>29,043 140</cell><cell>36</cell><cell>4</cell><cell>9.4e-6</cell></row><row><cell cols="2">MUTAG 23,644</cell><cell>23</cell><cell>74,227 272</cell><cell>68</cell><cell>2</cell><cell>5.7e-6</cell></row><row><cell>BGS</cell><cell>333,845</cell><cell>103</cell><cell>916,199 117</cell><cell>29</cell><cell>2</cell><cell>8.0e-8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Kwok (Fellow, IEEE) received the Ph.D. degree in computer science from The Hong Kong University of Science and Technology in 1996. He is a Professor with the Department of Computer Science and Engineering, Hong Kong University of Science and Technology. His research interests include machine learning, deep learning, and artificial intelligence. He received the IEEE Outstanding 2004 Paper Award and the Second Class Award in Natural Sciences by the Ministry of Education, China, in 2008. He is serving as an Associate Editor for the IEEE Transactions on Neural Networks and Learning Systems, Neural Networks, Neurocomputing, Artificial Intelligence Journal, International Journal of Data Science and Analytics, Editorial Board Member of Machine Learning, Board Member, and Vice President for Publications of the Asia Pacific Neural Network Society. He also served/is serving as Senior Area Chairs / Area Chairs of major machine learning / AI conferences including NIPS, ICML, ICLR, IJCAI, AAAI and ECML.</figDesc><table><row><cell>James T.</cell></row><row><cell>Yongqi Zhang (Member, IEEE) is a senior</cell></row><row><cell>researcher in 4Paradigm. He obtained his Ph.D.</cell></row><row><cell>degree at the Department of Computer Science</cell></row><row><cell>and Engineering of Hong Kong University of</cell></row><row><cell>Science and Technology (HKUST) in 2020 and</cell></row><row><cell>received his bachelor degree at Shanghai Jiao</cell></row><row><cell>Tong University (SJTU) in 2015. He has pub-</cell></row><row><cell>lished five top-tier conference/journal papers as</cell></row><row><cell>first-author, including NeurIPS, ACL, WebConf,</cell></row><row><cell>ICDE, VLDB-J. His research interests focus</cell></row><row><cell>on knowledge graph embedding, automated</cell></row><row><cell>machine learning and graoh learning. He was a Program Committee</cell></row><row><cell>for AAAI 2020-2022, IJCAI 2020-2022, CIKM 2021, KDD 2022, ICML</cell></row><row><cell>2022, and a reviewer for TKDE and NEUNET.</cell></row><row><cell>Quanming Yao (Member, IEEE) is a tenure-</cell></row><row><cell>track assistant professor at Department of Elec-</cell></row><row><cell>tronic Engineering, Tsinghua University. Before</cell></row><row><cell>that, he spent three years from a researcher to</cell></row><row><cell>a senior scientist in 4Paradigm INC, where he</cell></row><row><cell>set up and led the company's machine learning</cell></row><row><cell>research team. He is a receipt of Wunwen</cell></row><row><cell>Jun Prize of Excellence Youth of Artificial In-</cell></row><row><cell>telligence (issued by CAAI), the runner up of</cell></row><row><cell>Ph.D. Research Excellence Award (School of</cell></row><row><cell>Engineering, HKUST), and a winner of Google</cell></row><row><cell>Fellowship (in machine learning). Currently, his main research topics are</cell></row><row><cell>Automated Machine Learning (AutoML) and neural architecture search</cell></row><row><cell>(NAS). He was an Area Chair for ICLR 2022, IJCAI 2021 and ACML</cell></row><row><cell>2021; Senior Program Committee for IJCAI 2020 and AAAI 2020-2021;</cell></row><row><cell>and a guest editor of IEEE TPAMI AutoML special issue in 2019.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>, ?h, t, we haveG ht =[?] ht +[G] ht =? :,h g K (A,?)? :,t +? :,h g K (A,r)? :,t ,</figDesc><table><row><cell>=</cell><cell cols="2">KN m,n? m,h {g K (A,?)} m,n?n,t + 0</cell><cell>(29)</cell></row><row><cell></cell><cell>+ 0 +</cell><cell>KN m,n? m,h {g K (A,r)} m,n?n,t</cell><cell>(30)</cell></row><row><cell>=</cell><cell cols="2">2KN,2KN (i,j) mod 2=(1,1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 13</head><label>13</label><figDesc>Distribution of relation types in the testing set.</figDesc><table><row><cell></cell><cell cols="4">symmetry anti-symmetry general asymmetry inverse</cell></row><row><cell>WN18</cell><cell>23.4%</cell><cell>72.1%</cell><cell>4.5%</cell><cell>4.5%</cell></row><row><cell>FB15k</cell><cell>9.3%</cell><cell>5.2%</cell><cell>85.5%</cell><cell>74.9%</cell></row><row><cell>WN18RR</cell><cell>37.4%</cell><cell>59.0%</cell><cell>3.6%</cell><cell>0.0%</cell></row><row><cell>FB15k237</cell><cell>3.0%</cell><cell>8.5%</cell><cell>88.5%</cell><cell>10.5%</cell></row><row><cell>YAGO3-10</cell><cell>3.4%</cell><cell>0.7%</cell><cell>95.9%</cell><cell>8.2%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Obtained from https://github.com/thunlp/OpenKE and https:// github.com/Sujit-O/pykg2vec</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>sign(A ? ?1 (i),? ?1 (j) ) ? h * ? ?1 (i) , r * |A ? ?1 (i),? ?1 (j) | , t * ? ?1 (j) ,</p><p>In the third to fourth line, we set i = ? ?1 (i) and j = ? ?1 (j). Finally, based on Lemma 10, we have A ? A . (ii) We can permute the corresponding chunks in relation embedding to get the same scores.</p><p>If there exists a permutation matrix ? ? {0, 1} K?K that ? A = ?? A , we will have |A i,j | = ?(|A ij |), |A ij | = ? ?1 (|A ij |) and sign(A ij ) = sign(A ij ). Given E * , R * as the optimal embeddings trained by</p><p>In this way, we alway have</p><p>In this way, we always have</p><p>with |A i,j | = ?(|A ij |). Finally, based on Lemma 10, we have A ? A . (iii) We can flip the signs of corresponding chunks in relation embedding to get the same scores. If there exists a sign vector s ? {?1} K that [? A ] k,? = s k ?[? A ] k,? , ?k = 1 . . . K, we will have A ij = s k ?A ij and A ij = s k ? A ij with k = |A ij | = |A ij | and s k ? {?1}. Given E * , R * as the optimal embedding trained by f A (h, r, t), we can set E , R with E = E * , R k = s k ? R * k , k = 1 . . . K. In this way, we always have</p><p>sign(A ij ) ? h * i , r * |Aij | , t * j , = f A (h, r, t), with s 2 |Aij | = 1. Similarly, given E * , R * as the optimal embeddings trained by f A (h, r, t), we can set E, R with E =</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Introducing the knowledge graph: Things, not strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Official Google blog</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Query2box: Reasoning over knowledge graphs in vector space using box embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Embedding logical queries on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2026" to="2037" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Transitionbased knowledge graph embedding with relational mapping properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PACLIC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">RotatE: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge graph completion via complex tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4735" to="4772" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Analogical inference for multirelational embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2168" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SimplE embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Quaternion knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge vault: A webscale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Convolutional 2D knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to exploit long-term relational dependencies in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2505" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Compositionbased multi-relational graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge representation learning: A quantitative review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10901</idno>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On evaluating embedding models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RepL4NLP@ACL, Tech. Rep</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Taking human out of learning applications: A survey on automated machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13306</idno>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automated machine learning: Methods, systems, challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient and robust automated machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2962" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">AutoSF: Searching scoring functions for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">On multi-relational link prediction with bilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tucker: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interstellar: Searching recurrent architecture for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno>pp. 10 030-10 040</idno>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the equivalence of holographic and complex embeddings for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="554" to="559" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An overview of bilevel optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Colson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Savard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="549" to="558" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="4095" to="4104" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via proximal iterations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jonathon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Greed is good: Algorithmic results for sparse approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIT</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2231" to="2242" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Evolutionary algorithms in theory and practice: evolution strategies, evolutionary programming, genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on CVSMC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Yago: A core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>WWW</publisher>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22" to="118" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Yago3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pairre: Knowledge graph embeddings via paired relation vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4360" to="4369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Nase: Learning knowledge graph embedding for link prediction via neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIKM, 2020</title>
		<imprint>
			<biblScope unit="page" from="2089" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">On evaluating embedding models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meilicke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="104" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Probability calibration for knowledge graph embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tabacof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Costabello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>K?gl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Rdf2vec: Rdf graph embeddings for data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="498" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Knowledge graph embedding for link prediction: A comparative analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Firmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matinata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merialdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barbosa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">If given the optimal embedding E * and R * for the scoring function f A (h, r, t) there exist E and R</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Proof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denote P * = {e * , R * }</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P * = {e *</forename><surname>{e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R * }</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>{e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R }</forename><surname>Then</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">we have P * = arg max P M (F (P ; A), S) and P * = arg max P M (F (P ; A ), S)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>such that f A (h, r, t) = f A (h, r, t), we will have</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
