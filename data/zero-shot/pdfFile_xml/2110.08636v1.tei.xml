<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DPC: Unsupervised Deep Point Correspondence via Cross and Self Construction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itai</forename><surname>Lang</surname></persName>
							<email>itailang@mail.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University Tel Aviv University Tel Aviv University Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaaaaadvir</forename><surname>Ginzburg</surname></persName>
							<email>dvirginzburg@mail.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University Tel Aviv University Tel Aviv University Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avidan</forename><surname>Aaaaaaadan Raviv</surname></persName>
							<email>avidan@eng.tau.ac.ildarav@tauex.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University Tel Aviv University Tel Aviv University Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DPC: Unsupervised Deep Point Correspondence via Cross and Self Construction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new method for real-time non-rigid dense correspondence between point clouds based on structured shape construction. Our method, termed Deep Point Correspondence (DPC), requires a fraction of the training data compared to previous techniques and presents better generalization capabilities. Until now, two main approaches have been suggested for the dense correspondence problem. The first is a spectral-based approach that obtains great results on synthetic datasets but requires mesh connectivity of the shapes and long inference processing time while being unstable in real-world scenarios. The second is a spatial approach that uses an encoder-decoder framework to regress an ordered point cloud for the matching alignment from an irregular input. Unfortunately, the decoder brings considerable disadvantages, as it requires a large amount of training data and struggles to generalize well in cross-dataset evaluations. DPC's novelty lies in its lack of a decoder component. Instead, we use latent similarity and the input coordinates themselves to construct the point cloud and determine correspondence, replacing the coordinate regression done by the decoder. Extensive experiments show that our construction scheme leads to a performance boost in comparison to recent state-of-the-art correspondence methods. Our code is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the rise of the availability and vast deployment of 3D data sensors in various fields, 3D computer vision research has thrived. A core problem in 3D vision is shape correspondence. That is, finding a dense mapping from one shape to another. This information opens the door to a variety of applications, including non-rigid human body alignment, articulated motion transfer, face swapping, and more.</p><p>The shape correspondence problem has been thoroughly investigated for 3D mesh data <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b1">2]</ref>. Recent works have 1 https://github.com/dvirginz/DPC * Equal contribution aaReference shape aaOur result <ref type="figure">Figure 1</ref>. Correspondence by DPC. Our method learns a finegrained point-to-point mapping between point clouds without matching supervision. It is versatile and can be applied to various non-rigid shapes, including humans and animals. The correspondence is visualized by transferring colors from the left to the right shape according to our resulting matches. taken a spectral approach by computing the functional mapping between the projected features of the shapes onto their Laplace-Beltrami Operator (LBO) eigenbasis <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b4">5]</ref>. Functional mapping methods rely on the global geometric shape structure and learn a transformation between the eigendecomposition of source and target shapes, which is then translated to a point-to-point correspondence. Spectral matching techniques for 3D meshes have been proven to be extremely successful. However, the computation of the eigenbasis for each shape is a resourcedemanding and time-consuming pre-processing step. Even more consequential, the use of such methods in deep learn-ing pipelines is unstable, making them impractical in many cases, as demonstrated in the literature <ref type="bibr" target="#b8">[9]</ref> and encountered again in this paper. Moreover, the computation of the LBO basis functions <ref type="bibr" target="#b23">[24]</ref> requires the connectivity information between the mesh vertices. Such information is often absent in a real-world scenario when the data originates from a 3D sensing device and contains only the point coordinate information.</p><p>Recently, correspondence methods for point cloud data have been proposed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">32]</ref>. Point-based techniques typically employ an encoder-decoder architecture, where the decoder is used as a proxy for determining point assignments. For example, 3D-CODED <ref type="bibr" target="#b10">[11]</ref> deformed a given template shape to reconstruct different point clouds. The decoder regressed each point cloud's coordinates and the correspondence was set according to proximity to the given points in the template.</p><p>The use of a decoder burdens the learning process of point cloud matching. Since the decoder performs a regression operation, it demands a large amount of training data. Additionally, the decoder is adapted to the distribution of the training shapes and limits the generalization capability of point-based methods. Up-to-date, neither shape correspondence method has overcome the shortcoming in generalization power while presenting real-time performance.</p><p>In this work, we present a novel technique for real-time dense point cloud correspondence that learns discriminative point embedding without a decoder. Instead, for each source point, we approximate the corresponding point by a weighted average of the target points, where the weights are determined according to a local neighborhood in the learned feature space. We call this operation cross-construction, as it uses the existing target points themselves and the latent affinity to construct the target shape rather than regress the points with a decoder. Similarly, we use a self-construction operation for each shape to constrain the learned features to be spatially smooth, thus, reducing outlier matches and improving the correspondence accuracy. Our method does not require matching points supervision, as we use the given input point clouds in the optimization of the construction results.</p><p>Extensive evaluations on major shape matching benchmarks of human and non-human figures demonstrate the advantages of our method. Compared to previous works, we show substantial performance gains while using a fraction of the training data. To summarize, our key contributions are as follows:</p><p>? We propose a novel unsupervised real-time approach for dense point cloud correspondence 2 . It employs point similarity in a learned feature space and structured construction modules, rather than a point regression by a decoder network, as done in existing works; 2 38 shape pairs per second, where each shape contains 1024 points.</p><p>? Our method can be trained on small datasets and exhibits compelling generalization capability; ? We surpass the performance of existing methods by a large margin for human and animal datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Feature extraction Powerful feature learning techniques have been proposed for 3D point clouds <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref>. PointNet <ref type="bibr" target="#b20">[21]</ref> and PointNet++ <ref type="bibr" target="#b21">[22]</ref> pioneered an MLPbased learning approach directly on raw point clouds. DGCNN <ref type="bibr" target="#b29">[30]</ref> extended these works and proposed an edgeconvolution operator, along with dynamic feature aggregation. As DGCNN has been proven to be a discriminative feature extractor, we use it in our work as the backbone network for deep point embedding.</p><p>Shape correspondence A substantial body of research has been dedicated to the task of dense shape correspondence <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b1">2]</ref>. The goal of this task is to find a point-to-point mapping between a pair of shapes. A prominent approach for mesh-represented shapes is based on functional maps, in which a linear operator is optimized for spectral shape bases alignment <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. One advantage of this technique is the structured correspondence prediction using the learned functional map. However, the calculation of the spectral basis comes at the cost of high computational demand and pre-processing time and requires mesh connectivity information. Lately, Marin et al. <ref type="bibr" target="#b16">[17]</ref> suggested a spectral matching approach for point clouds. They circumvented the LBO-based eigendecomposition by employing a neural network to learn basis functions for the shapes. Then, these functions were used in a functional-maps framework to derive point correspondences.</p><p>In another line of work, a spatial approach has been taken. This kind of works can operate directly on raw 3D point clouds without the need for a costly pre-processing step. Recently, Groueix et al. <ref type="bibr" target="#b10">[11]</ref> and Deprelle et al. <ref type="bibr" target="#b3">[4]</ref> proposed to extract point correspondences via a learned deformation of a template shape. Zeng et al. <ref type="bibr" target="#b31">[32]</ref> presented a deep neural network to learn a matching permutation between two point clouds, guided by the reconstruction of both shapes. These methods rely on a decoder network to regress the shape coordinates for reconstruction. This decoder trammels the learning process, as in addition to point representation, the model also needs to learn weights for the reconstruction part. In contrast, we employ a structured shape construction approach. Rather than regressing the point cloud coordinates, we use its original points and a similarity measure between learned point representations to cross-construct the shapes for determining correspondence.</p><p>Point cloud construction Our construction operation relates to works in the literature from other domains, such as point cloud sampling and registration <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Lang et al. <ref type="bibr" target="#b13">[14]</ref> learned to sample a point cloud as a combination of points in the complete point set. As opposed to our work, they employed a similarity metric in the raw point space and used sparse point clouds. Wang and Solomon <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> used a learned feature embedding to map one point cloud to another other for computing a global rigid transformation between the two point clouds that represent the same rigid shape. Our work differs fundamentally. We seek to find per-point assignments between point clouds that represent non-rigid shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>A point cloud is a set of unordered 3D point coordinates X ? R n?3 , where n is the number of points. Given two point clouds X , Y ? R n?3 , referred to as source and target, respectively, our goal is to find a mapping f : X ? Y, such that for each point x i ? X , we obtain the corresponding point y j * ? Y, where 1 ? i, j * ? n.</p><p>Our approach operates on raw point clouds and does not require point connectivity information. A diagram of the method is presented in <ref type="figure">Figure 2</ref>. We divide the method into three components: deep feature extraction, affinity matrix computation, and cross and self-construction. The first part leverages a deep neural network to learn a high-dimensional point feature embedding. Next, we measure similarity between points in the two point sets based on their learned representation. Finally, we use unsupervised construction modules that drive the learning process to produce discriminative and smooth point feature fields, which are suitable for the dense matching problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Per-point Feature Embedding</head><p>To learn a high-dimensional point representation for correspondence F X , F Y ? R n?c , where c is the feature dimension, we use a variant of the DGCNN model <ref type="bibr" target="#b29">[30]</ref>. DGCNN applies a series of learned convolutions on the difference between the features of a point and its neighbors. The neighbors are set dynamically according to their feature representation for exploiting non-local point information. This architecture has been proven very efficient for high-level tasks, such as point cloud classification and semantic segmentation. In these tasks, there are typically a few tens of classes.</p><p>However, in our case, the point affiliation is in the order of thousands of candidate corresponding points. Moreover, as the 3D shapes often have co-occurred similar segments, like human arms or animal legs, such a dynamic neighborhood frequently leaks information from one segment to the other, causing symmetry inconsistencies, as discussed previously in the literature <ref type="bibr" target="#b9">[10]</ref>. Thus, we use a static neighborhood graph and describe each point according to its local geometry to increase the granularity of the point feature representation. The complete architecture details are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Similarity Measure</head><p>The matching between the source and target point clouds is determined according to the similarity between the points' latent embedding ( <ref type="figure">Figure 3</ref>). We measure proximity as the cosine of the angle between their feature vectors:</p><formula xml:id="formula_0">s ij = F i X ? (F j Y ) T ||F i X || 2 ||F j Y || 2 ,<label>(1)</label></formula><p>where F i X , F j Y ? R c are the i'th and j'th rows of F X and F Y , respectively, and (?) T denotes a transpose operation. S X Y denotes the affinity matrix between the point sets, where S ij X Y = s ij . We use the cosine similarity since it inherently includes feature vector normalization and its range is bounded. These properties contribute to the stability of our correspondence learning pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Construction Modules</head><p>Our dense correspondence learning process is unsupervised. To learn the point representation without matching annotations, we use two novel modules: cross-construction and self-construction. The cross-construction operator promotes unique point matches between the shape pair, while the self-construction operator acts as a regularizer and encourages the correspondence map to be smooth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-construction</head><p>The cross-construction module uses the latent proximity between source and target points and the target point coordinates to construct the target shape. The module's operation is depicted in <ref type="figure">Figure 4</ref>. For each source point x i ? X , we employ a softmax operation and normalize the similarity to its k cc nearest neighbors, to form a distribution function:</p><formula xml:id="formula_1">w ij = e sij l?N Y (xi) e s il ,<label>(2)</label></formula><p>where N Y (x i ) is the latent cross-neighborhood, which contains the k cc indices of x i 's latent nearest neighbors in Y. Then, we compute an approximate matching point? xi as:</p><formula xml:id="formula_2">y xi = j?N Y (xi) w ij y j .<label>(3)</label></formula><p>The cross-construction of the target Y by the source X is denoted as</p><formula xml:id="formula_3">Y X ? R n?3 , where Y i X =? xi .</formula><p>The point? xi approximates the target point corresponding to x i . In order to encourage a unique matching between the source and target points, we would like that each point in Y X will have a close point in Y and vice versa. Thus, we minimize the Chamfer Distance <ref type="bibr" target="#b0">[1]</ref> between the two.  <ref type="figure">Figure 2</ref>. The proposed method. Our method operates directly on point clouds and does not require point connectivity information nor ground-truth correspondence annotations. The method consists of three parts: a per-point learned feature embedding, cross and selfsimilarity measure in the feature space, and cross and self-construction modules. The construction modules guide the learning process to produce discriminative yet smooth point representation, suitable for a point-wise mapping between the shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross Similarity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Target Correspondence <ref type="figure">Figure 3</ref>. Cross-similarity illustration. On the left shape pair, we mark an example source point (enlarged) and its closest points in the feature space in the other shape. The features are optimized via our construction modules to obtain a similar and unique target point embedding (enlarged), which results in a dense correspondence map between the point clouds. The mapping is color-coded in the shape pair on the right.</p><p>The Chamfer Distance between two point clouds P and Q is given by:</p><formula xml:id="formula_4">CD(P, Q) = 1 |P| p?P min q?Q ||p ? q|| 2 2 + 1 |Q| q?Q min p?P ||q ? p|| 2 2 . (4)</formula><p>Thus, we define a target cross-construction loss as:</p><formula xml:id="formula_5">L cc (Y, Y X ) = CD(Y, Y X ).<label>(5)</label></formula><p>Since the target can also be matched to the source, we can exploit this mapping direction as another training example for our model. Thus, we define a loss for the crossconstruction of the source by the target, where the source and the target switch roles:</p><formula xml:id="formula_6">L cc (X , X Y ) = CD(X , X Y ),<label>(6)</label></formula><p>Cross Similarity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent KNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross Construction</head><p>Softmax <ref type="figure">Figure 4</ref>. The cross-construction module. The module leverages similarity in the latent feature space to find a neighborhood of a source point xi in the target point cloud Y. The neighbor points are weighted according to their affinity to the source point to construct the point?x i , which approximates the correspondence of xi in Y. and use it in the overall training objective function of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-construction</head><p>A desired property of a dense correspondence field is smoothness <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref>. That is, geometrically close points should have similar latent representations. To promote this property, we propose the self-construction module. Like in the cross-construction operation, a point x i ? X is approximated by its k sc neighboring latent points:</p><formula xml:id="formula_7">x i = l?N X (xi) w il x l ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_8">N X (x i ) is the latent self-neighborhood of x i in X \ x i , w il are computed as in Equation 2 with N X (x i ) instead of N Y (x i )</formula><p>, and the similarity in Equation 1 is measured between F i X and F l X instead of F i X and F j Y . This process is repeated for the target shape as well. Finally, we would like to minimize the self-construction loss terms, de-fined as:</p><formula xml:id="formula_9">L sc (X , X X ) = CD(X , X X ) L sc (Y, Y Y ) = CD(Y, Y Y ).<label>(8)</label></formula><p>When these loss terms are minimized, a point is approximated by its neighbors in the shape. Thus, they have similar features and the smoothness property is achieved, which in turn reduces outlier matches and improves the correspondence accuracy.</p><p>We note that our self-construction loss is analogous to the Laplace-Beltrami operator (LBO) <ref type="bibr" target="#b23">[24]</ref>, which computes the difference between a point and the average of its neighbors. However, while in the LBO operator the neighbors are set according to proximity in the raw Euclidean space, in our loss term, the neighbors and their weights are determined by the affinity measure in a learned feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Objective and Matching Inference</head><p>In addition to the construction modules, we regularize the mapping of neighboring points such that close points in the source will correspond to close points in the target. To this end, we use the following mapping loss:</p><formula xml:id="formula_10">L m (X , Y X ) = 1 nk m i l?N X (xi) v il ||? xi ?? x l || 2 2 , (9) where N X (x i ) is the Euclidean neighborhood of x i in X \x i of size k m ,? xi is defined in Equation 3, v il = e ?||xi?x l || 2 2 /?</formula><p>weight the loss elements according to the proximity of the source points, and ? is a hyperparameter. We note that this loss is complementary to the self-construction loss. The mapping loss further assists in reducing outlier matches and improves the mapping accuracy. Similar to the other loss terms, we also define this loss for the mapping from the target to the source, namely, L m (Y, X Y ). An analysis of the importance of the different loss terms is discussed in an ablation study in sub-section 4.5. The overall objective function of our point correspondence learning scheme is:</p><formula xml:id="formula_11">L total = ? cc (L cc (Y, Y X ) + L cc (X , X Y ))+ ? sc (L sc (X , X X ) + L sc (Y, Y Y ))+ ? m (L m (X , Y X ) + L m (Y, X Y )),<label>(10)</label></formula><p>where ? cc , ? sc , and ? m are hyperparameters, balancing the contribution of the different loss terms. At inference time, we set the target point with the highest weight in the latent cross-neighborhood of x i to be its corresponding point. That is:</p><formula xml:id="formula_12">f (x i ) = y j * , j * = argmax j?N Y (xi) w ij .<label>(11)</label></formula><p>This selection rule can be viewed as a local classification of the source point to the most similar point in its latent crossneighborhood in the target. The point y j * is also the closest latent neighbor to x i from Y, as:</p><formula xml:id="formula_13">j * = argmax j?N Y (xi) w ij = argmax j?N Y (xi) s ij = argmax j s ij ,<label>(12)</label></formula><p>where the first transition is due to Equation 2 and the second is from the latent cross-neighborhood N Y (x i ) definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, we present the results of our DPC model on several well-established datasets and compare the model's performance to recent state-of-the-art techniques for shape matching. Additionally, we evaluate the run time of the different methods and discuss ablation experiments that validate the design choices in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets To demonstrate the flexibility of our method, we evaluate it on both human and non-human datasets. For human figures, we use the SURREAL training dataset of 3D-CODED <ref type="bibr" target="#b10">[11]</ref>, consisting of 230000 training shapes. This is a synthetic dataset generated from the parametric human model SMPL <ref type="bibr" target="#b15">[16]</ref>. During training, we select shapes from the dataset at random and use them as training pairs. For testing, we follow the evaluation protocol of previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">32]</ref> and use the challenging SHREC'19 <ref type="bibr" target="#b17">[18]</ref> dataset. This dataset contains 44 real human scans, paired into 430 annotated test examples.</p><p>For non-human shapes, SMAL <ref type="bibr" target="#b32">[33]</ref> and TOSCA <ref type="bibr" target="#b2">[3]</ref> datasets are adopted for training and evaluation, respectively. SMAL is a parametric model for animals: cat, dog, cow, horse, and hippo. We use the model to create 2000 examples in various poses for each animal type and obtain a train set of 10000 shapes in total. For training, we randomly select shape pairs from within the same animal category. The TOSCA collection includes 80 objects of animals and humans. Different from SMAL, it is not a model-based dataset and also includes animal species other than those in SMAL. We consider the 41 animal figures from TOSCA and pair shapes from the same category to form a test set of 286 examples.</p><p>The number of points in the shapes substantially varies from one dataset to another. Thus, to have a shared baseline, we randomly sample n = 1024 points from each shape to create point clouds for training and evaluation, as done in CorrNet3D work <ref type="bibr" target="#b31">[32]</ref>. In the supplementary, we also report results for higher point cloud resolutions.</p><p>Evaluation metrics A common evaluation metric for shape correspondence is the average geodesic error <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5]</ref>, which requires knowing the point adjacency matrix. However, as we are in the realm of point clouds, we assume that aReference target 3D-CODED <ref type="bibr" target="#b10">[11]</ref> aaElementary <ref type="bibr" target="#b3">[4]</ref> aCorrNet3D <ref type="bibr" target="#b31">[32]</ref> aaaDPC (ours) aaGround-truth <ref type="figure">Figure 5</ref>. Visual comparison for human shapes for a SHREC'19 test pair. The training was done on the SURREAL dataset. Previous methods suffer from correspondence errors, such as matching the knee to the hand or mixing between the limbs (marked with arrows and zoomed-in). In contrast, our method produces an accurate result, which is close to the ground truth correspondence map (color-coded).  The methods were trained on the SURREAL dataset and evaluated on the official SHREC'19 test pairs. The number of training shapes (in thousands) is stated in the squared brackets. Our method achieves a substantial performance gain compared to the alternatives while being trained on much less training data. this information is unavailable and adopt a Euclidean-based measure instead <ref type="bibr" target="#b31">[32]</ref>. For a pair of source and target shapes (X , Y), the correspondence error is defined as:</p><formula xml:id="formula_14">err = 1 n xi?X ||f (x i ) ? y gt || 2 ,<label>(13)</label></formula><p>where y gt ? Y is the ground truth corresponding point to x i 3 . Additionally, we measure the correspondence accuracy, defined as:</p><formula xml:id="formula_15">acc( ) = 1 n xi?X 1(||f(x i ) ? y gt || 2 &lt; d),<label>(14)</label></formula><p>where 1(?) is the indicator function, d is the maximal Euclidean distance between points in Y, and ? [0, 1] is an <ref type="bibr" target="#b2">3</ref> Ground truth data is only used during testing DPC, not during its training. Higher accuracy and lower error reflect a better result. Spectral methods on meshes are grayed out to emphasize the difference in their operation requirements compared to ours. We achieve better results than the other point-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SURREAL/ SHREC/ SHREC SHREC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>error tolerance. We note that these evaluation metrics are analogous to the ones used for mesh-represented shapes, with the geodesic distance replaced by the Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We implement our method in PyTorch <ref type="bibr" target="#b19">[20]</ref> and adapt the open-source DGCNN <ref type="bibr" target="#b29">[30]</ref> implementation for our feature extractor module. For the cross and self-construction operations and the mapping loss, we use a neighborhood size of k cc = k sc = k m = 10. ? cc , ? sc , and ? m in Equation 10 are set to 1, 10, and 1, respectively. Additional implementation details are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline methods</head><p>We contrast our method with the recent state-of-the-art point-based matching techniques Diff-  <ref type="table">Table 2</ref>. Processing time comparison. We report the time requirements (in milliseconds) for different methods for the correspondence computation. Our method has the best time performance, which is about 100? faster compared to the spectral methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>FMaps <ref type="bibr" target="#b16">[17]</ref>, 3D-CODED <ref type="bibr" target="#b10">[11]</ref>, and Elementary Structures work <ref type="bibr" target="#b3">[4]</ref>. These methods require ground-truth matching supervision. Additionally, we compare with the most recent CorrNet3D work <ref type="bibr" target="#b31">[32]</ref> that learns point cloud correspondence in an unsupervised manner. For the completeness of the discussion, we also include in our evaluation the unsupervised SURFMNet <ref type="bibr" target="#b22">[23]</ref> and the supervised GeoFM-Net <ref type="bibr" target="#b4">[5]</ref> methods for meshes. For all the examined baselines, training and evaluation are done using their publicly available official source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Human Datasets</head><p>Cross-dataset generalization <ref type="figure" target="#fig_2">Figure 6</ref> presents the correspondence accuracy (Equation 14) for point-based methods trained on SURREAL and evaluated on the SHREC'19 test set. For a fair comparison, we report the results without any post-processing and include methods that operate on point sets and do not require the additional connectivity information, nor the intense eigendecomposition preprocessing step, as the spectral approaches demand. As seen in the figure, our method outperforms the competing approaches by a large margin. For example, at a 5% error tolerance, it achieves 50% accuracy, an improvement of 23% over CorrNet3D <ref type="bibr" target="#b31">[32]</ref>. Visual examples in <ref type="figure">Figure 5</ref> demonstrate the improved accuracy of our method compared to the previous ones. Diff-FMaps <ref type="bibr" target="#b16">[17]</ref> has learned basis functions from the synthetic SURREAL data, which are less suitable for aligning the different SHREC'19 shapes. The other compared methods employ a learned reconstruction module as a proxy for point matches. This module requires a large amount of training data and limits the generalization capability to the unseen test dataset. In contrast, we take a spatial approach and exclude the point regression of the decoder. Instead, we replace it with our structured construction modules and concentrate on point feature learning for the correspondence task. It enables our method to be trained on only a small fraction (1%) of 2000 SURREAL shapes, while the other methods utilize all the available 230000 instances.</p><p>Training on a small dataset To further demonstrate the ability of our method to learn discriminative point representations from a small amount of data, we train it on random shape pairs from the 44 human instances in SHREC'19. These pairs do not have matching annotations and are suitable only for unsupervised techniques. <ref type="table" target="#tab_0">Table 1</ref> reports the correspondence accuracy at 1% tolerance, which represents a near-perfect hit, and the average correspondence error (Equation 13) for training either on SURREAL or SHREC shapes and testing on the official 430 SHREC'19 pairs.</p><p>The spectral methods SURFMNet <ref type="bibr" target="#b22">[23]</ref> and GeoFM-Net <ref type="bibr" target="#b4">[5]</ref> show an outstanding result for the correspondence error. However, they are less accurate at 1% tolerance. We believe that this is due to the projection of the vertex feature maps on the highly smooth spectral functions, which reduces the overall error but compromises the near-perfect accuracy. We also show in sub-section 4.3 that the spectral methods are approximately 100? slower compared to DPC in terms of total inference run-time, making them impractical for real-time usage. Among point-based approaches, we achieve the best results on both the accuracy and error measures. Notably, our method reaches a comparable performance when trained either on SURREAL or SHREC, where the latter contained only 44 shapes, two orders of magnitude less than the former (which includes 2000 shapes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Time Analysis</head><p>We evaluate the average processing time of different methods for computing the correspondence between a pair of shapes from the SHREC'19 test set. The measurements were done on an NVIDIA RTX 2080Ti GPU. <ref type="table">Table 2</ref> summarizes the results. The spectral methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5]</ref> require the time-consuming LBO-based spectral decomposition of each shape, which results in a long overall time duration. In contrast, DPC operates directly on raw input point clouds without any pre-processing and runs faster than the other spectral and point-based alternatives. Its inference time is only 26.3 milliseconds, providing a real-time processing rate of 38 point cloud pairs per second. To sum up, our method offers a sweet spot of strong generalization capabilities, along with real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Non-human Shapes</head><p>We demonstrate the flexibility of our method by applying it to the dense alignment of animal point sets. Similar to the evaluation for human figures, we examine its and the other methods' generalization power by training on the modelbased SMAL dataset and testing on the diverse animal objects from the TOSCA set. <ref type="figure" target="#fig_3">Figure 7</ref> shows visualizations and <ref type="figure">Figure 8</ref> depicts the correspondence accuracy results.</p><p>Both 3D-CODED <ref type="bibr" target="#b10">[11]</ref> and Elementary Structures <ref type="bibr" target="#b3">[4]</ref> rely on the deformation of a template shape to the source Reference target 3D-CODED <ref type="bibr" target="#b10">[11]</ref> Elementary <ref type="bibr" target="#b3">[4]</ref> CorrNet3D <ref type="bibr" target="#b31">[32]</ref> DPC (ours) Ground-truth   <ref type="figure">Figure 8</ref>. Correspondence accuracy for non-humans. The compared works were trained on the SMAL dataset and tested on animal shapes from the TOSCA benchmark. Our DPC model outperforms the other methods by a large margin across all the error tolerance range. and target point clouds for deducing the correspondence map between them. In the SMAL dataset, this template takes the form of a standing cat. However, the TOSCA set includes substantially different poses and shapes, such as a howling wolf. Thus, these methods struggle to generalize to this test case.</p><p>CorrNet3D <ref type="bibr" target="#b31">[32]</ref>, on the other hand, does not depend on a template shape and improves over 3D-CODED and Elementary Structures. Still, it includes a decoder module that is fitted to the characteristics of the SMAL data and compromises CorrNet3D generalization capability to TOSCA's animal objects. Our method neither uses a template nor a decoder component. Instead, it learns robust local point descriptors, which enables it to operate on shapes out of the training distribution. As visualized in <ref type="figure" target="#fig_3">Figure 7</ref> and quantified in <ref type="figure">Figure 8</ref>, our DPC consistently surpasses the performance of the alternatives for point cloud matching.</p><p>Lastly, we refer the reader to the supplementary for an evaluation on SMAL and TOSCA, where we present our accuracy at 1% tolerance and the average correspondence error for these datasets as well. Since SURFMNet <ref type="bibr" target="#b22">[23]</ref> and GeoFMNet <ref type="bibr" target="#b4">[5]</ref> are LBO-based architectures, they fail to be applied to the SMAL and TOSCA sets since these methods are numerically unstable under non-watertight or topologyintersected meshes, as present in the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>In the supplementary, we present a thorough ablation study verifying the design choices in our DPC model. From the ablation study, we recognize that the local neighborhood for cross-construction contributes the most to the method's performance. While considering all target points for mapping a source point is a common approach in previous works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6]</ref>, we find it less effective, as it reduces our correspondence accuracy by 15.5%. Instead, for each source point, we consider a local latent neighborhood in the target shape. It focuses the model on exploring only relevant candidates for matching and eases the learning process. Additionally, the ablation study highlights the importance of our self-construction module, which regularizes the learned point representation and is crucial for an accurate correspondence result. Without it, the performance drops by 14.3%. For further details, please see the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we presented a novel unsupervised method for real-time dense correspondence, which operates directly on point clouds without connectivity information. We leverage similarity in a learned embedding space, together with cross and self-construction modules, to compute point-topoint matches without ground truth supervision. Different from previous approaches for point sets, we do not rely on a template shape nor a decoder for point regression but rather concentrate the model on learning discriminative point features for matching. It enables our method to be trained on small datasets while having a compelling generalization ability to other test benchmarks. Evaluation on wellestablished human and animal datasets showed that our approach surpasses the performance of recent state-of-the-art works on point cloud correspondence by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In the following sections, we provide more information regarding our point cloud correspondence method. Sections A and B present additional results for human and animal shapes, respectively. An ablation study is reported in Section C. In Section D, we detail experimental settings, including network architecture and optimization parameters of DPC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results for Human Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Robustness Evaluations</head><p>We evaluated the ability of our model to infer correspondence for point clouds with different resolutions than the training point clouds. We applied the model that was trained on SURREAL point clouds with 1024 points to SHREC'19 point sets, randomly sampled with a higher number of points in the source and target shapes. The results are quantified in <ref type="figure">Figure 9</ref>. As the figure shows, our method can successfully operate on a point set with 4? higher resolution, with a small performance drop.</p><p>To further test the robustness of DPC to the point resolution, we applied the model to source and target point clouds that differ in their point number. The result in <ref type="figure" target="#fig_6">Figure 10</ref> demonstrates that our model can gracefully infer the correspondence in this case as well. These experiments suggest that DPC has learned unique and robust point descriptors that vary smoothly between neighboring points, as intended by our constructions modules.</p><p>Additionally, we applied our model for matching noisy point clouds and show an example result in <ref type="figure">Figure 11</ref>. The figure suggests that our method exhibits noise resilience to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Influence of the Train Set Size</head><p>We examined the effect of the train set size on our method's performance by training it on a varying number of point clouds from the SURREAL dataset and evaluating the average correspondence error (equation 13 in the paper) on the SHREC'19 test set. When DPC was trained on 200 shapes, it resulted in an average error of 6.5 centimeters, an increase of 0.4 centimeters compared to error for using 2000 instances ( <ref type="table" target="#tab_0">Table 1</ref> in the paper). The error reduction for training on 20000 examples was negligible (less than 0.1 centimeters). We conclude that a small training set suffices for our model to converge and achieve an accurate correspondence result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Intra-dataset Generalization</head><p>In the paper, we evaluated the generalization of our model across datasets, when it was trained on SURREAL and tested on SHREC'19. Here, we apply the model to  <ref type="figure">Figure 9</ref>. Correspondence accuracy for higher point resolutions. DPC was trained on the SURREAL dataset with a point clouds of 1024 points. The same model, without any modifications, was evaluated on the SHREC'19 test set, with point clouds of higher resolutions (indicated in the squared brackets). Our model can generalize to denser point clouds than those is was trained on, with a mild reduction in performance.</p><p>Target 1024 Source 1024 Source 512 Source 4096   <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref> and we randomly paired them to 1000 test samples. <ref type="figure">Figure 12</ref> depicts the results. DPC's correspondence result is more accurate than all the compared methods, including the LBO-based approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5]</ref> that use the mesh connectivity and the additional spectral eigenbases information. Notably, our method archives a perfect hit rate of 64%, an improvement of 44% over SURFMNet <ref type="bibr" target="#b22">[23]</ref>. We attribute this result to the descriptive quality of our method, which computes local point representations with a high level of granularity.</p><p>A.4. Additional Visual Results <ref type="figure">Figure 13</ref> shows additional visual comparison for human shapes. As seen in the figure, our method computes more accurate correspondence results, which are closer to the ground-truth mapping between the point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Limitations</head><p>There are some sources of error for our method. One of them is when body parts are glued to each other. Another is when the body is in a highly unusual position. Examples of such cases are shown in <ref type="figure">Figure 14</ref>. In the first case, the hands are held tight to the body and the matching of the palms and hips is mixed. In the second, the leg is up in the air and is wrongly mapped to the hand. A possible solution is to augment the training data with such shape pairs, which we leave for future work. Notably, the other compared methods also struggle in these challenging cases and fail to align the shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Results for Animal Shapes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Correspondence Accuracy and Average Error</head><p>In <ref type="table">Table 3</ref> we report the correspondence accuracy at 1% tolerance (Equation 14) and the average matching error (Equation 13) for training either on the SMAL or TOSCA datasets and testing on TOSCA's 286 intra-category animal pairs. For the latter setting, all animal pairs were considered during training without using ground-truth correspondence data. As seen from the table, our model achieves the best results for both measures in both evaluation settings compared to the other point-based methods.</p><p>We note that the training examples in the SMAL dataset were generated from a parametric model for animals <ref type="bibr" target="#b32">[33]</ref>. The model may result in intersecting geometry, such as a leg crossing the body or another leg of the animal. The learning pipeline of spectral correspondence methods, such as SURFMNet <ref type="bibr" target="#b22">[23]</ref> and GeoFMNet <ref type="bibr" target="#b4">[5]</ref>, requires the Cholesky decomposition <ref type="bibr" target="#b12">[13]</ref> of the matrix of features projected on the shape's spectral basis <ref type="bibr" target="#b14">[15]</ref>. Unfortunately, in the case of non-watertight or topology-intersected meshes, the Cholesky decomposition is numerically unstable <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9]</ref> and the spectral methods cannot be trained on such a dataset. A similar phenomenon occurred for the TOSCA  <ref type="figure">Figure 12</ref>. Intra-dataset correspondence test. Training and evaluation was done on SURREAL train and test shape pairs, respectively. The method's type is stated in the squared brackets. Our DPC outperforms the other models, with large performance gains at near-perfect hit rate (i.e., correspondence accuracy for a low error tolerance). training set. Thus, we did not report results for these methods in table 3. <ref type="figure">Figure 15</ref> presents an additional qualitative comparison for the TOSCA test set. Similar to the findings in the paper, DPC outputs a more accurate matching map between the point clouds compared to the other works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Additional Visual Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>We verified the design choices in our method by an ablation study, where each time one element in the system was changed and the others were kept the same. The following settings were examined: (a) non-local feature aggregation in the point embedding module (as in the original DGCNN [30] model) instead of aggregation from local point neighbors; (b) unbounded dot-product similarity (the numerator of Equation 1) instead of the cosine similarity; (c) considering all neighbors for the cross-construction operation (k cc = n); (d) excluding the self-construction module (? sc = 0); and (e) turning off the mapping loss (? m = 0). <ref type="table">Table 4</ref> summarizes the results.</p><p>The table indicates the necessity of the proposed components and their configuration, as all the ablative settings compromise the model's performance. Local feature aggregation enables the model to extract a discriminative point representation and the bounded cosine similarity contributes to the numerical stability of the learning process. Additionally, the ablative experiments validate that a local latent neighborhood for the cross-construction operation and the employment of the self-construction module are highly important considerations in our method. aReference target 3D-CODED <ref type="bibr" target="#b10">[11]</ref> aaElementary <ref type="bibr" target="#b3">[4]</ref> aaCorrNet3D <ref type="bibr" target="#b31">[32]</ref> aaaDPC (ours) aGround-truth <ref type="figure">Figure 13</ref>. Additional visual comparison for the SHREC'19 test set. The training was done on the SURREAL dataset. First and third rows: color-coded point matches. Second and fourth rows: heat-map of correspondence errors magnitude, normalized by the maximal distance between points in the reference target shape. The compared methods result in relatively high errors. In contrast, our DPC better succeeds in aligning the shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Feature Extraction Architecture</head><p>We adopt the DGCNN architecture <ref type="bibr" target="#b29">[30]</ref>, with a local point neighborhood rather than a dynamic non-local one. The network includes 4 per-point convolution layers with filter sizes <ref type="bibr">(96,</ref><ref type="bibr">192,</ref><ref type="bibr">384,</ref><ref type="bibr">768)</ref>. Batch normalization and a leaky ReLU activation with a negative slope of ?0.2 are used after each filer. The convolutions operate on the concatenation of the point features and its edge features, where the latter are differences between the features of the points and its 27 nearest Euclidean points. After each layer, the per-point features are max-pooled from the point's neighbors. Finally, the features from the different stages are concatenated to a vector of size 1440 = 96 + 192 + 384 + 768, which is passed through two last layers with (1044, 512) neurons, along with batch normalization and non-linearity as before, to produce a c = 512 dimensional feature vector for each point.</p><p>aaa Reference target 3D-CODED <ref type="bibr" target="#b10">[11]</ref> aElementary <ref type="bibr" target="#b3">[4]</ref> aCorrNet3D <ref type="bibr" target="#b31">[32]</ref> aaaDPC (ours) aaGround-truth <ref type="figure">Figure 14</ref>. Failure cases. We show failure examples of our method for the SHREC'19 test set, where wrong matches are indicated by arrows. For comparison, we include the outcome of other works for these cases as well. The point mappings are color-coded. All the compared methods result in an inaccurate correspondence map for these challenging shape pairs.</p><p>Reference target 3D-CODED <ref type="bibr" target="#b10">[11]</ref> Elementary <ref type="bibr" target="#b3">[4]</ref> CorrNet3D <ref type="bibr" target="#b31">[32]</ref> DPC (ours) Ground-truth <ref type="figure">Figure 15</ref>. Visual comparison for animal shapes from the TOSCA test set. The training was done on the SMAL dataset. While the result of the other models is patchy or noisy, our method computes an accurate correspondence result (color-coded). <ref type="table">Table 5</ref> summarizes the optimization parameters for our model. The same values were used for all the four training datasets (SURREAL, SHREC'19, SMAL, and TOSCA). We used an Adam optimizer with an initial learning rate of 0.0003, momentum 0.9, and weight decay of 0.0005. The learning rate is multiplied by a factor of 0.1 at epochs 6 and 9. The training was done on an NVIDIA RTX 2080Ti GPU.  <ref type="table">Table 3</ref>. Accuracy and error. We evaluate the accuracy at 1% tolerance (acc, in percentage) and the average correspondence error (err, in centimeters) for two train/test settings of animal datasets. Higher accuracy and lower error reflect a better result. The training on TOSCA was done without correspondence supervision, thus, the supervised techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4]</ref> were not applied in this setting. Our model achieves better results compared to the competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. DPC Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMAL</head><p>Setting acc ? err ? (a) Non-local feature aggregation 13.5% 6.4 (b) Dot-product similarity measure 12.0% 6.2 (c) All cross neighbors (k cc = n) 2.2% 7.4 (d) W/O self construction (? sc = 0)</p><p>3.4% 6.6 (e) W/O mapping loss (? m = 0) 11.4% 6.7 Our complete method 17.7% 6.1 <ref type="table">Table 4</ref>. Performance in ablative settings. We train our method on the SURREAL dataset and test it on the SHREC'19 benchmark. The evaluation metrics are the same as in <ref type="table" target="#tab_0">Table 1</ref> in the paper. The complete proposed method yields the best performance. Please see additional details in Section C.  <ref type="table">Table 5</ref>. Hyper-parameters. The tables details the hyperparameter values that we used for the training of DPC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>[230K training shapes] 3D-CODED [230K training shapes] Elementary [230K training shapes] CorrNet3D [230K training shapes] DPC (ours) [2K training shapes]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Correspondence accuracy for human point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Visual comparison for animal shapes from the TOSCA test set. The training was done on the synthetic SMAL dataset. 3D-CODED and Elementary Structures produce a patchy result. CorrNet3D's output is noisy and contains outlier matches. In contrast, our method produces a smooth and accurate alignment between the animal point clouds (color-coded).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>) [1024 points] DPC (ours) [2048 points] DPC (ours) [4096 points]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Inference for different number of point. DPC was trained on SURREAL point clouds with 1024 points. A result for a test pair with 1024 points from the SHREC'19 dataset is shown in the left side. The same model, without any changes, is successfully applied to point clouds with either lower or higher point number (shown on the right side), which implies on model's robustness to the point cloud's resolution. Point matches are indicated by similar colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Input</cell><cell>acc ?</cell><cell>err ?</cell><cell>acc ?</cell><cell>err ?a</cell></row><row><cell>SURFMNet [23]</cell><cell>LBO</cell><cell>4.3%</cell><cell>0.3</cell><cell>5.9%</cell><cell>0.2</cell></row><row><cell>GeoFMNet [5]</cell><cell>LBO</cell><cell>8.2%</cell><cell>0.2</cell><cell>*</cell><cell>*</cell></row><row><cell>Diff-FMaps [17]</cell><cell>Point</cell><cell>4.0%</cell><cell>7.1</cell><cell>*</cell><cell>*</cell></row><row><cell cols="2">3D-CODED [11] Point</cell><cell>2.1%</cell><cell>8.1</cell><cell>*</cell><cell>*</cell></row><row><cell>Elementary [4]</cell><cell>Point</cell><cell>2.3%</cell><cell>7.6</cell><cell>*</cell><cell>*</cell></row><row><cell>CorrNet3D [32]</cell><cell>Point</cell><cell>6.0%</cell><cell>6.9</cell><cell>0.4%</cell><cell>33.8</cell></row><row><cell>DPC (ours)</cell><cell cols="2">Point 17.7%</cell><cell>6.1</cell><cell>15.3%</cell><cell>5.6</cell></row></table><note>. Accuracy and error. We evaluate the accuracy at 1% tol- erance (acc, in percentage) and the average correspondence error (err, in centimeters) for two train/test settings of human datasets.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was partly funded by ISF grant number 1549/19 and the Zimin institute for Engineering solutions advancing better lives.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Representations and Generative Models for 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recent Trends, Applications, and Perspectives in 3d Shape Similarity Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biasotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="119" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Numerical Geometry of Non-rigid Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Elementary Structures for 3D Shape Generation and Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deprelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeuralIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Geometric Functional Maps: Robust Feature Learning for Shape Correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Donati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">NeuroMorph: Unsupervised Shape Interpolation and Correspondence in One Go</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eisenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kerchenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7473" to="7483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Shells: Unsupervised Shape Correspondence with Optimal Transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eisenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cyclic Functional Mapping: Self-supervised correspondence between non-isometric deformable shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ginzburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raviv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ginzburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raviv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14723</idno>
		<title level="m">Dual Geometric Graph Network (DG2N): Iterative Network for Deformable Shape Alignment</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep Weighted Consensus: Dense correspondence confidence maps for 3D shape registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ginzburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raviv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02714</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D-CODED: 3D Correspondences by Deep Deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Analysis of the Cholesky Decomposition of a Semi-definite Matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Oxford University Press</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix Inversion Using Cholesky Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 Signal Processing: Algorithms, architectures, arrangements, and applications (SPA)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="70" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SampleNet: Differentiable Point Cloud Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Functional Maps: Structured Prediction for Dense Shape Correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SMPL: A Skinned Multi-person Linear Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Correspondence Learning via Linearly-invariant Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-J</forename><surname>Rakotosaona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeuralIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeuralIPS)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matching Humans with Different Connectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Poulenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval. The Eurographics Association</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Functional Maps: A Flexible Representation of Maps between Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic Differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeuralIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeuralIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised Deep Learning for Structured Shape Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Roufosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Laplace-Beltrami Eigenfunctions for Deformation Invariant Shape Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Rustamov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eurographics Symposium on Geometry Processing</title>
		<meeting>the Eurographics Symposium on Geometry Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>The Eurographics Association</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and Deformable Convolution for Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Survey on Shape Correspondence. Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1681" to="1707" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Product Manifold Filter: Non-Rigid Shape Correspondence via Kernel Density Estimation in the Product Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vestner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3327" to="3336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Closest Point: Learning Representations for Point Cloud Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3523" to="3532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PRNet: Self-Supervised Learning for Partial-to-Partial Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (Neu-ralIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (Neu-ralIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic Graph CNN for Learning on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PointConv: Deep Convolutional Networks on 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CorrNet3D: Unsupervised End-to-end Learning of Dense Correspondence for 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D Menagerie: Modeling the 3D Shape and Pose of Animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
