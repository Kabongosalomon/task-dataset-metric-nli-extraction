<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling the Background for Incremental Learning in Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cermelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Torino</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Italian Institute of Technology</orgName>
								<address>
									<addrLine>3 Fondazione Bruno Kessler</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
							<email>mancini@diag.uniroma1.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Italian Institute of Technology</orgName>
								<address>
									<addrLine>3 Fondazione Bruno Kessler</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
							<email>samuel@mapillary.com</email>
							<affiliation key="aff3">
								<orgName type="department">Mapillary Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
							<email>eliricci@fbk.eu</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<email>barbara.caputo@polito.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Torino</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Italian Institute of Technology</orgName>
								<address>
									<addrLine>3 Fondazione Bruno Kessler</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling the Background for Incremental Learning in Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite their effectiveness in a wide range of tasks, deep architectures suffer from some important limitations. In particular, they are vulnerable to catastrophic forgetting, i.e. they perform poorly when they are required to update their model as new classes are available but the original training set is not retained. This paper addresses this problem in the context of semantic segmentation. Current strategies fail on this task because they do not consider a peculiar aspect of semantic segmentation: since each training step provides annotation only for a subset of all possible classes, pixels of the background class (i.e. pixels that do not belong to any other classes) exhibit a semantic distribution shift. In this work we revisit classical incremental learning methods, proposing a new distillation-based framework which explicitly accounts for this shift. Furthermore, we introduce a novel strategy to initialize classifier's parameters, thus preventing biased predictions toward the background class. We demonstrate the effectiveness of our approach with an extensive evaluation on the Pascal-VOC 2012 and ADE20K datasets, significantly outperforming state of the art incremental learning methods. Code can be found at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a fundamental problem in computer vision. In the last years, thanks to the emergence of deep neural networks and to the availability of largescale human-annotated datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39]</ref>, the state of the art has improved significantly <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref>. Current approaches are derived by extending deep architectures from image-level to pixel-level classification, taking advantage of Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b19">[20]</ref>. Over the years, semantic segmentation models based on FCNs have been improved in several ways, e.g. by exploiting multiscale representations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref>, modeling spatial dependencies and contextual cues <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref> or considering attention models <ref type="bibr" target="#b6">[7]</ref>. <ref type="figure">Figure 1</ref>: Illustration of the semantic shift of the background class in incremental learning for semantic segmentation. Yellow boxes denote the ground truth provided in the learning step, while grey boxes denote classes not labeled. As different learning steps have different label spaces, at step t old classes (e.g. person) and unseen ones (e.g. car) might be labeled as background in the current ground truth. Here we show the specific case of single class learning steps, but we address the general case where an arbitrary number of classes is added.</p><p>Still, existing semantic segmentation methods are not designed to incrementally update their inner classification model when new categories are discovered. While deep nets are undoubtedly powerful, it is well known that their capabilities in an incremental learning setting are limited <ref type="bibr" target="#b15">[16]</ref>. In fact, deep architectures struggle in updating their parameters for learning new categories whilst preserving good performance on the old ones (catastrophic forgetting <ref type="bibr" target="#b22">[23]</ref>).</p><p>While the problem of incremental learning has been traditionally addressed in object recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b14">15]</ref> and detection <ref type="bibr" target="#b31">[32]</ref>, much less attention has been devoted to semantic segmentation. Here we fill this gap, proposing an incremental class learning (ICL) approach for semantic segmentation. Inspired by previous methods on image classification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3]</ref>, we cope with catastrophic forgetting by resorting to knowledge distillation <ref type="bibr" target="#b13">[14]</ref>. However, we argue (and experimentally demonstrate) that a naive application of previous knowledge distillation strategies would not suffice in this setting. In fact, one peculiar aspect of semantic segmentation is the presence of a special class, the background class, indicating pixels not assigned to any of the given object categories. While the presence of this class marginally influences the design of traditional, offline semantic segmentation methods, this is not true in an incremental learning setting. As illustrated in <ref type="figure">Fig. 1</ref>, it is reasonable to assume that the semantics associated to the background class changes over time. In other words, pixels associated to the background during a learning step may be assigned to a specific object class in subsequent steps or vice-versa, with the effect of exacerbating the catastrophic forgetting. To overcome this issue, we revisit the classical distillation-based framework for incremental learning <ref type="bibr" target="#b17">[18]</ref> by introducing two novel loss terms to properly account for the semantic distribution shift within the background class, thus introducing the first ICL approach tailored to semantic segmentation. We extensively evaluate our method on two datasets, Pascal-VOC <ref type="bibr" target="#b10">[11]</ref> and ADE20K <ref type="bibr" target="#b38">[39]</ref>, showing that our approach, coupled with a novel classifier initialization strategy, largely outperform traditional ICL methods. To summarize, the contributions of this paper are as follows:</p><p>? We study the task of incremental class learning for semantic segmentation, analyzing in particular the problem of distribution shift arising due to the presence of the background class.</p><p>? We propose a new objective function and introduce a specific classifier initialization strategy to explicitly cope with the evolving semantics of the background class. We show that our approach greatly alleviates the catastrophic forgetting, leading to the state of the art.</p><p>? We benchmark our approach over several previous ICL methods on two popular semantic segmentation datasets, considering different experimental settings. We hope that our results will serve as a reference for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Semantic Segmentation. Deep learning has enabled great advancements in semantic segmentation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref>. State of the art methods are based on Fully Convolutional Neural Networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref> and use different strategies to condition pixel-level annotations on their global context, e.g. using multiple scales <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8]</ref> and/or modeling spatial dependencies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>. The vast majority of semantic segmentation methods considers an offline setting, i.e. they assume that training data for all classes is available beforehand. To our knowledge, the problem of ICL in semantic segmentation has been addressed only in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b23">24]</ref>. Ozdemir et al. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> describe an ICL approach for medical imaging, extending a standard image-level classification method <ref type="bibr" target="#b17">[18]</ref> to segmentation and devising a strategy to select relevant samples of old datasets for rehearsal. Taras et al. proposed a similar approach for segmenting remote sensing data. Differently, Michieli et al. <ref type="bibr" target="#b23">[24]</ref> consider ICL for semantic segmentation in a particular setting where labels are provided for old classes while learning new ones. Moreover, they assume the novel classes to be never present as background in pixels of previous learning steps. These assumptions strongly limit the applicability of their method.</p><p>Here we propose a more principled formulation of the ICL problem in semantic segmentation. In contrast with previous works, we do not limit our analysis to medical <ref type="bibr" target="#b25">[26]</ref> or remote sensing data <ref type="bibr" target="#b32">[33]</ref> and we do not impose any restrictions on how the label space should change across different learning steps <ref type="bibr" target="#b23">[24]</ref>. Moreover, we are the first to provide a comprehensive experimental evaluation of state of the art ICL methods on commonly used semantic segmentation benchmarks and to explicitly introduce and tackle the semantic shift of the background class, a problem recognized but largely overseen by previous works <ref type="bibr" target="#b23">[24]</ref>.</p><p>Incremental Learning. The problem of catastrophic forgetting <ref type="bibr" target="#b22">[23]</ref> has been extensively studied for image classification tasks <ref type="bibr" target="#b8">[9]</ref>. Previous works can be grouped in three categories <ref type="bibr" target="#b8">[9]</ref>: replay-based <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b24">25]</ref>, regularization-based <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10]</ref>, and parameter isolation-based <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref>. In replay-based methods, examples of previous tasks are either stored <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref> or generated <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b24">25]</ref> and then replayed while learning the new task. Parameter isolation-based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref> assign a subset of the parameters to each task to prevent forgetting. Regularization-based methods can be divided in prior-focused and data-focused. The former <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1]</ref> define knowledge as the parameters value, constraining the learning of new tasks by penalizing changes of important parameters for old ones. The latter <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10]</ref> exploit distillation <ref type="bibr" target="#b13">[14]</ref> and use the distance between the activations produced by the old network and the new one as a regularization term to prevent catastrophic forgetting.</p><p>Despite these progresses, very few works have gone beyond image-level classification. A first work in this direction is <ref type="bibr" target="#b31">[32]</ref> which considers ICL in object detection proposing a distillation-based method adapted from <ref type="bibr" target="#b17">[18]</ref> for tackling novel class recognition and bounding box proposals generation. In this work we also take a similar approach to <ref type="bibr" target="#b31">[32]</ref> and we resort on distillation. However, here we propose to address the problem of modeling the background shift which is peculiar of the semantic segmentation setting. <ref type="figure">Figure 2</ref>: Overview of our method. At learning step t an image is processed by the old (top) and current (bottom) models, mapping the image to their respective output spaces. As in standard ICL methods, we apply a cross-entropy loss to learn new classes (blue block) and a distillation loss to preserve old knowledge (yellow block). In this framework, we model the semantic changes of the background across different learning steps by (i) initializing the new classifier using the weights of the old background one (left), (ii) comparing the pixel-level background ground truth in the cross-entropy with the probability of having either the background (black) or an old class (pink and grey bars) and (iii) relating the background probability given by the old model in the distillation loss with the probability of having either the background or a novel class (green bar).</p><p>tation. Let us denote as X the input space (i.e. the image space) and, without loss of generality, let us assume that each image x ? X is composed by a set of pixels I with constant cardinality |I| = N . The output space is defined as Y N , with the latter denoting the product set of N -tuples with elements in a label space Y. Given an image x the goal of semantic segmentation is to assign each pixel x i of image x a label y i ? Y, representing its semantic class. Out-ofclass pixels can be assigned a special class, i.e. the background class b ? Y. Given a training set T ? X ? Y N , the mapping is realized by learning a model f ? with parameters ? from the image space X to a pixel-wise class probability vector, i.e. f ? : X ? IR N ?|Y| . The output segmentation mask is obtained as</p><formula xml:id="formula_0">y * = {arg max c?Y f ? (x)[i, c]} N i=1 , where f ? (x)[i, c]</formula><p>is the probability for class c in pixel i.</p><p>In the ICL setting, training is realized over multiple phases, called learning steps, and each step introduces novel categories to be learnt. In other terms, during the t th learning step, the previous label</p><formula xml:id="formula_1">set Y t?1 is expanded with a set of new classes C t , yielding a new label set Y t = Y t?1 ? C t . At learning step t we are also provided with a training set T t ? X ? (C t ) N that is used in conjunc- tion to the previous model f ? t?1 : X ? IR N ?|Y t?1 | to train an updated model f ? t : X ? IR N ?|Y t | .</formula><p>As in standard ICL, in this paper we assume the sets of labels C t that we obtain at the different learning steps to be disjoint, except for the special void/background class b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Incremental Learning for Semantic Segmentation with Background Modeling</head><p>A naive approach to address the ICL problem consists in retraining the model f ? t on each set T t sequentially. When the predictor f ? t is realized through a deep architecture, this corresponds to fine-tuning the network parameters on the training set T t initialized with the parameters ? t?1 from the previous stage. This approach is simple, but it leads to catastrophic forgetting. Indeed, when training using T t no samples from the previously seen object classes are provided. This biases the new predictor f ? t towards the novel set of categories in C t to the detriment of the classes from the previous sets. In the context of ICL for image-level classification, a standard way to address this issue is coupling the supervised loss on T t with a regularization term, either taking into account the importance of each parameter for previous tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref>, or by distilling the knowledge using the predictions of the old model f ? t?1 <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3]</ref>. We take inspiration from the latter solution to initialize the overall objective function of our problem. In particular, we minimize a loss function of the form:</p><formula xml:id="formula_2">L(? t ) = 1 |T t | (x,y)?T t ? t ce (x, y) + ? ? t kd (x)<label>(1)</label></formula><p>where ce is a standard supervised loss (e.g. cross-entropy loss), kd is the distillation loss and ? &gt; 0 is a hyperparameter balancing the importance of the two terms. As stated in Sec. 3.1, differently from standard ICL settings considered for image classification problems, in semantic segmentation we have that two different label sets C s and C u share the common void/background class b. However, the distribution of the background class changes across different incremental steps. In fact, background annotations given in T t refer to classes not present in C t , that might belong to the set of seen classes Y t?1 and/or to still unseen classes i.e. C u with u &gt; t (see <ref type="figure">Fig. 1</ref>). In the following, we show how we account for the semantic shift in the distribution of the background class by revisiting standard choices for the general objective defined in Eq. (1).</p><p>Revisiting Cross-Entropy Loss.</p><p>In Eq.(1), a possible choice for ce is the standard cross-entropy loss computed over all image pixels:</p><formula xml:id="formula_3">? t ce (x, y) = ? 1 |I| i?I log q t x (i, y i ) ,<label>(2)</label></formula><p>where y i ? Y t is the ground truth label associated to pixel i and q t</p><formula xml:id="formula_4">x (i, c) = f ? t (x)[i, c].</formula><p>The problem with Eq. <ref type="formula" target="#formula_3">(2)</ref> is that the training set T t we use to update the model only contains information about novel classes in C t . However, the background class in T t might include also pixels associated to the previously seen classes in Y t?1 . In this paper, we argue that, without explicitly taking into account this aspect, the catastrophic forgetting problem would be even more severe. In fact, we would drive our model to predict the background label b for pixels of old classes, further degrading the capability of the model to preserve semantic knowledge of past categories. To avoid this issue, in this paper we propose to modify the cross-entropy loss in Eq.(2) as follows:</p><formula xml:id="formula_5">? t ce (x, y) = ? 1 |I| i?I logq t x (i, y i ) ,<label>(3)</label></formula><p>where:q</p><formula xml:id="formula_6">t x (i, c) = q t x (i, c) if c = b k?Y t?1 q t x (i, k) if c = b .<label>(4)</label></formula><p>Our intuition is that by using Eq. <ref type="formula" target="#formula_5">(3)</ref> we can update the model to predict the new classes and, at the same time, account for the uncertainty over the actual content of the background class. In fact, in Eq.(3) the background class ground truth is not directly compared with its probabilities q t x (i, b) obtained from the current model f ? t , but with the probability of having either an old class or the background, as predicted by f ? t (Eq.(4)). A schematic representation of this procedure is depicted in <ref type="figure">Fig. 2</ref> (blue block). It is worth noting that the alternative of ignoring the background pixels within the cross-entropy loss is a sub-optimal solution. In fact, this would not allow to adapt the background classifier to its semantic shift and to exploit the information that new images might contain about old classes.</p><p>Revisiting Distillation Loss. In the context of incremental learning, distillation loss <ref type="bibr" target="#b13">[14]</ref> is a common strategy to transfer knowledge from the old model f ? t?1 into the new one, preventing catastrophic forgetting. Formally, a standard choice for the distillation loss kd is:</p><formula xml:id="formula_7">? t kd (x, y) = ? 1 |I| i?I c?Y t?1 q t?1 x (i, c) logq t x (i, c) ,<label>(5)</label></formula><p>whereq t x (i, c) is defined as the probability of class c for pixel i given by f ? t but re-normalized across all the classes in Y t?1 i.e.:</p><formula xml:id="formula_8">q t x (i, c) = 0 if c ? C t \ {b} q t x (i, c)/ k?Y t?1 q t x (i, k) if c ? Y t?1 .<label>(6)</label></formula><p>The rationale behind kd is that f ? t should produce activations close to the ones produced by f ? t?1 . This regularizes the training procedure in such a way that the parameters ? t are still anchored to the solution found for recognizing pixels of the previous classes, i.e. ? t?1 .</p><p>The loss defined in Eq.(5) has been used either in its base form or variants in different contexts, from incremental task <ref type="bibr" target="#b17">[18]</ref> and class learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref> in object classification to complex scenarios such as detection <ref type="bibr" target="#b31">[32]</ref> and segmentation <ref type="bibr" target="#b23">[24]</ref>. Despite its success, it has a fundamental drawback in semantic segmentation: it completely ignores the fact that the background class is shared among different learning steps. While with Eq.(3) we tackled the first problem linked to the semantic shift of the background (i.e. b ? T t contains pixels of Y t?1 ), we use the distillation loss to tackle the second: annotations for background in T s with s &lt; t might include pixels of classes in C t .</p><p>From the latter considerations, the background probabilities assigned to a pixel by the old predictor f ? t?1 and by the current model f ? t do not share the same semantic content.</p><p>More importantly, f ? t?1 might predict as background pixels of classes in C t that we are currently trying to learn. Notice that this aspect is peculiar to the segmentation task and it is not considered in previous incremental learning models. However, in our setting we must explicitly take it into account to perform a correct distillation of the old model into the new one. To this extent we define our novel distillation loss by rewritingq t x (i, c) in Eq.(6) as:</p><formula xml:id="formula_9">q t x (i, c) = q t x (i, c) if c = b k?C t q t x (i, k) if c = b .<label>(7)</label></formula><p>Similarly to Eq.(5), we still compare the probability of a pixel belonging to seen classes assigned by the old model, with its counterpart computed with the current parameters ? t . However, differently from classical distillation, in Eq.(7) the probabilities obtained with the current model are kept unaltered, i.e. normalized across the whole label space Y t and not with respect to the subset Y t?1 (Eq. <ref type="formula" target="#formula_8">(6)</ref>). More importantly, the background class probability as given by f ? t?1 is not directly compared with its counterpart in f ? t , but with the probability of having either a new class or the background, as predicted by f ? t (see <ref type="figure">Fig. 2</ref>, yellow block). We highlight that, with respect to Eq.(6) and other simple choices (e.g. excluding b from Eq.(6)) this solution has two advantages. First, we can still use the full output space of the old model to distill knowledge in the current one, without any constraint on pixels and classes. Second, we can propagate the uncertainty we have on the semantic content of the background in f ? t?1 without penalizing the probabilities of new classes we are learning in the current step t.</p><p>Classifiers' Parameters Initialization. As discussed above, the background class b is a special class devoted to collect the probability that a pixel belongs to an unknown object class. In practice, at each learning step t, the novel categories in C t are unknowns for the old classifier f ? t?1 . As a consequence, unless the appearance of a class in C t is very similar to one in Y t?1 , it is reasonable to assume that f ? t?1 will likely assign pixels of C t to b. Taking into account this initial bias on the predictions of f ? t on pixels of C t , it is detrimental to randomly initialize the classifiers for the novel classes. In fact a random initialization would provoke a misalignment among the features extracted by the model (aligned with the background classifier) and the random parameters of the classifier itself. Notice that this could lead to possible training instabilities while learning novel classes since the network could initially assign high probabilities for pixels in C t to b.</p><p>To address this issue, we propose to initialize the classifier's parameters for the novel classes in such a way that given an image x and a pixel i, the probability of the background q t?1</p><p>x (i, b) is uniformly spread among the classes in</p><formula xml:id="formula_10">C t , i.e. q t x (i, c) = q t?1 x (i, b)/|C t | ?c ? C t ,</formula><p>where |C t | is the number of new classes (notice that b ? C t ). To this extent, let us consider a standard fully connected classifier and let us denote as {? t c , ? t c } ? ? t the classifier parameters for a class c at learning step t, with ? and ? denoting its weights and bias respectively. We can initialize {? t c , ? t c } as follows:</p><formula xml:id="formula_11">? t c = ? t?1 b if c ? C t ? t?1 c otherwise (8) ? t c = ? t?1 b ? log(|C t |) if c ? C t ? t?1 c otherwise (9) where {? t?1 b , ? t?1 b</formula><p>} are the weights and bias of the background classifier at the previous learning step. The fact that the initialization defined in Eq.(8) and <ref type="bibr" target="#b8">(9)</ref> </p><formula xml:id="formula_12">leads to q t x (i, c) = q t?1 x (i, b)/|C t | ?c ? C t is easy to obtain from q t x (i, c) ? exp(? t b ? x + ? t b )</formula><p>. As we will show in the experimental analysis, this simple initialization procedure brings benefits in terms of both improving the learning stability of the model and the final results, since it eases the role of the supervision imposed by Eq.(3) while learning new classes and follows the same principles used to derive our distillation loss (Eq. <ref type="formula" target="#formula_9">(7)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ICL Baselines</head><p>We compare our method against standard ICL baselines, originally designed for classification tasks, on the considered segmentation task, thus segmentation is treated as a pixel-level classification problem. Specifically, we report the results of six different regularization-based methods, three prior-focused and three data-focused approaches.</p><p>In the first category, we chose Elastic Weight Consolidation (EWC) <ref type="bibr" target="#b16">[17]</ref>, Path Integral (PI) <ref type="bibr" target="#b35">[36]</ref>, and Riemannian Walks (RW) <ref type="bibr" target="#b3">[4]</ref>. They employ different strategies to compute the importance of each parameter for old classes: EWC uses the empirical Fisher matrix, PI uses the learning trajectory, while RW combines EWC and PI in a unique model. We choose EWC since it is a standard baseline employed also in <ref type="bibr" target="#b31">[32]</ref> and PI and RW since they are two simple applications of the same principle. Since these methods act at the parameter level, to adapt them to the segmentation task we keep the loss in the output space unaltered (i.e. standard cross-entropy across the whole segmentation mask), computing the parameters' importance by considering their effect on learning old classes.</p><p>For the data-focused methods, we chose Learning without forgetting (LwF) <ref type="bibr" target="#b17">[18]</ref>, LwF multi-class (LwF-MC) <ref type="bibr" target="#b27">[28]</ref> and the segmentation method of <ref type="bibr" target="#b23">[24]</ref> (ILT). We denote as LwF the original distillation based objective as implemented in Eq.(1) with basic cross-entropy and distillation losses, which is the same as <ref type="bibr" target="#b17">[18]</ref> except that distillation and cross-entropy share the same label space and classifier. LwF-MC is the single-head version of <ref type="bibr" target="#b17">[18]</ref> as adapted from <ref type="bibr" target="#b27">[28]</ref>. It is based on multiple binary classifiers, with the target labels defined using the ground truth for novel classes (i.e. C t ) and the probabilities given by the old model for the old ones (i.e. Y t?1 ). Since the background class is both in C t and Y t?1 we implement LwF-MC by a weighted combination of two binary cross-entropy losses, on both the ground truth and the probabilities given by f ? t?1 . Finally, ILT <ref type="bibr" target="#b23">[24]</ref> is the only method specifically proposed for ICL in semantic segmentation. It uses a distillation loss in the output space, as in our adapted version of LwF <ref type="bibr" target="#b17">[18]</ref> and/or another distillation loss in the features space, attached to the output of the network decoder. Here, we use the variant where both losses are employed. As done by <ref type="bibr" target="#b31">[32]</ref>, we do not compare with replay-based methods (e.g. <ref type="bibr" target="#b27">[28]</ref>) since they violate the standard ICL assumption regarding the unavailability of old data.</p><p>In all tables we report other two baselines: simple finetuning (FT) on each T t (e.g. Eq.(2)) and training on all classes offline (Joint). The latter can be regarded as an upper bound. In the tables we denote our method as MiB (Modeling the Background for incremental learning in semantic segmentation). All results are reported as mean Intersection-over-Union (mIoU) in percentage, averaged over all the classes of a learning step and all the steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For all methods we use the Deeplab-v3 architecture <ref type="bibr" target="#b5">[6]</ref> with a ResNet-101 <ref type="bibr" target="#b12">[13]</ref> backbone and output stride of 16. Since memory requirements are an important issue in semantic segmentation, we use in-place activated batch normalization, as proposed in <ref type="bibr" target="#b28">[29]</ref>. The backbone has been initialized using the ImageNet pretrained model <ref type="bibr" target="#b28">[29]</ref>. We follow <ref type="bibr" target="#b5">[6]</ref>, training the network with SGD and the same learning rate policy, momentum and weight decay. We use an initial learning rate of 10 ?2 for the first learning step and 10 ?3 for the followings, as in <ref type="bibr" target="#b31">[32]</ref>. We train the model with a batch-size of 24 for 30 epochs for Pascal-VOC 2012 and 60 epochs for ADE20K in every learning step. We apply the same data augmentation of <ref type="bibr" target="#b5">[6]</ref> and we crop the images to 512 ? 512 during both training and test. For setting the hyper-parameters of each method, we use the protocol of incremental learning defined in <ref type="bibr" target="#b8">[9]</ref>, using 20% of the training set as validation. The final results are reported on the standard validation set of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pascal-VOC 2012</head><p>PASCAL-VOC 2012 <ref type="bibr" target="#b10">[11]</ref> is a widely used benchmark that includes 20 foreground object classes. Following <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>, we define two experimental settings, depending on how we sample images to build the incremental datasets. Following <ref type="bibr" target="#b23">[24]</ref>, we define an experimental protocol called the disjoint setup: each learning step contains a unique set of images, whose pixels belong to classes seen either in the current or in the previous learning steps. Differently from <ref type="bibr" target="#b23">[24]</ref>, at each step we assume to have only labels for pixels of novel classes, while the old ones are labeled as background in the ground truth. The second setup, that we denote as overlapped, follows what done in <ref type="bibr" target="#b31">[32]</ref> for detection: each training step contains all the images that have at least one pixel of a novel class, with only the latter annotated. It is important to note a difference with respect to the previous setup: images may now contain pixels of classes that we will learn in the future, but labeled as background. This is a more realistic setup since it does not make any assumption on the objects present in the images.</p><p>As done by previous works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b23">24]</ref>, we perform three different experiments concerning the addition of one class (19-1), five classes all at once , and five classes sequentially (15-1), following the alphabetical order of the classes to split the content of each learning step.</p><p>Addition of one class . In this experiment, we perform two learning steps: the first in which we observe the first 19 classes, and the second where we learn the tvmonitor class. Results are reported in <ref type="table" target="#tab_0">Table 1</ref>. Without employing any regularization strategy, the performance on past classes drops significantly. FT, in fact, performs poorly, completely forgetting the first 19 classes. Unexpectedly, using PI as a regularization strategy does not provide benefits, while EWC and RW improve performance of nearly 15%. However, prior-focused strategies are not competitive with data-focused ones. In fact, LwF, LwF-MC, and ILT, outperform them by a large margin, confirming the effectiveness of this approch on preventing catastrophic forgetting. While ILT surpasses standard ICL baselines, our model is able to obtain a further boost. This improvement is remarkable for new classes, where we gain 11% in mIoU, while do not experience forgetting on old classes. It is especially interesting to compare our method with the baseline LwF which uses the same principles of ours but without modeling the background. Compared to LwF we achieve an average improvement of about 15%, thus demonstrating the importance of modeling the background in ICL for semantic segmentation. These results are consistent in both the disjoint and overlapped scenarios.</p><p>Single-step addition of five classes . In this setting we add, after the first training set, the following classes: plant, sheep, sofa, train, tv-monitor. Results are reported in <ref type="table" target="#tab_0">Table 1</ref>. Overall, the behavior on the first 15 classes is consistent with the 19-1 setting: FT and PI suffer a large performance drop, data-focused strategies (LwF, LwF-MC, ILT) outperform EWC and RW by far, while our method gets the  best results, obtaining performances closer to the joint training upper bound. For what concerns the disjoint scenario, our method improves over the best baseline of 4.6% on old classes, of 2% on novel ones and of 4% in all classes. These gaps increase in the overlapped setting where our method surpasses the baselines by nearly 10% in all cases, clearly demonstrating its ability to take advantage of the information contained in the background class.</p><p>Multi-step addition of five classes . This setting is similar to the previous one except that the last 5 classes are learned sequentially, one by one. From <ref type="table" target="#tab_0">Table 1</ref> we can observe that performing multiple steps is challenging and existing methods work poorly for this setting, reaching performance inferior to 7% on both old and new classes. In particular, FT and prior-focused methods are unable to prevent forgetting, biasing their prediction completely towards new classes and demonstrating performances close to 0% on the first 15 classes. Even data-focused methods suffer a dramatic loss in performances in this setting, decreasing their score from the single to the multi-step scenarios of more than 50% on all classes. On the other side, our method is still able to achieve good performances. Compared to the other approaches, MiB outperforms all baselines by a large margin in both old (46.2% on the disjoint and 35.1% on the overlapped), and new (nearly 13% on both setups) classes. As the overall performance drop (11% on all classes) shows, the overlapped scenario is the most challenging one since it does not impose any constraint on which classes are present in the background.</p><p>Ablation Study. In <ref type="table" target="#tab_2">Table 3</ref> we report a detailed analysis of our contributions, considering the overlapped setup. We start from the baseline LwF <ref type="bibr" target="#b17">[18]</ref> which employs standard cross-entropy and distillation losses. We first add to the baseline our modified cross-entropy (CE): this increases the ability to preserve old knowledge in all settings without harming   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">ADE20K</head><p>ADE20K <ref type="bibr" target="#b38">[39]</ref> is a large-scale dataset that contains 150 classes. Differently from Pascal-VOC 2012, this dataset contains both stuff (e.g. sky, building, wall) and object classes. We create the incremental datasets T t by splitting the whole dataset into disjoint image sets, without any constraint except ensuring a minimum number of images (i.e. 50) where classes on C t have labeled pixels. Obviously, each T t provides annotations only for classes in C t while other classes (old or future) appear as background in the ground truth. In <ref type="table" target="#tab_1">Table 2</ref> we report the mean IoU obtained averaging the results on two different class orders: the order proposed by <ref type="bibr" target="#b38">[39]</ref> and a random one. In this experiments, we compare our approach with data-focused methods only (i.e. LwF, LwF-MC, and ILT) due to their gap in performance with prior-focused ones.</p><p>Single-step addition of 50 classes (100-50). In the first experiment, we initially train the network on 100 classes and we add the remaining 50 all at once. From <ref type="table" target="#tab_1">Table 2</ref> we can observe that FT is clearly a bad strategy on large scale settings since it completely forgets old knowledge. Using a distillation strategy enables the network to reduce the catastrophic forgetting: LwF obtains 21.1% on past classes, ILT <ref type="figure">Figure 3</ref>: Qualitative results on the 100-50 setting of the ADE20K dataset using different incremental methods. The image demonstrates the superiority of our approach on both new (e.g. building, floor, table) and old (e.g. car, wall, person) classes. From left to right: image, FT, LwF <ref type="bibr" target="#b17">[18]</ref>, ILT <ref type="bibr" target="#b23">[24]</ref>, LwF-MC <ref type="bibr" target="#b27">[28]</ref>, our method, and the ground-truth. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>.9%, and LwF-MC 34.2%. Regarding new classes, LwF is the best strategy, exceeding LwF-MC by 18.9% and ILT by 6.6%. However, our method is far superior to all others, improving on the first classes and on the new ones. Moreover, we can observe that we are close to the joint training upper bound, especially considering new classes, where the gap with respect to it is only 0.3%. In <ref type="figure">Figure 3</ref> we report some qualitative results which demonstrate the superiority of our method compared to the baselines.</p><p>Multi-step addition of 50 classes (100-10). We then evaluate the performance on multiple incremental steps: we start from 100 classes and we add the remaining classes 10 by 10, resulting in 5 incremental steps. In <ref type="table" target="#tab_1">Table 2</ref> we report the results on all sets of classes after the last learning step. In this setting the performance of FT, LwF and ILT are very poor because they strongly suffers catastrophic forgetting. LwF-MC demonstrates a better ability to preserve knowledge on old classes, at the cost of a performance drop on new classes. Again, our method achieves the best trade-off between learning new classes and preserving past knowledge, outperforming LwF-MC by 11.6% considering all classes.</p><p>Three steps of 50 classes (50-50). Finally, in <ref type="table" target="#tab_1">Table 2</ref> we analyze the performance on three sequential steps of 50 classes. Previous ICL methods achieve different trade-offs between learning new classes and not forgetting old ones. LwF and ILT obtain a good score on new classes, but they forget old knowledge. On the contrary, LwF-MC preserves knowledge on the first 50 classes without being able to learn new ones. Our method outperforms all the baselines by a large margin with a gap of 11.9% on the best performing baseline, achieving the highest mIoU on every step. Remarkably, the highest gap is on the intermediate step, where there are classes that we must both learn incrementally and preserve from forgetting on the subsequent learning step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We studied the incremental class learning problem for semantic segmentation, analyzing the realistic scenario where the new training set does not provide annotations for old classes, leading to the semantic shift of the background class and exacerbating the catastrophic forgetting problem. We address this issue by proposing a novel objective function and a classifiers' initialization strategy which allows our network to explicitly model the semantic shift of the background, effectively learning new classes without deteriorating its ability to recognize old ones. Results show that our approach outperforms regularization-based ICL methods by a large margin, considering both small and large scale datasets. We hope that our problem formulation, our approach and our extensive comparison with previous methods will encourage future works on this novel research topic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean IoU on the Pascal-VOC 2012 dataset for different incremental class learning scenarios. -19 20 all 1-15 16-20 all 1-15 16-20 all 1-15 16-20 all 1-15 16-20 all FT 5.8 12.3 6.2 6.8 12.9 7.1 1.1 33.6 9.2 2.1 33.1 9.8 0.2 1.8 0.6 0.2 1.</figDesc><table><row><cell></cell><cell>19-1</cell><cell></cell><cell>15-5</cell><cell></cell><cell>15-1</cell></row><row><cell></cell><cell>Disjoint</cell><cell>Overlapped</cell><cell>Disjoint</cell><cell>Overlapped</cell><cell>Disjoint</cell><cell>Overlapped</cell></row><row><cell>Method</cell><cell cols="6">1-19 20 all 18 0.6</cell></row><row><cell>PI [36]</cell><cell cols="6">5.4 14.1 5.9 7.5 14.0 7.8 1.3 34.1 9.5 1.6 33.3 9.5 0.0 1.8 0.4 0.0 1.8 0.5</cell></row><row><cell>EWC [17]</cell><cell cols="6">23.2 16.0 22.9 26.9 14.0 26.3 26.7 37.7 29.4 24.3 35.5 27.1 0.3 4.3 1.3 0.3 4.3 1.3</cell></row><row><cell>RW [4]</cell><cell cols="6">19.4 15.7 19.2 23.3 14.2 22.9 17.9 36.9 22.7 16.6 34.9 21.2 0.2 5.4 1.5 0.0 5.2 1.3</cell></row><row><cell>LwF [18]</cell><cell cols="6">53.0 9.1 50.8 51.2 8.5 49.1 58.4 37.4 53.1 58.9 36.6 53.3 0.8 3.6 1.5 1.0 3.9 1.8</cell></row><row><cell cols="7">LwF-MC [28] 63.0 13.2 60.5 64.4 13.3 61.9 67.2 41.2 60.7 58.1 35.0 52.3 4.5 7.0 5.2 6.4 8.4 6.9</cell></row><row><cell>ILT [24]</cell><cell cols="6">69.1 16.4 66.4 67.1 12.3 64.4 63.2 39.5 57.3 66.3 40.6 59.9 3.7 5.7 4.2 4.9 7.8 5.7</cell></row><row><cell>MiB</cell><cell cols="6">69.6 25.6 67.4 70.2 22.1 67.8 71.8 43.3 64.7 75.5 49.4 69.0 46.2 12.9 37.9 35.1 13.5 29.7</cell></row><row><cell>Joint</cell><cell cols="6">77.4 78.0 77.4 77.4 78.0 77.4 79.1 72.6 77.4 79.1 72.6 77.4 79.1 72.6 77.4 79.1 72.6 77.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Mean IoU on the ADE20K dataset for different incremental class learning scenarios.</figDesc><table><row><cell></cell><cell></cell><cell>100-50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100-10</cell><cell></cell><cell></cell><cell></cell><cell cols="2">50-50</cell><cell></cell></row><row><cell>Method</cell><cell cols="13">1-100 101-150 all 1-100 100-110 110-120 120-130 130-140 140-150 all 1-50 51-100 101-150 all</cell></row><row><cell>FT</cell><cell>0.0</cell><cell>24.9</cell><cell>8.3</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>16.6</cell><cell>1.1 0.0</cell><cell>0.0</cell><cell>22.0</cell><cell>7.3</cell></row><row><cell>LwF [18]</cell><cell>21.1</cell><cell cols="3">25.6 22.6 0.1</cell><cell>0.0</cell><cell>0.4</cell><cell>2.6</cell><cell>4.6</cell><cell>16.9</cell><cell cols="2">1.7 5.7 12.9</cell><cell cols="2">22.8 13.9</cell></row><row><cell cols="2">LwF-MC [28] 34.2</cell><cell cols="3">10.5 26.3 18.7</cell><cell>2.5</cell><cell>8.7</cell><cell>4.1</cell><cell>6.5</cell><cell>5.1</cell><cell cols="2">14.3 27.8 7.0</cell><cell cols="2">10.4 15.1</cell></row><row><cell>ILT [24]</cell><cell>22.9</cell><cell cols="3">18.9 21.6 0.3</cell><cell>0.0</cell><cell>1.0</cell><cell>2.1</cell><cell>4.6</cell><cell>10.7</cell><cell>1.4 8.4</cell><cell>9.7</cell><cell cols="2">14.3 10.8</cell></row><row><cell>MiB</cell><cell>37.9</cell><cell cols="3">27.9 34.6 31.8</cell><cell>10.4</cell><cell>14.8</cell><cell>12.8</cell><cell>13.6</cell><cell cols="3">18.7 25.9 35.5 22.2</cell><cell cols="2">23.6 27.0</cell></row><row><cell>Joint</cell><cell>44.3</cell><cell cols="3">28.2 38.9 44.3</cell><cell>26.1</cell><cell>42.8</cell><cell>26.7</cell><cell>28.1</cell><cell cols="3">17.3 38.9 51.1 38.3</cell><cell cols="2">28.2 38.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of the proposed method on the Pascal-VOC 2012 overlapped setup. CE and KD denote our cross-entropy and distillation losses, while init our initialization strategy.</figDesc><table><row><cell></cell><cell>19-1</cell><cell>15-5</cell><cell>15-1</cell></row><row><cell></cell><cell cols="3">1-19 20 all 1-15 16-20 all 1-15 16-20 all</cell></row><row><cell cols="4">LwF [18] 51.2 8.5 49.1 58.9 36.6 53.3 1.0 3.9 1.8</cell></row><row><cell>+ CE</cell><cell cols="3">57.6 9.9 55.2 63.2 38.1 57.0 12.0 3.7 9.9</cell></row><row><cell>+ KD</cell><cell cols="3">66.0 11.9 63.3 72.9 46.3 66.3 34.8 4.5 27.2</cell></row><row><cell>+ init</cell><cell cols="3">70.2 22.1 67.8 75.5 49.4 69.0 35.1 13.5 29.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>or even improving performances on the new classes. Second, we add our distillation loss (KD) to the model. Our KD provides a boost on the performances for both old and new classes. The improvement on old classes is remarkable, especially in the 15-1 scenario (i.e. 22.8%). For the novel classes, the improvement is constant and is especially pronounced in the 15-5 scenario (7%). Notice that this aspect is peculiar of our KD since standard formulation work only on preserving old knowledge. This shows that the two losses provide mutual benefits. Finally, we add our classifiers' initialization strategy (init). This component provides an improvement in every setting, especially on novel classes: it doubles the performance on the 19-1 setting (22.1% vs 11.9%) and triplicates on the 15-1 (4.5% vs 13.5%). This confirms the importance of accounting for the background shift at the initialization stage to facilitate the learning of new classes.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Method3.1. Problem Definition and NotationBefore delving into the details of ICL for semantic segmentation, we first introduce the task of semantic segmen-</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol?s</forename><surname>Mar?n-Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Riemannian walk for incremental learning: Understanding forgetting and intransigence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Continual learning: A comparative study on how to defy forgetting in classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning without memorizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat Vikram</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.1" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measuring catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Abitino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incremental learning techniques for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV-WS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to remember: A synaptic plasticity driven framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksiy</forename><surname>Ostapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Jahnichen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learn the new, keep the old: Extending pretrained models with new anatomy and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firat</forename><surname>Ozdemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fuernstahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orcun</forename><surname>Goksel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Extending pretrained segmentation networks with additional anatomical structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firat</forename><surname>Ozdemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orcun</forename><surname>Goksel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progressive neural networks</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Jung Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Incremental learning of object detectors without catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Incremental learning for semantic segmentation of large-scale remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Tasar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliya</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3524" to="3537" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Memory replay gans: Learning to generate new categories without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenshen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Raducanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large scale incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
