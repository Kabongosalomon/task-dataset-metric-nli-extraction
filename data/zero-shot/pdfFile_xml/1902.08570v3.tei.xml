<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jet Tagging via Particle Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huilin</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>93106</postCode>
									<settlement>Santa Barbara</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Loukas Gouskos ? CERN</orgName>
								<address>
									<postCode>CH-1211</postCode>
									<settlement>Geneva 23</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jet Tagging via Particle Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How to represent a jet is at the core of machine learning on jet physics. Inspired by the notion of point clouds, we propose a new approach that considers a jet as an unordered set of its constituent particles, effectively a "particle cloud". Such a particle cloud representation of jets is efficient in incorporating raw information of jets and also explicitly respects the permutation symmetry. Based on the particle cloud representation, we propose ParticleNet, a customized neural network architecture using Dynamic Graph Convolutional Neural Network for jet tagging problems. The ParticleNet architecture achieves state-of-the-art performance on two representative jet tagging benchmarks and is improved significantly over existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A jet is one of the most ubiquitous objects in protonproton collision events at the LHC. In essence, a jet is a collimated spray of particles. It serves as a handle to probe the underlying elementary particle produced in the hard scattering process that initiates the cascade of particles contained in the jet.</p><p>One of the most important questions about a jet is which type of elementary particle initiates it. Jets initiated by different particles exhibit different characteristics. For example, jets initiated by gluons tend to have a broader energy spread than jets initiated by quarks. High-momentum heavy particles (e.g., top quarks and W, Z, and Higgs bosons) that decay hadronically can lead to jets with distinct multi-prong structures. Therefore, the identity of the source particle can be inferred from properties of the reconstructed jet. Such particle identity information provides powerful insights into the collision events under study and therefore can help greatly in separating events originating from different physics processes and improving the sensitivity of both searches for new particles and measurements of the standard model processes.</p><p>The study on jet tagging, i.e., the identification of the elementary particle initiating a jet, has a long history. Methods based on the QCD theory have been proposed and continuously improved for discriminating quark and gluon jets <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, tagging jets originating from high-momentum heavy particles <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>, etc. See Refs. <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> for more in-depth reviews. Recently, machine learning (ML) has injected fresh blood in jet tagging. Jets are regarded as images <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref><ref type="bibr" target="#b33">[33]</ref><ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref> or as sequences <ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref><ref type="bibr" target="#b40">[40]</ref><ref type="bibr" target="#b41">[41]</ref><ref type="bibr" target="#b42">[42]</ref><ref type="bibr" target="#b43">[43]</ref><ref type="bibr" target="#b44">[44]</ref><ref type="bibr" target="#b45">[45]</ref><ref type="bibr" target="#b46">[46]</ref><ref type="bibr" target="#b47">[47]</ref><ref type="bibr" target="#b48">[48]</ref>, trees <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50]</ref>, graphs <ref type="bibr" target="#b51">[51]</ref>, or sets <ref type="bibr" target="#b52">[52]</ref> of particles, and ML techniques, most notably deep neural networks (DNNs), are used to build new jet tagging algorithms automatically from (labeled) simulated samples or even * hqu@ucsb.edu ? loukas.gouskos@cern.ch (unlabeled) real data <ref type="bibr" target="#b53">[53]</ref><ref type="bibr" target="#b54">[54]</ref><ref type="bibr" target="#b55">[55]</ref><ref type="bibr" target="#b56">[56]</ref>, leading to new insights and improvements in jet tagging. In this paper, we propose a new deep-learning approach for jet tagging using a novel way to represent jets. Instead of organizing a jet's constituent particles into an ordered structure (e.g., a sequence or a tree), we treat a jet as an unordered set of particles <ref type="bibr" target="#b57">[57]</ref>. This is very analogous to the point cloud representation of threedimensional (3D) shapes used in computer vision, where each shape is represented by a set of points in space, and the points themselves are also unordered. Therefore, a jet can be viewed as a "particle cloud". Based on Dynamic Graph Convolutional Neural Network (DGCNN) <ref type="bibr" target="#b58">[58]</ref>, we design ParticleNet, a customized neural network architecture that operates directly on particle clouds for jet tagging. The ParticleNet architecture is evaluated on two jet tagging benchmarks and is found to achieve significant improvements over all existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. JET REPRESENTATIONS</head><p>The efficiency and effectiveness of ML techniques on jet physics relies heavily on how a jet is represented. In this section, we review the mainstream jet representations and introduce the particle cloud representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image-based representation</head><p>The image representation has its root in the reconstruction of jets with calorimeters. A calorimeter measures the energy deposition of a jet on fine-grained spatial cells. Treating the energy deposition on each cell as the pixel intensity naturally creates an image for a jet. When jets are formed by particles reconstructed with the full detector information (e.g., using a particle-flow algorithm <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b60">60]</ref>), a jet image can be constructed by mapping each particle onto the corresponding calorimeter cell, and sum up the energy if more than one particle is mapped to the same cell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1902.08570v3 [hep-ph] 30 Mar 2020</head><p>The image-based approach has been extensively studied for various jet tagging tasks, e.g., W boson tagging <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b35">35]</ref>, top tagging <ref type="bibr" target="#b32">[32]</ref><ref type="bibr" target="#b33">[33]</ref><ref type="bibr" target="#b34">[34]</ref> and quark-gluon tagging <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">31]</ref>. Convolutional neural networks (CNNs) with various architectures were explored in these studies, and they were found to achieve sizable improvement in performance compared to traditional multivariate methods using observables motivated by QCD theory. However, the architectures investigated in these papers are in general much shallower compared to state-of-the-art CNN architectures used in image classification tasks (e.g., ResNet <ref type="bibr" target="#b61">[61]</ref> or Inception <ref type="bibr" target="#b62">[62]</ref>); therefore, it remains to be seen that if deeper architectures can further improve the performance.</p><p>Despite the promising performance, the image-based representation has two main shortcomings. While it can include all information without loss when a jet is measured by only the calorimeter, once the jet constituent particles are reconstructed, how to incorporate additional information of the particles is unclear, as it involves combining non-additive quantities (e.g., the particle type) of multiple particles entering the same cell. Moreover, treating jets as images also leads to a very sparse representation: a typical jet has O(10) to O(100) particles, while a jet image typically needs O(1000) pixels (e.g., 32 ? 32) in order to fully contain the jet; therefore, more than 90% of the pixels are blank. This makes the CNNs highly computationally inefficient on jet images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Particle-based representation</head><p>A more natural way to represent a jet, when particles are reconstructed, is to simply view the jet as a collection of its constituent particles. This approach allows for the inclusion of any kind of features for each particle and therefore is significantly more flexible than the image representation. It is also much more compact compared to the image representation, though at the cost of being variable length, as each jet may contain a different number of particles.</p><p>A collection of particles, though, is a rather general concept. Before applying any deep-learning algorithm, a concrete data structure has to be chosen. The prevailing choice is a sequence, in which particles are sorted in a specific way (e.g., with decreasing transverse momentum) and organized into a one-dimensional (1D) list. Using particle sequences as inputs, jet tagging tasks have been tackled with recurrent neural networks (RNNs) <ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref><ref type="bibr" target="#b45">45]</ref>, 1D CNNs <ref type="bibr" target="#b40">[40]</ref><ref type="bibr" target="#b41">[41]</ref><ref type="bibr" target="#b42">[42]</ref><ref type="bibr" target="#b43">[43]</ref><ref type="bibr" target="#b44">[44]</ref> and physics-oriented neural networks <ref type="bibr" target="#b46">[46]</ref><ref type="bibr" target="#b47">[47]</ref><ref type="bibr" target="#b48">[48]</ref>. Another interesting choice is a binary tree, which is well motivated from the QCD theory perspective. Recursive neural networks (RecNNs) are then a natural fit and have been studied in Refs. <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50]</ref>.</p><p>One thing to note about the sequence or tree representation is that they both need the particles to be sorted in some way, as the order of the particles is used implicitly in the corresponding RNNs, 1D CNNs, or the RecNNs.</p><p>However, the constituent particles in a jet have no intrinsic order; thus, the manually imposed order may turn out to be suboptimal and impair the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Jet as a particle cloud</head><p>An even more natural representation than particle sequences or trees would be an unordered, permutationinvariant set of particles. As a special case of the particlebased representations, it shares all the advantages of particle-based representations, especially the flexibility to include arbitrary features for each particle. We refer to such representation of a jet as a particle cloud, analogous to the point cloud representation of 3D shapes used in computer vision. They are actually highly similar, as both are essentially unordered sets of entities distributed irregularly in space. In both clouds, the elements are not unrelated individuals but are rather correlated, as they represent higher-level objects (i.e., jets or 3D shapes) that have rich internal structures. Therefore, deep-learning algorithms developed for point clouds are likely to be helpful for particle clouds, i.e., jets, as well.</p><p>The idea of regarding jets as unordered sets of particles was also proposed in Ref. <ref type="bibr" target="#b52">[52]</ref> and is in parallel to our work. The Deep Sets framework <ref type="bibr" target="#b63">[63]</ref> was adapted to construct the infrared and collinear safe Energy Flow Network and the more general Particle Flow Network. However, different from the DGCNN [58] approach adopted in this paper, the Deep Sets approach does not explicitly exploit the local spatial structure of particle clouds, but only processes the particle clouds in a global way. Another closely related approach is to represent a jet as a graph whose vertices are the particles. Message-passing neural networks (MPNNs) with different variants of adjacency matrices were explored on such jet graphs and were found to show better performance than the RecNNs <ref type="bibr" target="#b51">[51]</ref>. However, depending on how the adjacency matrix is defined, the MPNNs may not respect the permutation symmetry of the particles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NETWORK ARCHITECTURE</head><p>The permutation symmetry of the particle cloud makes it a natural and promising representation of jets. However, to achieve the best possible performance, the architecture of the neural network has to be carefully designed to fully exploit the potential of this representation. In this section, we introduce ParticleNet, a CNN-like deep neural network for jet tagging with particle cloud data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Edge convolution</head><p>CNNs have achieved overwhelming success in all kinds of machine-learning tasks on visual images. Two key features of CNNs contribute significantly to their success.</p><p>First, the convolution operation exploits translational symmetry of images by using shared kernels across the whole image. This not only greatly reduces the number of parameters in the network but also allows the parameters to be learned more effectively, as each set of weights will use all locations of the image for learning. Second, CNNs exploit a hierarchical approach <ref type="bibr" target="#b64">[64]</ref> for learning image features. The convolution operations can be effectively stacked to form a deep network. Different layers in the CNNs have different receptive fields and therefore can learn features at different scales, with the shallower layers exploiting local neighborhood information and the deeper layers learning more global structures. Such a hierarchical approach proves an effective way to learn images.</p><p>Motivated by the success of CNNs, we would like to adopt a similar approach for learning on point (particle) cloud data. However, regular convolution operation cannot be applied on point clouds, as the points there can be distributed irregularly, rather than following some uniform grids as the pixels in an image. Therefore, the basis for a convolution, i.e., a "local patch" of each point on which the convolution kernel operates, remains to be defined for point clouds. Moreover, a regular convolution operation, typically in the form j K j x j where K is the kernel and x j denotes the features of each point, is not invariant under permutation of the points. Thus, the form of a convolution also needs to be modified to respect the permutation symmetry of point clouds.</p><p>Recently, the edge convolution ("EdgeConv") operation has been proposed in Ref. <ref type="bibr" target="#b58">[58]</ref> as a convolution-like operation for point clouds. EdgeConv starts by representing a point cloud as a graph, whose vertices are the points themselves, and the edges are constructed as connections between each point to its k nearest neighboring points. In this way, a local patch needed for convolution is defined for each point as the k nearest neighboring points connected to it. The EdgeConv operation for each point x i then has the form</p><formula xml:id="formula_0">x i = k j=1 h ? (x i , x ij ),<label>(1)</label></formula><p>where x i ? R F denotes the feature vector of the point x i and {i 1 , ..., i k } are the indices of the k nearest neighboring points of the point x i . The edge function h ? :  <ref type="bibr" target="#b65">[65]</ref>.</p><formula xml:id="formula_1">R F ? R F ? R F is</formula><p>In this paper, we follow the choice in Ref. <ref type="bibr" target="#b58">[58]</ref> to use a specialized form of the edge function,</p><formula xml:id="formula_2">h ? (x i , x ij ) =h ? (x i , x ij ? x i ),<label>(2)</label></formula><p>where the feature vectors of the neighbors, x ij , are substituted by their differences from the central point x i andh ? can be implemented as a multilayer perceptron (MLP) whose parameters are shared among all edges. For the aggregation operation , however, we use mean, i.e., 1 k , throughout this paper, which shows better performance than the max operation used in the original paper.</p><p>One important feature of the EdgeConv operation is that it can be easily stacked, just as regular convolutions. This is because EdgeConv can be viewed as a mapping from a point cloud to another point cloud with the same number of points, only possibly changing the dimension of the feature vector for each point. Therefore, another EdgeConv operation can be applied subsequently. This allows us to build a deep network using EdgeConv operations which can learn features of point clouds hierarchically.</p><p>The stackability of EdgeConv operations also brings another interesting possibility. Basically, the feature vectors learned by EdgeConv can be viewed as new coordinates of the original points in a latent space, and then, the distances between points, used in the determination of the k nearest neighbors, can be computed in this latent space. In other words, the proximity of points can be dynamically learned with EdgeConv operations. This results in the DGCNN <ref type="bibr" target="#b58">[58]</ref>, in which the graph describing the point clouds are dynamically updated to reflect the changes in the edges, i.e., the neighbors of each point. Reference <ref type="bibr" target="#b58">[58]</ref> demonstrates that this leads to better performance than keeping the graph static.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ParticleNet</head><p>The ParticleNet architecture makes extensive use of EdgeConv operations and also adopts the dynamic graph update approach. However, a number of different design choices are made in ParticleNet compared to the original DGCNN to better suit the jet tagging task, including the number of neighbors, the configuration of the MLP in EdgeConv, the use of shortcut connection, etc. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the structure of the EdgeConv block implemented in this paper. The EdgeConv block starts with finding the k nearest neighboring particles for each particle, using the "coordinates" input of the Edge-Conv block to compute the distances. Then, inputs to the EdgeConv operation, the "edge features", are constructed from the "features" input using the indices of k nearest neighboring particles. The EdgeConv operation is implemented as a three-layer MLP. Each layer consists of a linear transformation, followed by a batch normalization <ref type="bibr" target="#b66">[66]</ref> and then the a rectified linear unit (ReLU) <ref type="bibr" target="#b67">[67]</ref>. Inspired by ResNet <ref type="bibr" target="#b61">[61]</ref>, a shortcut connection running parallel to the EdgeConv operation is also included in each block, allowing the input features to pass through directly. An EdgeConv block is characterized by two hyperparameters, the number of neighbors k, and the num- The ParticleNet architecture used in this paper is shown in <ref type="figure" target="#fig_1">Fig. 2a</ref>. It consists of three EdgeConv blocks. The first EdgeConv block uses the spatial coordinates of the particles in the pseudorapidity-azimuth space to compute the distances, while the subsequent blocks use the learned feature vectors as coordinates. The number of nearest neighbors k is 16 for all three blocks, and the number of channels C for each EdgeConv block is <ref type="bibr" target="#b64">(64,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b64">64)</ref>, (128, 128, 128), and (256, 256, 256), respectively. After the EdgeConv blocks, a channel-wise global average pooling operation is applied to aggregate the learned features over all particles in the cloud. This is followed by a fully connected layer with 256 units and the ReLU activation. A dropout layer <ref type="bibr" target="#b68">[68]</ref> with a drop probability of 0.1 is included to prevent overfitting. A fully connected layer with two units, followed by a softmax function, is used to generate the output for the binary classification task.</p><p>A similar network with reduced complexity is also investigated. Compared to the baseline ParticleNet architecture, only two EdgeConv blocks are used, with the number of nearest neighbors k reduced to 7 and the number of channels C reduced to <ref type="bibr" target="#b32">(32,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b32">32)</ref> and <ref type="bibr" target="#b64">(64,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b64">64)</ref> for the two blocks, respectively. The number of units in the fully connected layer after pooling is also lowered to 128. This simplified architecture is denoted as "ParticleNet-Lite" and is illustrated in <ref type="figure" target="#fig_1">Fig. 2b</ref>. The number of arithmetic operations is reduced by almost an order of magnitude in ParticleNet-Lite, making it more suitable when computational resources are limited.</p><p>The networks are implemented with Apache MXNet <ref type="bibr" target="#b69">[69]</ref>, and the training is performed on a single Nvidia GTX 1080 Ti graphics card (GPU). A batch size of 384 (1024) is used for the ParticleNet (ParticleNet-Lite) architecture due to GPU memory constraint. The AdamW optimizer <ref type="bibr" target="#b70">[70]</ref>, with a weight decay of 0.0001, is used to minimize the cross entropy loss. The one-cycle learning rate (LR) schedule <ref type="bibr" target="#b71">[71]</ref> is adopted in the training, with the LR selected following the LR range test described in Ref. <ref type="bibr" target="#b71">[71]</ref>, and slightly tuned afterward with a few trial trainings. The training of ParticleNet (ParticleNet-Lite) network uses an initial LR of 3 ? 10 ?4 (5 ? 10 ?4 ), rising to the peak LR of 3 ? 10 ?3 (5 ? 10 ?3 ) linearly in eight epochs and then decreasing to the initial LR linearly in another eight epochs. This is followed by a cooldown phase of four epochs which gradually reduces the LR to 5 ? 10 ?7 (1 ? 10 ?6 ) for better convergence. A snapshot of the model is saved at the end of each epoch, and the model snapshot showing the best accuracy on the validation dataset is selected for the final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>The performance of the ParticleNet architecture is evaluated on two representative jet tagging tasks: top tagging and quark-gluon tagging. In this section, we show the benchmark results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Top tagging</head><p>Top tagging, i.e., identifying jets originating from hadronically decaying top quarks, is commonly used in searches for new physics at the LHC. We evaluate the performance of the ParticleNet architecture on this task using the top tagging dataset <ref type="bibr" target="#b72">[72]</ref>, which is an extension of the dataset used in Ref. <ref type="bibr" target="#b46">[46]</ref> with some modifications. Jets in this dataset are generated with Pythia8 <ref type="bibr" target="#b73">[73]</ref> and passed through Delphes <ref type="bibr" target="#b74">[74]</ref> for fast detector I: Input variables used in the top tagging task (TOP) and the quark-gluon tagging task (QG) with and without PID information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variable</head><p>Definition TOP QG QG-PID ?? difference in pseudorapidity between the particle and the jet axis x x x ?? difference in azimuthal angle between the particle and the jet axis x x x log p T logarithm of the particle's p T x x x log E logarithm of the particle's energy x x x log pT pT (jet) logarithm of the particle's p T relative to the jet p T x x x log E E(jet) logarithm of the particle's energy relative to the jet energy x x x ?R angular separation between the particle and the jet axis ( (??) 2 + (??) 2 ) x x x q electric charge of the particle x isElectron if the particle is an electron x isMuon if the particle is a muon x isChargedHadron if the particle is a charged hadron x isNeutralHadron if the particle is a neutral hadron x isPhoton if the particle is a photon x simulation. No multiple parton interaction or pileup is included in the simulation. Jets are clustered from the Delphes E-Flow objects with the anti-k T algorithm <ref type="bibr" target="#b76">[75]</ref> using a distance parameter R = 0.8. Only jets with transverse momentum p T ? [550, 650] and pseudorapidity |?| &lt; 2 are considered. Each signal jet is required to be matched to a hadronically decaying top quark within ?R = 0.8, and all three quarks from the top decay also within ?R = 0.8 of the jet axis. The background jets are obtained from a QCD dijet process. This dataset consists of 2 million jets in total, half signal and half background. The official splitting for training (1.2M jets), validation (400k jets) and testing (400k jets) is used in the development of the ParticleNet model for this dataset. In this dataset, up to 200 jet constituent particles are stored for each jet. Only kinematic information, i.e., the 4-momentum (p x , p y , p z , E), of each particle is available. The ParticleNet model takes up to 100 constituent particles with the highest p T for each jet, and uses seven variables derived from the 4-momentum for each particle as inputs, which are listed in <ref type="table">Table I</ref>. The (??, ??) variables are used as coordinates to compute the distances between particles in the first EdgeConv block. They are also used together with the other five variables, log p T , log E, log p T p T (jet) , log E E(jet) and ?R, to form the input feature vector for each particle.</p><p>We compare the performance of ParticleNet with three alternative models <ref type="bibr" target="#b77">[76]</ref>:</p><p>? ResNeXt-50: The ResNeXt-50 model is a very deep two-dimensional (2D) CNN using jet images as inputs. The ResNeXt architecture <ref type="bibr" target="#b78">[77]</ref> was proposed for generic image classification, and we modify it slightly for the jet tagging task. The model is trained on the top tagging dataset starting from randomly initialized weights. The implementation details can be found in Appendix A. Note that the ResNeXt-50 architecture is much deeper and therefore has a much larger capacity than most of the CNN architectures <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref><ref type="bibr" target="#b33">[33]</ref><ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref> explored for jet tagging so far, so evaluating its performance on jet tagging will shed light on whether architectures for generic image classification are also applicable to jet images.</p><p>? P-CNN: The P-CNN is a 14-layer 1D CNN using particle sequences as inputs. The P-CNN architecture was proposed in the CMS particle-based DNN boosted jet tagger <ref type="bibr" target="#b42">[42]</ref> and showed significant improvement in performance compared to a traditional tagger using boosted decision trees and jet-level observables. The model is also trained on the top tagging dataset from scratch, with the implementation details in Appendix B.</p><p>? PFN: The Particle Flow Network (PFN) <ref type="bibr" target="#b52">[52]</ref> is a recent architecture for jet tagging which also treats a jet as an unordered set of particles, the same as the particle cloud approach in this paper. However, the network is based on the Deep Sets framework <ref type="bibr" target="#b63">[63]</ref>, which uses global symmetric functions and does not exploit local neighborhood information explicitly as the EdgeConv operation. Since the performance of PFN on this top tagging dataset has already been reported in Ref. <ref type="bibr" target="#b52">[52]</ref>, we did not reimplement it but just include the results for comparison.</p><p>The results are summarized in <ref type="table">Table II</ref> and also shown in <ref type="figure" target="#fig_2">Fig. 3</ref> in terms of receiver operating characteristic (ROC) curves. A number of metrics are used to evaluate the performance, including the accuracy, the area under the ROC curve (AUC), and the background rejection (1/? b , i.e., the reciprocal of the background misidentification rate) at a certain signal efficiency (? s ) of 50% or 30%. The background rejection metric is particularly relevant to physics analysis at the LHC, as it is directly related to the expected contribution of background, and is commonly used to select the best jet tagging algorithm. The II: Performance comparison on the top tagging benchmark dataset. The ParticleNet, ParticleNet-Lite, P-CNN and ResNeXt-50 models are trained on the top tagging dataset starting from randomly initialized weights. For each model, the training is repeated for 9 times using different randomly initialized weights. <ref type="table">The table shows</ref> the result from the median-accuracy training, and the standard deviation of the 9 trainings is quoted as the uncertainty to assess the stability to random weight initialization. Uncertainty on the accuracy and AUC are negligible and therefore omitted. The performance of PFN on this dataset is reported in Ref. <ref type="bibr" target="#b52">[52]</ref>, and the uncertainty corresponds to the spread in 10 trainings.  ParticleNet model achieves state-of-the-art performance on the top tagging benchmark dataset and improves over previous methods significantly. Its background rejection power at 30% signal efficiency is roughly 1.8 (2.1) times as good as PFN (P-CNN), and about 40% better than ResNeXt-50. Even the ParticleNet-Lite model, with significantly reduced complexity, outperforms all the previous models, achieving about 10% improvement with respect to ResNeXt-50. The large performance improvement of the ParticleNet architecture over the PFN architecture is likely due to a better exploitation of the local neighborhood information with the EdgeConv operation.</p><formula xml:id="formula_3">Accuracy AUC 1/? b at ? s = 50% 1/? b at ? s =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quark-gluon tagging</head><p>Another important jet tagging task is quark-gluon tagging, i.e., discriminating jets initiated by quarks and by gluons. The quark-gluon tagging dataset from Ref. <ref type="bibr" target="#b52">[52]</ref> is used to evaluate the performance of the ParticleNet architecture on this task. The signal (quark) and background (gluon) jets are generated with Pythia8 using the Z(? ??) + (u, d, s) and Z(? ??) + g processes, respectively. No detector simulation is performed. The final state non-neutrino particles are clustered into jets using the anti-k T algorithm <ref type="bibr" target="#b76">[75]</ref> with R = 0.4. Only jets with transverse momentum p T ? [500, 550] and rapidity |y| &lt; 2 are considered. This dataset consists of 2 million jets in total, half signal and half background. We follow the recommended splitting of 1.6M/200k/200k for training, validation and testing in the development of the ParticleNet model on this dataset.</p><p>One important difference of the quark-gluon tagging dataset is that it includes not only the four momentum, but also the type of each particle (i.e., electron, photon, pion, etc.). Such particle identification (PID) information can be quite helpful for jet tagging. Therefore, we include this information in the ParticleNet model and compare it with the baseline version using only the kinematic information. The PID information is included in an experimentally realistic way by using only five particle types (electron, muon, charged hadron, neutral hadron, and photon), as well as the electric charge, as inputs. These six additional variables, together with the seven kinematic variables, form the input feature vector of each particle for models with PID information, as shown in <ref type="table">Table I</ref>. <ref type="table">Table III</ref> compares the performance of the ParticleNet model with a number of alternative models introduced in Sec. IV A. Model variants with and without PID inputs are also compared. Note that for the ResNeXt-50 model only the version without PID inputs is presented, as it is based on jet images which cannot incorporate PID information straightforwardly. The corresponding ROC curves are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Overall, the addition of PID III: Performance comparison on the quark-gluon tagging benchmark dataset. The ParticleNet, ParticleNet-Lite, P-CNN, and ResNeXt-50 models are trained on the quark-gluon tagging dataset starting from randomly initialized weights. The training is repeated 9 times for the ParticleNet model using different randomly initialized weights. The table shows the result from the median-accuracy training, and the standard deviation of the 9 trainings is quoted as the uncertainty to assess the stability to random weight initialization. Because of limited computational resources, the training of other models is performed only once, but the uncertainty due to random weight initialization is expected to be fairly small. The performance of PFN on this dataset is reported in Ref. <ref type="bibr" target="#b52">[52]</ref>, and the uncertainty corresponds to the spread in ten trainings. Note that a number of PFN models with different levels of PID information are investigated in Ref. <ref type="bibr" target="#b52">[52]</ref>, and "PFN-Ex", also using experimentally realistic PID information, is shown here for comparison.  inputs has a large impact on the performance, increasing the background rejection power by 10%-15% compared to the same model without using PID information. This clearly demonstrates the advantage of particle-based jet representations, including the particle cloud representation, as they can easily integrate any additional information for each particle. The best performance is obtained by the ParticleNet model with PID inputs, achieving almost 15% improvement on the background rejection power compared to the PFN-Ex (PFN using experimentally realistic PID information) and P-CNN models. The ParticleNet-Lite model achieves the second-best performance and shows about 7% improvement with respect to the PFN-Ex and P-CNN models.</p><formula xml:id="formula_4">Accuracy AUC 1/? b at ? s = 50% 1/? b at ? s =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. MODEL COMPLEXITY</head><p>Another aspect of machine-learning models is the complexity, e.g., the number of parameters and the computational cost. <ref type="table" target="#tab_6">Table IV</ref> compares the number of parameters and the computational cost of all the models used in the top tagging task in Sec. IV A. The computational cost is evaluated using the inference time per object, which is a more relevant metric than the training time for real-life applications of machine-learning models. The inference time of each model is measured on both the CPU and the GPU, using the implementations with Apache MXNet. For the CPU, to mimic the event processing workflow typically used in collider experiments, a batch size of 1 is used, and the inference is performed in single-thread mode. For the GPU, a batch size of 100 is used instead, as the full power of the GPU cannot be revealed with a very small batch size (e.g., 1) due to the overhead in data transfer between the CPU and the GPU. The ParticleNet model achieves the best classification performance at the cost of speed, being more than an order of magnitude slower than the PFN and the P-CNN models, but still it is not prohibitively slow even on the CPU. In addition, the current implementation of the EdgeConv operation used in the ParticleNet model is not as optimized as the regular convolution operation; therefore, further speed-up is expected from an optimized implementation of EdgeConv. On the other hand, the ParticleNet-Lite model provides a good balance between speed and performance, showing more than 40% improvement in performance while being only a few times slower than the PFN and P-CNN models. Notably, it is also the most economical model, outperforming all previous approaches with only 26k parameters, thanks to the effective exploitation of the permutation symmetry of the particle clouds. Overall, PFN is the fastest model on both the CPU and the GPU, making it a suitable choice for extremely time-critical tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we present a new approach for machine learning on jets. The core of this approach is to treat jets as particle clouds, i.e., unordered sets of particles. Based on this particle cloud representation, we introduce ParticleNet, a network architecture tailored to jet tagging tasks. The performance of the ParticleNet architecture is compared with alternative deep-learning architectures, including the jet image-based ResNeXt-50 model, the particle sequence-based P-CNN model and the particle set-based PFN model. On both the top tagging and the quark-gluon tagging benchmarks, Par-ticleNet achieves state-of-the-art performance and improves significantly over existing methods. Although the very deep image-based ResNeXt-50 model also shows significant performance improvement over shallower models like P-CNN and PFN on the top-tagging benchmark, indicating that deeper architectures can generally lead to better performance, the gain with the ParticleNet architecture is more substantial. Moreover, the high performance is achieved in a very economical way as the number of trainable parameters is a factor of 4 (56) lower in ParticleNet (ParticleNet-Lite) compared to ResNeXt-50. Such lightweight models are particularly useful for applications in high-energy physics experiments, especially for online event processing in which low latency and memory consumption is critical.</p><p>While we only demonstrate the power of the particle cloud representation in jet tagging tasks, we think that it is a natural and generic way of representing jets (and even the whole collision event) and can be applied to a broad range of particle physics problems. Applications of the particle cloud approach to, e.g., pileup identification, jet grooming, jet energy calibration, etc., would be particularly interesting and worth further investigation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG. 1 :</head><label>1</label><figDesc>The structure of the EdgeConv block.ber of channels C = (C 1 , C 2 , C 3 ), corresponding to the number of units in each linear transformation layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG. 2 :</head><label>2</label><figDesc>The architectures of the ParticleNet and the ParticleNet-Lite networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIG. 3 :</head><label>3</label><figDesc>(AUC = 0.9837) P-CNN (AUC = 0.9803) ParticleNet-Lite (AUC = 0.9844) ParticleNet (AUC = 0.9858) Performance comparison in terms of ROC curves on the top tagging benchmark dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIG. 4 :</head><label>4</label><figDesc>(AUC = 0.8915) ParticleNet-Lite (AUC = 0.8993) ParticleNet (AUC = 0.9014) P-CNN (w/ PID) (AUC = 0.9002) ParticleNet-Lite (w/ PID) (AUC = 0.9079) ParticleNet (w/ PID) (AUC = 0.9116) Performance comparison in terms of ROC curves on the quark-gluon tagging benchmark dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>some function parametrized by a set of learnable parameters ?, and is a channel-wise symmetric aggregation operation, e.g., max, sum, or mean. The parameters ? of the edge function are shared for all points in the point cloud. This, together with the choice of a symmetric aggregation operation , makes Edge-Conv a permutationally symmetric operation on point clouds</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Number of parameters, inference time per object, and background rejection of different models. The CPU inference time is measured on an Intel Core i7-6850K CPU with a single thread using a batch size of 1. The GPU inference time is measured on a Nvidia GTX 1080 Ti GPU using a batch size of 100.</figDesc><table><row><cell></cell><cell>Parameters</cell><cell>Time (CPU) [ms]</cell><cell>Time (GPU) [ms]</cell><cell>1/? b at ? s = 30%</cell></row><row><cell>ResNeXt-50</cell><cell>1.46M</cell><cell>7.4</cell><cell>0.22</cell><cell>1147 ? 58</cell></row><row><cell>P-CNN</cell><cell>348k</cell><cell>1.6</cell><cell>0.020</cell><cell>759 ? 24</cell></row><row><cell>PFN</cell><cell>82k</cell><cell>0.8</cell><cell>0.018</cell><cell>888 ? 17</cell></row><row><cell>ParticleNet-Lite</cell><cell>26k</cell><cell>2.4</cell><cell>0.084</cell><cell>1262 ? 49</cell></row><row><cell>ParticleNet</cell><cell>366k</cell><cell>23</cell><cell>0.92</cell><cell>1615 ? 93</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Gregor Kasieczka </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Implementation details of ResNeXt-50</head><p>The ResNeXt-50 model uses jet images as inputs. Each image is constructed from the constituent particles by projecting them onto a 2D grid of 64 ? 64 pixels in size, corresponding to a granularity of 0.025 rad in the pseudorapidity-azimuth space. The intensity of each pixel is the sum of p T of all the particles within the pixel rescaled by the inverse of the jet p T .</p><p>The original 50-layer ResNeXt architecture <ref type="bibr" target="#b78">[77]</ref> was developed for images of size 224 ? 224 and a classification task with 1000 classes. To adapt to the smaller size of the jet images and the significantly fewer number of output classes, the number of channels in all but the first convolutional layers is reduced by a factor of 4, and a dropout layer with a drop probability of 0.5 is added after the global pooling layer.</p><p>The network is implemented with Apache MXNet and trained with the Adam optimizer with a minibatch size of 256. The network is trained for 30 epochs, with a starting learning rate of 0.01, and subsequently reduced by a factor of 10 at the 10th and 20th epochs. A snapshot of the model is saved at the end of each epoch, and the model snapshot showing the best accuracy on the validation dataset is selected for the final evaluation.</p><p>Appendix B: Implementation details of P-CNN The particle-level convolutional neural network (P-CNN) <ref type="bibr" target="#b42">[42]</ref> is a deep 1D CNN architecture customized for boosted jet tagging. Each input jet is represented as a sequence of particles with a fixed length of 100. The particles are organized in descending order of p T . The sequence is padded with zeros if a jet has less than 100 particles and truncated if it has more than 100 particles.</p><p>The P-CNN architecture is similar to the ResNet model <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b80">79]</ref> for image classification but uses 1D convolution instead. It features a total of 14 convolutional layers, all with a kernel size of 3. The number of channels for the 1D convolutions is either 32, 64, or 128. The convolutions are followed by a global pooling, then by a fully connected layer of 512 units with ReLU activation and a dropout layer with a drop rate of 0.5, before producing the classification output.</p><p>The network is implemented with Apache MXNet and trained with the Adam optimizer with a minibatch size of 1024. The network is trained for 30 epochs, with a starting learning rate of 0.001, and subsequently reduced by a factor of 10 at the 10th and 20th epochs. A snapshot of the model is saved at the end of each epoch, and the model snapshot showing the best accuracy on the validation dataset is selected for the final evaluation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quark and Gluon Tagging at the LHC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gallicchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.107.172001</idno>
		<idno type="arXiv">arXiv:1106.3076</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">172001</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quark and Gluon Jet Substructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gallicchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP04(2013)090</idno>
		<idno type="arXiv">arXiv:1211.7038</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">04</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaining (Mutual) Information about Quark/Gluon Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Larkoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Waalewijn</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP11(2014)129</idno>
		<idno type="arXiv">arXiv:1408.3122</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">129</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Associated jet and subjet rates in light-quark and gluon jet discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattacherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Nojiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Webber</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP04(2015)131</idno>
		<idno type="arXiv">arXiv:1501.04794</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">04</biblScope>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quark-Gluon tagging with Shower Deconstruction: Unearthing dark matter and Higgs couplings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferreira De Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spannowsky</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.95.034001</idno>
		<idno type="arXiv">arXiv:1607.06031</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">34001</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>hepph</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Systematics of quark/gluon tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>H?che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Larkoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>L?nnblad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pl?tzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Si?dmok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Skands</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soyez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP07(2017)091</idno>
		<idno type="arXiv">arXiv:1704.03878</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page">91</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Casimir Meets Poisson: Improved Quark/Gluon Discrimination with Counting Observables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Larkoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP09(2017)083</idno>
		<idno type="arXiv">arXiv:1704.06266</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">09</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Top Tagging: A Method for Identifying Boosted Hadronically Decaying Top Quarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rehermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tweedie</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.101.142001</idno>
		<idno type="arXiv">arXiv:0806.0848</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">142001</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">W-jet Tagging: Optimizing the Identification of Boosted Hadronically-Decaying W Bosons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.83.074023</idno>
		<idno type="arXiv">arXiv:1012.2077</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">74023</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How to Improve Top Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spannowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takeuchi</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.85.034029</idno>
		<idno type="arXiv">arXiv:1111.5034</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">34029</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding top quarks with shower deconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Soper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spannowsky</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.87.054012</idno>
		<idno type="arXiv">arXiv:1211.3140</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">54012</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Benchmarking an even better top tagger algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bernaciak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasieczka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schell</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.89.074047</idno>
		<idno type="arXiv">arXiv:1312.1504</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">74047</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Resonance Searches with an Updated Top Tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasieczka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strebler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Salam</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP06(2015)203</idno>
		<idno type="arXiv">arXiv:1503.05921</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page">203</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identifying Boosted Objects with N-subjettiness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Tilburg</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP03(2011)015</idno>
		<idno type="arXiv">arXiv:1011.2268</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">03</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximizing Boosted Top Identification by Minimizing N-subjettiness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Tilburg</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP02(2012)093</idno>
		<idno type="arXiv">arXiv:1108.2701</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Energy Correlation Functions for Jet Substructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Larkoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Salam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP06(2013)108</idno>
		<idno type="arXiv">arXiv:1305.0007</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page">108</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">New Angles on Energy Correlation Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Moult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Necib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP12(2016)153</idno>
		<idno type="arXiv">arXiv:1609.07483</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Soft Drop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Larkoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soyez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP05(2014)146</idno>
		<idno type="arXiv">arXiv:1402.2657</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">05</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Boosted objects: A Probe of beyond the Standard Model physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdesselam</surname></persName>
		</author>
		<idno type="DOI">10.1140/epjc/s10052-011-1661-y</idno>
		<idno type="arXiv">arXiv:1012.5412</idno>
	</analytic>
	<monogr>
		<title level="j">Eur. Phys. J</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">1661</biblScope>
			<date type="published" when="2010-06-22" />
		</imprint>
	</monogr>
	<note>Boost. hep-ph</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Jet Substructure at the Tevatron and LHC: New results, new tools, new benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Altheimer</surname></persName>
		</author>
		<idno type="DOI">10.1088/0954-3899/39/6/063001</idno>
		<idno type="arXiv">arXiv:1201.0008</idno>
	</analytic>
	<monogr>
		<title level="j">J. Phys</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">63001</biblScope>
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
	<note>BOOST. hep-ph</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boosted objects and jet substructure at the LHC. Report of BOOST2012, held at IFIC Valencia, 23rd-27th of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Altheimer</surname></persName>
		</author>
		<idno type="DOI">10.1140/epjc/s10052-014-2792-8</idno>
		<idno type="arXiv">arXiv:1311.2708</idno>
	</analytic>
	<monogr>
		<title level="m">BOOST 2012 Valencia</title>
		<meeting><address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">2792</biblScope>
		</imprint>
	</monogr>
	<note>hep-ex</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards an Understanding of the Correlations in Jet Substructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Adams</surname></persName>
		</author>
		<idno type="DOI">10.1140/epjc/s10052-015-3587-2</idno>
		<idno type="arXiv">arXiv:1504.00679</idno>
	</analytic>
	<monogr>
		<title level="j">Eur. Phys. J</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Jet Substructure at the Large Hadron Collider: A Review of Recent Advances in Theory and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Larkoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Moult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nachman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.physrep.2019.11.001</idno>
		<idno type="arXiv">arXiv:1709.04464</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rept</title>
		<imprint>
			<biblScope unit="volume">841</biblScope>
			<biblScope unit="page" from="1" to="63" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Jet Substructure at the Large Hadron Collider: Experimental Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kogler</surname></persName>
		</author>
		<idno type="DOI">10.1103/RevModPhys.91.045003</idno>
		<idno type="arXiv">arXiv:1803.06991</idno>
	</analytic>
	<monogr>
		<title level="j">Rev. Mod. Phys</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">45003</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>hep-ex</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Jet-Images: Computer Vision Inspired Techniques for Jet Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwarztman</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP02(2015)118</idno>
		<idno type="arXiv">arXiv:1407.5675</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Playing Tag with ANN: Boosted Top Identification with Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Backovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cliche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perelstein</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP07(2015)086</idno>
		<idno type="arXiv">arXiv:1501.05968</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Jet-images -deep learning edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwartzman</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP07(2016)069</idno>
		<idno type="arXiv">arXiv:1511.05190</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Jet Substructure Classification in High-Energy Physics with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whiteson</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.93.094034</idno>
		<idno type="arXiv">arXiv:1603.09349</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">94034</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>hep-ex</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parton Shower Uncertainties in Jet Substructure Analyses with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Dawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajcic</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.95.014018</idno>
		<idno type="arXiv">arXiv:1609.00607</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">14018</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Komiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Metodiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning in color: towards automated quark/gluon jet discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP01(2017)110</idno>
		<idno type="arXiv">arXiv:1612.01551</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<idno>ATL-PHYS-PUB-2017-017</idno>
		<title level="m">Quark versus Gluon Jet Tagging Using Jet Images with the ATLAS Detector</title>
		<meeting><address><addrLine>Geneva</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>CERN</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep-learning Top Taggers or The End of QCD?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasieczka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schell</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP05(2017)006</idno>
		<idno type="arXiv">arXiv:1701.08784</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">05</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pulling Out All the Tops with Computer Vision and Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Macaluso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shih</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP10(2018)121</idno>
		<idno type="arXiv">arXiv:1803.00107</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">121</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Infrared Safety of a Neural-Net Top Tagging Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perelstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01263</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Lund Jet Plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Salam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soyez</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP12(2018)064</idno>
		<idno type="arXiv">arXiv:1807.04758</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Jet Flavor Classification in High-Energy Physics with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whiteson</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.94.112002</idno>
		<idno type="arXiv">arXiv:1607.08633</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">112002</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>hep-ex</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Jet Constituents for Deep Neural Network Based Top Quark Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pearkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedorko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02124</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>hep-ex</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Long Short-Term Memory (LSTM) networks with jet constituents for boosted top tagging at the LHC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedorko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pearkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09059</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>hep-ex</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Jet Charge and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP10(2018)093</idno>
		<idno type="arXiv">arXiv:1803.08066</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>hepph</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">CMS Phase 1 heavy flavour identification performance and developments</title>
		<idno>CMS- DP-2017-013</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Performance of the DeepJet b tagging algorithm using 41.9/fb of data from proton-proton collisions at 13TeV with Phase 1 CMS detector</title>
		<idno>CMS-DP-2018-058</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Boosted jet identification using particle candidates and deep neural networks</title>
		<idno>CMS-DP-2017-049</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DeepJet: Generic physics object based jet multiclass classification for LHC experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kieseler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gouskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verzetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Physical Sciences Workshop at the 31st Conference on Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Machine learning-based identification of highly Lorentz-boosted hadronically decaying particles at the CMS experiment</title>
		<idno>CMS-PAS-JME- 18-002</idno>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Geneva</pubPlace>
		</imprint>
		<respStmt>
			<orgName>CERN</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Identification of Jets Containing b-Hadrons with Recurrent Neural Networks at the AT-LAS Experiment</title>
		<idno>ATL-PHYS-PUB-2017-003</idno>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Geneva</pubPlace>
		</imprint>
		<respStmt>
			<orgName>CERN</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep-learned Top Tagging with a Lorentz Layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasieczka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.21468/SciPostPhys.5.3.028</idno>
		<idno type="arXiv">arXiv:1707.08966</idno>
	</analytic>
	<monogr>
		<title level="j">Sci-Post Phys</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Quark-Gluon Tagging: Machine Learning vs Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasieczka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Thompson</surname></persName>
		</author>
		<idno type="DOI">10.21468/SciPostPhys.6.6.069</idno>
		<idno type="arXiv">arXiv:1812.09223</idno>
	</analytic>
	<monogr>
		<title level="j">SciPost Phys</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Erdmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rieger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09722</idno>
		<title level="m">Lorentz Boost Networks: Autonomous Physics-Inspired Feature Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>hepex</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">QCD-Aware Recursive Neural Networks for Jet Physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Becot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP01(2019)057</idno>
		<idno type="arXiv">arXiv:1702.00748</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recursive Neural Networks in Quark/Gluon Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/s41781-018-0007-y</idno>
		<idno type="arXiv">arXiv:1711.02633</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Softw. Big Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Jet Physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Henrion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rochette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Physical Sciences Workshop at the 31st Conference on Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Energy Flow Networks: Deep Sets for Particle Jets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Komiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Metodiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP01(2019)121</idno>
		<idno type="arXiv">arXiv:1810.05165</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page">121</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Classification without labels: Learning from mixed samples in high energy physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Metodiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP10(2017)174</idno>
		<idno type="arXiv">arXiv:1708.02949</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">174</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to classify from impure samples with high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Komiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Metodiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.98.011502</idno>
		<idno type="arXiv">arXiv:1801.10158</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">11502</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">JUNIPR: a Framework for Unsupervised Machine Learning in Particle Physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Feige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.1140/epjc/s10052-019-6607-9</idno>
		<idno type="arXiv">arXiv:1804.09720</idno>
	</analytic>
	<monogr>
		<title level="j">Eur. Phys. J</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An operational definition of quark and gluon jets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Komiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Metodiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
		<idno type="DOI">10.1007/JHEP11(2018)059</idno>
		<idno type="arXiv">arXiv:1809.01140</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The idea of regarding jets as unordered sets of particles was also proposed in Ref</title>
		<imprint/>
	</monogr>
	<note>52] independently while this work was being finalized. We provide comparison to their approach in later sections</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3326362</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Particle-flow reconstruction and global event description with the CMS detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Sirunyan</surname></persName>
			<affiliation>
				<orgName type="collaboration">CMS</orgName>
			</affiliation>
		</author>
		<idno type="DOI">10.1088/1748-0221/12/10/P10003</idno>
		<idno type="arXiv">arXiv:1706.04965</idno>
	</analytic>
	<monogr>
		<title level="j">JINST</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">10003</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>physics.ins-det</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Jet reconstruction and performance using particle flow with the ATLAS Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aaboud</surname></persName>
			<affiliation>
				<orgName type="collaboration">ATLAS</orgName>
			</affiliation>
		</author>
		<idno type="DOI">10.1140/epjc/s10052-017-5031-2</idno>
		<idno type="arXiv">arXiv:1703.10485</idno>
	</analytic>
	<monogr>
		<title level="j">Eur. Phys. J</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">466</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>hepex</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10590-1_53</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">EdgeConv is not designed as a universal approximator for any permutation-invariant functions. Specifically, the permutation invariance of the EdgeConv layer refers to the fact that the output does not depend on the ordering of the input points. However, to use EdgeConv, one needs to specify a set of features to be used as the &quot;coordinates&quot; for the computation of distances needed by the nearest neighbor finding. This choice of a &quot;coordinate system&quot; then leads to a canonical ordering of the points in that space where the neighbor relationship is fully determined. Since EdgeConv is performed with the k nearest neighbors for each point</title>
		<imprint/>
	</monogr>
	<note>Unlike other approaches in the literature (e.g, Deep Sets [63]). any permutation of the points that changes the neighbor relationship will actually lead to a change in the network output</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
	</analytic>
	<monogr>
		<title level="m">Workshop on Machine Learning Systems at the 29st Conference on Neural Information Processing Systems (NIPS) (Learn-ingSys.org</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasieczka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Russel</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.2603256</idno>
		<title level="m">Top quark tagging reference dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">An Introduction to PYTHIA 8.2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sj?strand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ilten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mrenna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prestel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Z</forename><surname>Skands</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cpc.2015.01.024</idno>
		<idno type="arXiv">arXiv:1410.3012</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Phys. Commun</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page" from="159" to="177" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Favereau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delaere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Demin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giammanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lema?tre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Selvaggi</surname></persName>
		</author>
		<imprint>
			<publisher>DELPHES</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">DELPHES 3, A modular framework for fast simulation of a generic collider experiment</title>
		<idno type="DOI">10.1007/JHEP02(2014)057</idno>
		<idno type="arXiv">arXiv:1307.6346</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>hep-ex</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The antikt jet clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cacciari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Salam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soyez</surname></persName>
		</author>
		<idno type="DOI">10.1088/1126-6708/2008/04/063</idno>
		<idno type="arXiv">arXiv:0802.1189</idno>
	</analytic>
	<monogr>
		<title level="j">JHEP</title>
		<imprint>
			<biblScope unit="volume">04</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">A comprehensive comparison between a wide range of machine learning approaches on this top tagging dataset is presented in Ref</title>
		<imprint/>
	</monogr>
	<note>where an earlier version of Par-ticleNet is also included</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.634</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The Machine Learning Landscape of Top Taggers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butter</surname></persName>
		</author>
		<idno type="DOI">10.21468/SciPostPhys.7.1.014</idno>
		<idno type="arXiv">arXiv:1902.09914</idno>
	</analytic>
	<monogr>
		<title level="j">SciPost Phys</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>hep-ph</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_38</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
