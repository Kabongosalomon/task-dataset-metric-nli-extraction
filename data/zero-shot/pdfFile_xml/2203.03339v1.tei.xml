<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">L2CS-NET: FINE-GRAINED GAZE ESTIMATION IN UNCONSTRAINED ENVIRONMENTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A</forename><surname>Abdelrahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Information Technology</orgName>
								<orgName type="institution">Neuro-Information Technology Group Otto-von-Guericke-University</orgName>
								<address>
									<settlement>Magdeburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Hempel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Information Technology</orgName>
								<orgName type="institution">Neuro-Information Technology Group Otto-von-Guericke-University</orgName>
								<address>
									<settlement>Magdeburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aly</forename><surname>Khalifa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Information Technology</orgName>
								<orgName type="institution">Neuro-Information Technology Group Otto-von-Guericke-University</orgName>
								<address>
									<settlement>Magdeburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayoub</forename><surname>Al-Hamadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Information Technology</orgName>
								<orgName type="institution">Neuro-Information Technology Group Otto-von-Guericke-University</orgName>
								<address>
									<settlement>Magdeburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">L2CS-NET: FINE-GRAINED GAZE ESTIMATION IN UNCONSTRAINED ENVIRONMENTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Appearance-based gaze estimation</term>
					<term>Gaze Analysis</term>
					<term>Gaze Tracking</term>
					<term>Convolutional Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human gaze is a crucial cue used in various applications such as human-robot interaction and virtual reality. Recently, convolution neural network (CNN) approaches have made notable progress in predicting gaze direction. However, estimating gaze in-the-wild is still a challenging problem due to the uniqueness of eye appearance, lightning conditions, and the diversity of head pose and gaze directions. In this paper, we propose a robust CNN-based model for predicting gaze in unconstrained settings. We propose to regress each gaze angle separately to improve the per-angel prediction accuracy, which will enhance the overall gaze performance. In addition, we use two identical losses, one for each angle, to improve network learning and increase its generalization. We evaluate our model with two popular datasets collected with unconstrained settings. Our proposed model achieves state-of-theart accuracy of 3.92 ? and 10.41 ? on MPIIGaze and Gaze360 datasets, respectively. We make our code open source at https://github.com/Ahmednull/L2CS-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Eye gaze is one of the essential cues used in a large variety of applications. It indicates the user's level of engagement in human-robot interaction <ref type="bibr">[1,</ref><ref type="bibr" target="#b0">2]</ref>, and open dialogue systems <ref type="bibr" target="#b1">[3]</ref>. Furthermore, it is used in augmented reality <ref type="bibr" target="#b2">[4]</ref> to predict the users' attention, which improves the device's awareness and reduces power consumption. Therefore, researchers developed multiple methods and techniques for accurately estimating the human gaze. These methods are divided into two categories: model-based and appearance-based approaches. Model-based methods generally require dedicated hardware that makes them difficult to use in an unconstrained environment. On the other hand, appearance-based methods regress the human gaze directly from the images captured by inexpensive off-the-shelf cameras, making them easy to generate in different locations with unconstrained settings. Recently, CNN-based appearance-based methods are the most commonly used gaze estimation methods as they provide better gaze performance <ref type="bibr" target="#b3">[5]</ref><ref type="bibr" target="#b4">[6]</ref><ref type="bibr" target="#b5">[7]</ref><ref type="bibr" target="#b6">[8]</ref>. Most of the related work <ref type="bibr">[5-7, 9, 10]</ref> focussed on developing novel CNN-based networks which mainly consist of popular backbones (e.g. VGG <ref type="bibr" target="#b9">[11]</ref>, ResNet-18 <ref type="bibr" target="#b7">[9]</ref>, ResNet-50 <ref type="bibr" target="#b10">[12]</ref>) to extract gaze features and finally outputs the gaze direction. The input to these networks can be a single stream <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b10">12]</ref>(e.g., face or eye images) or multiple streams <ref type="bibr" target="#b3">[5]</ref> (e.g., face and eye images). The most common loss function used for the gaze estimation task is the mean square loss or 2 loss. However, Petr et al. <ref type="bibr" target="#b7">[9]</ref> proposed a novel pinball loss that estimates the gaze direction and error bounds together, which improves the accuracy of gaze estimation, especially in unconstrained settings. Although CNNbased methods achieve improved gaze accuracy, they lack robustness and generalization, especially in unconstrained environments.</p><p>In this paper, we introduce a new method to estimate 3D gaze angles from RGB images using a multi-loss approach. We propose to regress each gaze angle (yaw, pitch) independently using two fully-connected layers to enhance the prediction accuracy of each angle. Furthermore, we use two separate loss functions for each gaze angle. Each loss consists of gaze binary classification and regression components. Finally, the two losses are backpropagated through the network, which accurately fine-tunes the network weights and increases network generalization. We perform gaze bin classification by utilizing a softmax layer along with crossentropy loss so that the network estimates the neighborhood of the gaze angle in a robust manner. Based on the proposed loss function and the softmax layer ( 2 loss+ cross-entropy loss+ softmax layer), we present a new network (L2CS-Net) to predict 3D gaze vector in unconstrained settings. Finally, we evaluate the robustness of our network on two popular datasets, MPIIGaze <ref type="bibr" target="#b8">[10]</ref> and Gaze360 <ref type="bibr" target="#b7">[9]</ref>. The proposed L2CS-Net achieved state-of-the-art performance on MPI-IGaze and Gaze360 datasets.</p><p>Conventional gaze estimation methods use a regression function to create a person-specific mapping function to the human gaze, e.g., adaptive linear regression <ref type="bibr" target="#b11">[13]</ref> and gaussian process regression <ref type="bibr" target="#b12">[14]</ref>. These methods show reasonable accuracy in constrained setup (e.g., subject-specific and fixed head pose and illumination), however they significantly decrease when tested on unconstrained settings.</p><p>Recently, researchers have gained more interest in CNNbased gaze estimation methods, as they can model a highly nonlinear mapping function between images and gaze. Zhang et al. <ref type="bibr" target="#b8">[10]</ref> first proposed a simple VGG CNN-based architecture to predict gaze using a single eye image. Also, they designed a spatial weights CNN in <ref type="bibr" target="#b14">[15]</ref> to give more weight to those regions of the face that related to the gaze in appearance. Krafka et al. <ref type="bibr" target="#b15">[16]</ref> proposed a multichannel network that takes eye images, full-face images, and face grid information as inputs.</p><p>Combining statistical models with deep learning is a good solution for gaze estimation as in <ref type="bibr" target="#b16">[17]</ref>, which they introduced a mixed effect model that integrates information from statistics within CNN architecture based on eye images. Chen et al. <ref type="bibr" target="#b5">[7]</ref> adopted dilated convolutions to make use of the high-level features extracted from images without decreasing spatial resolution. In addition, they expanded their work by proposing GEDDNet <ref type="bibr" target="#b17">[18]</ref>, which utilizes both gaze decomposition with dilated convolutions and reported better performance than using dilated convolutions only. Fischer et al. <ref type="bibr" target="#b9">[11]</ref> append the head pose vector along with features extracted using a VGG CNN with eye crops to predict gaze angels. Additionally, they used an ensemble scheme to improve gaze accuracy.</p><p>Motivated by the two-eye asymmetry property, Cheng et al. <ref type="bibr" target="#b18">[19]</ref> proposed FAR-Net that estimates 3D gaze angels for both eyes with an asymmetric approach. They give asymmetric weights to each loss of the two eyes and finally sum these losses. The proposed model presented a top performance in multiple public datasets. Wang et al. <ref type="bibr" target="#b19">[20]</ref> integrated adversarial learning with the Bayesian approach in one framework, which demonstrates an increased gaze generalization performance. Cheng et al. <ref type="bibr" target="#b4">[6]</ref> proposed a coarse-to-fine adaptive network (CA-Net) that first uses face image to predict primary gaze angels and adapt it with the residual estimated from eye crops. Then, they proposed a bi-gram model to bridge the primary gaze with the eye residual. Kellnhofer et al. <ref type="bibr" target="#b7">[9]</ref> used a temporal model (LSTM) with a sequence of 7 frames to predict gaze angels. In addition, they adopt pinball loss to jointly regress the gaze direction and error bounds together to improve gaze accuracy.</p><p>The most recent work with gaze estimation is AGE-Net <ref type="bibr" target="#b3">[5]</ref> which they propose two parallel networks for each eye image, one is used to generate a feature vector using CNN, and the other is used to generate a weight feature vector using an attention-based network. The output of the two parallel networks is multiplied and then refined with the output of VGG CNN of the face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proposed loss function</head><p>Most CNN-based gaze estimation models predict 3D gaze as the gaze direction angles (yaw, pitch) in spherical coordinates. Further, they adopt the mean-squared error ( 2 loss) for penalizing their networks. We propose to use two identical losses for each gaze angle. Each loss contains a combined crossentropy loss and mean-squared error. Instead of directly predicting continuous gaze angels, we used a softmax layer with cross-entropy to predict binned gaze classification. Then, we estimate the expectation of the gaze binned output to finegrain the predictions. Finally, we add a mean-squared error to the output to improve the gaze predictions. Using 2 together with Softmax can tune the nonlinear softmax layer with immense flexibility.</p><p>The cross entropy loss is defined as:</p><formula xml:id="formula_0">H(y, p) = ? i y i log p i</formula><p>And the mean-squared error is defined as:</p><formula xml:id="formula_1">M SE(y, p) = 1 N N 0 (y ? p) 2</formula><p>Our proposed loss for each gaze angle is a linear combination of the mean-squared error and cross-entropy losses, which is defined as:</p><formula xml:id="formula_2">CLS(y, p) = H(y, p) + ? ? M SE(y, p)</formula><p>Where CLS is the combined loss, p is the predicted values, y is the ground-truth values and ? is the regression coefficient. We change the weight of the mean-squared loss during the experiments in Section 4 to obtain the best gaze performance.</p><p>To the best of our knowledge, all related works which estimated gaze using CNN-based methods do not consider the combined classification and regression loss in their techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">L2CS-Net Architecture</head><p>We propose a simple network architecture (L2CS-Net) based upon the proposed classification and regression losses. It takes face images as input and feeds them to ResNet-50 as a backbone to extract spatial gaze features from images. In contrast to most previous work that regresses the two gaze angles together in one fully-connected layer, we predict each angle separately using two fully-connected layers. These two fully-connected layers share the same convolution layers in the backbone. Also, we use two loss functions, one for each gaze angle (yaw, pitch). Using this approach will improve network learning, as it has two signals that backpropagate through the network.</p><p>For each output from the fully-connected layer, we first use a softmax layer to convert the network output logits into a probability distribution. Then, a cross-entropy loss is applied to calculate the bin classification loss between output probabilities and target bin labels. Next, we calculate the expectation of the probability distribution to get fine-grained gaze predictions. Finally, we calculate the mean square error for this prediction and add it to the classification loss. The detailed architecture of L2CS-Net is shown in <ref type="figure" target="#fig_1">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Datasets</head><p>With the development of appearance-based gaze estimation methods, large-scale datasets have been proposed to improve gaze performance. These datasets were collected with different procedures, varying from laboratory-constrained settings to unconstrained indoor and outdoor environments. In order to get a valuable evaluation of our network, we train and evaluate our model using two popular datasets collected with unconstrained settings: Gaze360 and MPIIGaze.</p><p>Gaze360 <ref type="bibr" target="#b7">[9]</ref> provides the widest range of 3D gaze annotations with a range of 360 degrees. It contains 238 subjects of different ages, genders, and ethnicity. Its images are captured using a Ladybug multi-camera system in different indoor and outdoor environmental settings like lighting conditions and backgrounds.</p><p>MPIIGaze <ref type="bibr" target="#b14">[15]</ref> provides 213.659 images from 15 subjects captured during their daily routine over several months. Consequently, it contains images with diverse backgrounds, time, and lighting that make it suitable for unconstrained gaze estimation. It was collected using software that asks the participants to look at randomly moving dots on their laptops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data preprocessing</head><p>We follow the same procedures in <ref type="bibr" target="#b14">[15]</ref> to normalize images in the two datasets. In summary, this process applies rotation and translation to the virtual camera to remove the head's roll angle and keep the same distance between the virtual camera and a reference point (the center of the face). Furthermore, we split up the continuous gaze target in each dataset (pitch and yaw angles) into bins with binary labels for classification based on the range of the gaze annotations. As a result, both datasets have two different target annotations: continuous and binned labels make them suitable for our combined regression and classification losses. Furthermore, we change the regression coefficient in the combined loss function during the experiments to obtain the best gaze performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training and results</head><p>We use an ImageNet-pretrained ResNet-50 as the backbone network. Our proposed network (L2CS-Net) was trained in PyTorch framework using Adam optimizer with a learning rate of 0.00001. We train our proposed network for 50 epochs using a batch size of 16. We evaluate our proposed network on MPIIGaze and Gaze360 datasets. We change the regression coefficient during the experiments and compare the output performance with the state-of-the-art gaze estimation methods. We utilize gaze angular error ( ? ) as the evaluation metric following most gaze estimation methods. Assuming the ground-truth gaze direction is g R 3 and the predicted gaze vector is? R 3 , the gaze angular error ( ? ) can be computed as:</p><formula xml:id="formula_3">L angular = g ?? g ?</formula><p>Methods MPIIFaceGaze iTracker (AlexNet) <ref type="bibr" target="#b15">[16]</ref> 5.6 ? MeNets <ref type="bibr" target="#b16">[17]</ref> 4.9 ? FullFace (Spatial weights CNN) <ref type="bibr" target="#b14">[15]</ref> 4.8 ? Dilated-Net <ref type="bibr" target="#b5">[7]</ref> 4.8 ? RT-Gene(1 model) <ref type="bibr" target="#b9">[11]</ref> 4.8 ? GEDDNet <ref type="bibr" target="#b17">[18]</ref> 4.5 ? RT-Gene(4 ensemble) <ref type="bibr" target="#b9">[11]</ref> 4.3 ? Bayesian Approach <ref type="bibr" target="#b19">[20]</ref> 4.3 ? FAR-Net <ref type="bibr" target="#b18">[19]</ref> 4.3 ? CA-Net <ref type="bibr" target="#b4">[6]</ref> 4.1 ? AGE-Net <ref type="bibr" target="#b3">[5]</ref> 4.09  <ref type="table">Table 2</ref>: Comparison of mean angular error between our proposed model and SOTA methods on Gaze360 dataset</p><formula xml:id="formula_4">? L2CS-Net (? = 1) L2CS-Net (? = 2) 3.96 ? 3.92 ?</formula><p>We adapt leave-one-subject-out cross-validation on MPI-IGaze dataset as used in the related works <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b14">15]</ref>. In order to obtain the best performance, we train L2CS-Net on the MPIIGaze dataset with different regression coefficients (?) of 1 and 2. <ref type="table">Table.</ref> 1 shows the comparison of mean angular error between our proposed model and state-of-the-art methods on MPIIGaze dataset. Our proposed L2CS-Net (? = 1) achieved state-of-the-art gaze performance with 3.92 ? mean angular error. Furthermore, we present the gaze accuracy of each subject of the MPIIGaze dataset and compare it with FARE-Net <ref type="bibr" target="#b18">[19]</ref> as they presented the subject-wise gaze accuracy. Out of 15 subjects, our proposed method achieves better gaze accuracy for 11 subjects, as shown in <ref type="figure" target="#fig_3">Fig 2.</ref> Kellnhofer et al. <ref type="bibr" target="#b7">[9]</ref> divided the Gaze360 dataset into train-val-test sets and presented three evaluation scopes based on the range of gaze angles: all 360 ? , front 180 ? , and frontfacing (within 20 ? ). We follow the same evaluation criteria in <ref type="bibr" target="#b7">[9]</ref>, but only with the front 180 ? and front-facing for a fair comparison with all related methods that are trained and evaluated on datasets within 180 ? range. We trained L2CS-Net on Gaze360 dataset with different regression coefficients of 1 and 2. <ref type="table">Table. 2</ref>  between our proposed model and State-of-the-art methods on Gaze360 dataset. We used the results from <ref type="bibr" target="#b20">[21]</ref> as they implement typical gaze estimation methods on the Gaze360 dataset. Our proposed L2CS-Net (? = 1) achieves state-ofthe-art gaze performance with 10.41 ? mean angular error on front 180 ? and 9.02 ? on front facing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a robust CNN-based model (L2CS-Net) for predicting 3D gaze directions in unconstrained environments. We propose to predict each gaze angle individually with two fully-connected layers and a ResNet-50 backbone. In order to improve the network learning, we used two separate loss functions for each gaze angle, each of them is a linear combination of regression and classification losses. Further, we use a reliable softmax layer to predict gaze bins. Furthermore, we changed the regression coefficient during the experiments to obtain the best gaze performance. To show the robustness of our model, we validate our network using two of the most unconstrained gaze datasets: MPIIGaze and Gaze360, and we followed the same evaluation criteria used in each dataset. Our model achieved state-of-the-art gaze accuracy with the lowest angular error in both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">References</head><p>[1] Thorsten Hempel and Ayoub Al-Hamadi, "Slam-based multistate tracking system for mobile human-robot interaction," in International Conference on Image Analysis and Recognition. Springer, 2020, pp. 368-376.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This research was funded by the Federal Ministry of Education and Research of Germany (BMBF) RoboAssist no. 03ZZ0448L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>L2CS-Net with combined classification and regression losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>shows the comparison of mean angular error p Comparison of subject gaze accuracy between our proposed model and FARE-Net [19] on MPIIGaze dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of mean angular error between our proposed model and SOTA methods on MPIIGaze dataset</figDesc><table><row><cell>Methods</cell><cell cols="2">Front 180 ? Front Facing</cell></row><row><cell>FullFace</cell><cell>14.99 ?</cell><cell>N/A</cell></row><row><cell>Dilated-Net</cell><cell>13.73 ?</cell><cell>N/A</cell></row><row><cell>RT-Gene (4 ensemble)</cell><cell>12.26 ?</cell><cell>N/A</cell></row><row><cell>CA-Net</cell><cell>12.26 ?</cell><cell>N/A</cell></row><row><cell>Gaze360 (LSTM) [9]</cell><cell>11.4 ?</cell><cell>11.1 ?</cell></row><row><cell>L2CS-Net (? = 2)</cell><cell>10.54 ?</cell><cell>9.13 ?</cell></row><row><cell>L2CS-Net (? = 1)</cell><cell>10.41 ?</cell><cell>9.02 ?</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Related WorkAccording to the literature, appearance-based gaze estimation can be divided into conventional and CNN-based methods. ?2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robot system assistant (rosa): Towards intuitive multi-modal and multi-device human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominykas</forename><surname>Strazdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aly</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Ahmed A Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayoub</forename><surname>Hempel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Hamadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">923</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision-based attention estimation and selection for social robot to perform natural interaction in the open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinguo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeow</forename><surname>Kee Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="183" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards foveated rendering for gazetracked virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjul</forename><surname>Patney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Kaplanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Wyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Benty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luebke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Lefohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation using attention and difference mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradipta</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A coarse-to-fine adaptive network for appearance-based gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10623" to="10630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation using dilated-convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="309" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gaze estimation with multi-scale channel and spatial attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 9th International Conference on Computing and Pattern Recognition</title>
		<meeting>the 2020 9th International Conference on Computing and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="303" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gaze360: Physically unconstrained gaze estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6912" to="6921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mpiigaze: Real-world dataset and deep appearance-based gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="162" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rt-gene: Real-time eye gaze estimation in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demiris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eth-xgaze: A large scale dataset for gaze estimation under extreme head pose and gaze variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive linear regression for appearance-based gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2033" to="2046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse and semi-supervised visual mapping with the s3 gp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="230" to="237" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">It&apos;s written all over your face: Fullface appearance-based gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2299" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eye tracking for everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Krafka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harini</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchendra</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2176" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mixed effects neural networks (menets) with applications to gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyunwoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7743" to="7752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Geddnet: A network for gaze estimation with dilation and decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09284</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gaze estimation by exploring two-eye asymmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5259" to="5272" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalizing eye tracking with bayesian adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11907" to="11916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Appearance-based gaze estimation with deep learning: A review and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12668</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
