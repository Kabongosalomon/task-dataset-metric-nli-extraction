<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyao</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SenseTime</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
							<email>daijifeng@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>qiaoyu@pjlab.org.cnzhuwalter</email>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Long-tailed Recognition, Vision-Language Models</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, computer vision foundation models such as CLIP and ALI-GN, have shown impressive generalization capabilities on various downstream tasks. But their abilities to deal with the long-tailed data still remain to be proved. In this work, we present a novel framework based on pre-trained visual-linguistic models for long-tailed recognition (LTR), termed VL-LTR, and conduct empirical studies on the benefits of introducing text modality for long-tailed recognition tasks. Compared to existing approaches, the proposed VL-LTR has the following merits. (1) Our method can not only learn visual representation from images but also learn corresponding linguistic representation from noisy classlevel text descriptions collected from the Internet; (2) Our method can effectively use the learned visual-linguistic representation to improve the visual recognition performance, especially for classes with fewer image samples. We also conduct extensive experiments and set the new state-of-the-art performance on widelyused LTR benchmarks. Notably, our method achieves 77.2% overall accuracy on ImageNet-LT, which significantly outperforms the previous best method by over 17 points, and is close to the prevailing performance training on the full ImageNet. Code is available at https://github.com/ChangyaoTian/ VL-LTR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Real-world data always presents a long-tailed distribution, where only a few head classes encompass most of the data, and most tail classes have very few samples. Such phenomenon is not conducive to the practical application of deep-learning based models. Because of this, a number of works have emerged and tried to alleviate the class imbalance problem from different aspects, such as re-sampling the training data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41]</ref>, reweighting the loss functions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28]</ref>, or employing transfer learning methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b28">29]</ref> (see <ref type="figure" target="#fig_0">Figure 1</ref> (a)). Despite their great contributions, most of these works still restrict themselves to only relying on the image modality for solving this problem.  <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54]</ref> mainly focus on the class imbalance problem on the image modality, while (b) our method addresses the LTR task by combining the advantages of image and text modalities. (c) and (d) give intuitive explanations for the correlations and differences between the image and text modalities.</p><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (c)(d), there are some inner connections between images and text descriptions of the same class, especially when it comes to some visual concepts and attributes. However, different from the image modality that usually presents concrete low-level features (e.g., shape, color, texture) of the object or scene, the text modality typically contains much high-level and abstract information. Furthermore, text descriptions are prior knowledge that can be summarized by experts, which could be useful when there are no sufficient images to learn general class-wise representation for recognition.</p><p>Although there have been some visual-linguistic approachs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b35">36]</ref> for visual recognition, their performance is still not satisfactory, due to the gap between image and text representation and the lack of robustness to noisy text. Recently, the rise of visuallinguistic foundation models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b18">19]</ref> has provided an effective way to learn powerful representation that can connect the image and text modalities. Motivated by this, we present a visual-linguistic framework for long-tailed recognition, termed VL-LTR, which can utilize the advantages of both visual and linguistic representation for visual recognition tasks as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b). Our method mainly consists of two key components, which are (1) a class-wise visual-linguistic pre-training (CVLP) framework for linking images and text descriptions at the class level, and (2) a language-guided recognition (LGR) head designed to perform long-tailed recognition according to the learned visual-linguistic representation.</p><p>Overall, the proposed VL-LTR possesses the following merits. (1) Compared to the visual-linguistic pre-training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b20">21]</ref>, our method can learn visual-linguistic representation at the class level, and take the advantages of class-wise linguistic representation to improve visual recognition performance, especially in the long-tailed scenario;</p><p>(2) Compared to previous visual-linguistic classifiers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b35">36]</ref>, our method can not only effectively bridge the gap between visual and linguistic representation, but also be more flexible and robust to noisy text descriptions.</p><p>To verify the effectiveness of our method, we conduct extensive experiments on three challenging long-tailed recognition (LTR) benchmarks, including ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>, Places-LT <ref type="bibr" target="#b28">[29]</ref>, and iNaturalist 2018 <ref type="bibr" target="#b45">[46]</ref>. As shown in <ref type="figure">Figure 2</ref>, using ResNet-50 <ref type="bibr" target="#b13">[14]</ref> as backbone, our method achieves an overall accuracy of 70.1% on ImageNet-LT, which is 10.1 points higher than the previous best method PaCo <ref type="bibr" target="#b6">[7]</ref> (ResNeXt-101 <ref type="bibr" target="#b49">[50]</ref>). For tail classes, the medium and few-shot accuracy of our method reaches 67.0% and 50.8% respectively, which significantly outperform that of the prior arts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b47">48]</ref>   <ref type="bibr" target="#b28">[29]</ref>. Our VL-LTR (ResNet-50 <ref type="bibr" target="#b13">[14]</ref>) significantly outperforms prior arts, including PaCo <ref type="bibr" target="#b6">[7]</ref>, TADE <ref type="bibr" target="#b52">[53]</ref>, RIDE (4 Experts) <ref type="bibr" target="#b47">[48]</ref>, and ResLT <ref type="bibr" target="#b5">[6]</ref>, which use heavier ResNeXt-50/101 <ref type="bibr" target="#b49">[50]</ref> as backbone.</p><p>In summary, our main contributions are three-fold.</p><p>(1) We provide a detailed analysis on the connection and differences between image and text modalities, and point out that class descriptions can serve as a supplement to images, which is conducive to long-tailed visual recognition.</p><p>(2) We present a new visuallinguistic framework for longtailed visual recognition (VL-LTR), which contains two tailored components, including a class-wise text-image pre-training (CVLP) to bridge the class-level images and text descriptions, and a language-guided recognition (LGR) head to perform classification based on the learned visual-linguistic representation.</p><p>(3) The proposed VL-LTR has achieved state-of-the-art performance on prevailing ImageNet-LT, Places-LT, and iNaturalist 2018 datasets. Notably, our method gets the best overall accuracy of 77.2% on ImageNet-LT, outperforming the old record by 17.2 points, and even approaching the performance training on the full ImageNet <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Long-Tailed Visual Recognition</head><p>Class re-balanced strategy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b17">18]</ref> has been comprehensively studied for long-tailed visual recognition. One type of the class re-balanced strategy is Data Resampling <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41]</ref>, which generates class-balanced data by adjusting the sampling rate of tail classes and head classes, yet they might take the risk of over-fitting on data-scarced classes. Besides that, some recent methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> augment tail class samples with head classes ones, to alleviate the over-fitting problem. Another kind of class re-balanced strategy is to design re-weighting loss functions, where tail classes would be emphasized by using large weights or margins <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b1">2]</ref>, or ignoring negative gradients for tail classes <ref type="bibr" target="#b43">[44]</ref>.</p><p>In addition, researchers also address the long-tailed recognition task from the aspect of transfer learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b55">56]</ref>. Liu et al. <ref type="bibr" target="#b28">[29]</ref> and Zhu et al. <ref type="bibr" target="#b56">[57]</ref> transfer knowledge from head classes's features to tail classes by maintaining memory bank and modeling intra-class variance, respectively. After that, Samuel et al. <ref type="bibr" target="#b38">[39]</ref> proposes a late-fusion framework for long-tail learning with class descriptors. Some decoupling methods <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b20">21]</ref> also can be regarded as transferring head classes frozen feature to tail classes when fine-tuning classifiers. Recently, some studies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53]</ref> also transfer the representation learned by contrastive learning or self-supervised learning for long-tailed problems.</p><p>The aforementioned methods mainly focus on addressing the class imbalance problem based on image modalities, while rarely exploring the possibility of integrating text modalities on this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual-linguistic Model</head><p>In this section, we mainly discuss visual-linguistic pre-training and classification related to our work.</p><p>Visual-linguistic pre-training <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref> have achieved great success on a number of downstream vision tasks. Zhang et al. <ref type="bibr" target="#b51">[52]</ref> show the importance of visual features in visual-linguistic pre-training and obtain more strong visual representations from large object detectors. Li et al. <ref type="bibr" target="#b26">[27]</ref> find that a larger transformer visual-linguistic model can learn more powerful representation from a larger visuallinguistic corpus. In addition, Huang et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref> proposed a visual-linguistic pretraining model by extracting patch features from the convolutional layers without the proposal computation. Recently, CLIP <ref type="bibr" target="#b36">[37]</ref> and ALIGN <ref type="bibr" target="#b18">[19]</ref> learns powerful visuallinguistic representation via contrastive learning on large-scale image-text pairs.</p><p>Prior to these works, there have been some visual-linguistic approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b35">36]</ref> designed for tasks related to image classification. He et al. <ref type="bibr" target="#b14">[15]</ref> propose a two-stream model, which directly combines visual and linguistic representation for fine-grained image classification. Mu et al. <ref type="bibr" target="#b35">[36]</ref> present a few-shot visual recognition model that is regularized with text descriptions during training. Similar to He et al. <ref type="bibr" target="#b14">[15]</ref>, Zhuang et al. <ref type="bibr" target="#b57">[58]</ref> design a multi-modal model for automatic fish classification, with a CNN encoder for images and a RNN encoder for class text. However, these methods (1) cannot effectively model the connection between images and text, leading to a considerable gap between visual and linguistic representation; and (2) require high-quality text annotations, which is usually expensive and thus limits their practical application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>In order to make effective use of the linguistic modality in the visual recognition task, we propose a two-stage framework, as depicted in <ref type="figure">Figure 3</ref>. (1) The first stage is classwise visual-linguistic pre-training (CVLP), which is used to link the images and text descriptions of the same class via contrastive learning. (2) In the second stage, a languageguided recognition (LGR) head is designed to collect the overall linguistic representation of each class to guide the image recognition. As a result, the proposed VL-LTR  <ref type="figure">Fig. 3</ref>: Overall architecture of VL-LTR. The entire model has two stages. In the first stage, class-wise visual-linguistic pre-training (CVLP) takes both the images and text of each class as inputs, learning to connect the representation of the two modalities through class-wise contrastive learning. In the second stage, the language-guided recognition (LGR) head uses the learned visual-linguistic representation to perform image classification.</p><p>is able to combine the advantages of visual and linguistic representation and achieve impressive long-tailed recognition performance. When training VL-LTR models, we first pre-train the visual and linguistic encoders by class-wise visual-linguistic contrastive learning and the pre-training loss L pre . During pre-training, an image and a sentence from the same class would be regarded as a positive pair, and otherwise is a negative pair. After pre-training, the weights of the linguistic encoder are frozen, and the anchor sentences of each class are then selected by filtering out the low-scored sentences in text descriptions. The visual encoder and LGR head are fine-tuned by the recognition loss L rec . Details of the aforementioned loss functions will be introduced in the later sections.</p><p>In the inference phase, given a query image and pre-populated text embeddings of anchor sentences, we first feed the image to the visual encoder and obtain an image embedding. Then, the image embedding passes through the LGR head and is categorized into a class according to the image embedding itself as well as the text embeddings of anchor sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Class-wise Visual-Linguistic Pre-training</head><p>The goal of this stage is to learn the visual-linguistic representation of images and text descriptions at the class level. To this end, we design a class-wise visual-linguistic pre-training (CVLP) framework. Unlike previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19]</ref> that use instance-wise image-text pairs for pre-training, our framework is expected to fuse the class-wise linguistic information into the visual space.</p><p>During pre-training, as shown in <ref type="figure">Figure 3</ref>, we first randomly sample a batch of</p><formula xml:id="formula_0">images I = {I i } N i=1 , and the corresponding text sentences T = {T i } N i=1</formula><p>, where N denotes the batch size. Then, the images I and texts T are fed to the visual encoder E vis (?) and linguistic encoder E lin (?) respectively, yielding image and text embeddings as Eqn. 1:</p><formula xml:id="formula_1">E I i = E vis (I i ), E T i = E lin (T i ),<label>(1)</label></formula><p>where both E I i and E T i are of D dimensions. After that, a class-wise contrastive learning (CCL) loss is used to optimize the visual and linguistic encoders. Let us denote the cosine similarity of E I i and E T j as S i,j , and then the CCL loss can be formulated as:</p><formula xml:id="formula_2">L ccl =L vis + L lin = ? 1 |T + i | Tj ?T + i log exp(S i,j /? ) T k ?T exp(S i,k /? ) ? 1 |I + i | Ij ?I + i log exp(S j,i /? ) I k ?I exp(S k,i /? ) ,<label>(2)</label></formula><p>where L vis and L lin denote the loss of visual and linguistic side respectively, while T + i denotes a subset of T , in which each text shares the same class with the image I i . Correspondingly, all images in I + i share the same class with the text T i . ? is a learnable parameter with an initial value of 0.07.</p><p>In addition to CCL, we also distill the knowledge from the CLIP <ref type="bibr" target="#b36">[37]</ref> pre-trained model, to reduce the risk of over-fitting caused by limited text corpus in the pre-training stage. The distillation loss L dis can be written as Eqn. 3:</p><formula xml:id="formula_3">L dis = ? exp(S ? i,i /? ) Tj ?T exp(S ? i,j /? ) log exp(S i,i /? ) T k ?T exp(S i,k /? ) ? exp(S ? i,i /? ) Ij ?I exp(S ? j,i /? ) log exp(S i,i /? ) I k ?I exp(S k,i /? ) .<label>(3)</label></formula><p>Here, S ? is the cosine similarity matrix produced by the frozen CLIP model. Our pre-training framework has two merits as follows: (1) It is convenient to add new training samples for image or text modality in our framework, since the image and text description for a specific class is independent of each other, which greatly reduces the cost of data collection; (2) The text description of each image sample is different in each iteration, which serves as an additional regularization to prevent the model from learning some fixed trivial correlation within a certain image-text pair, and thus our framework is robust to the noisy text from the Internet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Language-Guided Recognition</head><p>In this stage, we design (1) an anchor sentence selection strategy to filter out noise texts, and (2) a language-guided recognition head to effectively use visual and linguistic representation learned in the pre-training stage. Anchor Sentence Selection. Most text descriptions in our corpus are crawled from the Internet, which are noisy and might degrade the recognition performance. To address this problem, we propose an anchor sentence selection (AnSS) strategy to find the most discriminative sentences for each class. Specifically, we first construct a "special" image batch I ? , which contains at most 50 images (if any) of each class. Then, for each text sentence T i , we score each sentence T i by computing the L lin between the sentence and the image batch I ? . Finally, we select M text sentences with the smallest L lin as the anchor sentences for the follow-up visual recognition. Language-Guided Recognition Head. After obtaining the anchor sentences of each class, we design a language-guided recognition (LGR) head, to adjust the weights of these sentences based on the attention scores with the input image. In this way, visual and linguistic features can be flexibly and dynamically combined according to the query image.</p><p>As shown in <ref type="figure">Figure 3</ref>, given an image embedding E I ? R D , as well as the embeddings of all classes' anchor sentences E T ? R C?M ?D , where C is the class number, and M is the maximum number of sentences for each class. Then the LGR head can be formulated as:</p><formula xml:id="formula_4">Q = Linear(LayerNorm(E I )),<label>(4)</label></formula><formula xml:id="formula_5">K = Linear(LayerNorm(E T )), V = E T ,<label>(5)</label></formula><formula xml:id="formula_6">G = ?( QK T ? D )V,<label>(6)</label></formula><formula xml:id="formula_7">P = P I + P T = ?(MLP(E I )) + ?( E I , G /? ).<label>(7)</label></formula><p>Here, Q ? R D , K ? R C?M ?D , and V ? R C?M ?D are query, key and value of the attention operation. G ? R C?D is the gather of the M anchor sentence embeddings of each class. ?(?) denotes Softmax function. MLP(?) denotes two linear layers sandwich a ReLU in the middle. E I , G is the cosine similarity of E I and G. P is the classification probability of the image I q , P I and P T are the classification probabilities based on visual and linguistic representation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>As mentioned in Section 3.1, the training process of our method has two stages, namely pre-training and fine-tuning respectively. In the pre-training stage, the visual encoder and linguistic encoder are jointly optimized by the CCL loss L ccl and distillation loss L dis . So the overall pre-training loss can be written as:</p><formula xml:id="formula_8">L pre = ?L ccl + (1 ? ?)L dis ,<label>(8)</label></formula><p>where ? ? [0, 1] is a hyperparameter to balance L ccl and L dis .</p><p>In the fine-tuning stage, after computing the classification probabilities P I and P T , we simply calculate their corresponding CrossEntropy loss L CE with the ground truth label y as Eqn. 9:</p><formula xml:id="formula_9">L rec = L CE (P I , y) + L CE (P T , y).<label>(9)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We perform extensive experiments on three challenging long-tailed visual recognition benchmarks, namely ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>, Places-LT <ref type="bibr" target="#b28">[29]</ref>, and iNaturalist 2018 <ref type="bibr" target="#b45">[46]</ref>. Among these benchmarks, ImageNet-LT is constructed from ImageNet-2012 <ref type="bibr" target="#b8">[9]</ref> by sampling a subset following the Pareto distribution with the power value ? = 6, which contains 1,000 classes. The training set has 115.8K images, and the number of images per class ranges from 1,280 to 5 images. Both the validation set and the test set are balanced, containing 20K and 50K images respectively. We select the hyperparameters on the validation set and report numerical results on the test set. Similar to ImageNet-LT, Places-LT is a long-tailed version of the large-scale scene classification dataset Places <ref type="bibr" target="#b54">[55]</ref>. It consists of 62.5K images from 365 categories with class cardinality ranging from 5 to 4,980. iNaturalist 2018 is a real-world, naturally long-tailed dataset, which is composed of 8,142 fine-grained species. The training set contains 437.5K images, and its imbalance factor is equal to 500. We use the official validation set to test our approach, which has 3 images per class. We also collect the class-level text descriptions for the three datasets. The text descriptions mainly come from Wikipedia 1 , an open-source online encyclopedia that contains millions of articles for free. We first use the original class name as an initial query to get the best matching entry on Wikipedia. After cleaning and filtering out some obviously irrelevant sections such as "references" or "external links" of these entries, we split the left into sentences to form the original text candidate set for each class. Noting that some classes have relatively much fewer sentences than others, we also add 80 additional prompt sentences for each class to alleviate the data scarcity problem. These sentences, which are in the form of 'a photo of a {label}', are auto-generated based on the prompt templates provided in <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Protocol</head><p>Following common practices <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b6">7]</ref>, we evaluate our proposed models on the corresponding balanced validation/test set and report the overall top-1 accuracy. To diagnose the source of improvement, we also report the top-1 accuracy of the three subsets split by the number of training samples in each class, namely many-shot (?100 samples), medium-shot (20?100 samples), and few-shot (?20 samples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on ImageNet-LT</head><p>Settings. To verify the effectiveness of our method, we conduct extensive experiments on ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>. We use ResNet-50 <ref type="bibr" target="#b13">[14]</ref> or ViT-Base/16 <ref type="bibr" target="#b9">[10]</ref> as the visual encoder, and a 12-layer Transformer <ref type="bibr" target="#b37">[38]</ref> as the linguistic encoder. All models are optimized by AdamW <ref type="bibr" target="#b30">[31]</ref> with a momentum of 0.9 and a weight decay of 5 ? 10 ?2 . We use the same data augmentation as <ref type="bibr" target="#b44">[45]</ref> (w/o distillation). In the pre-training phase, the maximum length of text tokens is set to 77 (including [SOS] and [EOS] tokens), and the pre-trained weights of CLIP <ref type="bibr" target="#b36">[37]</ref> is loaded. The initial learning rate is set to 5 ? 10 ?5 and decays following the cosine schedule <ref type="bibr" target="#b29">[30]</ref>. During this phase, models are pre-trained for 50 epochs, with a mini-batch size of 256. In the fine-tuning phase, we select 64 sentences for each class and fine-tune models with the mini-batch size of 128 for another 50 epochs. We set the initial learning rate to 1 ? 10 ?3 and still decrease it with the cosine schedule. In both stages, we adopt the input size of 224 ? 224 and the square-root data sampling strategy <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> unless specifically mentioned. <ref type="figure">Fig. 4</ref>: Absolute accuracy score of our method over the baseline using ViT-Base/16 <ref type="bibr" target="#b9">[10]</ref> as the backbone on ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>. Our method enjoys more performance gains on classes with fewer image samples.</p><p>For a fair comparison, we also build a baseline that is only based on visual modality while keeping other settings exactly the same as our proposed method, except that the baseline models are directly initialized with CLIP pre-trained weights and finetuned for 100 epochs. In addition, we re-implement and report the performance of some representative methods as well, such as ? -normalized, cRT, NCM, and LWS <ref type="bibr" target="#b20">[21]</ref>, which are all initialized with CLIP pre-trained weights.</p><p>Results. In <ref type="table">Table 1</ref>, we can see that our VL-LTR models are superior to conventional visionbased methods with similar visual encoders (i.e., backbones). For example, when using ResNet-50 (R-50) <ref type="bibr" target="#b13">[14]</ref> as the backbone, the overall accuracy of our method reaches 70.1%, which outperforms baseline by 9.6 points (70.1% vs.60.5%), and 10.1 points better than previous best PaCo <ref type="bibr" target="#b6">[7]</ref> (70.1% vs.60.0%).</p><p>Moreover, from the aspect of few-shot accuracy, the performance of our method is more promising, which is 16.3 points and 7.3 points better than baseline (50.8% vs.34.5%) and the second-best method <ref type="bibr" target="#b52">[53]</ref> (50.8% vs.43.5%). When replacing the backbone with heavy ViT-Base/16 (ViT-B) <ref type="bibr" target="#b9">[10]</ref>, the overall accuracy of our method can further boost up to 77.2%, which is the current new state-of-the-art of ImageNet-LT, and near the prevailing performance (i.e., 80%) training on the full ImageNet <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Accuracy (%) Overall Many Medium Few Cross Entropy <ref type="bibr" target="#b25">[26]</ref> ResNeXt-50 44.4 65.9 37.5 7.7 OLTR <ref type="bibr" target="#b28">[29]</ref> ResNeXt-50 46.3 ---SSD <ref type="bibr" target="#b25">[26]</ref> ResNeXt-50 <ref type="bibr" target="#b55">56</ref>  <ref type="table">Table 1</ref>: Results on ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>. Our method outperforms prior arts when using a similar backbone network. "*" indicates the corresponding backbone is initialized with CLIP <ref type="bibr" target="#b36">[37]</ref> weights.</p><p>In <ref type="figure">Figure 4</ref>, we visualize the class-level performance improvement, which is measured by the absolute accuracy gains of our method against the baseline, both of which use ViT-B as the visual backbone. We see that there is more gains on tail classes, which indicates that our method can help mitigate the data-scarce problem under long-tail settings by introducing class-level text descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments on Places-LT</head><p>Settings. We also investigate our method on Places-LT <ref type="bibr" target="#b28">[29]</ref>, a dataset with a different domain. The experimental setting of Places-LT is the same as Section 4.3. Results. As reported in <ref type="table">Table 2</ref>, using ResNet-50 (R-50) as backbone, our model achieves 48.0% overall accuracy, surpassing counterparts by at least 6.8 points (48.0% vs.41.2%), including state-of-the-art PaCo <ref type="bibr" target="#b6">[7]</ref>, TADE <ref type="bibr" target="#b52">[53]</ref>, and ResLT <ref type="bibr" target="#b5">[6]</ref>, while all of them use ResNet-152 <ref type="bibr" target="#b13">[14]</ref> as backbone. The performance are also impressive for the medium-(47.2%) and few-shot (38.4%) classes. Once again, the model with ViT-Base/16 (ViT-B) <ref type="bibr" target="#b9">[10]</ref> gives the top overall accuracy of 50.1%, which is a new state-of-the-art on this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Accuracy (%) Overall Many Medium Few OLTR <ref type="bibr" target="#b28">[29]</ref> ResNet  <ref type="table">Table 2</ref>: Results on Places-LT <ref type="bibr" target="#b28">[29]</ref>. "*" indicates the corresponding backbone is initialized with CLIP <ref type="bibr" target="#b36">[37]</ref> weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">iNaturalist 2018</head><p>Settings. We further test our VL-LTR on iNaturalist 2018, a long-tailed fine-grained benchmark. Following the common practice <ref type="bibr" target="#b44">[45]</ref>, we adopt a long training schedule. To be specific, our models are pre-trained for 100 epochs, and fine-tuned for 360 epochs. The initial learning rate of the pre-training and fine-tuning phase is set to 5 ? 10 ?4 and 2 ? 10 ?5 , respectively. Correspondingly, the baseline has the same fine-tuning epochs and initial learning rate as the proposed method. All other experimental settings are the same as Section 4.3.</p><p>Results. <ref type="table">Table 3</ref> shows the top-1 accuracy on iNaturalist 2018 of different methods. We see that when using ResNet-50 (R-50) <ref type="bibr" target="#b13">[14]</ref> as the backbone, our models can achieve a 74.6% overall accuracy, surpassing previous methods with the same backbone by at least 1.4 points. Besides that, when equipped with a strong backbone ViT-Base/16 (ViT-B) <ref type="bibr" target="#b9">[10]</ref>, our model can have an overall accuracy of 76.8%, which outperforms the stateof-the-art PaCo (ResNet-152) by 1.6 points (76.8% vs.75.2%). Moreover, our model can also benefit from a larger image input size (i.e., 384 ? 384), and achieve 81.0% top-1 accuracy, which is 1.5 points higher than DeiT-B/16-384 <ref type="bibr" target="#b44">[45]</ref> (81.0% vs.79.5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>Settings. In order to provide a deep analysis of our proposed method, we also conduct ablation studies on the ImageNet-LT dataset. In these experiments, we use ResNet-50 as the default backbone. All other settings remain the same as Section 4.3 unless specifically mentioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Accuracy (%) CB-Focal <ref type="bibr" target="#b1">[2]</ref> ResNet-50 61.1 LDAM+DRW <ref type="bibr" target="#b1">[2]</ref> ResNet-50 68.0 BBN <ref type="bibr" target="#b55">[56]</ref> ResNet-50 69.6 SSD <ref type="bibr" target="#b25">[26]</ref> ResNet-50 71.5 RIDE (4 experts) <ref type="bibr" target="#b47">[48]</ref> ResNet-50 72.6 smDRAGON <ref type="bibr" target="#b38">[39]</ref> ResNet-50 69.1 ResLT <ref type="bibr" target="#b5">[6]</ref> ResNet-50 72.3 TADE <ref type="bibr" target="#b52">[53]</ref> ResNet-50 72.9 PaCo <ref type="bibr" target="#b6">[7]</ref> ResNet-50 73.2 NCM <ref type="bibr" target="#b20">[21]</ref> ResNet-50 63.1 cRT <ref type="bibr" target="#b20">[21]</ref> ResNet-50 67.6 ? -normalized <ref type="bibr" target="#b20">[21]</ref> ResNet-50 69.3 LWS <ref type="bibr" target="#b20">[21]</ref> ResNet-50 69.5 NCM <ref type="bibr" target="#b20">[21]</ref> ResNet-50* 65.3 cRT <ref type="bibr" target="#b20">[21]</ref> ResNet-50* 69.9 ? -normalized <ref type="bibr" target="#b20">[21]</ref> ResNet-50* 71.2 LWS <ref type="bibr" target="#b20">[21]</ref> ResNet-50* 71.0 Zero-Shot CLIP <ref type="bibr" target="#b36">[37]</ref> ResNet-50*</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Baseline</head><p>ResNet-50* 72.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VL-LTR (ours)</head><p>ResNet-50* 74.6 PaCo <ref type="bibr" target="#b6">[7]</ref> ResNet-152 75.2 DeiT-B/16 <ref type="bibr" target="#b44">[45]</ref> -73.2 DeiT-B/16-384 <ref type="bibr" target="#b44">[45]</ref> -79.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VL-LTR (ours)</head><p>ViT-Base* 76.8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VL-LTR-384 (ours)</head><p>ViT-Base* 81.0 <ref type="table">Table 3</ref>: Results on iNaturalist 2018 <ref type="bibr" target="#b45">[46]</ref>. "*" indicates the corresponding backbone is initialized with CLIP weights. "*-384" means the input size of 384 ? 384.</p><p>Class-wise Visual-Linguistic Pre-training. To examine the effectiveness of our classwise visual-linguistic pre-training (CVLP) framework, we remove it by directly performing the fine-tuning process on the pre-trained weights of CLIP <ref type="bibr" target="#b36">[37]</ref>. As reported in the #1 and #2 of <ref type="table">Table 4</ref>, the model with CVLP outperforms the one without CVLP by 7.3 points on the overall accuracy. Such gap might be attributed to the inconsistency between image and text representation, which can be alleviated by our CVLP.</p><p>To verify this, we visualize some concepts by retrieving images with the greatest cosine similarity. As shown in <ref type="figure">Figure 6</ref>, both CLIP and our method can learn common visual concepts, such as the "blue" color, but CLIP <ref type="bibr" target="#b36">[37]</ref> fails to capture rare concepts, such as "stick" shape and "spot" texture. More examples are provided in the supplementary material. CLIP Pre-trained Weights. To analyze the influence of CLIP pre-trained weights, we train our method with randomly initialized weights. Comparing the #1 and #3 of <ref type="table">Table  4</ref>, we can see that initializing with CLIP pre-trained weights benefits our VL-LTR. We also plot the curves of training and validation loss in the fine-tuning stage in <ref type="figure" target="#fig_1">Figure  5</ref>, where CLIP pre-trained weights (see red curves) can help alleviate the over-fitting problem. This phenomenon is caused by the limited text corpus for pre-training. There </p><formula xml:id="formula_10">(%) 1 ? - ? LGR AnSS 70.1 2 ? - - LGR AnSS 62.8 3 - ? - LGR AnSS 46.8 4 ? ? - LGR AnSS 66.2 5 ? - ? FC - 62.1 6 ? - ? KNN - 63.9 7 ? - ?</formula><p>LGR Cut Off 69.7 <ref type="table">Table 4</ref>: Ablation studies on ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>. "Head" denotes the recognition head used in the fine-tuning stage, and "SS" denotes the sentence selection strategy.</p><p>are only 1000 class descriptions (about 127K sentences) for ImageNet-LT, it is easy to overfit an image to a specific set of sentences without a pre-trained linguistic encoder. Distillation Loss. Similar to the role of pre-trained weights, the distillation loss L dis is also used to reduce the risk of over-fitting in the pre-training phase. Comparing the red and blue curves in <ref type="figure" target="#fig_1">Figure 5</ref>, the over-fitting problem is alleviated in the model with L dis . From the #1 and #4 of <ref type="table">Table 4</ref>, we also see that the model with L dis performs better than the one without L dis (70.1% vs.66.2%). Linguistic-Guided Recognition. We verify the effectiveness of linguistic-guided recognition (LGR) by comparing it with other recognition heads, including FC (vision-based), and KNN (vision-language-based). As reported in #1, #5, and #6 of <ref type="table">Table 4</ref>, the proposed LGR performs better than FC and KNN by 8.0% and 6.2% points in overall accuracy respectively. It is notable that, as a vision-language-based recognition head, KNN also works better than FC. These results indicate the effectiveness of LGR and the power of visual-linguistic representation.</p><p>Anchor Sentence Selection. We study the effectiveness of anchor sentence selection (AnSS) by replacing it with "Cut Off" strategy, where we simply select the first M sentences from text descriptions as the anchor sentences for visual recognition. As shown in <ref type="table">Table 4</ref>, the model with AnSS (see the #1 of <ref type="table">Table 4</ref>) outperforms the model with "Cut Off" on the overall accuracy, which proves the effectiveness of AnSS to filter out some noisy sentences. Note that, AnSS is a training-free module, which can bring considerable improvements in noisy scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blue</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spot</head><p>Stick CLIP Ours <ref type="figure">Fig. 6</ref>: Concept visualization, where "freq" and "rare" mean concepts appear frequently and rarely, respectively. Our method can effectively learn common visual concepts, and even the rare concepts where CLIP <ref type="bibr" target="#b36">[37]</ref> makes mistakes, such as "spot" texture and "stick" shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Limitations</head><p>Although the proposed VL-LTR achieves good performance on multiple long-tailed recognition benchmarks, it still has some flaws. First, due to the limited text corpus, our method currently relies on existing pre-trained foundation models to learn high-quality linguistic representation. Second, like most LTR works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref>, our VL-LTR is a twostage method as well, which does not support end-to-end training. But we believe these problems could be well addressed in the future with the enrichment of text data and the development of visual-linguistic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we introduce VL-LTR, a new visual-linguistic framework for long-tailed recognition. We develop a class-level visual-linguistic pre-training (CVLP) to connect images and text descriptions at class level, and a language-guided recognition (LGR) head to make effective use of visual-linguistic representation for visual recognition. Extensive experiments on various long-tailed recognition benchmarks verify that our method works better than well-designed vision-based methods. We hope this work could provide a strong baseline for vision-language-based long-tailed visual recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Methodology Details</head><p>For convenience, we summarize all the notations used in the paper in <ref type="table">Table 5</ref>. <ref type="table">Table 5</ref>: Summary of notations used in the paper. </p><formula xml:id="formula_11">Notation Meaning I = {Ii} N i=1 A batch of N image samples T = {Ti} N i=1 A batch of N text</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Class-level Corpus Preparation</head><p>As described in Section 4.1, we collect class-level text descriptions from Wikipedia and prompt templates provided in <ref type="bibr" target="#b36">[37]</ref>. In <ref type="figure">Figure 7</ref>, we display part of text descriptions collected for ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>, Places-LT <ref type="bibr" target="#b28">[29]</ref>, and iNaturalist-2018 <ref type="bibr" target="#b45">[46]</ref> datasets. We see that since these texts are all crawled from the Internet, it is inevitable to have some noisy text within them. In addition, we report detailed statistics of the collected text descriptions in <ref type="table" target="#tab_5">Table 6</ref>, where we find that even if all the corpus comes from Wikipedia, the text quantity of different classes varies greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Computation Overhead</head><p>As mentioned in Section 3.1, our VL-LTR is a two-stage framework with two encoders. Nevertheless, we would like to point out that the computational cost of our method is almost the same as the vision-based method, since the linguistic encoder is not necessary at the inference stage. Specifically, after pre-training, the text embeddings of anchor sentences can be pre-populated offline. During inference, we only need to load the prepopulated text embeddings to perform visual recognition. As reported in <ref type="table" target="#tab_6">Table 7</ref>, the GFLOPs and the inference speed of our method are similar to the baseline. These results are tested with a batch size of 128 on one V100 GPU and one 2.20GHz CPU in a single thread. Moreover, we believe such conclusion also applies to other backbones such as ViT, Swin, TransFG, and complemental attention, since our framework is orthogonal to the backbone's structure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Comparison with Zero-Shot CLIP</head><p>In <ref type="table" target="#tab_7">Table 8</ref>, we compare our results and the zero-shot results of CLIP <ref type="bibr" target="#b36">[37]</ref> on ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>, Places-LT <ref type="bibr" target="#b28">[29]</ref> and iNaturalist 2018 <ref type="bibr" target="#b45">[46]</ref> datasets, respectively. We see that the performance of CLIP drops sharply when the domain of target data (e.g., iNaturalist 2018) is inconsistent with its training data, while our method can achieve significant improvement on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Comparison of Different Distillation Methods in CVLP</head><p>To further study the influence of distillation in the pre-training phase, we try to use the pre-trained CLIP model <ref type="bibr" target="#b36">[37]</ref> as the teacher model to distill the visual and linguistic encoder of our model at the feature level, in addition to the logits distillation mentioned in Section 3.2. As reported in <ref type="table" target="#tab_8">Table 9</ref>, both feature distillation and logits distillation can improve recognition accuracy, and our method achieves the highest accuarcy on ImageNet-LT <ref type="bibr" target="#b28">[29]</ref> when using logits distillation with the loss weight ? of 0.5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Comparison of Different Text Description Sources</head><p>In <ref type="table">Table 10</ref>, we compare the results of models using different kinds of text descriptions on ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>. Specifically, we use the prompt sentences provided in <ref type="bibr" target="#b36">[37]</ref> as the source of text description. We mark this model as "prompt only", and compare it with the default model that uses both Wikipedia and prompt templates as the source of text description (i.e., "wiki + prompt"). We see that "wiki + prompt" outperforms "prompt only" in overall, medium, and few accuracy, which demonstrates the effectiveness of corpus from Wikipedia.</p><p>We also notice that although "prompt only" is not the best, its performance is still relatively competitive compared to the vision-based methods (e.g., the strong Baseline established in this work). We attribute this phenomenon to reasons as follows: (1) Our method can make effective use of the pre-trained image and text encoder of CLIP <ref type="bibr" target="#b36">[37]</ref>, while vision-based methods can only use image encoder; (2) Some class names themselves contain discriminative language information, such as "gold fish", "tree frog", and "mountain bike".</p><p>? It is commonly kept as a pet in indoor aquariums, and is one of the most popular aquarium fish. ? Goldfish released into the wild have become an invasive pest in parts of North America. ... ? A photo of a goldfish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goldfish</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-LT</head><p>? A tree frog (or treefrog) is any species of frog that spends a major portion of its lifespan in trees, known as an arboreal state. ? Several lineages of frogs among the Neobatrachia have given... ... ? A photo of a tree frog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tree Frog</head><p>? Oreaster reticulatus, commonly known as the red cushion sea star or the West Indian sea star, is a species of marine invertebrate, a starfish in the family Oreasteridae. ... ? A photo of a oreaster reticulatus.</p><p>Oreaster Reticulatus iNaturalist 2018 ? Its historical range, by 9000 BC, is described as the great bison belt, a tract of rich grassland that ran from Alaska to the Gulf of Mexico, east to the Atlantic Seaboard... ... ? A photo of a bison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bison</head><p>Places-LT ? The holiday cottage exists in many cultures under different names. ? In American English, "cottage" is one term for such holiday homes, although they... ... ? A photo of a cottage.</p><p>Cottage ? The most common kinds of true arch are the fixed arch, the twohinged arch, and the three-hinged arch.The fixed arch is most often used in reinforced concrete bridges and tunnels... ... ? A photo of an arch. Arch <ref type="figure">Fig. 7</ref>: Examples of text descriptions crawled from Wikipedia for ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>, Places-LT <ref type="bibr" target="#b28">[29]</ref> and iNaturalist-2018 <ref type="bibr" target="#b45">[46]</ref>, in which both redundant useful and noise information can be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Visualization of AnSS</head><p>To intuitively show the effectiveness of our anchor sentence selection (AnSS), we also present some sentences recommended or filtered out by our AnSS of different classes in <ref type="figure">Figure 8</ref>. We see that our method can reserve useful texts and drop the useless ones effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H More Examples of Concept Visualization</head><p>In this section, we provide more concept visualization results of VL-LTR (ResNet-50) trained on ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>. As shown in <ref type="figure">Figure 9</ref>, our models can not only learn some appearance attributes such as the shape and texture, but also understand highlevel concepts like "wall" and "sky". Moreover, benefiting from CVLP, our method can cover more visual concepts than CLIP. <ref type="table">Table 10</ref>: Results of using different text source on ImageNet-LT <ref type="bibr" target="#b28">[29]</ref> and Places-LT <ref type="bibr" target="#b28">[29]</ref>, where we see that "wiki + prompt" outperforms "prompt only" in overall, medium, and few accuracy.  <ref type="figure">Fig. 9</ref>: Examples of concept visualization. Our method can not only learn the texture (e.g., mesh) and shape (e.g., linear) of objects, but can also understand some visual attributes (e.g., fur and fin) and high-level concepts (e.g., wall and sky). In addition, compared to the original CLIP <ref type="bibr" target="#b36">[37]</ref>, our method can cover more visual concepts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>the upper half is brownish-black while the lower is light blue. The habitat of the indigo bunting is brushy forest edges. Low-Level / Concrete / Posteriori (c) Image Modality (d) Text Modality vs. High-Level / Abstract / Priori Comparison of different long-tailed recognition (LTR) frameworks and different modalities. (a) Previous LTR methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Train and validation loss curves of VL-LTR (ResNet-50) on ImageNet-LT<ref type="bibr" target="#b28">[29]</ref> under different settings. Both without CLIP<ref type="bibr" target="#b36">[37]</ref> weights (w/o CLIP) and without distillation lead to a certain degree of overfitting. Ldis w/ Ldis Head SS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>as well.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">VL-LTR (ours)</cell><cell></cell><cell cols="3">PaCo</cell><cell cols="3">TADE</cell><cell></cell><cell>RIDE</cell><cell cols="3">ResLT</cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>40 50 60 70</cell><cell>70.1</cell><cell>60.0</cell><cell>58.8</cell><cell>56.8</cell><cell>55.1</cell><cell>50.8</cell><cell>41.0</cell><cell>43.5</cell><cell>36.0</cell><cell>40.3</cell><cell>67.0</cell><cell>58.7</cell><cell>57.0</cell><cell>53.8</cell><cell>53.3</cell><cell>68.2</cell><cell>66.5</cell><cell>68.2</cell><cell>63.3</cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Overall</cell><cell></cell><cell></cell><cell cols="3">Few</cell><cell></cell><cell cols="5">Medium</cell><cell cols="3">Many</cell><cell></cell></row></table><note>Fig. 2: Performance comparison on ImageNet-LT</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Detailed statistics of the class-level text descriptions for each dataset, where M min , M max , M mean , and M Med denotes the minimum, maximum, mean, and median number of sentences of classes respectively, and L Avg denotes the average number of tokens per sentence.</figDesc><table><row><cell>Dataset</cell><cell>Mmin</cell><cell>Mmax</cell><cell>Mmean</cell><cell>MMed</cell><cell>LAvg</cell></row><row><cell>ImageNet-LT [29]</cell><cell>1</cell><cell>721</cell><cell>127</cell><cell>89</cell><cell>29</cell></row><row><cell>Places-LT [29]</cell><cell>2</cell><cell>610</cell><cell>116</cell><cell>77</cell><cell>29</cell></row><row><cell>iNaturalist 2018 [46]</cell><cell>1</cell><cell>1774</cell><cell>33</cell><cell>17</cell><cell>26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Computation overhead comparison of our VL-LTR (ResNet-50) and the baseline (ResNet-50). Our method has almost the same GFLOPs and inference speed to the baseline. GFLOPs is calculated under the input scale of 224 ? 224.</figDesc><table><row><cell>Method</cell><cell>GFLOPs</cell><cell>Time Cost (ms)</cell></row><row><cell>Baseline</cell><cell>5.4</cell><cell>1.1</cell></row><row><cell>VL-LTR (ours)</cell><cell>5.5</cell><cell>1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparison with Zero-Shot CLIP. Our method achieves improvements on all datasets and is robust to datasets of different domains.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Overall</cell><cell cols="2">Accuracy(%) Many Medium</cell><cell>Few</cell></row><row><cell></cell><cell>Zero-Shot</cell><cell>59.8</cell><cell>60.8</cell><cell>59.3</cell><cell>58.6</cell></row><row><cell>ImageNet-LT</cell><cell>Baseline</cell><cell>60.5</cell><cell>74.4</cell><cell>56.9</cell><cell>34.5</cell></row><row><cell></cell><cell>VL-LTR (ours)</cell><cell>70.1</cell><cell>77.8</cell><cell>67.0</cell><cell>50.8</cell></row><row><cell></cell><cell>Zero-Shot</cell><cell>38.0</cell><cell>37.5</cell><cell>37.5</cell><cell>40.1</cell></row><row><cell>Places-LT</cell><cell>Baseline</cell><cell>39.7</cell><cell>50.8</cell><cell>38.6</cell><cell>22.7</cell></row><row><cell></cell><cell>VL-LTR (ours)</cell><cell>48.0</cell><cell>51.9</cell><cell>47.2</cell><cell>38.4</cell></row><row><cell></cell><cell>Zero-Shot</cell><cell>3.4</cell><cell>6.1</cell><cell>3.3</cell><cell>2.9</cell></row><row><cell>iNaturalist 2018</cell><cell>Baseline</cell><cell>72.6</cell><cell>76.6</cell><cell>74.1</cell><cell>70.2</cell></row><row><cell></cell><cell>VL-LTR (ours)</cell><cell>74.6</cell><cell>78.3</cell><cell>75.5</cell><cell>72.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Results of different types of distillation in CVLP on ImageNet-LT<ref type="bibr" target="#b28">[29]</ref>. Our method achieves the highest accuarcy when using logits distillation with the loss weight ? of 0.5.</figDesc><table><row><cell>Distill Level</cell><cell>?</cell><cell>Overall</cell><cell cols="2">Accuracy (%) Many Medium</cell><cell>Few</cell></row><row><cell>-</cell><cell>0</cell><cell>66.2</cell><cell>76.9</cell><cell>63.5</cell><cell>42.5</cell></row><row><cell>Feature</cell><cell>0.1 0.5</cell><cell>67.3 68.0</cell><cell>77.3 77.6</cell><cell>64.4 65.2</cell><cell>44.0 45.5</cell></row><row><cell>Logits</cell><cell>0.1 0.5 (ours)</cell><cell>68.3 70.1</cell><cell>77.9 77.8</cell><cell>65.3 67.0</cell><cell>45.1 50.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://en.wikipedia.org/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Good</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bad</head><p>Class Name: Goldfinch ? Tree Frogs are usually tiny as their weight has to be carried by the branches and twigs in their habitats. (?!"#= 3.57) ? A tree frog is any species of frog that spends a major portion of its lifespan in trees, known as an arboreal state. (?!"#= 3.61) ? A few also occur in East Asia. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Good</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bad</head><p>Class Name: Snorkel <ref type="figure">Fig. 8</ref>: Some "good" and "bad" sentences and their corresponding L lin of classes in ImageNet-LT <ref type="bibr" target="#b28">[29]</ref>. The value of L lin can reflect the usefulness of these sentences to some extent, which thereby supports the effectiveness of our AnSS.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ar?chiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf</title>
		<meeting>Advances in Neural Inf</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis. (2020) 1</title>
		<meeting>Eur. Conf. Comp. Vis. (2020) 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Reslt: Residual learning for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10633</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parametric contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt</title>
		<meeting>IEEE Conf. Comp. Vis. Patt</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Recogn. (2021) 2, 3, 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations (2021) 9</title>
		<meeting>Int. Conf. Learn. Representations (2021) 9</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">C4. 5, class imbalance, and cost sensitivity: why undersampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on learning from imbalanced datasets II</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf</title>
		<meeting>Advances in Neural Inf</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Intell. Comput</title>
		<meeting>Int. Conf. Intell. Comput</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained image classification via combining vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<title level="m">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring balanced feature spaces for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. &amp; Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">M2m: Imbalanced classification via major-to-minor translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HERO: hierarchical encoder for video+language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A closer look at the robustness of vision-and-language pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08673</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self supervision to distillation for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf</title>
		<meeting>Advances in Neural Inf</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf</title>
		<meeting>Advances in Neural Inf</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shaping visual representations with language for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From generalized zero-shot learning to long-tail with class descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter. Conf. Applications of Comp. Vis. (2021)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Distributional robustness loss for long-tail learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03066</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LXMERT: learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Equalization loss for longtailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Training dataefficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt</title>
		<meeting>IEEE Conf. Comp. Vis. Patt</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Contrastive learning based hybrid networks for long-tailed image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Long-tailed recognition by routing diverse distribution-aware experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations (2020) 2, 3</title>
		<meeting>Int. Conf. Learn. Representations (2020) 2, 3</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feature transfer learning for face recognition with under-represented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Test-agnostic long-tailed recognition by test-time aggregating diverse experts with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.09249</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unequal-training for deep face recognition with long-tailed noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (2020) 3, 4</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn. (2020) 3, 4</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Inflated episodic memory with region self-attention for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Wildfish++: A comprehensive fish benchmark for multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
