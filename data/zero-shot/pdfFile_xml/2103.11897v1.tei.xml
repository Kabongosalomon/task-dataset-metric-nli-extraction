<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Aware Layout to Image Generation with Enhanced Object Appearance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Liao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">TNT</orgName>
								<orgName type="institution">Leibniz University Hannover</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">SUG</orgName>
								<orgName type="institution" key="instit2">University of Twente</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">TNT</orgName>
								<orgName type="institution">Leibniz University Hannover</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Aware Layout to Image Generation with Enhanced Object Appearance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A layout to image (L2I) generation model aims to generate a complicated image containing multiple objects (things) against natural background (stuff), conditioned on a given layout. Built upon the recent advances in generative adversarial networks (GANs), existing L2I models have made great progress. However, a close inspection of their generated images reveals two major limitations: (1) the object-to-object as well as object-to-stuff relations are often broken and (2) each object's appearance is typically distorted lacking the key defining characteristics associated with the object class. We argue that these are caused by the lack of context-aware object and stuff feature encoding in their generators, and location-sensitive appearance representation in their discriminators. To address these limitations, two new modules are proposed in this work. First, a context-aware feature transformation module is introduced in the generator to ensure that the generated feature encoding of either object or stuff is aware of other coexisting objects/stuff in the scene. Second, instead of feeding location-insensitive image features to the discriminator, we use the Gram matrix computed from the feature maps of the generated object images to preserve location-sensitive information, resulting in much enhanced object appearance. Extensive experiments show that the proposed method achieves state-of-the-art performance on the COCO-Thing-Stuff and Visual Genome benchmarks. Code available at: https://github.com/wtliao/layout2img.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in generative adversarial networks (GANs) <ref type="bibr" target="#b10">[11]</ref> have made it possible to generate photorealistic images for a single object, e.g., faces, cars, cats <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. However, generating complicated images containing multiple objects (things) of different classes against natural backgrounds (stuff) still remains a challenge <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref>. This is due to the large appearance variations * Equal contribution  <ref type="figure">Figure 1</ref>. Illustration of the limitations of existing L2I models and how our model overcome them. From left to right: ground truth layout, images generated by the state-of-the-art LostGAN-v2 <ref type="bibr" target="#b38">[39]</ref>, and by our model with the layout as input. In the middle and right column, regions with key differences in the generation quality between LostGAN-v2 and our model are highlighted in dashed boxes. See text for more details.</p><p>for objects of different classes, as well as the complicated relations between both object-to-object and object-to-stuff. A generated object needs to be not only realistic on its own, but in harmony with surrounding objects and stuff. Without any conditional input, the mode collapse <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b5">6]</ref> problem is likely to be acute for GANs trained to generate such complicated natural scenes. Consequently, various inputs have been introduced to provide some constraints on the image generation process. These include textual description of image content <ref type="bibr" target="#b30">[31]</ref>, scene graph representing objects and their relationship <ref type="bibr" target="#b17">[18]</ref>, and semantic map providing pixel-level annotation <ref type="bibr" target="#b29">[30]</ref>. This work focuses on the conditional image generation task using the layout <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref> that defines a set of bounding boxes with specified size, location and categories (see <ref type="figure">Fig. 1</ref>). Layout is a user-friendly input format on its own and can also be used as an intermediate input step of other tasks, e.g., scene graph and text to image generation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Since the seminal work <ref type="bibr" target="#b47">[48]</ref> in 2019, the very recent layout to image (L2I) generation models <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39]</ref> have made great progresses, thanks largely to the advances made in GANs <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b19">20]</ref> as they are the key building blocks. From a distance, the generated images appear to be realistic and adhere to the input layout (see <ref type="figure">Fig. 1</ref> and more in <ref type="figure" target="#fig_1">Fig. 3</ref>). However, a closer inspection reveals two major limitations. First, the relations between objects and object-to-stuff are often broken. This is evident from the food example in <ref type="figure">Fig. 1</ref> (Top-Middle) -the input layout clearly indicates that the four bowls are overlapping with each other. Using the state-of-the-art LostGAN-v2 <ref type="bibr" target="#b38">[39]</ref>, the occluded regions between objects are poorly generated. Second, each generated object's appearance is typically distorted lacking classdefining characteristics. For instance, the surfing example in <ref type="figure">Fig. 1</ref> (Middle) and the giraffe example in <ref type="figure">Fig. 1</ref> (Bottom) show that the object appearance has as if been touched by Picasso -one can still recognize the surfing person or giraffe, but key body parts are clearly misplaced.</p><p>We believe these limitations are caused by two major design flaws in existing L2I models in both their GAN generators and discriminators. (1) Lack of context-aware modeling in the generator: Existing models generate the feature for the object/stuff in each layout bounding box first, and then feed the generated feature into a generator for image generation. However, the feature generation process for each object/stuff is completely independent of each other, therefore offering no chance for capturing the inter-object and object-to-stuff relations. (2) Lack of location-sensitive appearance representation in the discriminator: As in any GAN model, existing L2I models deploy a discriminator that is trained to distinguish the generated whole image and individual object/stuff images from the real ones. Such a discriminator is essentially a CNN binary classifier whereby globally pooled features extracted from the CNN are fed to a real-fake classifier. The discriminator thus cares only about the presence/absence and strength of each semantic feature, rather than where they appear in the generated images. This lack of location-sensitive appearance representation thus contributes to the out-of-place object part problem in <ref type="figure">Fig. 1</ref> (Middle).</p><p>In this paper, we provide solutions to overcome both limitations. First, to address the lack of context-aware modeling problem, we propose to introduce a context-aware feature transformation module in the generator of a L2I model. This module updates the generated feature for each object and stuff after each has examined its relations with all other objects/stuff co-existing in the image through selfattention. Second, instead of feeding location-insensitive globally pooled object image features to the discriminator, we use the Gram matrix computed from the feature maps of the generated object images. The feature map Gram matrix captures the inter-feature correlations over the vectorized feature map, and is therefore locations sensitive. Adding it to the input of the real-fake classifier in the discriminator, the generated images preserve both shape and texture characteristics of each object class, resulting in much enhanced object appearance (see <ref type="figure">Fig. 1 (Right)</ref>).</p><p>The contributions of this work are as follows: (1) For the first time, we identify two major limitations of existing L2I models for generating complicated multi-object images. (2) Two novel components, namely a context-aware feature transformation module and a location-sensitive object appearance representation are introduced to address these two limitations. (3) The proposed modules can be easily integrated into any existing L2I generation models and improve them significantly. (4) Extensive experiments on both the COCO-Thing-Stuff <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b4">5]</ref> and Visual Genome <ref type="bibr" target="#b21">[22]</ref> datasets show that state-of-the-art performance is achieved using our model. The code and trained models will be released soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generative Adversarial Networks Generative adversarial networks (GANs) <ref type="bibr" target="#b10">[11]</ref>, which play a min-max game between a generator and a discriminator, is the mainstream approach used in recent image generation works. However, the training of a GAN is often unstable and known to be prone to the mode collapse problem. To address this, techniques like Wasserstein GAN <ref type="bibr" target="#b1">[2]</ref> and Unrolled GAN <ref type="bibr" target="#b27">[28]</ref> were developed. Meanwhile, noise injection and weight penalizing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref> were used in the discriminator to alleviate the non-convergence problem for further stabilization of the training. To generate high fidelity and resolution images, architectures like Progressive GAN <ref type="bibr" target="#b18">[19]</ref> and BigGAN <ref type="bibr" target="#b3">[4]</ref> were also proposed.</p><p>Conditional Image Generation Conditional image generation, which generates an image based on a given condition (e.g., class label, sentence description, image, semantic mask, sketch, and scene graph) has been studied intensively <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref> due to its potential in generating complicated natural images. In general, there are two popular architectures for the conditional image generation. The first one is the encoder-decoder architecture used in Pix2pix <ref type="bibr" target="#b16">[17]</ref> and CycleGAN <ref type="bibr" target="#b50">[51]</ref>, where the encoder directly takes the conditional input and embeds it to a latent space. The decoder then transfers the embedded representation into the target image. The second popular architecture is the decoder-only architecture used in StyleGAN <ref type="bibr" target="#b19">[20]</ref> and GauGAN <ref type="bibr" target="#b29">[30]</ref>, where a decoder starts with a random input, and then progressively transforms it to produce the desired output. In this architecture, the conditional input is used to generate part of the parameters in the decoder, e.g., the affine transformation parameters in the normalization layers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> or the weight parameters in convolutional kernels <ref type="bibr" target="#b25">[26]</ref>.</p><p>Layout to Image Generation Though the previous work <ref type="bibr" target="#b14">[15]</ref> has already touched the concept of layout to image generation (L2I), it is just used as an intermediate step for a different generation task. The first stand-alone solution appeared in <ref type="bibr" target="#b47">[48]</ref>. Compared to other conditional inputs such as text and scene graph, layout is a more flexible and richer format. Therefore, more studies followed up by introducing more powerful generator architectures <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, or new settings <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>. Sun et al. <ref type="bibr" target="#b37">[38]</ref> proposed a new architecture inspired by StyleGAN <ref type="bibr" target="#b19">[20]</ref>, which allows their model to generate higher resolution images with better quality. Li et al. <ref type="bibr" target="#b22">[23]</ref> introduced a new setting for high resolution street scene generation. Their model retrieves a background from a database based on the given foreground layout. Recently, Ma et al. <ref type="bibr" target="#b26">[27]</ref> introduced attribute guided layout generation, which is more controllable on the generated objects. As mentioned earlier, all these existing models have two limitations, namely lack of context-aware modeling in their generators, and lack of location-sensitive appearance representation in their discriminators. Both limitations are overcome in this work, resulting in much improved L2I generation performance (see Sec. 5).</p><p>Context Modeling Context plays an important role in many discriminative scene analysis tasks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b42">43]</ref>. The main idea in context-based analysis is to tie each object instance in the scene with the global context, such that their relationship or interaction can be better understood. However, context has drawn little attention in image generation. One exception is SAGAN <ref type="bibr" target="#b45">[46]</ref> which applied self-attention to refine the feature map in the generator for single object image generation. In this work, we introduce context modeling for layout to image generation, a more complicated image generation task with a focus on inter-objects and object-to-stuff relation modeling.</p><p>Appearance Representation in CNNs Works on CNN visualization clear show that feature channels, especially those at the top layers of a CNN capture semantically meaningful concepts such as body parts; and the activations of these feature channel at different locations indicate where these concepts are <ref type="bibr" target="#b49">[50]</ref>. However, when it comes to object recognition <ref type="bibr" target="#b34">[35]</ref> or real-fake discriminator in GAN <ref type="bibr" target="#b10">[11]</ref>, these feature maps are globally pooled before being fed into a binary classification layer. Location-sensitive information is thus largely lost, and the focus is on the presence/absence of the semantic concepts rather than where. We therefore propose to use the Gram matrix computed on the feature maps to complement the semantics-only appearance representation used in existing discriminators in order to induce location-sensitivity in object image generation. Such a Gram matrix based appearance representation has been used in style transfer <ref type="bibr" target="#b9">[10]</ref> for style/texture representation, which seems to suggest that it only captures feature distribution but contains no spatial information. However, as pointed out in <ref type="bibr" target="#b23">[24]</ref>, this is because the use of entry-wise mean-square distance in <ref type="bibr" target="#b9">[10]</ref> removes the location sensitivity in the feature map Gram matrix. In our model, we pass the raw matrix instead of mean-square distance to the discriminator classifier, therefore preserving the location sensitivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><formula xml:id="formula_0">Let L = {(y i , b i ) N i=1 } be a layout with N bounding boxes, where y i ? C is the class of the bounding box and b i = [x i , y i , w i , h i ]</formula><p>is the position and size of the bounding box in the image lattice (H ? W ). The goal of the layout to image (L2I) generation task is to build a model G, which can generate a realistic photo I g ? R 3?H?W , given the coarse information in the layout L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Prior Models</head><p>Before introducing our proposed method in Sec. 4, we first briefly describe prior L2I models. In all previous models, the first step is always to generate a feature representation for each bounding box based on their classes:</p><formula xml:id="formula_1">p i = ? 0 ([e i , n i ]),<label>(1)</label></formula><p>where p i ? R d l +dn is the feature representation of the i th bounding box in the layout, ? 0 is a linear transformation layer, e i ? R d l is the label embedding of y i , and n i ? R dn is a random noise sampled from a zero-mean unit-variance multivariate Gaussian distribution. The generated feature vector set {p i } N i=1 is then fed into a generator G for image generation. Depending on how the generator uses the feature vector set to generate the image, the existing models can be grouped into the following two categories. L2I Models with Encoder-Decoder Generators These models deploy an encoder-decoder generator <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b26">27]</ref> which takes the feature vector set as input, and then transfers the feature vector set into a sequence of feature maps. Each feature map is generated by filling the corresponding feature vector into the region in the image lattice based on their bounding box. The generated feature maps are then fed into an encoder, which embeds each feature map into a latent space separately. Those embedded feature maps are merged into a single one through a convolutional LSTM network <ref type="bibr" target="#b36">[37]</ref>. Finally, a decoder transforms the combined feature into the target image. Mathematically, the encoder-decoder based method can be formulated as:  <ref type="figure">Figure 2</ref>. A schematic of our method with a decoder-only generator as in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref>. The feature generation module generates the raw representation for each bounding box based on their class label, the context-aware feature transformation module integrates the global context into the representation of each bounding box. Then the transformed bounding boxes' representation and the box coordinates (bi) are fed into the generator for image generation. Finally the generated image is compared with real images by a discriminator with three losses, namely image-level and object-level semantic loss (Lim and Lo) and object-level Gram matrix loss (La).</p><formula xml:id="formula_2">I g = D(cLSTM(E({F(p i , b i )} N i=1 ))),<label>(2)</label></formula><p>where F(?, ?) is a filling operation, E is the encoder, cLSTM is the convolutional LSTM network, and D is the decoder. L2I Models with Decoder-Only Generators These models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> use a decoder-only generator to first generate an auxiliary mask 1 for each bounding box for a fine-grained shape or structure prediction:</p><formula xml:id="formula_3">M i = R S (?(p i ), b i ),<label>(3)</label></formula><p>where ? is a small convolutional neural network, ?(p i ) ? R H?W , and R S (?, ?) is a resize operator, which resizes each generated mask and fit it to the corresponding region in the image lattice via up/down sampling. Then the decoder receives a zero-mean unit-variance multivariate random noise n 0 ? R C0?H0?W0 as input, and decode it into the target image by modulating the affine transformation in the normalization layer:</p><formula xml:id="formula_4">f l = BatchNorm(f l , ? l ( N i=1 p i ? M li )),<label>(4)</label></formula><p>wheref l and f l are the feature maps before and after normalization at the l th layer in the decoder, ? l is a small convolutional block to generate the pixel-wise affine transformation parameters, M li is the resized version of M i to match the corresponding feature map's scale, and ? is the outer product, by which a vector p i and a matrix M li produce a 3D tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Proposed Method</head><p>The main architecture of our proposed method is illustrated in <ref type="figure">Fig. 2</ref>. The proposed context-aware feature transformation module and location-sensitive Gram matrix based object appearance representation are integrated into the generator and discriminator respectively of a decoder-only L2I <ref type="bibr" target="#b0">1</ref> The mask is not a strictly binary mask, as it is the output of a layer with sigmoid activation. generation architecture <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. Similarly they can be easily integrated with those employing an encoder-decoder architecture <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Context-Aware Feature Generation</head><p>Let us first look at the feature transformation module. It is clear that the prior models process each bounding box independently (either in the feature generation stage or the mask generation stage in the decoder-only methods), disrespecting the other objects and stuff in the scene. As a result, the generated objects do not appear in harmony with other co-existing objects and stuff in the scene and often appear to be out of place (see <ref type="figure" target="#fig_1">Fig. 1 and Fig. 3</ref>). To overcome this limitation, we propose a context-aware transformation module, which integrates contextual information into the feature representation of each bounding box by allowing each feature to cross-examine all other features via self-attention <ref type="bibr" target="#b41">[42]</ref>. Concretely, the contextualized representation of each bounding box is computed as:</p><formula xml:id="formula_5">p c i = N j=1 w i,j p j W v ,<label>(5)</label></formula><formula xml:id="formula_6">w i,j = exp(? i,j ) N k=1 exp(? i,k ) ,<label>(6)</label></formula><formula xml:id="formula_7">? i,j = (p i W q )(p j W k ) T ,<label>(7)</label></formula><p>where W q , W k and W v ? R (d l +dn)?(d l +dn) are linear transformation layers. With the transformation, the contextualized representation of each bounding box not only has its own information, but also the global context in the layout. It is thus able to avoid the poor occlusion region generation problem shown in <ref type="figure">Fig. 1</ref> (Top-Middle). Note that this module can be used for feature map filling in the encoderdecoder based methods, as well as the mask generation and the feature modulation steps in the decoder-only methods. The contextualized feature representation is then fed into the generator for image generation (see <ref type="figure">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Location-Sensitive Appearance Representation</head><p>To address the issue of lacking location-sensitive appearance representation in the discriminators of existing L2I models, we introduce a feature map Gram matrix based appearance representation. In existing models' discriminators, the input image I im is first processed by a convolutional neural network ? D , and represented as</p><formula xml:id="formula_8">f im ? R C?H D ?W D : f im = ? D (I im ).<label>(8)</label></formula><p>Existing L2I models then apply two losses in the discriminator to train the whole model: an image-level loss L im according to the globally pooled feature of f im , and an object-level conditional loss L o based on the ROI pooled <ref type="bibr" target="#b31">[32]</ref> feature of each object in the image, concatenated with its corresponding class information. These losses are designed to boost the realism of the generated image and the objects in the image respectively. However, using pooled feature as appearance representation means that both losses are location-insensitive, i.e., they only care about the presence/absence and strength of each learned semantic feature; much less about where the corresponding visual concept appear in the image.</p><p>To address this problem, we propose to introduce an additional appearance discriminator loss, which directly penalizes the spatial misalignment of each semantic feature between the generated and real images. Concretely, we use object feature maps' Gram matrix <ref type="bibr" target="#b9">[10]</ref> as a new appearance representation and feed it to the discriminator classification layer. Formally, we define the appearance of a generated object in the image as:</p><formula xml:id="formula_9">A i = s i s T i /d s ,<label>(9)</label></formula><p>where d s = C is the channel dimension of the feature map,</p><formula xml:id="formula_10">s i ? R C?(H D ?W D )</formula><p>is the spatial dimension vectorized feature representation of the i th generated object in the image, computed as:</p><formula xml:id="formula_11">s i = R A (f im , b i ),<label>(10)</label></formula><p>where R A (?, ?) is the ROI align operator <ref type="bibr" target="#b11">[12]</ref>. For simplicity, the vectorization operation is omitted here. The new appearance loss is then defined as:</p><formula xml:id="formula_12">L a (G, D) = E A r ?p r data (A r ) [log(D(A r |y)] + E A g ?p g data (A g ) [1 ? log(D(A g |y)],<label>(11)</label></formula><p>where A r and A g are the Gram matrices of object feature maps in real and generated images respectively, y is their corresponding class label. More specifically, for the i th object in an image, its appearance loss is computed as:</p><formula xml:id="formula_13">D(A i |y) = 1 C C j=1 [A i,j , E(y i )]W A ,<label>(12)</label></formula><p>where E(y i ) ? R k is the label embedding, and W A ? R (C+K)?1 is a linear layer. The Gram matrix here captures the correlation between different feature channels and is clearly location-sensitive: each entry only assumes a large value when the corresponding two features are both present and activated at the same location. This loss is thus complementary to the two conventional losses (L im and L o ) which emphasize the presence of the semantics only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Objectives</head><p>The final model is trained with the proposed appearance loss, together with image and object level losses <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b37">38]</ref>:</p><formula xml:id="formula_14">G * = arg min D max G L a (G, D)+? im L im (G, D)+? o L o (G, D),<label>(13)</label></formula><p>where ? im and ? o are the loss weight hyperparameters, and L im and L o are computed as:</p><formula xml:id="formula_15">L im (G, D) = E I r im ?p r data (I r im ) [log(D(I r im )] + E I g im ?p g data (I g im ) [1 ? log(D(I g im )], L o (G, D) = E O r ?p r data (O r ) [log(D(O r |y)] + E O g ?p g data (O g ) [1 ? log(D(O g |y)],<label>(14)</label></formula><p>where I r im and I g im are real and generated images respectively, and O r and O g are objects in the real and generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets Two widely used benchmarks, COCO-Thing-Stuff <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b4">5]</ref> and Visual Genome <ref type="bibr" target="#b21">[22]</ref> are used in our experiments. COCO-Thing-Stuff includes bounding box annotations of the 91 stuff classes in <ref type="bibr" target="#b4">[5]</ref> and the 80 thing/object classes in <ref type="bibr" target="#b24">[25]</ref>. Following <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b37">38]</ref>, only images with 3 to 8 bounding boxes are used in our experiments. Visual Genome is originally built for complex scene understanding. The annotations in Visual Genome contain bounding boxes, object attributes, relationships, region descriptions, and segmentation. As per standard in L2I generation, we only use the bounding boxes annotation in our experiments, and each layout contains 3 to 30 bounding boxes. We follow the splits in prior works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b37">38]</ref> on both datasets to train and test our model. Implementation Details Our model is implemented with PyTorch. To show the general applicability of our proposed method, and for fair comparison with prior works, we adopt both encoder-decoder and decoder-only generators in the two instantiations of our method (termed Ours-ED and Ours-D respectively). The encoder-decoder generator has the same architecture as used in <ref type="bibr" target="#b47">[48]</ref>, and the decoderonly generator shares the same architecture as used in <ref type="bibr" target="#b37">[38]</ref>. Following <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b37">38]</ref>, the resolution of generated images is 64 ? 64 for the encoder-decoder generator and 128 ? 128 for the decoder-only generator. The learning rate is set to  1e ?4 for both generator and discriminator in all the experiments. We train our model for 200 epochs. The loss weight hyperparameters ? im and ? o are set to 0.1 and 1, respectively.</p><p>Evaluation Metrics We evaluate our model both automatically and manually. In automatic evaluation, we adopt three widely used metrics, namely Inception Score <ref type="bibr" target="#b35">[36]</ref>, Fr?chet Inception Distance (FID) <ref type="bibr" target="#b13">[14]</ref> and Diversity Score <ref type="bibr" target="#b46">[47]</ref>. Inception Score evaluates the quality of the generated images. FID computes the statistical distance between the generated images and the real images. Diversity Score compares the difference between the generated image and the real image from the same layout. Following prior evalu-  ation protocol <ref type="bibr" target="#b2">[3]</ref>, for each layout, we generate five images in COCO-Thing-Stuff and one image in Visual Genome. In manual evaluation, we run perceptual studies on Amazon Mechanical Turk (AMT) to compare the quality of the generated images from different models. Ten participants engaged in the evaluation. Each participant was given 100 randomly sampled layouts from the testing dataset as well as the corresponding generated images from different mod-els. All participants were asked to vote for their preferred image according to the image's quality and the matching degree to the paired layout. We compute the preference rate of each model from all participants. Due to the difference in generated image's resolution and for fair comparison, we compare our encoder-decoder generator based instantiation (Ours-ED) with the state-of-the-art encoder-decoder generator based baseline Layout2im <ref type="bibr" target="#b47">[48]</ref> and decoder-only instantiation (Ours-D) with the state-of-the-art decoder-only generator based baseline LostGAN-v2 <ref type="bibr" target="#b38">[39]</ref>. In both comparisons, the generated images are of the same resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>We compare our method with existing L2I models <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b26">27]</ref>, the pix2pix model <ref type="bibr" target="#b16">[17]</ref> which takes the input feature maps constructed from layout as implemented in <ref type="bibr" target="#b47">[48]</ref>, and the Grid2Im model <ref type="bibr" target="#b2">[3]</ref> which receives scene graph as input. The following observations can be made on the quantitative results shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>(1) Our method outperforms all compared methods on all benchmarks with both architectures and under all three automatic evaluations metrics, particularly for Inception Score and FID.</p><p>(2) The more recent L2I methods take a decoderonly generator. Taking the same architecture but with the two new components, our method (Ours-D) achieves new state-of-the-art. <ref type="figure" target="#fig_3">Fig 5 shows</ref> detailed statistics in the human evaluation on AMT. Among all 40 evaluation sets, our model won 32 sets. The preference rate is clearly higher at the higher resolution with more complex images (i.e., 128 ? 128, VG dataset). Some qualitative results are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. It is evident from these examples that the images generated using our method are much more contextaware, i.e., different objects co-exist in harmony with each other and the background. Importantly, each generated object has sharper texture, clearer shape boundary with respect to background inside the object bounding box, and overall much more spatially-coherent than those generated by existing L2I models.</p><p>Ablation Study In this experiment, we adopt LostGAN-v1 <ref type="bibr" target="#b37">[38]</ref> as our baseline and evaluate the effects of introducing our context transformation module and locationsensitive appearance representation. The quantitative results are shown in <ref type="table" target="#tab_2">Table 2</ref>. We can see that both our context-aware feature transformation module and new appearance representation improve the baseline significantly on their own and when combined give a further boost. Some qualitative results are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. It is clear that the model trained with our appearance representation can generate objects with much better appearance both in terms of shape and texture (TV in <ref type="figure" target="#fig_2">Fig. 4(b)</ref> and person in <ref type="figure" target="#fig_2">Fig. 4</ref>(a)(f)(g)). Context transformation also plays an important role: the generated occluded regions become more natural ( <ref type="figure" target="#fig_2">Fig. 4</ref>(b)(f)); each object's pose is also more intune with its surrounding objects and background, e.g. the surfing person's body pose is more physically plausible in <ref type="figure" target="#fig_2">Fig. 4(a)</ref>; so is the person's head pose in the presence of the laptop in <ref type="figure" target="#fig_2">Fig. 4(f)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How Our Context Transformation Module Works</head><p>In the decoder-only generator, a mask is generated using the representation of each bounding box to predict the finegrained shape or structure of the object in each bounding box (Eq. 3). Without the context information in the feature representation, the generated masks would interfere with each other. This could result in irregular or incomplete object shape particularly in the occluded regions, which would further affect the feature modulation defined in Eq. 4. We investigate this effect by adding more bounding boxes into a layout, and visualizing the predicted masks as well as the generated images. The visualization results in <ref type="figure">Fig. 6</ref> show clearly that the context-aware feature transformation  <ref type="figure">Figure 6</ref>. Qualitative examples about the contribution of context transformation in the complex scene generation. From left to right, at each time, we add one more bounding box into the previous layout, visualizing the predicted masks as well as the generated image by a model with our context transformation (Ours-D), and the same model without context transformation. Regions to pay more attention to are highlighted in dashed boxes. module reduced the negative inter-object appearance interference in a complex scene when occlusion exists, yielding better appearance for the generated objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed a novel context feature transformation module and a location-sensitive appearance representation to improve existing layout to image (L2I) generation models. In particular, they are designed to address existing models' limitations on lacking context-aware modeling in their generator and spatially sensitive appearance representation in their discriminator. Extensive experiments demonstrate the effectiveness of our method, yielding new state-of-the-art on two benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results comparing Ours-D against two representative baselines Layout2im<ref type="bibr" target="#b48">[49]</ref> and LostGAN-v2<ref type="bibr" target="#b38">[39]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative ablative experimental results. Regions with clear generation quality differences are highlighted using red dashed boxes for close examination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The preference rate of our model. A bar higher than dark dashed horizontal line indicates that our model is judged to be better than the compared baseline by the AMT workers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparative results on COCO-Thing-Stuff and Visual Genome. E-D means encoder-decoder based generator, D means decoderonly based generator. ? means improved decoder-only generator.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="2">Resolution Generator</cell><cell cols="2">Inception Score ? COCO VG</cell><cell cols="2">FID ? COCO</cell><cell>VG</cell><cell>Diversity Score ? COCO VG</cell></row><row><cell>Real images</cell><cell></cell><cell>64 ? 64</cell><cell>-</cell><cell>16.3 ? 0.4</cell><cell>13.9 ? 0.5</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Real images</cell><cell></cell><cell>128 ? 128</cell><cell>-</cell><cell>22.3 ? 0.5</cell><cell>20.5 ? 1.5</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>pix2pix [17]</cell><cell></cell><cell>64 ? 64</cell><cell>E-D</cell><cell>3.5 ? 0.1</cell><cell>2.7 ? 0.02</cell><cell cols="3">121.97 142.86</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">Layout2im [48]</cell><cell>64 ? 64</cell><cell>E-D</cell><cell>9.1 ? 0.1</cell><cell>8.1 ? 0.1</cell><cell>38.14</cell><cell cols="2">40.07</cell><cell>0.15 ? 0.06 0.17 ? 0.09</cell></row><row><cell>Ours-ED</cell><cell></cell><cell>64 ? 64</cell><cell>E-D</cell><cell>10.27 ? 0.25</cell><cell>8.53 ? 0.13</cell><cell>31.32</cell><cell cols="2">33.91</cell><cell>0.39 ? 0.09</cell><cell>0.4?0.09</cell></row><row><cell>Grid2Im [3]</cell><cell></cell><cell>128 ? 128</cell><cell>E-D</cell><cell>11.22 ? 0.15</cell><cell>-</cell><cell>63.44</cell><cell></cell><cell>-</cell><cell>0.28 ? 0.11</cell><cell>-</cell></row><row><cell cols="2">LostGAN-v1 [38]</cell><cell>128 ? 128</cell><cell>D</cell><cell>13.8 ? 0.4</cell><cell>11.1 ? 0.6</cell><cell>29.65</cell><cell cols="2">29.36</cell><cell>0.40 ? 0.09 0.43 ? 0.09</cell></row><row><cell cols="2">LostGAN-v2 [49]</cell><cell>128 ? 128</cell><cell>D  ?</cell><cell>14.21 ? 0.4</cell><cell>10.71 ? 0.76</cell><cell>24.76</cell><cell cols="2">29.00</cell><cell>0.55 ? 0.09 0.53 ? 0.09</cell></row><row><cell cols="2">OC-GAN [40]</cell><cell>128 ? 128</cell><cell>D</cell><cell>14.0 ? 0.2</cell><cell>11.9 ? 0.5</cell><cell>36.04</cell><cell cols="2">28.91</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">AG-Layout2im [27] 128 ? 128</cell><cell>E-D</cell><cell>-</cell><cell>8.5 ? 0.1</cell><cell>-</cell><cell cols="2">39.12</cell><cell>-</cell><cell>0.15 ? 0.09</cell></row><row><cell>Ours-D</cell><cell></cell><cell>128 ? 128</cell><cell>D</cell><cell cols="2">15.62 ? 0.05 12.69 ? 0.45</cell><cell>22.32</cell><cell cols="2">21.78</cell><cell>0.55 ? 0.09 0.54 ? 0.09</cell></row><row><cell>clouds</cell><cell></cell><cell>person</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Building-other</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bridge</cell><cell cols="2">Skyscrape</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>boat sea boat</cell><cell>r</cell><cell>Wall-other</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ti</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>e</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on COCO-Thing-Stuff dataset. baseline [38] context appearance Inception Score FID 13.8 ? 0.4 29.65 14.97 ? 0.27 24.05 15.28 ? 0.24 21.73 15.62 ? 0.05 22.32</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Specifying object attributes and relations in interactive scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mode regularized generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepfacedrawing: deep generation of face images from sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanchao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="72" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Context refinement for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sketchycoco: Image generation from freehand scene sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengying</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image captioning: Transforming objects into words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simao</forename><surname>Herdade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kofi</forename><surname>Boakye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bachgan: High-resolution image synthesis from salient object layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Demystifying neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to predict layout-to-image conditional convolutions for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attribute-guided image generation from layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC, 2020. 3, 4</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00951</idno>
		<title level="m">Encoding in style: a stylegan encoder for image-to-image translation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stabilizing training of generative adversarial networks through regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image synthesis from reconfigurable layout and style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning layout and style reconfigurable gans for controllable image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11571</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Object-centric image generation from layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Sylvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07449</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Context-based vision system for place and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust object detection under occlusion with contextaware compositionalnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angtian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatial-aware graph relation network for large-scale object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image generation from layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Layout2image: Image generation from layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
