<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D AGSE-VNet: An Automatic Brain Tumor MRI Data Segmentation Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Medical Technology and Information Engineering</orgName>
								<orgName type="institution">Zhejiang Chinese Medical University</orgName>
								<address>
									<postCode>310053</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Cardiovascular Research Centre</orgName>
								<orgName type="institution">Royal Brompton Hospital</orgName>
								<address>
									<postCode>SW3 6NP</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">National Heart and Lung Institute</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Ye</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">First Affiliated Hospital</orgName>
								<orgName type="institution" key="instit2">Gannan Medical University</orgName>
								<address>
									<postCode>341000</postCode>
									<settlement>Ganzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiji</forename><surname>Yang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">College of Life Science</orgName>
								<orgName type="institution">Zhejiang Chinese Medical University</orgName>
								<address>
									<postCode>310053</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomei</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Medical Technology and Information Engineering</orgName>
								<orgName type="institution">Zhejiang Chinese Medical University</orgName>
								<address>
									<postCode>310053</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Medical Technology and Information Engineering</orgName>
								<orgName type="institution">Zhejiang Chinese Medical University</orgName>
								<address>
									<postCode>310053</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Medical Technology and Information Engineering</orgName>
								<orgName type="institution">Zhejiang Chinese Medical University</orgName>
								<address>
									<postCode>310053</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D AGSE-VNet: An Automatic Brain Tumor MRI Data Segmentation Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Manuscript Type: Original Research Articles Main Body Word Count: 6708 Abstract Word Count: 311 Figures: 12 Tables: 4 * Co-first authors contributed equally. ? Corresponding Authors: G. Yang (g.yang@imperial.ac.uk) and X. Lai (dmia_lab@zcmu.edu.cn)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Brain Tumor</term>
					<term>Magnetic Resonance Imaging</term>
					<term>VNet</term>
					<term>Automatic Segmentation</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background Glioma is the most common brain malignant tumor, with a high morbidity rate and a mortality rate of more than three percent, which seriously endangers human health. The main method of acquiring brain tumors in the clinic is MRI. Segmentation of brain tumor regions from multi-modal MRI scan images is helpful for treatment inspection, post-diagnosis monitoring, and effect evaluation of patients. However, the common operation in clinical brain tumor segmentation is still manual segmentation, lead to its time-consuming and large performance difference between different operators, a consistent and accurate automatic segmentation method is urgently needed.</p><p>With the continuous development of deep learning, researchers have designed many automatic segmentation algorithms; however, there are still some problems: 1) The research of segmentation algorithm mostly stays on the 2D plane, this will reduce the accuracy of 3D image feature extraction to a certain extent. 2) MRI images have grayscale offset fields that make it difficult to divide the contours accurately.</p><p>Methods To meet the above challenges, we propose an automatic brain tumor MRI data segmentation framework which is called AGSE-VNet. In our study, the Squeeze and Excite (SE) module is added to each encoder, the Attention Guide Filter (AG) module is added to each decoder, using the channel relationship to automatically enhance the useful information in the channel to suppress the useless information, and use the attention mechanism to guide the edge information and remove the influence of irrelevant information such as noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We used the BraTS2020 challenge online verification tool to evaluate our approach. The focus of verification is that the Dice scores of the whole tumor (WT), tumor core (TC) and enhanced tumor (ET) are 0.68, 0.85 and 0.70, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion Although MRI images have different intensities, AGSE-VNet is not</head><p>affected by the size of the tumor, and can more accurately extract the features of the three regions, it has achieved impressive results and made outstanding contributions to the clinical diagnosis and treatment of brain tumor patients.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Glioma is one of the common types of primary brain tumors, accounting for about 50% of intracranial tumors <ref type="bibr" target="#b0">[1]</ref>. According to the WHO classification criteria, gliomas can be divided into four grades according to different symptoms, of which I and II are low-grade gliomas (LGG), III and IV are high-grade gliomas (HGG) <ref type="bibr" target="#b2">[2]</ref>. Due to the high mortality rate of glioma, it can appear in any part of the brain and people of any age, with various histological subregions and varying degrees of invasiveness <ref type="bibr" target="#b3">[3]</ref>.</p><p>Therefore, it has attracted widespread attention in the medical field. Because glioblastoma (GBM) cells are immersed in the healthy brain parenchyma and infiltrate the surrounding tissues, they can grow and spread rapidly near the protein fibers, and the deterioration process is very rapid. Therefore, early diagnosis and treatment are essential.</p><p>At present, the methods of acquiring brain tumors in clinical practice are mainly computed tomography (CT), positron emission tomography (PET), and magnetic resonance imaging (MRI) <ref type="bibr" target="#b4">[4]</ref>. Among them, MRI has become the preferred medical imaging method for brain diagnosis and treatment planning. Because it provides images with high-contrast soft tissue and high spatial resolution <ref type="bibr" target="#b5">[5]</ref>, it is a good representation of the anatomical structure of the cranial nerve soft tissue and the image of the lesion.</p><p>At the same time, MRI images can obtain multiple sequence information of brain tumors in different spaces through one scan. This information includes four sequences of T1 weighting (T1), T1-weighted contrast-enhanced (T1-CE), and T2 weighting (T2), fluid attenuation inversion recovery (FLAIR) <ref type="bibr" target="#b6">[6]</ref><ref type="bibr" target="#b7">[7]</ref>. However, manually segmenting tumors from MRI images requires professional prior knowledge, which is timeconsuming and labor-intensive, and is prone to errors, which is very dependent on the doctor's experience. Therefore, the development of an accurate, reliable, and fully automatic brain tumor segmentation algorithm has strong clinical significance.</p><p>With the development of computer vision and pattern recognition, convolutional neural networks have been implemented to solve many challenging tasks. For example, classification, segmentation and target detection capabilities have been greatly improved. In addition, deep learning technology shows great potential in medical image processing. So far, plenty of research studies on medical image segmentation have been developed in both academia and industry. VNet <ref type="bibr" target="#b8">[8]</ref> has good segmentation performance in single-modal images, but there are still some shortcomings for multi-modal segmentation. In this article, inspired by the integration of the "Project and Excite" (PE) module into the 3D U-net proposed by Anne-Marie et al <ref type="bibr" target="#b9">[9]</ref> , we proposed an automatic brain tumor MRI Data segmentation framework, which is called 3D AGSE-VNet. The network structure is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The main contributions of this paper are: 1)</p><p>Propose a combined segmentation model based on VNet, integrating SE module and AG module. 2) Using volume input, three-dimensional convolution is used to process MRI images. 3) Get excellent segmentation results, have the potential clinical application. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Traditional machine learning</head><p>At present, in clinical medicine, it is the goal that experts and scholars have been pursuing to use fully automatic segmentation methods to replace tedious manual segmentation or semi-automatic segmentation. It is also the focus and key technology of medical impact research in recent years. Traditional image processing brain tumor segmentation algorithms use threshold-based segmentation methods, region-based segmentation methods, and boundary-based segmentation methods. Image segmentation based on the threshold is one of the simplest and most traditional methods in image processing. Tustison et al proposed a two-stage segmentation framework based on Random Forest-derived probabilities, using the output of the first classifier to improve the segmentation result of the second classifier <ref type="bibr" target="#b10">[10]</ref>. Stadlbauer et al <ref type="bibr" target="#b11">[11]</ref> proposed using the normal distribution of data to obtain the threshold. According to the intensity change of each region, an adaptive threshold segmentation method was proposed to separate the foreground image from the background. However, this method has high limitations, and segmentation fails when multiple organizational structures overlap. Amiri et al <ref type="bibr" target="#b12">[12]</ref> proposed a multi-layer structure in which structural Random Forest and Bayesian networks are embedded to learn tumor features better, but inputting a large number of features can easily lead to dimensional disasters and waste plenty of time <ref type="bibr" target="#b13">[13]</ref> uses a seed region growth algorithm to process brain MRI images according to the threshold T and the generation of PD images, and then uses the Markov logic algorithm to process them further to improve segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep learning</head><p>In recent years, convolutional neural networks have become the most popular method in image classification, and they are widely used in medical image analysis. S?rgio Pereira et al. <ref type="bibr" target="#b14">[14]</ref> proposed an automatic positioning method based on Convolutional Neural Network (CNN) to explore the 3?3 core size, use a small core to design a deeper architecture, and use intensity normalization is used as a preprocessing step to train the core validation data set in BraTS 2015. In the article by Hao et al., a fully automatic brain tumor segmentation method based on U-net deep convolutional network was proposed and evaluated on the BraTS 2015 dataset. Cross-validation shows that it can effectively obtain promising segmentation <ref type="bibr" target="#b15">[15]</ref>. Wang et al. proposed a cascade network. The first step is to segment the entire tumor. The second step is to segment the tumor nucleus using the obtained bounding box and segment the enhanced tumor nucleus according to the bounding box of the tumor nucleus segmentation result.</p><p>Use anisotropic convolution and unfolded convolution, combined with multi-view fusion methods to reduce false positives <ref type="bibr" target="#b16">[16]</ref>. Andriy Myronenko proposed a 3D MRI tumor subregion segmentation semantic network based on the encoder-decoder structure, which uses auto-encoder branches to reconstruct images, and won first place in the 2018 BraTS Challenge <ref type="bibr" target="#b17">[17]</ref>. Feng Xue proposed an integrated 3D U-net brain tumor segmentation method. Using an integrated modelling method, the encoder and decoder are input into 6 networks with different colour block sizes and loss weights, and training has improved various performances <ref type="bibr" target="#b18">[18]</ref>. In 2019, Nabilibtehaz et al.</p><p>developed a novel architecture based on u-net, multires-unet, which increased the extension of residual connections and proposed the residual path (respath). It has verified its use in ISIC and BraTS. Good segmentation performance on the dataset <ref type="bibr" target="#b20">[19]</ref>. proposed an effective 3D residual neural network for brain tumor segmentation, using a computationally efficient network 3D shuffleNetV2 as an encoder, and introducing a decoder with residual blocks to achieve high-efficiency segmentation <ref type="bibr" target="#b22">[21]</ref>. Saman et al.</p><p>proposed an active contour model driven by optimized energy function for MR brain tumor segmentation with uneven intensity correction and a method to identify and segment brain tumor slices in MRI images <ref type="bibr" target="#b23">[22]</ref>. Liu et al. studied a deep learning model based on learnable group convolution and deep supervision. This method replaces the convolution in the feature extraction stage with learnable group convolution. Tests on the BraTS2018 dataset show that the segmentation effect on the core area of the tumor is perfect, surpassing the winning method NVDLMED <ref type="bibr" target="#b24">[23]</ref>. In addition, CNN has also been widely used in other medical image analysis tasks. For example, Yurttakal et al.</p><p>used the convolutional neural network method for laryngeal histopathological image segmentation, which is of great help to the early detection, monitoring and treatment of laryngeal cancer, and rapid and accurate tumor segmentation <ref type="bibr" target="#b25">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Our work</head><p>Although many experts and scholars have proposed a variety of deep learning network structures, and have achieved good results in the field of brain tumor segmentation. However, due to the inherent anisotropy of brain glial tumors, MRI images show a high degree of non-uniformity and irregular shapes <ref type="bibr" target="#b26">[25]</ref>. Secondly, the segmentation method of deep learning requires large-scale annotation data, while brain tumor data is generally small and complex, and its inherent high heterogeneity will cause intra-class differences between the sub-regions of the brain tumor area and the tumor area, the difference between classes and non-tumor areas, etc <ref type="bibr" target="#b27">[26]</ref>, these problems all affect the accuracy of brain tumor segmentation.</p><p>In this article, to meet the above challenges, we use a combined model, integrate the "Squeeze and Excite" (SE) module and the "Attention Guide Filter" (AG) module into the VNet model for image segmentation of 3D MRI glioma brain tumors, it is an end-to-end network structure. We input data into the model in the form of volume input and use three-dimensional convolution to process MRI images. When the image is compressed along with different encoder blocks, the resolution is halved, and the number of channels increases. After the image is convolved, the compression and compression module is performed. The importance of each feature channel is automatically obtained through learning. Then according to this important level to promote useful functions, and cancel the less useful functions of the current task. Each decoder receives the characteristics of the corresponding stage of downsampling and decompresses the image, in the upsampling, the AG module is integrated, the Attention block is used to eliminate the influence of noise and irrelevant background, and the Guide Image Filtering is used to guide image features and structural information (edge information), it is worth mentioning that the idea of skip connection is used in the model to avoid the disappearance of the gradient. Besides, we also use the Categorical_Dice loss function as the optimization function of the model, which effectively solves the problem of pixel imbalance.</p><p>We tested the performance of this model on the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2020 dataset and compared it with the results of other teams participating in the challenge. The results show that our model has a good segmentation effect and has the potential for clinical trials. The innovations of this article are: 1) Clever use of channel relationships, using global information to enhance useful information in the channel, to suppress useless information in the channel. 2)</p><p>The attention mechanism is added, and the network structure is also full of jump connections. The information extracted by the downsampling can be quickly captured to enhance the performance of the model. 3) Use the Categorical_Dice loss function to solve the problem of imbalance between foreground voxels and background voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method summary</head><p>Our task is to segment multiple sequences of 3D MRI brain tumor images. In order to obtain good segmentation performance, we propose a new network structure called AGSE-VNet, which combines SE (Squeeze-and-Excitation) <ref type="bibr" target="#b28">[27]</ref> module with AG (Attention Guided Filter) module <ref type="bibr" target="#b29">[28]</ref> is integrated into the network structure, allowing the network to use global information to enhance useful feature channels selectively and suppress useless feature channels, cleverly solving the mutual dependence of feature maps, effectively suppressing the background information of the image, and enhancing the accuracy of model segmentation. In the next section, we will introduce the network structure of AGSE-VNet in detail. in the figure is a standard convolution operation, as shown in formula (1), input as X, X?R Z ' ?W ' ?H ' ?C ' , where is the depth, is the height is the width, is the number of channels, the output is , ? ? ? ? , is a three-dimensional spatial convolution, means that each channel acts on the corresponding channel feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Squeeze-and-Excitation Blocks</head><formula xml:id="formula_0">= ? = ? ? =1 ? (1) Fig. 2. SE network module diagram.</formula><p>(?) is the squeeze operation. As shown in formula <ref type="formula">(2)</ref>, the feature first passes the Squeeze operation. It compresses the features along the spatial gradient and aggregates the feature maps into the feature maps of dimension ? as the feature descriptor. Each three-dimensional feature channel becomes a real number, which responds to the global distribution on the feature channel, to a certain extent, the real number at this time is closer to the global receptive field. This operation transforms the input of ? ? into 1 ? 1 ? output.</p><formula xml:id="formula_1">= ( ) = 1 ? ? = ? ? ? ( , , ) =1 =1 =1 (2)</formula><p>As shown in Equation 3, in order to limit the complexity and generalization of the model, using two fully connected layers as a parameterized gating mechanism. In Equation 3, 1 ? represents a fully connected layer operation. The dimension of</p><formula xml:id="formula_2">1 is ? .</formula><p>Here is a scaling parameter. In this article, we set = 4</p><p>empirically. The parameter aims is to reduce the number of channels and thus reduce the number of calculations. Then through a ReLU layer, the output dimension remains unchanged and then multiplying with 2 . This process of multiplying with 2 is also an operation of a fully connected layer. The dimension of 2 is ? , and finally, through the Sigmoid function, the parameter is obtained.</p><formula xml:id="formula_3">= ( , ) = ( ( , )) = ( 2 ( 1 , ))<label>(3)</label></formula><p>Where is the ReLU operation, 1 ? ? , 2 ? ? , and finally a 1 ? 1 ? real number sequence is combined with , recalibrated, and the final output is obtained by formula <ref type="formula" target="#formula_4">(4)</ref>.</p><formula xml:id="formula_4">?= ( , ) = ?<label>(4)</label></formula><p>Among them, = [ 1 , 2 , ? , ] and ( , ) refer to the corresponding channel between the feature map ? ? and the scalar .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Guided Filter Blocks</head><p>Attention Guided Filter (AG) module combines attention block and guided image filtering. The Attention Guided Filter filters the low-resolution feature maps and highresolution feature maps to recover spatial information and merge structural information from feature maps of different resolutions.   Different from the guided filtering proposed by Kaiming He <ref type="bibr" target="#b30">[29]</ref>, the attention feature map is generated from the filtered feature map ( ) through the Attention Block module. First, the guided feature map is down-sampled to obtain a lowresolution feature map , which is similar to the feature map Same size. Then minimize the reconstruction error of and to obtain the coefficients and of the attention guided filter. After that, by down-sampling and or coefficients ? and ? , finally get the high resolution generated by the attention filter Feature map ?. Among them, the attention filter is essentially a specific window with a radius of . In particular, the attention guide filter will construct a square window , and the radius of at each position is . In our study, we set = 16 and = 0.1 2 empirically based on the final segmentation performance. ( , ) is also the only certain constant coefficient, as shown in formula <ref type="formula">(6)</ref>, where ridge regression with a standard term is used to calculate the minimum reconstruction error.</p><formula xml:id="formula_5">min , ( , ) ? ? ( 2 ( + ? ) 2 + 2 ) ? (6)</formula><p>Where is the attention weight at position , is the regularization parameter, and the calculation of ( , ) is shown in formula <ref type="bibr" target="#b7">(7)</ref>.</p><formula xml:id="formula_6">= 1 | | ? ( ? ???? ) ? 2 + , = ???? ?<label>(7)</label></formula><p>Where is the average value of the window pixels in the image , 2 is the variance of the window , | | is the sum of the window pixels, and is the</p><formula xml:id="formula_7">average pixel value = 1 | | ? ?</formula><p>of the image to be filtered in the window , so that the non-edge area can be found if a pixel is surrounded by multiple windows, calculate the average value of all windows containing the pixel at that pixel for such a pixel, as shown in formula (8).</p><formula xml:id="formula_8">= 1 | | ? ( + ) = * , ? +<label>(8)</label></formula><p>Get ? and ? through upsampling, and finally get the output ?= ? * + ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Downsamplings</head><p>In <ref type="figure" target="#fig_0">Fig. 1</ref>, we provide a schematic diagram of AGSE-VNet. The network structure is divided into encoder and decoder in total, as shown in <ref type="figure">Fig</ref> and excitation layer and a downsampling layer, the processing process of the SE module is shown on the right side of <ref type="figure" target="#fig_6">Fig. 5(a)</ref>. The feature extraction is performed by convolution with a step size of 2. The convolution is as follows (9) (10) shows:</p><formula xml:id="formula_9">= + ( ? 1)( ? 1) (9) = [ +2 ? + 1]<label>(10)</label></formula><p>Where is the input size, is the output size after filling, is the step size, is the filling size, is the convolution kernel size, and is the output size. When the image is compressed along with different encoder blocks, its resolution is halved and the number of channels doubled. This is achieved by convolution of 3 ? 3 ? 3 voxels with a step size of 2. After the convolution operation, the squeeze and compression module is performed, which ingeniously solves the relationship between the channels and improves the effective information transmission in the channels. It is worth mentioning that all convolutional layers have adopted normalization and dropout processing, and the ReLU activation function has also been applied to various positions in the network structure. Besides, a jump connection method is also used in the model to avoid the disappearance of the gradient as the network structure deepens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Upsampling</head><p>After downsampling the model, we introduced the AG module to solve the problem of restoring spatial information and fusing structural information from lowresolution feature maps to high-resolution feature maps. The AG module is similar to the SE module. Based on not changing the dimensions of input and output, the features are enhanced. Therefore, we replace the splicing module in the VNet model with the AG module and integrate it into the decoder. The structure diagram is shown in <ref type="figure" target="#fig_6">Fig. 5</ref>.</p><p>Each decoder block includes an upsampling layer, an AG module, and three layers of convolution, the processing flow of the AG module is shown in the box on the right side of <ref type="figure" target="#fig_6">Fig.5(b)</ref>. The decoder decompresses the image. In the up-sampling, this article uses deconvolution with a step size of 2 to fill in the image feature information. The deconvolution is shown in formula (11):</p><formula xml:id="formula_10">= ( ? 1) + 2 ? + 2<label>(11)</label></formula><p>Each decoder block receives the characteristics of the corresponding stage of downsampling. The convolution kernel used in the last layer of the network structure keeps the number of output channels consistent with the number of categories. Finally, the channel value is converted into a probability value output through the sigmoid function, and the voxel is converted into a brain tumor gangrene area. The idea of skip connection is adopted in each decoder block. The feature map after processing by the encoder and decoder is shown in <ref type="figure" target="#fig_8">Fig. 6</ref>., where <ref type="figure" target="#fig_8">Fig. 6(a)</ref> is a feature map processed by the encoder, and <ref type="figure" target="#fig_8">Fig. 6(b)</ref> is a feature map processed by the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Skip connection</head><p>To further make up for the information lost in the downsampling of the encoder, concat is used between the encoder and decoder of the network to fuse the feature maps of the corresponding positions in the two processes. In particular, the method extracted in this article uses the AG (Attention Guided Filter Blocks) module instead of concat, so that the decoder can obtain information during upsampling. With more highresolution information, the detailed information in the original image can be restored more perfectly, and the segmentation accuracy can be improved. We introduced adjacent layer feature reconstruction and cross-layer feature reconstruction in the network. The cross-layer feature reconstruction module is based on the encoder-decoder structure. In the process of network communication, as the network continues to deepen, the acceptance domain of the corresponding feature map will become larger and larger, but the retained detailed information will become less and less. Based on the encoder-decoder symmetric structure, the splicing layer is used to splice the feature maps extracted from the down-sampling in the encoder process and the new features obtained from the up-sampling in the decoder process to perform channel-dimensional splicing. Retaining more important feature information is conducive to achieving a better segmentation effect. Adjacent layer feature reconstruction is to establish a branch between each pair of adjacent convolutional layers with the same size feature map, that is, use the splicing layer to convolve the feature map obtained through the convolution of the previous layer and the next layer.</p><p>Obtaining the channel size achieves the purpose of maximizing the use of feature information in all previous layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Loss function</head><p>At present, the segmentation of medical images faces the problem of the imbalance between the foreground and the background regions. We also face such challenges in our tasks. Therefore, we choose the Categorical_Dice loss function as the optimization function of our model. Heavy to solve this problem by adjusting the weight of each forecast category. We set the weight of gangrene, edema, and enhanced tumor to 1, and the weight of background to 0.1. The Categorical_Dice loss function is shown in formula <ref type="formula" target="#formula_11">(12)</ref>: </p><formula xml:id="formula_11">( , ) = 2| ? | | |+| |<label>(12)</label></formula><p>The weight distribution of the loss function of each node is shown in formula <ref type="bibr" target="#b14">(14)</ref>, </p><formula xml:id="formula_13">and</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>In this research, we use the dataset of the BraTS 2020 challenge to train and test our model <ref type="bibr" target="#b31">[30]</ref><ref type="bibr" target="#b32">[31]</ref>. The data set contains two types, namely low-grade glioma (LGG) and glioblastoma (HGG), each category has four modal images: T1 weighting (T1), T1weighted contrast-enhanced (T1-CE), and T2 weighting (T2), fluid attenuation inversion recovery (FLAIR). The mask of the brain tumor includes the gangrene area, edema area, and enhancement area. Our task is to segment the three sub-regions formed by nesting tags, which are enhancement tumor (ET), whole tumor (WT), and tumor core (TC).</p><p>There are 369 cases in the training set and 125 cases in the validation set. The masks corresponding to these cases are not used for training, and their functions are mainly used for evaluating the model after training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Design Detail</head><p>In deep learning training, the setting of hyperparameters is very essential, and it will determine the performance of our model. But often in training, the initial value of the hyperparameter is set by experience. In the training of the AGSE-VNet model, the initial learning rate is set to 0.0001, the dropout is set to 0.5, the number of training steps is about 350,000, and then the learning rate is adjusted to 0.00003. The dataset is halved every time it is traversed, and the data is shuffled to enhance the robustness and generalization ability of the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pre-Processing</head><p>Since our data set has four modalities, T1, T1-CE, T2, and FLAIR, there is a problem of different contrast, which may cause the gradient to disappear during the training process, so we use standardization to process the image, from the image pixel.</p><p>The image data is normalized by subtracting the average value and dividing by the standard deviation. Calculated as follows:</p><formula xml:id="formula_14">?= ?<label>(15)</label></formula><p>where, donates the mean of the image, donates standard deviation, donates the image matrix, ? is the normalized image matrix.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation Metrics</head><p>We use the dice coefficient, specificity, sensitivity, and Hausdorff95 distance to measure the performance of our model. Dice coefficient is calculated as:</p><formula xml:id="formula_15">= 2 + +2<label>(16)</label></formula><p>where , and are the number of true positive, false positive, and false negative respectively. Specificity can be used to evaluate the number of true negative and false positive, it is used to measure the model ability to predict the background area, defined as:</p><formula xml:id="formula_16">= +<label>(17)</label></formula><p>where is the number of true negative. Sensitivity can be used to evaluate the number of the true positive and false negative, it is used to measure the sensitivity of the model to segmented regions, defined as:</p><formula xml:id="formula_17">= +<label>(18)</label></formula><p>The Hausdorff95 distance measures the distance between the surface of the real area and the predicted area, which is more sensitive to the segmented boundary, defined as:</p><formula xml:id="formula_18">95( , ) = max {sup ? , ? ( , ) , sup ? , ? ( , )}<label>(19)</label></formula><p>where inf denotes the infimum and sup denotes the supremum, and donate the points on the surface of the ground-truth area and the surface of the pre dataset dictated area. Besides, (?,?) calculates the distance between the assembly point and the assembly point .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on AGSE-VNet model.</head><p>Our data set includes a training set and a test set. The training set contains 369 cases and the test set contains 125 cases. The mask of the tumor includes the gangrene area, edema area, enhancement area, and background area. The labels correspond to 1, 2, 4, and 0, respectively. These labels are merged into three nested sub-areas, namely the enhancing tumor (ET), the whole tumor (WT), and the tumor core (TC), for these sub-regions, we use four indicators of sensitivity, specificity, dice coefficient, and</p><p>Hausdorff95 distance to measure the performance of the model. We use the data set of BraTS 2020 for training and verification, and the average index obtained is shown in <ref type="table" target="#tab_2">Table 1</ref>. From <ref type="table" target="#tab_2">Table 1</ref>, we observe that the model has a better segmentation effect on the WT region. The Dice and Sensitivity of the training set and the validation set are 0.846, 0.849, 0.825, and 0.833, respectively, which are significantly better than other regions. On this basis, we conduct a statistical analysis of the experimental results. <ref type="figure" target="#fig_13">Fig. 8</ref> and <ref type="figure" target="#fig_14">Fig. 9</ref>  Specificity are at a higher level, which shows that the segmentation effect of our proposed model is located in a higher area. In the results of the four indicators, the sensitivity results are all concentrated at a higher level. It can be seen that the fluctuation range is small. Observing the scatter diagram on the left side, it can be seen that the data are all clustered at a higher position, indicating that our model is the background area has a high level of prediction, which can effectively alleviate the problem of imbalance between foreground pixels and background. We randomly selected several slices from the training set and compared the actual situation with the results predicted by our model, as shown in <ref type="figure" target="#fig_0">Fig. 10(a)</ref>, the first line is the original image, the second line is the label, and the third line is the tumor subregion predicted by our model. At the same time, we also selected two of them to display in <ref type="figure" target="#fig_0">Fig. 10(b)</ref>. Among them, the green area is the whole tumor (WT), the red area is the tumor core (TC), and the area combining yellow and red represents the enhancing tumor(ET). We show the 3D image of the segmentation result in the last two columns. From the comparison of the segmentation results, we can find that our model has a good effect on brain tumor segmentation, especially the whole tumor (WT) region segmentation effect is excellent. However, the segmentation prediction of the tumor core (TC) is slightly biased, which may not be suitable for extraction due to the small features of the tumor core.  After the training is completed, we randomly selected several segmentation slices in the validation set for display, as shown in <ref type="figure" target="#fig_0">Fig. 11(a)</ref>. Similarly, in <ref type="figure" target="#fig_0">Fig. 11(b)</ref>, we also</p><p>show the three-dimensional image of the segmentation result and annotate the accuracy value of the ET region, as can be seen from the figure, our model has a good segmentation effect for MRI images of different intensities, and can accurately segment tumor sub-regions, which has a certain potential in brain tumor image segmentation.</p><p>In our research, we proposed the AGSE-VNet model to segment 3D MRI brain tumor images and obtained better segmentation results on the BraTS 2020 dataset. In order to further verify the effect of our segmentation, compare our experimental method with the methods proposed by other outstanding teams participating in the competition. indicating that the method we proposed has a certain potential in segmentation. <ref type="figure" target="#fig_0">Fig. 11</ref>. Display of segmentation results in the validation set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Discussion</head><p>The method proposed in this paper cleverly solves the problem of interdependence between channels, and autonomously extracts effective features from channels to suppress useless feature channels. After the features extracted by the encoder, lowresolution feature maps and high-resolution feature maps are filtered through the Attention module, to recover spatial information and fusion structural information from feature maps of different resolutions, our method is not affected by the size and location of the tumor. For MRI images of different intensities, the tumor area can be automatically identified, and the tumor sub-regions can be feature extracted and segmented, and the segmentation effect obtained has a good performance. This is beneficial to radiologists and oncologists, who can quickly predict the condition of the tumor and assist in the treatment of the patient. Comparing the results in <ref type="table" target="#tab_4">Table 2</ref> and <ref type="table" target="#tab_5">Table 3</ref>, we find that our model performs well in the whole tumor (WT) area, but does not perform well in the enhancing tumor (ET) and the tumor core (TC) areas, this may be because the target in the ET area is small and the feature is fuzzy and difficult to extract. At the same time, we compare our method with some classic algorithms for brain tumor segmentation. The results are shown in  Analyzing <ref type="table" target="#tab_6">Table 4</ref>, we found that our model has certain advantages in segmentation, there are still differences in TC regional accuracy, and the model has limitations. In future work, we will propose solutions to this situation, such as how to further segment the region of interest after our model has extracted it, in order to improve the accuracy of the enhancing tumor (ET) and the tumor core (TC) areas, more characteristic information can be captured. Besides, the algorithms proposed in many top methods have their areas of excellent performance. How we combine the advantages of these algorithms and integrate them into our model is the focus of our future work. In clinical treatment, it helps experts to understand the patient's current situation more quickly and accurately, saving experts time, and realizing a leap in the quality of automatic medical segmentation.</p><p>In addition, in order to verify the robustness of our model to resist noise interference, we have now added Gaussian noises in the frequency domain (k-space) of the testing data to simulate realistic noise contaminations. The comparison results are shown in <ref type="figure" target="#fig_0">Fig.12</ref>. From the noisy and no-noise segmentation results, we have found that the segmentation results of our AGSE-VNet model for the three regions are not much different. These results can demonstrate that our model has a significant advantage in generalization when noises are present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>All in all, we have implemented a good method to segment 3D MRI brain tumor images, this method can automatically segment the three regions of the enhancing tumor (ET), the whole tumor (WT), and the tumor core (TC) of the brain tumors. We conducted experiments on the BraTS 2020 data set and got good results. The AGSE-VNet model is improved based on VNet. There are five encoder blocks and four decoder blocks. Each encoder block has an extrusion and excitation block, and each decoder has an Attention Guild Filter block. Such a design can be embedded in our model without affecting the size mismatch of the network structure under the condition that the input ratio and output ratio are unchanged. After the SE module processes the model, the network learns the global information and selects the useful information in the enhancement channel, and then uses the attention mechanism of the Attention Guild Filter block to quickly capture its dependencies and enhance the performance of the model. Secondly, we also introduced a new loss function Categorical_Dice, set different weights for unused masks, set the weight of the background area to 0.1, and set the tumor area of interest to 1, Ingeniously solve the problem of the voxel imbalance between the foreground and the background. Our online verification tool on the BraTS Challenge website evaluated this approach. It is found that our model is still different from the top methods for the segmentation of the enhancing tumor (ET) and the tumor core (TC) regions. It may be because the features of these two regions are small and difficult to extract. How to improve the accuracy of these two regions is our future work direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The overall architecture of the proposed 3D AGSE-VNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Xu et al. proposed progressive sequential causality to synthesize high-quality LGRequivalent images and accurately segment all tissues related to the diagnosis to obtain highly accurate diagnostic indicators in a real clinical environment [20]. Zhou et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>is a schematic diagram of the SE module, which mainly includes the Squeeze module and the Excitation module. The core of the module is to recalibrate the characteristic response of the channel adaptively by explicitly modeling the interdependence between the channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>is a schematic diagram of the Attention Block, where and are the input of the attention guided filter, and the attention map obtained by the calculation. Attention Block is extremely critical in this method. It effectively solves the influence of the background on the foreground and has the effect of highlighting the foreground and reducing the background. For the given feature maps and , use convolution with a channel of 1 ? 1 ? 1 to perform a linear transformation, and then combine the two converted feature maps with the ReLU layer through element addition, and then use a 1 ? 1 ? 1 . The convolution is again linearly transformed, and the sigmoid is most used to activate the final attention feature map .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Attention Block schematic diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 Fig. 4 .</head><label>44</label><figDesc>is a schematic diagram of the results of the AG module. The input is the guided feature map ( ) and the filtered feature map ( ), and the output is the highresolution feature map (?), which is the product of the joint action of and , as shown in formula<ref type="bibr" target="#b5">(5)</ref>. Attention Guided Filter Blocks structure diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>. 5 .</head><label>5</label><figDesc>Among them, Fig. a is the encoder, and the coding area mainly performs compression path, and Fig. b is the decoder, and the decoding area performs decompression. Downsampling is composed of four encoder blocks, each of which includes 2-3 layers of convolution, an extrusion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>The architecture of encoder block and decoder block with AGSE-VNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Feature map processed by encoder and decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>the weight value is [0.1,1.0,1.0,1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>After normalization, we merge the images of the four modalities with the same contrast to form a three-dimensional image with four channels. The original image size is, and the combined image size becomes. The size of the label is, and its pixel value contains 4 different values. Channel 0 is the normal tissue area, 1 is the gangrene area, 2 is the edema area, and 3 is the enhanced tumor area. Then, divide the image and mask into multiple blocks and perform the patch operation. Each case generates 175 images with a size of 128?128?64. Finally, save it in the corresponding folder in NumPy .npy format (https://numpy.org/doc/stable/reference/). The preprocessed image is shown inFig. 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Preprocessed result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>are the scatter plots and box plots of the four evaluation indicators of the training set and test set, reflecting the distribution characteristics of the results. It can be seen from the box plot that there are fewer outliers of various indicators and minimal fluctuation of results. The horizontal line in the box plot represents the median of this set of data. It can be observed that the three indicators of Dice, Sensitivity, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 .</head><label>8</label><figDesc>A collection of scatter plots and box plots of four indicators in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 9 .</head><label>9</label><figDesc>A collection of scatter plots and box plots of four indicators in the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 10 .</head><label>10</label><figDesc>Display of segmentation results in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 12 .</head><label>12</label><figDesc>Comparison of segmentation results without noise and noise added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative valuation on the training set and validation set.</figDesc><table><row><cell></cell><cell></cell><cell>Dice</cell><cell></cell><cell></cell><cell>Sensitivity</cell><cell></cell><cell></cell><cell>Specificity</cell><cell></cell><cell cols="3">Hausdorff95</cell></row><row><cell></cell><cell>ET</cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>WT</cell><cell>TC</cell></row><row><cell>Training</cell><cell>0.70</cell><cell>0.85</cell><cell>0.77</cell><cell>0.72</cell><cell>0.83</cell><cell>0.74</cell><cell>0.99</cell><cell>0.99</cell><cell cols="4">0.99 35.70 8.96 17.40</cell></row><row><cell cols="2">Validation 0.68</cell><cell>0.85</cell><cell>0.69</cell><cell>0.68</cell><cell>0.83</cell><cell>0.65</cell><cell>0.99</cell><cell>0.99</cell><cell cols="4">0.99 47.40 8.44 31.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The results of other teams are available on the official website of the BraTS Multimodal</figDesc><table><row><cell>Brain</cell><cell>Tumor</cell><cell>Segmentation</cell><cell>Challenge</cell><cell>2020</cell></row><row><cell cols="5">(https://www.cbica.upenn.edu/BraTS20/lboardTraining.html). The comparison results</cell></row></table><note>of the training set are shown in Table 2, and the comparison results of the verification set are shown in Table 3. From the results in the table, we can find that our model performs well in the whole tumor (WT) region and obtains relatively excellent results,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The results of various indicators in the training set</figDesc><table><row><cell></cell><cell></cell><cell>Dice</cell><cell></cell><cell></cell><cell>Sensitivity</cell><cell></cell><cell></cell><cell>Specificity</cell><cell></cell><cell></cell><cell>Hausdorff95</cell><cell></cell></row><row><cell>Team</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ET</cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>WT</cell><cell>TC</cell></row><row><cell>proposed</cell><cell>0.70</cell><cell>0.85</cell><cell>0.77</cell><cell>0.72</cell><cell>0.83</cell><cell>0.74</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>35.70</cell><cell>8.96</cell><cell>17.40</cell></row><row><cell>mpstanford</cell><cell>0.60</cell><cell>0.78</cell><cell>0.72</cell><cell>0.56</cell><cell>0.80</cell><cell>0.75</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>35.95</cell><cell>17.68</cell><cell>17.21</cell></row><row><cell>agussa</cell><cell>0.67</cell><cell>0.87</cell><cell>0.79</cell><cell>0.69</cell><cell>0.87</cell><cell>0.82</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>39.25</cell><cell>15.75</cell><cell>17.05</cell></row><row><cell>ovgu_seg</cell><cell>0.65</cell><cell>0.81</cell><cell>0.75</cell><cell>0.72</cell><cell>0.78</cell><cell>0.76</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>34.79</cell><cell>9.50</cell><cell>8.93</cell></row><row><cell>AI-Strollers</cell><cell>0.59</cell><cell>0.73</cell><cell>0.61</cell><cell>0.52</cell><cell>0.73</cell><cell>0.64</cell><cell>0.99</cell><cell>0.97</cell><cell>0.98</cell><cell>38.87</cell><cell>20.81</cell><cell>24.22</cell></row><row><cell>uran</cell><cell>0.48</cell><cell>0.79</cell><cell>0.64</cell><cell>0.45</cell><cell>0.74</cell><cell>0.61</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>37.92</cell><cell>7.72</cell><cell>14.07</cell></row><row><cell>CBICA</cell><cell>0.54</cell><cell>0.78</cell><cell>0.57</cell><cell>0.64</cell><cell>0.82</cell><cell>0.53</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>20.00</cell><cell>46.30</cell><cell>39.60</cell></row><row><cell>unet3d-sz</cell><cell>0.69</cell><cell>0.81</cell><cell>0.75</cell><cell>0.77</cell><cell>0.93</cell><cell>0.83</cell><cell>0.99</cell><cell>0.96</cell><cell>0.98</cell><cell>37.71</cell><cell>19.57</cell><cell>18.36</cell></row><row><cell>iris</cell><cell>0.76</cell><cell>0.88</cell><cell>0.81</cell><cell>0.78</cell><cell>0.90</cell><cell>0.83</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>32.30</cell><cell>18.07</cell><cell>14.70</cell></row><row><cell>VuongHN</cell><cell>0.74</cell><cell>0.81</cell><cell>0.82</cell><cell>0.84</cell><cell>0.98</cell><cell>0.84</cell><cell>0.95</cell><cell>0.93</cell><cell>0.99</cell><cell>21.97</cell><cell>12.32</cell><cell>8.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The results of various indicators in the validation set</figDesc><table><row><cell></cell><cell>Dice</cell><cell></cell><cell></cell><cell cols="2">Sensitivity</cell><cell></cell><cell cols="2">Specificity</cell><cell></cell><cell cols="2">Hausdorff95</cell><cell></cell></row><row><cell>Team</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ET</cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>WT</cell><cell>TC</cell></row><row><cell>proposed</cell><cell>0.68</cell><cell>0.85</cell><cell>0.69</cell><cell>0.68</cell><cell>0.83</cell><cell>0.65</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>47.40</cell><cell>8.44</cell><cell>31.60</cell></row><row><cell>mpstanford</cell><cell>0.49</cell><cell>0.72</cell><cell>0.62</cell><cell>0.49</cell><cell>0.81</cell><cell>0.69</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>61.89</cell><cell>26.00</cell><cell>28.02</cell></row><row><cell>agussa</cell><cell>0.59</cell><cell>0.83</cell><cell>0.69</cell><cell>0.60</cell><cell>0.87</cell><cell>0.71</cell><cell>0.99</cell><cell>0.99</cell><cell>.0.99</cell><cell>56.58</cell><cell>23.23</cell><cell>29.59</cell></row><row><cell>ovgu_seg</cell><cell>0.60</cell><cell>0.79</cell><cell>0.68</cell><cell>0.66</cell><cell>0.79</cell><cell>0.67</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>54.07</cell><cell>12.05</cell><cell>19.10</cell></row><row><cell>AI-Strollers</cell><cell>0.58</cell><cell>0.74</cell><cell>0.61</cell><cell>0.52</cell><cell>0.77</cell><cell>0.62</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>47.23</cell><cell>24.03</cell><cell>31.54</cell></row><row><cell>uran</cell><cell>0.75</cell><cell>0.88</cell><cell>0.76</cell><cell>0.77</cell><cell>0.85</cell><cell>0.71</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>36.42</cell><cell>6.62</cell><cell>19.30</cell></row><row><cell>CBICA</cell><cell>0.63</cell><cell>0.82</cell><cell>0.67</cell><cell>0.76</cell><cell>0.78</cell><cell>0.75</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>9.60</cell><cell>10.70</cell><cell>28.20</cell></row><row><cell>unet3d-sz</cell><cell>0.70</cell><cell>0.84</cell><cell>0.72</cell><cell>0.71</cell><cell>0.87</cell><cell>0.79</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>42.09</cell><cell>10.48</cell><cell>12.32</cell></row><row><cell>iris</cell><cell>0.68</cell><cell>0.86</cell><cell>0.73</cell><cell>0.67</cell><cell>0.90</cell><cell>0.70</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>44.13</cell><cell>23.87</cell><cell>20.02</cell></row><row><cell>VuongHN</cell><cell>0.79</cell><cell>0.90</cell><cell>0.83</cell><cell>0.80</cell><cell>0.89</cell><cell>0.80</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell><cell>21.43</cell><cell>6.74</cell><cell>7.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>In the BraTS Challenge, 2018, zhou et al.[32] and others proposed a lightweight one-step multi-task segmentation model, by learning the shared parameters of joint features and the composition features of distinguishing specific task parameters, the imbalance factors of tumor types are effectively alleviated, uncertain information is suppressed, and the segmentation result is improved. In the method proposed by Zhao et al., a new segmentation framework was developed, using a fully convolutional neural network to assign different labels to the image in pixel units, optimize the output results of FCNNs by using the recurrent neural network constructed by the conditional random place, this method was verified on the BraTS 2016 dataset and got a good segmentation effect. Pereira et al. proposed an automatic positioning method for convolutional neural networks, which achieved good results in the BraTS 2015 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison of our proposed AGSE-VNet model with classic methods</figDesc><table><row><cell>Method</cell><cell>Dice_ET</cell><cell>Dice_WT</cell><cell>Dice_TC</cell><cell>Dataset</cell></row><row><cell>Proposed</cell><cell>0.67</cell><cell>0.85</cell><cell>0.69</cell><cell>BraTs 2020</cell></row><row><cell>Zhou et al.</cell><cell>0.65</cell><cell>0.87</cell><cell>0.75</cell><cell>BraTs 2018</cell></row><row><cell>Zhao et al.</cell><cell>0.62</cell><cell>0.84</cell><cell>0.73</cell><cell>BraTs 2016</cell></row><row><cell>Pereira et al.</cell><cell>0.65</cell><cell>0.78</cell><cell>0.75</cell><cell>BraTs 2015</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The automatic segmentation of brain tumors in the medical field has been a longterm research problem. How to design an automatic segmentation algorithm with short time and high accuracy, and then form a complete system is the current direction of a large number of researchers. Therefore, we must continue to optimize our segmentation model to achieve a qualitative leap in the field of automatic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics approval and consent to participate</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent for publication</head><p>-All authors contributed to the article and approved the submitted version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>The datasets analysed during this current study are available in the BRATS 2020.</p><p>https://www.med.upenn.edu/cbica/brats2020/data.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work is funded in part by the National Natural Science Foundation of China </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Automated Brain Tumor Segmentation Using Spatial Accuracy-Weighted Hidden Markov Random Field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Setayesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><forename type="middle">G</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Comput Med Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="431" to="441" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation. Progression Assessment, and Overall Survival Prediction in the BRATS Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with Vander Lugt correlator based active contour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Essadike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ouabida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">A</forename><surname>Bouzid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="103" to="117" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Brain Tumor Segmentation with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Learning for Brain MRI Segmentation: State of the Art and Future Directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galimzianova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="459" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation of Glioma Tumors in Brain Using Deep Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Majid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">282</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparison of unsupervised classification methods for brain tumor segmentation using multi-parametric MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sauwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Acou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cauter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veraart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Himmelreich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Achten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage: Clinical</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="753" to="764" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentatio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Project &amp; Excite&apos; Modules for Segmentation of Volumetric Medical Scans. Image and Video Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rickmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sarasua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N?</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal Symmetric Multimodal Templates and Concatenated Random Forests for Supervised Brain Tumor Segmentation (Simplified) with ANTsR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shrinidhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wintermark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Durst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved delineation of brain tumour margins using whole-brain track-density mapping. Ismrm-esmrmb Joint Meeting: Clinical Needs &amp; Technological Solutions. International Society of Magnetic Resonance in Medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Crozier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bourgeat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N?</forename><surname>Dowson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O?</forename><surname>Salvado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?</forename><surname>Raniga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pannek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coulthard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?</forename><surname>Fay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian Network and Structured Random Forest Cooperative Deep Learning for Automatic Multi-label Brain Tumor Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mahjoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rekik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Agents and Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fuzzy cc-mean based brain MRI segementation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balafar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell Rev</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="449" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on medical imaging</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automatic brain tumor detection and segmentation using U-Net based fully convolutional networks. Annual conference on medical image understanding and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic Brain Tumor Segmentation using Cascaded Anisotropic Convolutional Neural Networks. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">3D MRI brain tumor segmentation using autoencoder regularizatio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Brain Tumor Segmentation using an Ensemble of 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">U-Nets and Overall Survival Prediction using Radiomic Features. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabilibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Multiresunet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">121</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Contrast agent-free synthesis and segmentation of ischemic heart disease images using progressive sequential causal GANs. Medical Image Analysis 101668</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu L?ohorodnyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename><surname>M?li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ERV-Net: An efficient 3D residual neural network for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Active contour model driven by optimized energy functionals for MR brain tumor segmentation with intensity inhomogeneity correction. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="21925" to="21954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Deep-Learning Model with Learnable Group Convolution and Deep Supervision for Brain Tumor Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Problems in engineering</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Segmentation of Larynx Histopathology Images via Convolutional Neural Networks. Intelligent and Fuzzy Techniques: Smart and Innovative Solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yurttakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erbay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="949" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A deep learning model integrating fcnns and crfs for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="98" to="111" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pediatric Gliomas: Current Concepts on Diagnosis, Biology, and Clinical Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dtw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Oncology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">2370</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention Guided Network for Retinal Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention -MICCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Guided Image Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keyvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical lmaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kirby</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2017.117</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Scientific Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">One-Pass Multi-Task Networks With Cross-Task Guided Attention for Brain Tumor Segmentation. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
