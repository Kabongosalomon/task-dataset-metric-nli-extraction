<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parameter-Efficient Person Re-identification in the 3D Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Parameter-Efficient Person Re-identification in the 3D Space</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person re-identification</term>
					<term>3D human representa- tion</term>
					<term>Image retrieval</term>
					<term>Point cloud</term>
					<term>Graph convolutional networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People live in a 3D world. However, existing works on person re-identification (re-id) mostly consider the semantic representation learning in a 2D space, intrinsically limiting the understanding of people. In this work, we address this limitation by exploring the prior knowledge of the 3D body structure. Specifically, we project 2D images to a 3D space and introduce a novel parameter-efficient Omni-scale Graph Network (OG-Net) to learn the pedestrian representation directly from 3D point clouds. OG-Net effectively exploits the local information provided by sparse 3D points and takes advantage of the structure and appearance information in a coherent manner. With the help of 3D geometry information, we can learn a new type of deep re-id feature free from noisy variants, such as scale and viewpoint. To our knowledge, we are among the first attempts to conduct person re-identification in the 3D space. We demonstrate through extensive experiments that the proposed method (1) eases the matching difficulty in the traditional 2D space, (2) exploits the complementary information of 2D appearance and 3D structure, (3) achieves competitive results with limited parameters on four large-scale person reid datasets, and (4) has good scalability to unseen datasets. Our code, models and generated 3D human data are publicly available at https://github.com/layumi/person-reid-3d.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P ERSON re-identification is usually regarded as an image retrieval problem of spotting the person in nonoverlapping cameras <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Due to the rising demand of public safety and the fast development of camera network, person re-id has received increasing interests. These studies aim to save the human resource and efficiently find the person of interest, e.g., lost child in the airport, from thousands of candidate images. In recent years, the advance of person reid is mainly due to two factors: 1) the availability of largescale datasets and 2) the deeply-learned person representation. On one hand, deeply-learned models are usually data-hungry. The large-scale datasets <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref> facilitate the data-driven approaches. On the other hand, the development of Convolutional Neural Network (CNN) also provides the technical breakthrough of the pedestrian representation learning. Many efforts have been paid to improve the CNN-based model capability <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Recently, some researchers and companies also claim that the model can surpass the human performance <ref type="bibr" target="#b14">[15]</ref>. <ref type="bibr">Zhedong</ref>   Our brain generally associates the 2D appearance with prior knowledge of the 3D body shape. In this work, we intend to simulate this process and explore robust pedestrian representation with a lightweight model. (Dash arrows are missing in prevailing re-id methods.) However, one inherent problem still remains: does the model really understand the person? People live in a 3D world. In contrast, we notice that most prevailing person reid methods ignore the prior knowledge that human is a 3D non-rigid object, and only focus on learning the representation in 2D space. Although some pioneering works <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> consider the 3D human structure, the pedestrian representation is still learned from the projected 2D images. For instance, one of the existing works, PersonX <ref type="bibr" target="#b16">[17]</ref>, has applied the game engine to build 3D person models. However, representation learning is conducted in the 2D space by projecting the 3D model back to 2D images. This line of works is effective in data augmentation but might be sub-optimal in representation learning. It is because the 2D data space intrinsically limits the model to understand the 3D geometry information of the person.</p><p>Inspired by the human ability of associating the 2D appearance with the 3D geometry structure (see <ref type="figure" target="#fig_0">Figure 1</ref>), we argue that the key to learning an effective and scalable person representation is to consider the complementary information of 2D human appearance and 3D geometry structure. With the prior knowledge of 3D human geometry information, we could learn a depth-aware model, thus making the representation robust to real-world scenarios. As shown in <ref type="figure">Figure 2</ref>, we map the visible surface to the human mesh, and make the person free from the 2D space. The intuition is that after mapping to the 3D space, the appearance information is correlated/aligned with the human structure. Without the need to worry about the part matching from two different viewpoints, the 3D data structure eases the matching difficulty in nature. The model could concentrate on learning the identity-related features, and dealing with the other intra-class variants, such as illumination conditions.</p><p>To fully take advantage of the 3D structure and 2D appearance, we propose a novel Omni-scale Graph Network for person re-id in the 3D space, called OG-Net. OG-Net is a parameter-efficient model based on graph neural network arXiv:2006.04569v3 [cs.CV] 31 Jul 2021 <ref type="figure">Fig. 2</ref>. Person is a 3D non-rigid object. In this work, we conduct the person re-identification in the 3D space, and learn a new type of robust re-id feature. Given one 2D image (a), we first (b) estimate the 3D pose via the off-the-shelf model <ref type="bibr" target="#b17">[18]</ref>, followed by (c) mapping the RGB color of visible surfaces to corresponding points. The invisible parts are made transparent for visualization. (d) The appearance information is aligned with the human structure. We make the person free from the 2D space, and thus ease the matching difficulty.</p><p>(GNN) to communicate between the discrete cloud points of arbitrary locations. Given the 3D point cloud and the corresponding color information, OG-Net predicts the person identity and outputs the robust human representation for subsequent matching. Following the spirit of the conventional convolutional neural network (CNN), we utilize 3D points to build the location topology, and deploy the corresponding RGB color to extract appearance information. In particular, we propose Omni-scale module to aggregate the feature from multiple 3D receptive fields, which leverages multiscale information in 3D data. Even though the basic OG-Net only consists of four Omni-scale modules, it has achieved competitive performance on four person re-id datasets.</p><p>Contribution. Our contributions are as follows. <ref type="formula" target="#formula_0">(1)</ref> We study person re-identification in the 3D space -a realistic scenario which could better reflect the nature of the 3D nonrigid human. To our knowledge, this work is among the early attempts to address this problem. <ref type="bibr" target="#b1">(2)</ref> We propose a novel Omni-scale Graph Network to learn the feature from both human appearance and 3D geometry structure in a coherent manner. OG-Net leverages discrete 3D points to capture the multi-scale identity information. (3) Extensive experiments on four person re-id benchmarks show the proposed method could achieve competitive performance with limited parameters. A more realistic transfer learning setting is also studied in this paper. We observe that OG-Net has good scalability to the unseen person re-id dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Space for Person Re-id</head><p>Recent years, convolutional neural network (CNN) models have been explored to map the pedestrian inputs, e.g., images, into one shared semantic space, where the data of the same identity is close and the data of different identities is apart from each other <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Different optimization objectives have been studied. For instance, the contrastive loss is widelyused to discriminate different identities <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, while the identification loss deploys the identity classification as the pretext task <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. To simultaneously minimize the intra-class difference and maximize the inter-class gap, the triplet loss with different hard sampling strategies are also widely-studied <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. Xiao et al. <ref type="bibr" target="#b26">[27]</ref> propose the online instance matching loss to view the unlabeled data as negative samples, while Zheng et al. <ref type="bibr" target="#b8">[9]</ref> design one label smooth loss to take advantage of synthetic data. Besides, several works <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref> utilize person attributes, e.g., gender, to help the model learning intermediate features. This line of works is orthogonal to our work -any semantic spaces or optimization objectives can be used in our work and better ones can benefit our approach. In this work, we do not intend to pursue the best semantic space, but focus on verifying the effectiveness of the 3D space and the proposed OG-Net. We, therefore, deploy the basic identification loss for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Part Matching for Person Re-id</head><p>To obtain the discriminative pedestrian representation, one line of research works resorts to mining local patterns, such as bodies, legs and arms, on 2D image inputs. The part matching is usually conducted on two different levels, i.e., the pixel level <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref> and the feature level <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b36">[37]</ref>. The pixellevel part matching directly transforms the input image to one unified form. For instance, Su et al. <ref type="bibr" target="#b30">[31]</ref> and Zheng et al. <ref type="bibr" target="#b32">[33]</ref> deploy the off-the-shelf pose estimator <ref type="bibr" target="#b37">[38]</ref> to predict the human key points, followed by cropping and resizing body parts for representation learning. Similarly, Zhang et al. <ref type="bibr" target="#b31">[32]</ref> utilize the semantic segmentation predictor to crop and align body parts densely. Instead of cropping body parts, Saquib et al. <ref type="bibr" target="#b38">[39]</ref> concatenate the rgb input with key point heatmap as input, and let model to learn the part attention by itself. In contrast, another line of works align the parts coarsely on the feature level, given that pedestrians usually stand in the image and are horizontally aligned in nature. Based on this assumption, Sun et al. <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b39">[40]</ref> propose to split feature maps horizontally and learn the part feature in a relatively large receptive field. Taking one more step, MGN <ref type="bibr" target="#b34">[35]</ref> explores more partition strategies and fuses different loss functions, further improving the performance. To obtain more fine-grained information, several works et al. <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b44">[45]</ref> introduce one extra human parsing branch to provide part matching information in the feature level. Some pioneering works also explore the neural architecture search to learn fine-grained visual representation <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b47">[48]</ref>. Besides, to address the misdetection of the input image, Zheng et al. <ref type="bibr" target="#b48">[49]</ref> apply the spatial transformer network <ref type="bibr" target="#b49">[50]</ref> to re-align feature maps. Different from existing works on part alignment in 2D space, the proposed method explores the 3D body structure, which is more close to the prior knowledge of human -a 3D non-rigid object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning from Synthetic Data</head><p>Another active research line is to leverage the synthetic human data. Although most datasets <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b50">[51]</ref> provide more training data in recent years, the number of images per person is still limited <ref type="bibr" target="#b8">[9]</ref>. Therefore, the intra-class variants of every training pedestrian are limited, which largely compromise the model learning and hurt the model scalability to the realworld scenario. To address the data limitation, one line of existing works leverages the generative adversarial network (GAN) <ref type="bibr" target="#b51">[52]</ref> to synthesize more high-quality training images, and let the model "see" more appearance variants to learn the robust representation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b52">[53]</ref>- <ref type="bibr" target="#b57">[58]</ref>. Zheng et al. <ref type="bibr" target="#b8">[9]</ref> first propose a new label smooth regularization for outliers to leverage imperfect generated images. In a similar spirit, Huang et al. <ref type="bibr" target="#b58">[59]</ref> deploy the pseudo label learning to assign refined labels for synthetic data. Qian et al. <ref type="bibr" target="#b55">[56]</ref> modify the generation model and add pedestrian images with different poses into training set, yielding the pose-invariant features. Inspired by the conventional encoder-decoder manner, Ge et al. <ref type="bibr" target="#b53">[54]</ref> propose FD-GAN to learn one pose-invariant feature when encoding the input image. DG-Net <ref type="bibr" target="#b52">[53]</ref> disentangles the pedestrian image to two embeddings, i.e., appearance code and structure code, to generate diverse and realistic synthetic images. With the high-quality synthetic data, more discriminative feature can be learned, in turn, improving re-id performance. Furthermore, several works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b59">[60]</ref>- <ref type="bibr" target="#b61">[62]</ref> also apply GAN, i.e., CycleGAN <ref type="bibr" target="#b62">[63]</ref>, to cross-domain person re-identification by training the model with the targetstyle synthetic data. In contrast, another line of works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref> is close to our work, which applies the game engine to build 3D models. Sun et al. <ref type="bibr" target="#b16">[17]</ref> build a large number of 3D person models, and map models to 2D plane for generating more 2D training data. Yao et al. <ref type="bibr" target="#b64">[65]</ref> and Tang et al. <ref type="bibr" target="#b63">[64]</ref> manipulate the generation setting and leverage attributes, e.g., color and pose, to enable multi-task learning on 2D synthetic data. Lin et al. <ref type="bibr" target="#b65">[66]</ref> also leverage the synthetic data to learn the common knowledge of human structure, improving the model scalability on real data. However, different from our work, the above-mentioned studies are mostly investigated in the 2D space, and neglect the 3D geometry information of human bodies. In this work, we argue that the 3D space with the geometry knowledge could help to learn a new type of feature free from several intra-class visual variants, such as viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Learning from Point Clouds</head><p>The point cloud is a flexible geometric representation of 3D data structure, which could be obtained by most 3D data acquisition devices, such as radar. The point cloud data is usually unordered, and thus the conventional convolutional neural network (CNN) could not directly work on this kind of data. One of the earliest works, i.e., PointNet <ref type="bibr" target="#b66">[67]</ref>, proposes to leverage the multi-layer perceptron (MLP) networks and max-pooling layer to fuse the information from multiple points. PointNet++ <ref type="bibr" target="#b67">[68]</ref> takes one more step by introducing the sampling layer to distill salient points. To address the limitation in decoding, FoldingNet <ref type="bibr" target="#b68">[69]</ref> adds one constant 2D plane to simulate the surface of 3D objects. However, the communication between the 3D points is still limited, and each point is treated independently most of the time. Therefore, Wang et al. <ref type="bibr" target="#b69">[70]</ref> propose to leverage Graph Neural Network (GNN) <ref type="bibr" target="#b70">[71]</ref> to enable the information spread between the knearest points. Li et al. <ref type="bibr" target="#b71">[72]</ref> take one more step and propose to deploy a deeper graph neural network structure, further boosting the performance. Similarly, in this work, we regard every person as one individual graph, while every RGB pixel and the corresponding location are viewed as one node in the graph. More details are provided in Section III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>We show a schematic overview of our framework in Figure 3. We next introduce some notations and assumptions, followed by the details of how to learn from 3D points, and how to take advantage of 2D appearance information and 3D structure in one coherent manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries and Notations</head><p>To conduct person re-identification in the 3D space, we first change the data structure of inputs. In particular, given one person re-id dataset, 2D images are mapped to the 3D space via the off-the-shelf 3D pose estimation <ref type="bibr" target="#b17">[18]</ref>. We apply this mapping function to every image in the dataset to obtain 3D point clouds aligned with the 2D appearance. We denote the generated point sets and identity labels as S = {s n } N n=1 and Y = {y n } N n=1 , where N is the number of samples in the dataset, y n ? [1, K], and K is the number of the identity categories. We utilize the matrix format to illustrate the point cloud s n ? R m?6 , where m is the number of points, and 6 is the channel number. The former 3 channels contain 3D coordinates XYZ, while the latter 3 channels contain the corresponding RGB information. Given one 3D data s n ? R m?6 , our work intend to learn a mapping function F which projects the input s n to the identity-aware representation f n = F ? (s n ) with learnable parameters ?. Unlike the conventional image format, the 3D point clouds Given the point cloud of (m ? 6), we split the geometry location b 0 and the rgb color data a 0 . The 3D location information, i.e., (x,y,z), is to build the KNN graph, while the rgb data is to extract the appearance feature as the conventional 2D CNNs. We progressively downsample the number of selected points {m, 768, 384, 192, 96}, while increasing the appearance feature length {3, 64, 128, 256, 512}. For the last KNN Graph, we concatenate the position b 3 and the appearance feature a 3 to yield a non-local attention (see the red dash arrow). Finally, we concatenate the outputs of average pooling and max pooling layer, followed by one fully connected (FC) layer and one batch normalization (BN) layer. We adopt the conventional pretext task, i.e., identity classification L id , as the optimization objective to learn the pedestrian representation. When testing, we drop the last classifier and extract the compressed feature of 512 dimensions as the pedestrian representation for matching. are unordered and discrete. We can not directly apply the traditional 2D convolutional layer on m?6 to capture the local information, e.g., one 3 ? 3 receptive field, since unordered neighbor points may have limited connections to the center point. To address the limitation, we follow the idea of graph neural networks <ref type="bibr" target="#b70">[71]</ref> to build the graph G based on the distance between points. Next we illustrate one basic component, i.e., dynamic graph convolution, to learn from the graph G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic Graph Convolution</head><p>To model the relationship between neighbor points, we adopt the k-nearest neighbor (KNN) graph G = (V, E), where V denotes the vertex set, and E denotes the edge set (E ? V ? V). The KNN graph is directed, and includes selfloop, meaning (i, i) ? E. It is worth to noticing that the selection of the k-nearest neighbors is based on the value of vertexes (points) rather than the initial input order, evading the problem of unordered 3D point clouds. Besides, recent works <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b71">[72]</ref> also show the dynamic graph is superior to the fixed graph structure during training GCN, which alleviates the over-smoothing problem and enlarges the receptive field of every node. Following the spirit of the dynamic graph, the KNN graph used in our work is not fixed, and we re-build the graph after every down-sampling layer. The down-sampling layers are to progressively remove redundant points (vertexes), and thus the computation cost of the proposed method is much less than the conventional implementation in <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b71">[72]</ref>.</p><p>To learn representation from the topology structure of the graph, we follow the spirit of the traditional 2D CNN and deploy one local convolutional layer based on neighbor points with connected edges. In particular, given one node feature x i , the output x i of the dynamic graph convolution could be formulated as:</p><formula xml:id="formula_0">x i = j:(i,j)?E, j =i (? i x i + ? j x j )<label>(1)</label></formula><p>where x j is the feature of neighbor points in the graph, and there is one edge from i to j. ? is the learnable parameter in ?. The main difference with the traditional convolution is the definition of the neighbor set. In this work, we combine two kinds of neighbor choices, i.e., position similarity and feature similarity. If the graph G is based on the 3D coordinate similarity, dynamic graph convolution equals to the conventional 2D CNN to capture the local pattern based on the position. We note that this operation is translation invariant, since the global translation, such as ShiftX, ShiftY and Rotation, could not change the connected neighbors in E.</p><p>On the other hand, if the graph G is built on the appearance feature, the dynamic graph convolution works as the non-local self-attention as <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, which ignores the local position but pays attention to the area with similar appearance patterns. We next take advantage of the dynamic graph convolution function to build the basic module -Omni-scale module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Omni-scale Module</head><p>To leverage the rich multi-scale information as the prevailing 2D CNNs, we propose one basic Omni-scale module, which could be easily stacked to form the whole network. The module treats the 3D location and the RGB input differently (see <ref type="figure" target="#fig_2">Figure 4</ref> (b)). We denote l ? [0, L ? 1] as the layer index. The RGB input is the first appearance feature a 0 of m ? 3, while the initial 3D position is b 0 of m ? 3. Different from the conventional graph CNN, the local k-nearest graph G l is dynamically generated according to the input location b l or the concatenation of a l and b l . Given the appearance feature a l of m ? c, the location b l of m ? 3 and the KNN graph G l ,  the Omni-scale module outputs the appearance feature a l+1 and the selected locations b l+1 . From the top to the bottom of the module, we first apply Dynamic Graph Convolution to aggregate the k-nearest neighbor features, which is similar to the conventional convolutional layer. Dynamic Graph Convolution does not change the number of points, and thus the shape of the output feature is m ? c . If down-sampling points is not applied, we will remain the channel number c = c following the conventional residual learning <ref type="bibr" target="#b74">[75]</ref> to obtain a l+1 followed by one batch normalization layer and one ReLU (see <ref type="figure" target="#fig_2">Figure 4 (a)</ref>). If down-sampling points is applied, we generally set c = 2c to enlarge the feature channel before downsampling. Then we downsample the location according to the farthest point sampling (FPS) <ref type="bibr" target="#b67">[68]</ref>. FPS selects the most distinguish points in the 3D space. We note that only the 3D position b l is used to calculate the distance and decide the selected points when downsampling. According to the selected location, we also downsample the appearance feature, and only keep the feature of the selected location. Therefore, the shape of the selected location is 1 2 m ? 3, while the selected feature shape is 1 2 m ?c . Next we deploy three branches with different grouping rates r = {8, 16, 32}, and the three branches do not share weights. In this way, we could capture the information with different receptive fields as the conventional 2D CNNs, i.e., InceptionNet <ref type="bibr" target="#b75">[76]</ref>. Each branch consists one grouping layer, two linear layers, two batch normalization (BN) layers, one squeeze-excitation (SE) block <ref type="bibr" target="#b76">[77]</ref> and one group max pooling layer to aggregate the local information. Specifically, grouping-r layer is to sample and duplicate the r nearest points for each point, followed by the linear layers, batch normalization and the SE block. We introduce SE-block <ref type="bibr" target="#b76">[77]</ref> as one adaptive gate function to re-scale the weight of each branch before the summarization of three branches. Group max pooling layer is to maximize the feature within each group. Finally, we adopt the 'add' to calculate the sum of three branches rather than concatenation, so that the different scale pattern of the same part, such as cloth logos, could be accumulated. The shape of the new appearance feature a l+1 is 1 2 m ? c , and the shape of the corresponding 3D position b l+1 is 1 2 m ? 3. Alternatively, we could add the short-cut connection to take advantage of the identity representation as ResNet <ref type="bibr" target="#b74">[75]</ref>.</p><formula xml:id="formula_1">( ) ( ) ( ) ( ) ( ) ( ) RGB</formula><p>To summarize, the key of Omni-scale Module is two crosspoint functions. The cross-point function indicates the function considers the neighbor points, while the pre-point function only considers the feature of one point itself. One crosspoint function is the dynamic graph convolution before downsampling, which could be simply formulated as h(x i , x j ), where h denotes a linear function. It mimics the conventional 2D CNN to aggregate the local patterns according to the position. The other is the max group pooling layer in each branch, which could be simply formulated as max h(x i ). It maximizes neighbor features in each group as the new point feature. Now we have the Omni-scale module to learn from both of the appearance and the geometry structure information in a coherent manner, and next we will utilize Omni-scale modules to build the Omni-scale Graph Network (OG-Net).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. OG-Net Architecture</head><p>The structure of OG-Net is as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, consisting four Omni-scale modules. We progressively decrease the number of selected points as the conventional CNN. Every time the point number decreases, the channel number of the appearance feature is doubled. After four Omni-scale modules, we could obtain 96 points with 512-dim appearance feature. Similar to <ref type="bibr" target="#b69">[70]</ref>, we apply the max pooling as well as average pooling to aggregate the point feature, and concatenate the two outputs, yielding the 1024-dim feature. We add one fullyconnected layer and one batch normalization layer to compress the feature to 512 dimensions as the pedestrian representation. When inference, we drop the last linear classifier for the pretext classification task, and extract the 512-dim feature to conduct image matching. Training Objective. We adopt the conventional identity classification as the pretext task to learn the identity-aware feature. The vanilla cross-entropy loss could be formulated as:</p><formula xml:id="formula_2">L id = E[?log(p(y n |s n ))]<label>(2)</label></formula><p>where p(y n |s n ) is the predicted possibility of s n belonging to the ground-truth class y n . The training objective demands that the model could discriminate different identities according to the input points. Besides, other training objectives are orthogonal to our work. 1) In this work, we intend to show the strong potential ability of the 3D space and the proposed OG-Net. We, therefore, only deploy the basic identification loss for a fair comparison with other networks. 2) We deploy the new-released circle loss <ref type="bibr" target="#b77">[78]</ref> to show that our work can be fused with better loss functions for further performance boost.</p><p>Relation to Existing Methods. The main difference with existing GNN-based networks <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref> is three-fold: <ref type="bibr" target="#b0">(1)</ref> We extract the multi-scale local information via the proposed Omni-scale Block, which can deal with the common scale variants in 3D person data; <ref type="bibr" target="#b1">(2)</ref> We split the XYZ position information and RGB color information, and treat them differently. RGB inputs are used to extract appearance features, while the geometry position is to build the graph for local representation learning; (3) Due to a large number of points in 3D person, we progressively reduce the number of nodes in the graph, facilitating efficient training for 3D person data. On the other hand, compared with PointNet <ref type="bibr" target="#b66">[67]</ref> and PointNet++ <ref type="bibr" target="#b67">[68]</ref>, the proposed OG-Net contains more cross-point functions, and provides topology information, enriching the representation power of the network. The graph could be built on the two kinds of neighbor choices, i.e., position similarity or feature similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT A. Implementation Details</head><p>OG-Net is trained with a mini-batch of 36. We deploy Adam optimizer <ref type="bibr" target="#b78">[79]</ref> with amsgrad <ref type="bibr" target="#b79">[80]</ref> and the initial learning rate is set to 8e?4. We gradually decrease the learning rate via the cosine policy <ref type="bibr" target="#b80">[81]</ref>, and the model is trained for 1000 epochs. To regularize the training, we transfer some traditional 2D data augmentation methods, such as random scale and position jittering, to the 3D space. For instance, position jittering is to add zero-mean Gaussian noise to every point. Following the setting in DGCNN <ref type="bibr" target="#b69">[70]</ref>, we set the neighbor number of KNNgraph to k = 20. The dynamic graph convolution in OG-Net can be any of the existing graph convolution operations, such as EdgeConv <ref type="bibr" target="#b69">[70]</ref>, SAGE <ref type="bibr" target="#b81">[82]</ref> and GAT <ref type="bibr" target="#b82">[83]</ref>. In practise, we adopt EdgeConv <ref type="bibr" target="#b69">[70]</ref>. Dropout with 0.7 drop probability is used before the last linear classification layer. Since the basic OG-Net is shallow, we do not use the short-cut connection. For the person re-id task, the input image is resized to 128 ? 64, and there are 8192 points with RGB color information. After mapping to the 3D space, we uniformly sample half points to train the OG-Net, and thus the number of input m in <ref type="figure" target="#fig_1">Figure 3</ref> equals to 4096. We note that, for other competitive 2D CNN methods, we still follow the common setting, and the 2D image input is resized to 256 ? 128 <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b52">[53]</ref> for a fair comparison. OG-Net. The channel number of the four Omni-scale Module in OG-Net is {64, 128, 256, 512}. The parameter number is 1.95M , which is much less than the prevailing CNN structure ResNet-50 (24.56M ). OG-Net-Small. To compare with lightweight models, we also introduce OG-Net-Small with fewer channel numbers, i.e., {48, 96, 192, 384}. The parameter number of the model is 1.20M , which is less than both widely-adopted mobile models, i.e., ShuffleNetV2 (1.78M ) and MobileNetV2 (4.16M ). OG-Net-Deep. We build one deep OG-Net with more Omniscale Modules. The channel numbers are {48, <ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr">192,</ref><ref type="bibr">192</ref>, 384, 384}. The short-cut connection is enabled. Further discussion on short-cut connection is provided in <ref type="table" target="#tab_6">Table V</ref>. The parameter number is 2.47M .</p><p>The models are trained from scratch on 3D point clouds. The whole training process costs about 2 days, with one NVIDIA 2080Ti. During testing, we extract the 512-dim feature before the classifier as the pedestrian representation. The feature is L2-normalized. Given one query image, we calculate the cosine similarity between the query feature and the candidate features of gallery images. We sort gallery images and return the ranking list according to the cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>We verify the effectiveness of the proposed method on four large-scale person re-id datasets, i.e., Market-1501 <ref type="bibr" target="#b6">[7]</ref>, DukeMTMC-reID <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b50">[51]</ref>, MSMT-17 <ref type="bibr" target="#b9">[10]</ref>, and CUHK03-NP <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref>. Market-1501 <ref type="bibr" target="#b6">[7]</ref> is collected in a university campus by 6 cameras, containing 12, 936 training images of 751 identities, 3, 368 query images and 19, 732 gallery images of the other 750 identities. There are no overlapping identities (classes) between the training and test set. Every identity in the training set has 17.2 photos on average. All images are automatically detected by the DPM detector <ref type="bibr" target="#b85">[86]</ref>. DukeMTMC-reID <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b50">[51]</ref> consists 16, 522 training images of 702 identites, 2, 228 query images of the other 702 identities <ref type="figure">Fig. 5. (a,b)</ref> Visualization of lossy compression in the 2D-to-3D mapping, which drops the body outliers, e.g., hair and dress. (c) We still introduce the 2D background to 3D space. and 17, 661 gallery images, which is mostly collected in winter by eight high-resolution cameras. It is challenging in that most pedestrians are in the similar clothes, and may be occluded by cars or trees. MSMT-17 <ref type="bibr" target="#b9">[10]</ref> is one of the newly-released large-scale datasets, including 126, 441 images collected in both indoor and outdoor scenarios with 15 cameras. It contains 32, 621 images of 1, 041 identities for training, 11, 659 query images with 82, 161 gallery images. CUHK03-NP <ref type="bibr" target="#b83">[84]</ref> is one of the early person re-identification datasets. We follow the new protocol in <ref type="bibr" target="#b84">[85]</ref> to split 767 identities as the training set, and the rest 700 identities are deployed to verify the model. We utilize the pedestrian images detected by DPM <ref type="bibr" target="#b85">[86]</ref> for training and testing, which is more close to the real-world scenario. Evaluation Metrics. We report Rank-1 accuracy (R@1) and mean average precision (mAP). Rank-i denotes the probability of the true match in the top-i of the retrieval results, while AP denotes the area under the Precision-Recall curve. The mean of the average precision (mAP) for all query images reflects the precision and recall rate of the retrieval performance. Besides, we also provide the number of model parameters (#params). Data Limitation. Before the experimental analysis, we would like to illustrate several data limitations. It is mainly due to lossy mapping in the 2D-to-3D process. Due to the restriction of the 3D human model, we could not build the 3D model for several body outliers, such as hair, bag, dress. However, these outliers contain discriminative identity information. For instance, as shown in <ref type="figure">Figure 5</ref> (a) and (b), the 3D model based on the visible part drops some part of hair and dress of the girl, which is not ideal for representation learning. We think it could be solved via the depth estimation devices, such as Kinect <ref type="bibr" target="#b86">[87]</ref>, or more sophisticated human models in the future. In this paper, we do not solve the 3D human reconstruction problem, but focus on the person re-identification task. Therefore, as a trade-off, we still introduce the 2D background, and project the corresponding pixel to the XY plane (see <ref type="figure">Figure 5</ref> (c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative Results</head><p>Comparisons to the 2D Space. We compare the results on three kinds of inputs, i.e., 2D input, 3D Visible Part and 3D Visible Part with 2D Background. For a fair comparison, the grid of the 2D input is also transformed to the point cloud format as (x, y, 0), while z is set to 0. We train OG-Net on three kinds of input data with the same hyper-parameters. As shown in <ref type="table" target="#tab_2">Table I</ref>, we observe that the retrieval result of the pure 3D Visible Part input is inferior to that of 2D Image. As discussed in Section IV-B, we speculate that it is due to the lossy 2Dto-3D mapping, which drops several discriminative parts, such as hair, dress, and carrying. In contrast, the 3D Visible Part + 2D Background has achieved superior performance 86.79% Rank@1 and 67.92% mAP to the result of 2D Image (85.72% Rank@1 and 67.28% mAP), which shows that the 3D position information is complementary to 2D color information. Similar results also can be observed on the DukeMTMC-reID dataset. The 3D information yields +1.84% Rank-1 and +1.76% mAP accuracy improvement. The 3D space could ease the matching difficulty and highlight the geometry structure. Person Re-id Performance. We compare the proposed method with three groups of competitive methods, i.e., prevailing 2D CNN models, light-weight CNN models, and popular point classification models. We note that the model pre-trained on the large-scale datasets, e.g., ImageNet <ref type="bibr" target="#b90">[91]</ref>, could yield the performance boost. For a fair comparison, models are trained from scratch with the same optimization objective, i.e., the cross-entropy loss. Since the proposed method is orthogonal to different metric learning losses, we also run experiments with the prevailing circle loss <ref type="bibr" target="#b77">[78]</ref>. As shown in <ref type="table" target="#tab_2">Table II</ref>, we can make the following observations:</p><p>(1) OG-Net has achieved competitive results of 69.02% mAP, 57.92% mAP, 21.57% mAP, and 39.28% mAP on four large-scale person re-id benchmarks with limited training parameters 1.95M . The mobile OG-Net-Small of less channel width also achieves a close result only with the cross-entropy loss.</p><p>(2) Comparing with the point-based methods, such as Point-Net++ <ref type="bibr" target="#b67">[68]</ref> and DGCNN <ref type="bibr" target="#b69">[70]</ref>, both OG-Net and OG-Net-Small have surpassed this line of works by a clear margin, which validates the effectiveness of the proposed Omni-scale module in capturing multi-scale neighbor information on point clouds.</p><p>(3) Comparing with light-weight 2D CNN models, i.e., ShuffleNetV2 <ref type="bibr" target="#b87">[88]</ref> and MobileNetV2 <ref type="bibr" target="#b88">[89]</ref>, OG-Net-Small has achieved competitive performance with fewer parameters (1.20M ).</p><p>(4) We apply the same setting to train the model with Circle loss. The strong supervision mechanism of Circle loss sometime compromises the training process. The training process is quite challenging, especially when the class number largely increases in the MSMT-17 dataset. We observe that <ref type="table" target="#tab_2">the  TABLE II  WE MAINLY COMPARE THREE GROUPS OF MODELS TRAINED FROM SCRATCH ON FOUR LARGE-SCALE PERSON RE-ID DATASETS,</ref> i.e., MARKET-1501 <ref type="bibr" target="#b6">[7]</ref>, DUKEMTMC-REID <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b50">[51]</ref>, MSMT-17 <ref type="bibr" target="#b9">[10]</ref> AND CUHK03-NP <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref>. WE REPORT RANK1(%), MAP(%) AND THE NUMBER OF MODEL PARAMTERS <ref type="table" target="#tab_2">(M). THE FIRST GROUP CONTAINS THE POINT-BASED METHODS THAT WE RE-IMPLEMENTED. THE SECOND GROUP CONTAINS THE  LIGHTWEIGHT CNN MODELS. THE THIRD GROUP CONTAINS PREVAILING 2D CNN MODELS WITH MORE</ref>  proposed model is shallow and relatively easy to converge, so Circle loss generally works well with the proposed structure, and yields performance boost. <ref type="bibr" target="#b4">(5)</ref> Comparing with prevailing 2D CNN models, i.e., ResNet-50 <ref type="bibr" target="#b74">[75]</ref> and DenseNet-121 <ref type="bibr" target="#b89">[90]</ref>, the proposed OG-Net surpasses these models. Furthermore, OG-Net-Deep with deeper structure has achieved better Rank@1 and mAP accuracy. Besides, we also observe that OG-Net is more robust than 2D CNNs, when facing the unseen data. We will discuss this aspect in the following section. Transferring to Unseen Datasets. To verify the scalability of OG-Net, we train the model on dataset A and directly test the model on dataset B (with no adaptation), which is close to the real-world deployment. We denote the direct transfer learning protocol as A ? B. Three groups of related works are considered. We observe that the modern CNN models are typically over-parameterized, which is prone to over-fit the training dataset. As shown in <ref type="table" target="#tab_2">Table III</ref>, both ResNet-50 and DenseNet-121 do not perform well given more parameters. The 3D point cloud-based methods are competitive to the conventional 2D methods. It is worth noting that the proposed OG-Net has outperformed the point-based methods as well as prevailing 2D networks. The results suggest that the proposed method has the potential to adapt one new re-id dataset of unseen environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>Visualization of Retrieval Results. As shown in <ref type="figure" target="#fig_3">Figure 6</ref>, we provide the original query, the corresponding 3D query and the top-5 retrieved candidates. Two different cases are studied. One is the typical case that the 3D human reconstruction is relatively good. OG-Net can successfully retrieve the truematches of different viewpoints (see <ref type="figure" target="#fig_3">Figure 6 (a)</ref>). On the other hand, we also show the challenging case, including the partially detected query and occlusion. Thanks to the prior knowledge of the human geometry structure, OG-Net can still provide reasonable retrieval results with large scale variants (see <ref type="figure" target="#fig_3">Figure 6</ref> (b)). It also verifies the robustness of the proposed approach.</p><p>V. FURTHER ANALYSIS AND DISCUSSIONS Effect of Different Components. In this section, we intend to study the mechanism of the Omni-scale Module. First, we compare the OG-Net without KNN Graph, i.e., k = 1. For a fair comparison, we apply one linear layer to replace the dynamic graph convolution. As shown in the second and the third column of <ref type="table" target="#tab_2">Table IV</ref>, the performance of OG-Net without leveraging the KNN neighbor information drops from 69.33% mAP to 65.50% mAP. The result suggests that the dynamic graph captures effective local information, which could not be replaced by pre-point function, e.g., linear layer. On the other hand, if we include too many neighbors, e.g., k = 64, the model loses the discriminative feature of local patterns, thus compromising the retrieval performance as well. To validate this points, we evaluate the sensitivity analysis on k = {4, 8, 16, 32, 64} (see <ref type="figure">Figure 7</ref>). The observation is consistent with the conventional k nearest neighbor algorithms <ref type="bibr" target="#b96">[97]</ref> on the neighbor number.</p><p>Next, we intend to verify the effectiveness of the last nonlocal graph. The last graph is built on the k-nearest neighbor of the appearance feature. (In practice, we append the 3channel position to the appearance feature for building the graph, which prevents duplicate nodes with the same node attribute in the graph.) For a fair comparison, we replace the last non-local graph with the graph based on 3D position only. As shown in the third and the fourth column of <ref type="table" target="#tab_2">Table IV</ref>, OG-Net with the last non-local block has surpassed the model with position graph +1.15% mAP, indicating that the last non-local graph provides effective long-distance attention.</p><p>Finally, we study two alternative components, i.e., SE block and short-cut connection. By default, Omni-scale Module deploys SE block but does not add the short-cut connection. As shown in the first and second column in <ref type="table" target="#tab_2">Table IV</ref>, we can observe that SE Block improves about +0.85% mAP from      well on the relatively deep network structure. The performance is improved from 68.49% mAP to 72.91% mAP, and the shortcut connections help the model optimization. Sensitivity Analysis on the Point Density. Our model is trained with 50% points, i.e., 4096, and thus the best performance is achieved with 50% points remaining. In practice, different depth estimation devices may provide different scan point density. To verify the robustness of the proposed OG-Net on point density, we synthesize the data similar to that in <ref type="figure" target="#fig_4">Figure 8</ref> (left) and conduct the inference. When 25% points remain, OG-Net still could arrive at 84.95% Rank@1 and 65.91% mAP. When 100% points are used, OG-Net arrives at 84.77% Rank@1 and 66.14% mAP. It is because too low/high density impacts the distribution of the k-nearest neighbors, compromising the retrieval performance. Despite the density changes, the relative performance drop is small. The result verifies OG-Net is robust to different point density (see <ref type="figure" target="#fig_4">Figure 8</ref> (right)). Evaluation of Point Cloud Classification Task. We also evaluate the proposed OG-Net on the traditional point cloud classification benchmark, i.e., ModelNet <ref type="bibr" target="#b91">[92]</ref>. The ModelNet dataset contains 12,311 meshed CAD models of 40 categories.</p><p>Following the train-test split in <ref type="bibr" target="#b69">[70]</ref>, 9,843 models are used for training, while the rest 2,468 models are for evaluation. Note that the ModelNet dataset does not provide RGB information. To verify the effectiveness of OG-Net, we du-plicate the xyz input as the appearance input to train OG-Net. Following other competitive approaches <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b69">[70]</ref>, the number of input points is fixed as 1024. As shown in <ref type="table" target="#tab_2">Table  VI</ref>, we compare with prevailing models in terms of mean-class accuracy and overall accuracy. Although the proposed method is not designed for cloud point classification task, OG-Net-Small has achieved a competitive result of 90.5% mean-class accuracy and 93.3% overall accuracy with 1.22M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we provide an early attempt to learn the pedestrian representation in the 3D space, easing the part matching on 2D images. The 3D assumption is aligned with the human visual system of associating the 2D appearance with the 3D geometry structure. Different from existing CNNbased approaches, the proposed Omni-scale Graph Network (OG-Net) takes the advantage of 3D prior knowledge and 2D appearance information in an end-to-end manner, starting from 3D human point clouds. Given 3D points and the nearest neighbour graph, the basic Omni-scale module can aggregate different-scale neighbor information in the topology, enriching the representation ability. This allows the proposed OG-Net efficiently learns discriminative feature via limited network parameters. Extensive experiments suggest that OG-Net exploits the complementary information of 3D geometry information and the 2D appearance, yielding the competitive performance on four person re-id benchmarks. The 3D prior knowledge also benefits the model generalizability on the unseen pedestrian data, which is close to the application in real-world scenarios.</p><p>The proposed OG-Net still have room for futher improvements. In experiment, the proposed method learns the representation from the generated 3D point clouds mapping from 2D images. Although it works, the original 2D images are usually resized and compressed in most person re-id datasets, compromising the body shape, e.g., height. We may consider collecting a new 3D dataset in the future. Furthermore, the proposed method has the potential to many related fields. Similar graph-based models can be employed to other potential fields, e.g., objects with a rigid structure like vehicles <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b98">[99]</ref> and products <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b100">[101]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our brain generally associates the 2D appearance with prior knowledge of the 3D body shape. In this work, we intend to simulate this process and explore robust pedestrian representation with a lightweight model. (Dash arrows are missing in prevailing re-id methods.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>OG-Net Architecture. OG-Net is simply built via stacking Omni-scale Modules. (m ? c) denotes the feature of m points with c-dim attribute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of Omni-scale Module. We provide the feature shape as the format of (?). For instance, (m ? c) denotes the feature of m points with c-dim attribute. (a) We show the basic Omni-scale module without downsampling. (b) We show the Omni-scale module with downsampling, which is similar to the conventional pooling layer. The module distills the number of the points and improves the training efficiency. The dash line denotes the short-cut connection. Besides, we highlight two function types, i.e., cross-point functions and per-point functions, in red. The cross-point function aggregates the feature among neighbor points, while the per-point function only considers the single-point feature. The proposed Omni-scale module consists of these two kinds of functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization of Retrieval Results. (a) Given one 3D query, we show the original 2D images and the top-5 retrieval results. (b) We also show the challenging case, such as occlusion and the partially detected query. The green index indicates the true-matches, while the red index denotes the false-matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Sensitivity analysis on the point density. (left) We visualize point clouds with different proportion of the point number. (right) We provide the corresponding re-id performance in terms of Rank@1(%) and mAP(%) against the point number variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Zheng and Yi Yang are with ReLER Lab, Australian Artificial Intelligence Institute, University of Technology Sydney, NSW 2007, Australia. E-mail: zdzheng12@gmail.com, yi.yang@uts.edu.</figDesc><table /><note>au Nenggan Zheng is with the School of Computer Science, Zhejiang Univer- sity, Hangzhou 310027, China. E-mail: zng@cs.zju.edu.cn</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDY OF DIFFERENT INPUTS ON MARKET-1501. ?: FOR A FAIR COMPARISON, THE MODEL IS TRAINED ON THE TRADITIONAL 2D IMAGE INPUTS WITH EXTRA 3DCOORDINATES (x, y, 0).</figDesc><table><row><cell>Inputs</cell><cell cols="2">Market-1501 DukeMTMC-reID R@1 mAP R@1 mAP</cell></row><row><cell>3D Visible Part</cell><cell>77.64 54.52 59.52</cell><cell>37.25</cell></row><row><cell>2D Image  ?</cell><cell>85.72 67.28 75.49</cell><cell>55.98</cell></row><row><cell cols="2">3D Visible Part + 2D Background 86.79 67.92 77.33</cell><cell>57.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III TRANSFERRING</head><label>III</label><figDesc>TO UNSEEN DATASETS. HERE WE DIRECTLY DEPLOY THE MODEL TRAINED ON THE DATASET A TO THE UNSEEN DATASET B. WE DENOTE THIS SETTING AS A ? B, WHICH COULD REFLECT THE SCALABILITY OF THE MODEL IN DIFFERENT SCENARIOS.WE OBSERVE THAT OGNET IS GENERALLY SUPERIOR TO THE RESNET-50 AND DENSENET-121 AS WELL AS LIGHTWEIGHT MODELS, SUCH AS SHUFFLENETV2 AND MOBILENETV2.</figDesc><table><row><cell>Method</cell><cell>Input Type</cell><cell>Loss Function</cell><cell cols="12">Market?Duke Duke?Market Market?MSMT MSMT?Market Duke?MSMT MSMT?Duke R@1 mAP R@1 mAP R@1 mAP R@1 mAP R@1 mAP R@1 mAP</cell></row><row><cell>DGCNN [70]</cell><cell>point clouds</cell><cell>CE</cell><cell>7.4</cell><cell>2.9</cell><cell>13.4</cell><cell>4.4</cell><cell>1.4</cell><cell>0.4</cell><cell>10.0</cell><cell>3.7</cell><cell>1.8</cell><cell>0.5</cell><cell>7.3</cell><cell>2.7</cell></row><row><cell cols="2">PointNet++ (SSG) [68] point clouds</cell><cell>CE</cell><cell>18.6</cell><cell>8.4</cell><cell>28.8</cell><cell>11.3</cell><cell>3.9</cell><cell>1.2</cell><cell>32.4</cell><cell>13.3</cell><cell>5.5</cell><cell>1.7</cell><cell>29.0</cell><cell>15.4</cell></row><row><cell cols="2">PointNet++ (MSG) [68] point clouds</cell><cell>CE</cell><cell>23.2</cell><cell>11.0</cell><cell>32.8</cell><cell>12.6</cell><cell>5.0</cell><cell>1.5</cell><cell>30.6</cell><cell>12.9</cell><cell>6.5</cell><cell>1.9</cell><cell>24.3</cell><cell>12.4</cell></row><row><cell cols="4">PointNet++ (MSG) [68] point clouds CE + Circle 25.4</cell><cell>12.2</cell><cell>35.1</cell><cell>14.5</cell><cell>5.4</cell><cell>1.7</cell><cell>35.6</cell><cell>15.1</cell><cell>6.4</cell><cell>1.9</cell><cell>31.4</cell><cell>17.3</cell></row><row><cell>ShuffleNetV2 [88]</cell><cell>images</cell><cell>CE</cell><cell>17.2</cell><cell>7.2</cell><cell>36.4</cell><cell>13.9</cell><cell>2.8</cell><cell>0.8</cell><cell>36.5</cell><cell>14.1</cell><cell>5.8</cell><cell>1.5</cell><cell>29.3</cell><cell>15.3</cell></row><row><cell>ShuffleNetV2 [88]</cell><cell>images</cell><cell cols="2">CE + Circle 18.7</cell><cell>8.5</cell><cell>36.2</cell><cell>13.7</cell><cell>3.4</cell><cell>1.0</cell><cell>36.4</cell><cell>14.4</cell><cell>6.0</cell><cell>1.6</cell><cell>29.4</cell><cell>15.1</cell></row><row><cell>MobileNetV2 [89]</cell><cell>images</cell><cell>CE</cell><cell>16.7</cell><cell>7.1</cell><cell>34.3</cell><cell>12.4</cell><cell>3.2</cell><cell>0.9</cell><cell>35.9</cell><cell>14.2</cell><cell>5.5</cell><cell>1.4</cell><cell>30.6</cell><cell>15.4</cell></row><row><cell>MobileNetV2 [89]</cell><cell>images</cell><cell cols="2">CE +Circle 18.5</cell><cell>8.0</cell><cell>34.1</cell><cell>13.3</cell><cell>3.5</cell><cell>0.9</cell><cell>32.1</cell><cell>13.3</cell><cell>5.3</cell><cell>1.4</cell><cell>30.3</cell><cell>15.5</cell></row><row><cell>DenseNet-121 [90]</cell><cell>images</cell><cell>CE</cell><cell>11.7</cell><cell>5.0</cell><cell>32.7</cell><cell>11.6</cell><cell>2.9</cell><cell>0.8</cell><cell>34.2</cell><cell>13.0</cell><cell>5.3</cell><cell>1.5</cell><cell>27.8</cell><cell>13.6</cell></row><row><cell>DenseNet-121 [90]</cell><cell>images</cell><cell cols="2">CE + Circle 12.3</cell><cell>5.3</cell><cell>32.6</cell><cell>11.9</cell><cell>2.6</cell><cell>0.8</cell><cell>31.9</cell><cell>12.0</cell><cell>5.3</cell><cell>1.4</cell><cell>25.2</cell><cell>12.8</cell></row><row><cell>ResNet-50 [75]</cell><cell>images</cell><cell>CE</cell><cell>12.1</cell><cell>5.2</cell><cell>34.3</cell><cell>13.5</cell><cell>2.7</cell><cell>0.7</cell><cell>34.7</cell><cell>13.5</cell><cell>5.4</cell><cell>1.5</cell><cell>28.1</cell><cell>14.4</cell></row><row><cell>ResNet-50 [75]</cell><cell>images</cell><cell cols="2">CE + Circle 15.5</cell><cell>6.9</cell><cell>35.7</cell><cell>13.9</cell><cell>2.9</cell><cell>0.8</cell><cell>32.4</cell><cell>12.2</cell><cell>6.1</cell><cell>1.6</cell><cell>24.3</cell><cell>12.0</cell></row><row><cell>OG-Net</cell><cell>point clouds</cell><cell>CE</cell><cell>26.5</cell><cell>13.1</cell><cell>35.9</cell><cell>14.5</cell><cell>5.9</cell><cell>1.7</cell><cell>40.1</cell><cell>17.6</cell><cell>6.8</cell><cell>1.9</cell><cell>35.2</cell><cell>19.3</cell></row><row><cell>OG-Net</cell><cell cols="3">point clouds CE + Circle 26.4</cell><cell>13.7</cell><cell>36.4</cell><cell>14.7</cell><cell>5.3</cell><cell>1.6</cell><cell>38.8</cell><cell>16.9</cell><cell>6.3</cell><cell>1.9</cell><cell>35.3</cell><cell>19.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV EFFECTIVENESS</head><label>IV</label><figDesc>OF DIFFERENT COMPONENTS. WE COMPARE THE NETWORK VARIANTS, INCLUDING SQUEEZE-EXCITATION (SE), THE USAGE OF KNN GRAPH AND THE LAST NON-LOCAL ATTENTION IN THE MODEL.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">Performance</cell><cell></cell></row><row><cell>with Squeeze-excitation?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with KNN Graph?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with Last Non-local?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rank@1</cell><cell>83.35</cell><cell>84.38</cell><cell>86.43</cell><cell>87.38</cell></row><row><cell>mAP(%)</cell><cell>64.65</cell><cell>65.50</cell><cell>69.33</cell><cell>70.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V EFFECTIVENESS</head><label>V</label><figDesc>OF THE SHORT-CUT CONNECTION. WE OBSERVE A SIMILAR RESULT WITH [75] THAT THE IMPROVEMENT FROM THE SHORT-CUT CONNECTION IS NOT SIGNIFICANT ON THE "SHALLOW" NETWORK, WHILE IT WORKS WELL ON THE RELATIVELY DEEP NETWORK STRUCTURE.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Fig. 7. Sensitivity analysis on the different number of neighbors k. We provide</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the corresponding re-id performance on Market-1501 in terms of Rank@1(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>and mAP(%).</cell></row><row><cell>Method</cell><cell>Short-cut</cell><cell>R@1</cell><cell>mAP</cell></row><row><cell>OG-Net</cell><cell>?</cell><cell>86.82</cell><cell>69.02</cell></row><row><cell>OG-Net</cell><cell></cell><cell>84.00</cell><cell>65.04</cell></row><row><cell>OG-Net-Deep</cell><cell>?</cell><cell>86.28</cell><cell>68.49</cell></row><row><cell>OG-Net-Deep</cell><cell></cell><cell>88.81</cell><cell>72.91</cell></row></table><note>64.65% to 65.50%. On the other hand, the short-cut connec- tions do not provide significant improvement or performance drop on OG-Net, since OG-Net is relatively shallow with four Omni-scale blocks. As shown in Table V, we deploy the OG- Net-Deep to further validate this point. The observation is consistent with ResNet [75]. The short-cut connection works</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI CLASSIFICATION</head><label>VI</label><figDesc>RESULTS ON MODELNET [92]. WE DO NOT FOCUS ON THE POINT CLOUD CLASSIFICATION PROBLEM, BUT SHOW THE FEASIBILITY OF THE PROPOSED OG-NET. ? : WE PROVIDE RESULTS BASED ON OUR RE-IMPLEMENTATION, WHICH IS SLIGHTLY HIGHER THAN THE REPORTED RESULT IN<ref type="bibr" target="#b67">[68]</ref>.</figDesc><table><row><cell>Method</cell><cell>#params(M)</cell><cell cols="2">Mean-class Overall Accuracy Accuracy</cell></row><row><cell>3DShapeNets [92]</cell><cell>-</cell><cell>77.3</cell><cell>84.7</cell></row><row><cell>VoxNet [93]</cell><cell>-</cell><cell>83.0</cell><cell>85.9</cell></row><row><cell>PointNet [67]</cell><cell>3.50</cell><cell>86.0</cell><cell>89.2</cell></row><row><cell>SpecGCN [94]</cell><cell>2.05</cell><cell>-</cell><cell>91.5</cell></row><row><cell>PointNet++(SSG)  ? [68]</cell><cell>1.62</cell><cell>89.5</cell><cell>92.0</cell></row><row><cell>PCNN by Ext [95]</cell><cell>1.40</cell><cell>-</cell><cell>92.2</cell></row><row><cell>PointNet++(MSG)  ? [68]</cell><cell>1.89</cell><cell>90.1</cell><cell>92.7</cell></row><row><cell>DGCNN [70]</cell><cell>1.81</cell><cell>90.2</cell><cell>92.9</cell></row><row><cell>Point Cloud Transformer [96]</cell><cell>2.88</cell><cell>-</cell><cell>93.2</cell></row><row><cell>OG-Net-Small</cell><cell>1.22</cell><cell>90.5</cell><cell>93.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The reidentification challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
	<note>in Person re-identification</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond intra-modality: A survey of heterogeneous person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-scale deep learning architectures for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Leader-based multiscale attention deep architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="371" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Looking beyond appearances: Synthetic training data for deep cnns in re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rognhaugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="50" to="62" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dissecting person re-identification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhancing person re-identification in a self-trained subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning of compact binary descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1501" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The devil is in the middle: Exploiting mid-level representations for crossdomain instance matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Features for multi-target multi-camera tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6036" to="6046" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person reidentification via structural deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2987" to="2998" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transferable joint attributeidentity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2275" to="2284" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attribute recognition by joint recurrent learning of context and correlation</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pose-invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4500" to="4509" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep coattention-based comparator for relative representation learning in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Iaunet: Global context-aware feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="420" to="429" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning part-based convolutional features for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?kmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pose-guided visible part matching for occluded person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pose-guided feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="542" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Magnifiernet: Towards semantic adversary and fusion for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">BMVC</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention-based neural architecture search for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3750" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A comprehensive survey of neural architecture search: Challenges and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2017" to="2025" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Fd-gan: Pose-guided feature distilling gan for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning disentangled representation for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pose-normalized image generation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pose transferrable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Joint disentangling and adaptation for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multipseudo regularized label for generated data in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1391" to="1403" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Imageimage domain adaptation with preserved self-similarity and domaindissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning to adapt invariance in memory for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to reduce dual-level discrepancy for infrared-visible person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pamtri: Pose-aware multi-task learning for vehicle re-identification using highly randomized synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Simulating content consistent vehicle datasets with attribute descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08855</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Crossdomain complementary learning using pose for multi-person part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud autoencoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<title level="m">Deepgcns: Can gcns go as deep as cnns?&quot; in ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Enhanced computer vision with microsoft kinect sensor: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1318" to="1334" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Pct: Point cloud transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09688</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">K-nearest neighbor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scholarpedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1883</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anastasiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Partguided attention learning for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TITS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07249</idno>
		<title level="m">Rpc: A large-scale retail product checkout dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Hi, magic closet, tell me what to wear</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">He is currently a Ph.D. student with the School of Computer Science at University of Technology Sydney, Australia. His research interests include robust learning for image retrieval, generative learning for data augmentation, and unsupervised domain adaptation</title>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Zhedong Zheng received the B.S. degree in computer science from Fudan University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">He is currently a professor in computer science with the Academy for Advanced Studies, Zhejiang University. His current research interests include artificial intelligence, embedded systems</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Nenggan Zheng received the B. E. degree in Biomedical Engineering and the Ph. D. degree in Computer Science, both from Zhejiang University</orgName>
		</respStmt>
	</monogr>
	<note>and brain-computer interface</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">His current research interest includes machine learning and its applications to multimedia content analysis and computer vision, such as multimedia indexing and retrieval, video analysis and video semantics understanding</title>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Hangzhou, China; Pittsburgh, PA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Yi Yang received the Ph.D. degree in computer science from Zhejiang University ; Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>He is currently a professor with University of Technology Sydney, Australia. He was a Post-Doctoral Research with the School of Computer Science</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
