<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNET 3+: A FULL-SCALE CONNECTED UNET FOR MEDICAL IMAGE SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanfen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofeng</forename><surname>Tong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjie</forename><surname>Hu</surname></persName>
							<email>hongjiehu@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Sir Run Run Shaw Hospital</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaowei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Sir Run Run Shaw Hospital</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Iwamoto</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">Ritsumeikan University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhua</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Wei</forename><surname>Chen</surname></persName>
							<email>yen-weichenchen@is.ritsumei.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">Ritsumeikan University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Research Center for Healthcare Data Science</orgName>
								<address>
									<addrLine>Zhejiang Lab</addrLine>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">Ritsumeikan University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UNET 3+: A FULL-SCALE CONNECTED UNET FOR MEDICAL IMAGE SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>*Corresponding Authors: Lanfen Lin (llf@zju.edu.cn), Hongjie Hu</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Segmentation</term>
					<term>Full-scale skip connection</term>
					<term>Deep supervision</term>
					<term>Hybrid loss function</term>
					<term>Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, a growing interest has been seen in deep learningbased semantic segmentation. UNet, which is one of deep learning networks with an encoder-decoder architecture, is widely used in medical image segmentation. Combining multi-scale features is one of important factors for accurate segmentation. UNet++ was developed as a modified Unet by designing an architecture with nested and dense skip connections. However, it does not explore sufficient information from full scales and there is still a large room for improvement. In this paper, we propose a novel UNet 3+, which takes advantage of full-scale skip connections and deep supervisions. The full-scale skip connections incorporate low-level details with high-level semantics from feature maps in different scales; while the deep supervision learns hierarchical representations from the full-scale aggregated feature maps. The proposed method is especially benefiting for organs that appear at varying scales. In addition to accuracy improvements, the proposed UNet 3+ can reduce the network parameters to improve the computation efficiency. We further propose a hybrid loss function and devise a classification-guided module to enhance the organ boundary and reduce the over-segmentation in a non-organ image, yielding more accurate segmentation results. The effectiveness of the proposed method is demonstrated on two datasets. The code is available at: github.com/ZJUGiveLab/UNet-Version</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic organ segmentation in medical images is a critical step in many clinical applications. Recently, convolutional neural networks (CNNs) greatly promoted to developed a variety of segmentation models, e.g. fully convolutional neural networks (FCNs) <ref type="bibr" target="#b0">[1]</ref>, UNet <ref type="bibr" target="#b1">[2]</ref>, PSPNet <ref type="bibr" target="#b2">[3]</ref> and a series of DeepLab version <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Especially, UNet, which is based on an encoder-decoder architecture, is widely used in medical image segmentation. It uses skip connections to combine the high-level semantic feature maps from the decoder and corresponding low-level detailed feature maps from the encoder. To recede the fusion of semantically dissimilar feature from plain skip connections in UNet, UNet++ <ref type="bibr" target="#b6">[7]</ref> further strengthened these connections by introducing nested and dense skip connections, aiming at reducing the semantic gap between the encoder and decoder. Despite achieving good performance, this type of approach is still incapable of exploring sufficient information from full scales.</p><p>As witnessed in many segmentation studies <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, feature maps in different scale explore distinctive information. Lowlevel detailed feature maps capture rich spatial information, which highlight the boundaries of organs; while high-level semantic feature maps embody position information, which locate where the organs are. Nevertheless, these exquisite signals may be gradually diluted when progressively down-and up-sampling. To make full use of the multi-scale features, we propose a novel U-shape-based architecture, named UNet 3+, in which we re-design the inter-connection between the encoder and the decoder as well as the intra-connection between the decoders to capture fine-grained details and coarsegrained semantics from full scales. To further learn hierarchical representations from the full-scale aggregated feature maps, each side output is connected with a hybrid loss function, which contributes to accurate segmentation especially for organs that appear at varying scales in the medical image volume. In addition to accuracy improvements, we also show that the proposed UNet 3+ can reduce the network parameters to improve the computation efficiency.</p><p>To address the demand for more accurate segmentation in medical image, we further investigate how to effectively reduce the false positives in non-organ images. Existing methods solve the problem by introducing attention mechanisms <ref type="bibr" target="#b7">[8]</ref> or conducting a pre-defined refinement approach such as CRF <ref type="bibr" target="#b3">[4]</ref> at inference. Different from these methods, we extend a classification task to predict the input image whether has organ, providing a guidance to the segmentation task.</p><p>In summary, our main contributions are four-fold: (i) devising a novel UNet 3+ to make full use of the multi-scale features by introducing full-scale skip connections, which incorporate low-level details with high-level semantics from feature maps in full scales, but with fewer parameters; (ii) developing a deep supervision to learn hierarchical representations from the full-scale aggregated feature maps, which optimizes a hybrid loss function to enhance the organ boundary; (iii) proposing a classification-guided module to reduce over-segmentation on none-organ image by jointly training with an image-level classification; (iv) conducting extensive experiments on liver and spleen datasets, where UNet 3+ yields consistent improvements over a number of baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Full-scale Skip Connections</head><p>The proposed full-scale skip connections convert the interconnection between the encoder and decoder as well as intraconnection between the decoder sub-networks. Both UNet with plain connections and UNet++ with nested and dense connections are short of exploring sufficient information from full scales, failing to explicitly learn position and boundary of an organ. To remedy the defect in UNet and UNet++, each decoder layer in UNet 3+ incorporates both smaller-and same-scale feature maps from encoder and larger-scale feature maps from decoder, which capturing fine-grained details and coarse-grained semantics in full scales.</p><p>As an example, <ref type="figure" target="#fig_4">Fig. 2</ref> illustrates how to construct the feature map of "# $ . Similar to the UNet, the feature map from the same-scale encoder layer %&amp; $ are directly received in the decoder. In contrast to the UNet, a set of inter encoder-decode skip connections delivers the low-level detailed information from the smaller-scale encoder layer %&amp; ' and %&amp; ( , by applying non-overlapping max pooling operation; while a chain of intra decoder skip connections transmits the high-level semantic information from larger-scale decoder layer "# ) and "# * , by utilizing bilinear interpolation. With the five sameresolution feature maps in hand, we need to further unify the number of channels, as well as reduce the superfluous information. It occurred to us that the convolution with 64 filters of size 3 ? 3 can be a satisfying choice. To seamlessly merge the shallow exquisite information with deep semantic information, we further perform a feature aggregation mechanism on the concatenated feature map from five scales, which consists of 320 filters of size 3 ? 3, a batch normalization and a ReLU activation function. Formally, we formulate the skip connections as follows: let i indexes the down-sampling layer along the encoder, refers to the total number of the encoder. The stack of feature maps represented by "# , is computed as: It is worth mentioning that our proposed UNet 3+ is more efficient with fewer parameters. In the encoder sub-network, UNet, UNet++, and UNet 3+ share the same structure, where %&amp; , has 32? 2 , channels. As for the decoder, the depth of feature map in UNet is symmetric to the encoder, and thus "# , also has 32 ? 2 , channels. The number of parameters in LM decoder stage of UNet ( O6"# , ) can be computed as:</p><formula xml:id="formula_0">O6"# , = Q ? Q ? "# ,A' ? "# , + "# , ( + %&amp; , + "# , ? "# ,<label>(2)</label></formula><p>where Q is the convolution kernel size, ? denotes the depth of the nodes. When it comes to UNet++, it makes use of a dense convolution block along each skip pathway, where </p><p>As can be seen, </p><p>For the sake of the channel reduction, the parameters in UNet 3+ is fewer than those in UNet and UNet++. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Full-scale Deep Supervision</head><p>In order to learn hierarchical representations from the fullscale aggregated feature maps, the full-scale deep supervision is further adopted in the UNet 3+. Compared with the deep supervision performed on generated full-resolution feature map in UNet++, the proposed UNet 3+ yields a side output from each decoder stage, which is supervised by the ground truth. To realize deep supervision, the last layer of each decoder stage is fed into a plain 3 ? 3 convolution layer followed by a bilinear up-sampling and a sigmoid function.</p><p>To further enhance the boundary of organs, we propose a multi-scale structural similarity index (MS-SSIM) <ref type="bibr" target="#b8">[9]</ref> loss function to assign higher weights to the fuzzy boundary. Benefiting from it, the UNet 3+ will keep eye on fuzzy boundary as the greater the regional distribution difference, the higher the MS-SSIM value. Two corresponding N?N sized patches are cropped from the segmentation result P and the ground truth mask G, which can be denoted as = { [ : = 1, ? , <ref type="bibr">(</ref>  </p><p>where is the total number of the scales, c , d and c , c are the mean and standard deviations of p and g, cd denotes their covariance. a and a define the relative importance of the two components in each scale, which are set according to <ref type="bibr" target="#b8">[9]</ref>. Two small constants ' = 0.01 ( and ( = 0.03 ( are added to avoid the unstable circumstance of dividing by zero. In our experiment, we set the scale to 5 based on <ref type="bibr" target="#b8">[9]</ref>.</p><p>By combining focal loss ( ? p: ) <ref type="bibr" target="#b9">[10]</ref>, MS-SSIM loss (? a;6;;,a ) and IoU loss (? ,qr ) <ref type="bibr" target="#b10">[11]</ref>, we develop a hybrid loss for segmentation in three-level hierarchy -pixel-, patch-and map-level, which is able to capture both large-scale and fine structures with clear boundaries. The hybrid segmentation loss (? ;#d )is defined as: ? ;#d = ? p: + ? a;6;;,a + ? ,qr</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Classification-guided Module (CGM)</head><p>In the most medical image segmentations, the appearance of false-positives in a non-organ image is an inevitable circums- tance. It is, in all probability, caused by noisy information from background remaining in shallower layer, leading to the phenomenon of over-segmentation. To achieve more accurate segmentation, we attempt to solve this problem by adding an extra classification task, which is designed for predicting the input image whether has organ or not. As depicted in <ref type="figure" target="#fig_6">Fig. 3</ref>, after passing a series of operations including dropout, convolution, maxpooling and sigmoid, a 2-dimensional tensor is produced from the deepest-level %&amp; * , each of which represents the probability of with/without organs. Benefiting from the richest semantic information, the classification result can further guide each segmentation sideoutput in two steps. First, with the help of the argmax function, 2-dimensional tensor is transferred into a single output of {0,1}, which denotes with/without organs. Subsequently, we multiply the single classification output with the side segmentation output. Due to the simpleness of binary classification task, the module effortlessly achieves accurate classification results under the optimization of Binary Cross Entropy loss function <ref type="bibr" target="#b11">[12]</ref>, which realizes the guidance for remedying the drawback of over-segmentation on none-organ image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and Implementation</head><p>The method was validated on two organs: the liver and spleen. The dataset for liver segmentation is obtained from the ISBI LiTS 2017 Challenge. It contains 131 contrast-enhanced 3D abdominal CT scans, of which 103 and 28 volumes are used for training and testing, respectively. The spleen dataset from the hospital passed the ethic approvals, containing 40 and 9 CT volumes for training and testing. In order to speed up training, the input image had three channels, including the slice to be segmented and the upper and lower slices, which was cropped to 320?320. We utilized the stochastic gradient descent to optimize our network and its hyper parameters were set to the default values. Dice coefficient was used as the evaluation metric for each case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison with UNet and UNet++</head><p>In this section, we first compared the proposed UNet 3+ with UNet and UNet++. The loss function used in each method is the focal loss.   <ref type="table" target="#tab_1">Table 1</ref> compares UNet, UNet++ and the proposed UNet 3+ architecture in terms of the number of parameters and segmentation accuracy on both liver and spleen datasets. As seen, UNet 3+ without deep supervision achieves a surpassing performance over UNet and UNet++, obtaining average improvement of 2.7 and 1.6 point between two backbones performed on two datasets. Taking account of both liver and spleen appearing at varying scales in CT slices, UNet 3+ combined with full-scale deep supervision further improved 0.4 point. (ii) Qualitative comparison: <ref type="figure" target="#fig_4">Figure 2</ref> exhibits the segmentation results of ResNet-101-based UNet, UNet++ and UNet 3+ with full-scale deep supervision on liver datasets. It can be observed that our proposed method not only accurately localizes organs but also produces coherent boundaries, even in small object circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with the State of the Art</head><p>We quantitatively compare our ResNet-101-based UNet 3+ with several recent state-of-the-art methods: PSPNet <ref type="bibr" target="#b2">[3]</ref>, De-epLabV2 <ref type="bibr" target="#b3">[4]</ref>, DeepLabV3 <ref type="bibr" target="#b4">[5]</ref>, DeepLabV3+ <ref type="bibr" target="#b5">[6]</ref> and Attention UNet <ref type="bibr" target="#b7">[8]</ref>. It is worth mentioning that all results are directly from single-model test without relying on any post-processing tools. Moreover, all networks were optimized by the loss function proposed in their own paper. <ref type="table" target="#tab_2">Table 2</ref> summarizes the quantitative comparison results. As seen, the proposed hybrid loss function greatly improves the performance by taking pixel-, patch-, map-level optimization in to consideration. Especially, the patch-level MS-SSIM loss function contributes to assign higher weights to the fuzzy boundary, yielding more enhance boundary-aware segmentation map. Moreover, taking advantages of the classification-guidance module, UNet 3+ skillfully avoids the oversegmentation in complex background. As it can be seen, this approach is outstanding compared to all other previous approaches. It is also worth noting that the proposed method outperforms the second best result on the liver (0.9675 against 0.9341) and spleen (0.9620 against 0.9324).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, we proposed a full-scale connected UNet, named UNet 3+ with deep supervision in order to make maximum use of feature maps in full scales for accurate segmentation and efficient network architecture with fewer parameters. The classification-guided module and a hybrid loss function is further introduced to yielding more accurate positionaware and boundary-aware segmentation map. Experimental results on both liver and spleen datasets demonstrated that UNet 3+ surpasses all previous state-of-the-art approaches, highlighting the organs and producing coherent boundaries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig . 1 :</head><label>1</label><figDesc>Comparison of UNet (a), UNet++(b) and proposed UNet 3+ (c). The depth of each node is presented below the circle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>gives simplified overviews of UNet, UNet++ and the proposed UNet 3+. Compared with UNet and UNet++, UNet 3+ combines the multi-scale features by re-designing skip connections as well as utilizing a full-scale deep supervision, which provides fewer parameters but yields a more accurate position-aware and boundary-enhanced segmentation map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where function ?(?) denotes a convolution operation, ?(?) realizes the feature aggregation mechanism with a convolution followed by a batch normalization and a ReLU activation function.(?) and (?) indicate up-and down-sampling operation respectively, and [?] represents the concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of how to construct the full-scale aggregated feature map of third decoder layer "# $ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>} and = [ : = 1, ? , ( , respectively. The MS-SSIM loss function of p and g is defined as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of classification-guided module (CGM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative comparisons of ResNet-101-based UNet, UNet++, and proposed UNet 3+ on liver dataset. Purple areas: true positive (TP); Yellow areas: false negative (FN); Green areas: the false positive (FP). (i) Quantitative comparison: Based on the backbone of Vgg-16 and ResNet-101,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of UNet, UNet++, the proposed UNet 3+ without deep supervision (DS) and UNet 3+ on liver and spleen datasets in terms of Dice metrics. The best results are highlighted in bold. The loss function used in each method is focal loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of UNet 3+ and other 5 state-of-the-art methods. The best results are highlighted in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>This work was supported in part by Major Scientific Research Project of Zhejiang Lab under the Grant No.2018DG0ZX01, in part by the Key Science and Technology Innovation Support Program of Hangzhou under the Grant No.20172011A038, and in part by the Grant-in Aid for Scientific Research from the Japanese Ministry for Education, Science, Culture and Sports (MEXT) under the Grant No. 18H03267 and No.17H00754.</figDesc><table><row><cell>Architecture</cell><cell>Params</cell><cell>Vgg-16 :,u#v</cell><cell>;c:##&amp;</cell><cell>Params</cell><cell>ResNet-101 :,u#v</cell><cell>;c:##&amp;</cell><cell>9u#v9d#</cell></row><row><cell>UNet</cell><cell>39.39M</cell><cell>0.9206</cell><cell>0.9023</cell><cell>55.90M</cell><cell>0.9387</cell><cell>0.9332</cell><cell>0.9237</cell></row><row><cell>UNet++</cell><cell>47.18M</cell><cell>0.9278</cell><cell>0.9230</cell><cell>63.76M</cell><cell>0.9475</cell><cell>0.9423</cell><cell>0.9352</cell></row><row><cell>UNet 3+ w/o DS</cell><cell>26.97M</cell><cell>0.9489</cell><cell>0.9437</cell><cell>43.55M</cell><cell>0.9580</cell><cell>0.9539</cell><cell>0.9511</cell></row><row><cell>UNet 3+</cell><cell>26.97M</cell><cell>0.9550</cell><cell>0.9496</cell><cell>43.55M</cell><cell>0.9601</cell><cell>0.9560</cell><cell>0.9552</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell>:,u#v</cell><cell>;c:##&amp;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSPNet [3]</cell><cell>0.9242</cell><cell>0.9240</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DeepLabV2 [4]</cell><cell>0.9021</cell><cell>0.9097</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DeepLabV3 [5]</cell><cell>0.9217</cell><cell>0.9217</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DeepLabV3+ [6]</cell><cell>0.9186</cell><cell>0.9290</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Attention UNet [8]</cell><cell>0.9341</cell><cell>0.9324</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">UNet 3+ (focal loss)</cell><cell>0.9601</cell><cell>0.9560</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">UNet 3+ (Hybrid loss)</cell><cell>0.9643</cell><cell>0.9588</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">UNet 3+ (Hybrid loss + CGM)</cell><cell>0.9675</cell><cell>0.9620</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep Learning in Medical Image Anylysis and Multimodal Learning for Clinical Decision Support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
	<note>UNet++: A Nested U-Net Architecture for Medical Image Segmentation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention u-net: Learning where to look for the pancreas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals, Systems &amp; Computers</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep-roadmapper: Extracting road topology from aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mattyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A tutorial on the cross-entropy method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Boer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
