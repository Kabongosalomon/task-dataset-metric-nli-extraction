<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Kempka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Science</orgName>
								<orgName type="institution">Poznan University of Technology</orgName>
								<address>
									<settlement>Pozna?</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Wydmuch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Science</orgName>
								<orgName type="institution">Poznan University of Technology</orgName>
								<address>
									<settlement>Pozna?</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Runc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Science</orgName>
								<orgName type="institution">Poznan University of Technology</orgName>
								<address>
									<settlement>Pozna?</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Toczek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Science</orgName>
								<orgName type="institution">Poznan University of Technology</orgName>
								<address>
									<settlement>Pozna?</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Ja?kowski</surname></persName>
							<email>wjaskowski@cs.put.poznan.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Science</orgName>
								<orgName type="institution">Poznan University of Technology</orgName>
								<address>
									<settlement>Pozna?</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>video games</term>
					<term>visual-based reinforcement learning</term>
					<term>deep reinforcement learning</term>
					<term>first-person perspective games</term>
					<term>FPS</term>
					<term>visual learning</term>
					<term>neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the firstperson perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual signals are one of the primary sources of information about the surrounding environment for living and artificial beings. While computers have already exceeded humans in terms of raw data processing, they still do not match their ability to interact with and act in complex, realistic 3D environments. Recent increase in computing power (GPUs), and the advances in visual learning (i.e., machine learning from visual information) have enabled a significant progress in this area. This was possible thanks to the renaissance of neural networks, and deep architectures in particular. Deep learning has been applied to many supervised machine learning tasks and performed spectacularly well especially in the field of image classification <ref type="bibr" target="#b17">[18]</ref>. Recently, deep architectures have also been successfully employed in the reinforcement learning domain to train human-level agents to play a set of Atari 2600 games from raw pixel information <ref type="bibr" target="#b21">[22]</ref>.</p><p>Thanks to high recognizability and an easy-to-use software toolkit, Atari 2600 games have been widely adopted as a benchmark for visual learning algorithms. Atari 2600 games have, however, several drawbacks from the AI research perspective. First, they involve only 2D environments. Second, the environments hardly resemble the world we live in. Third, they are third-person perspective games, which does not match a real-world mobile-robot scenario. Last but not least, although, for some Atari 2600 games, human players are still ahead of bots trained from scratch, the best deep reinforcement learning algorithms are already ahead on average. Therefore, there is a need for more challenging reinforcement learning problems involving first-person-perspective and realistic 3D worlds.</p><p>In this paper, we propose a software platform, ViZDoom 1 , for the machine (reinforcement) learning research from raw visual information. The environment is based on Doom, the famous first-person shooter (FPS) video game. It allows developing bots that play Doom using only the screen buffer. The environment involves a 3D world that is significantly more real-world-like than Atari 2600 games. It also provides a relatively realistic physics model. An agent (bot) in ViZDoom has to effectively perceive, interpret, and learn the 3D world in order to make tactical and strategic decisions where to go and how to act. The strength of the environment as an AI research platform also lies in its customization capabilities. The platform makes it easy to define custom scenarios which differ by maps, environment elements, non-player characters, rewards, goals, and actions available to the agent. It is also lightweight -on modern computers, one can play the game at nearly 7000 frames per second (the real-time in Doom involves 35 frames per second) using a single CPU core, which is of particular importance if learning is involved.</p><p>In order to demonstrate the usability of the platform, we perform two ViZDoom experiments with deep Q-learning <ref type="bibr" target="#b21">[22]</ref>. The first one involves a somewhat limited 2D-like environment, for which we try to find out the optimal rate at which agents should make decisions. In the second experiment, the agent has to navigate a 3D maze collecting some object and omitting the others. The results of the experiments indicate that deep reinforcement learning is capable of tackling first-person perspective 3D environments 2 .</p><p>FPS games, especially the most popular ones such as Unreal Tournament <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, Counter-Strike <ref type="bibr" target="#b14">[15]</ref> or Quake III Arena <ref type="bibr" target="#b7">[8]</ref>, have already been used in AI research. However, in these studies agents acted upon high-level information like positions of walls, enemies, locations of items, etc., which are usually inaccessible to human players. Supplying only raw visual information might relieve researchers of the burden of providing AI with high-level information and handcrafted features. We also hypothesize that it could make the agents behave more believable <ref type="bibr" target="#b15">[16]</ref>. So far, there has been no studies on reinforcement learning from visual information obtained from FPS games.</p><p>To date, there have been no FPS-based environments that allow research on agents relying exclusively on raw visual information. This could be a serious factor impeding the progress of vision-based reinforcement learning, since engaging in it requires a large amount of programming work. Existence of a ready-to-use tool facilitates conducting experiments and focusing on the goal of the research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>One of the earliest works on visual-based reinforcement learning is due to Asada et al. <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b2">[3]</ref>, who trained robots various elementary soccer-playing skills. Other works in this area include teaching mobile robots with visual-based Qlearning <ref type="bibr" target="#b9">[10]</ref>, learning policies with deep auto-encoders and batch-mode algorithms <ref type="bibr" target="#b18">[19]</ref>, neuroevolution for a vision-based version of the mountain car problem <ref type="bibr" target="#b5">[6]</ref>, and compressed neuroevolution with recurrent neural networks for vision-based car simulator <ref type="bibr" target="#b16">[17]</ref>. Recently, Mnih et al. have shown a deep Q-learning method for learning Atari 2600 games from visual input <ref type="bibr" target="#b21">[22]</ref>.</p><p>Different first-person shooter (FPS) video games have already been used either as AI research platforms, or application domains. The first academic work on AI in FPS games is due to Geisler <ref type="bibr" target="#b10">[11]</ref>. It concerned modeling player behavior in Soldier of Fortune 2. Cole used genetic algorithms to tune bots in Counter Strike <ref type="bibr" target="#b4">[5]</ref>. Dawes <ref type="bibr" target="#b6">[7]</ref> identified Unreal Tournament 2004 as a potential AI research test-bed. El Rhalib studied weapon selection in Quake III Arena <ref type="bibr" target="#b7">[8]</ref>. Smith devised a RETALIATE reinforcement learning algorithm for optimizing team tactics in Unreal Tournament <ref type="bibr" target="#b22">[23]</ref>. SARSA(?), another reinforcement learning method, was the subject of research in FPS games <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Recently, continuous and reinforcement learning techniques were applied to learn the behavior of tanks in the game BZFlag <ref type="bibr" target="#b23">[24]</ref>.</p><p>As far as we are aware, to date, there have been no studies that employed the genre-classical Doom FPS. Also, no previous study used raw visual information to develop bots in first-person perspective games with a notable exception of the Abel's et al. work on Minecraft <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VIZDOOM RESEARCH PLATFORM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Why Doom?</head><p>Creating yet another 3D first-person perspective environment from scratch solely for research purposes would be somewhat wasteful <ref type="bibr" target="#b26">[27]</ref>. Due to the popularity of the firstperson shooter genre, we have decided to use an existing game engine as the base for our environment. We concluded that it has to meet the following requirements:</p><p>1) based on popular open-source 3D FPS game (ability to modify the code and the publication freedom), 2) lightweight (portability and the ability to run multiple instances on a single machine), 3) fast (the game engine should not be the learning bottleneck), 4) total control over the game's processing (so that the game can wait for the bot decisions or the agent can learn by observing a human playing), 5) customizable resolution and rendering parameters, 6) multiplayer games capabilities (agent vs. agent and agent vs. human), 7) easy-to-use tools to create custom scenarios, 8) ability to bind different programming languages (preferably written in C++), 9) multi-platform.</p><p>In order to make the decision according to the above-listed criteria, we have analyzed seven recognizable FPS games: Quake III Arena, Doom 3, Half-Life 2, Unreal Tournament 2004, Unreal Tournament and Cube. Their comparison is shown in <ref type="table" target="#tab_0">Table I</ref>. Some of the features listed in the table are objective (e.g., 'scripting') and others are subjective ("code complexity"). Brand recognition was estimated as the number (in millions) of Google results (as of 26.04.2016) for phrases "game &lt;gamename&gt;", where &lt;gamename&gt; was 'doom', 'quake', 'half-life', 'unreal tournament' or 'cube'. The game was considered as low-resolution capable if it was possible to set the resolution to values smaller than 640 ? 480.</p><p>Some of the games had to be rejected right away in spite of high general appeal. Unreal Tournament 2004 engine is only accessible by the Software Development Kit and it lacks support for controlling the speed of execution and direct screen buffer access. The game has not been prepared to be heavily modified.</p><p>Similar problems are shared by Half-Life 2 despite the fact that the Source engine is widely known for modding capabilities. It also lacks direct multiplayer support. Although the Source engine itself offers multiplayer support, it involves client-server architecture, which makes synchronization and direct interaction with the engine problematic (network com- The client-server architecture was also one the reasons for rejection of Quake III: Arena. Quake III also does not offer any scripting capabilities, which are essential to make a research environment versatile. The rejection of Quake was a hard decision as it is a highly regarded and playable game even nowadays but this could not outweigh the lack of scripting support.</p><p>The latter problem does not concern Doom 3 but its high disk requirements were considered as a drawback. Doom 3 had to be ignored also because of its complexity, Windows-only tools, and OS-dependent rendering mechanisms. Although its source code has been released, its community is dispersed. As a result, there are several rarely updated versions of its sources.</p><p>The community activity is also a problem in the case of Cube as its last update was in August 2005. Nonetheless, the low complexity of its code and the highly intuitive map editor would make it a great choice if the engine was more popular.</p><p>Unreal Tournament, however popular, is not as recognizable as Doom or Quake but it has been a primary research platform for FPS games <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b25">[26]</ref>. It also has great capabilities. Despite its active community and the availability of the source code, it was rejected due to its high system requirements.</p><p>Doom (see <ref type="figure" target="#fig_0">Fig. 1</ref>) met most of the requirements and allowed to implement features that would be barely achievable in other <ref type="bibr" target="#b2">3</ref> GZDoom, the ZDoom's fork, is OpenGL-based.</p><p>games, e.g., off-screen rendering and custom rewards. The game is highly recognizable and runs on the three major operating systems. It was also designed to work in 320 ? 240 resolution and despite the fact that modern implementations allow bigger resolutions, it still utilizes low-resolution textures. Moreover, its source code is easy-to-understand.</p><p>The unique feature of Doom is its software renderer. Because of that, it could be run without the desktop environment (e.g., remotely in a terminal) and accessing the screen buffer does not require transferring it from the graphics card.</p><p>Technically, ViZDoom is based on the modernized, opensource version of Doom's original engine -ZDoom, which is still actively supported and developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Application Programming Interface (API)</head><p>ViZDoom API is flexible and easy-to-use. It was designed with reinforcement and apprenticeship learning in mind, and therefore, it provides full control over the underlying Doom process. In particular, it allows retrieving the game's screen buffer and make actions that correspond to keyboard buttons (or their combinations) and mouse actions. Some game state variables such as the player's health or ammunition are available directly.</p><p>ViZDoom's API was written in C++. The API offers a myriad of configuration options such as control modes and rendering options. In addition to the C++ support, bindings for Python and Java have been provided. The Python API example is shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Features</head><p>ViZDoom provides features that can be exploited in different kinds of AI experiments. The main features include different control modes, custom scenarios, access to the depth buffer and off-screen rendering eliminating the need of using a graphical interface.</p><p>1) Control modes: ViZDoom implements four control modes: i) synchronous player, ii) synchronous spectator, iii) asynchronous player, and iv) asynchronous spectator.</p><p>In asynchronous modes, the game runs at constant 35 frames per second and if the agent reacts too slowly, it can miss some frames. Conversely, if it makes a decision too quickly, it is blocked until the next frame arrives from the engine. Thus, for reinforcement learning research, more useful are the synchronous modes, in which the game engine waits for the decision maker. This way, the learning system can learn at its pace, and it is not limited by any temporal constraints.</p><p>Importantly, for experimental reproducibility and debugging purposes, the synchronous modes run deterministically.</p><p>In the player modes, it is the agent who makes actions during the game. In contrast, in the spectator modes, a human player is in control, and the agent only observes the player's actions.</p><p>In addition, ViZDoom provides an asynchronous multiplayer mode, which allows games involving up to eight players (human or bots) over a network.</p><p>2) Scenarios: One of the most important features of ViZ-Doom is the ability to run custom scenarios. This includes creating appropriate maps, programming the environment mechanics ("when and how things happen"), defining terminal conditions (e.g., "killing a certain monster", "getting to a certain place", "died"), and rewards (e.g., for "killing a monster", "getting hurt", "picking up an object"). This mechanism opens endless experimentation possibilities. In particular, it allows creating a scenario of a difficulty which is on par with the capabilities of the assessed learning algorithms.</p><p>Creation of scenarios is possible thanks to easy-to-use software tools developed by the Doom community. The two recommended free tools include Doom Builder 2 and SLADE 3. Both are visual editors, which allow defining custom maps and coding the game mechanics in Action Code Script. They also enable to conveniently test a scenario without leaving the editor.</p><p>ViZDoom comes with a few predefined scenarios. Two of them are described in Section IV.</p><p>3) Depth Buffer Access: ViZDoom provides access to the renderer's depth buffer (see <ref type="figure" target="#fig_3">Fig. 3</ref>), which may help an agent to understand the received visual information. This feature gives an opportunity to test whether the learning algorithms can autonomously learn the whereabouts of the objects in the environment. The depth information can also be used to simulate the distance sensors common in mobile robots. 4) Off-Screen Rendering and Frame Skipping: To facilitate computationally heavy machine learning experiments, we equipped ViZDoom with off-screen rendering and frame skipping features. Off-screen rendering lessens the performance burden of actually showing the game on the screen and makes it possible to run the experiments on the servers (no graphical interface needed). Frame skipping, on the other hand, allows omitting rendering selected frames at all. Intuitively, an effective bot does not have to see every single frame. We explore this issue experimentally in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ViZDoom's Performance</head><p>The main factors affecting ViZDoom performance are the number of the actors (like items and bots), the rendering resolution, and computing the depth buffer. <ref type="figure" target="#fig_4">Fig. 4</ref> shows how the number of frames per second depends on these factors.</p><p>The tests have been made in the synchronous player mode on Linux running on Intel Core i7-4790k. ViZDoom uses only a single CPU core.  The performance test shows that ViZDoom can render nearly 7000 low-resolution frames per second. The rendering resolution proves to be the most important factor influencing the processing speed. In the case of low resolutions, the time needed to render one frame is negligible compared to the backpropagation time of any reasonably complex neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Basic Experiment</head><p>The primary purpose of the experiment was to show that reinforcement learning from the visual input is feasible in ViZDoom. Additionally, the experiment investigates how the number of skipped frames (see Section III-C4) influences the learning process.</p><p>1) Scenario: This simple scenario takes place in a rectangular chamber (see <ref type="figure" target="#fig_5">Fig. 5</ref>). An agent is spawned in the center of the room's longer wall. A stationary monster is spawned at a random position along the opposite wall. The agent can strafe left and right, or shoot. A single hit is enough to kill the monster. The episode ends when the monster is eliminated or after 300 frames, whatever comes first. The agent scores 101 points for killing the monster, ?5 for a missing shot, and, additionally, ?1 for each action. The scores motivate the learning agent to eliminate the monster as quickly as possible, preferably with a single shot <ref type="bibr" target="#b3">4</ref> .</p><p>2) Deep Q-Learning: The learning procedure is similar to the Deep Q-Learning introduced for Atari 2600 <ref type="bibr" target="#b21">[22]</ref>. The problem is modeled as a Markov Decision Process and Qlearning <ref type="bibr" target="#b27">[28]</ref> is used to learn the policy. The action is selected by an -greedy policy with linear decay. The Q-function is approximated with a convolutional neural network, which is trained with Stochastic Gradient Decent. We also used experience replay but no target network freezing (see <ref type="bibr" target="#b21">[22]</ref>).</p><p>3) Experimental Setup: a) Neural Network Architecture: The network used in the experiment consists of two convolutional layers with 32 square filters, 7 and 4 pixels wide, respectively (see <ref type="figure" target="#fig_6">Fig. 6</ref>). Each convolution layer is followed by a max-pooling layer with max pooling of size 2 and rectified linear units for activation <ref type="bibr" target="#b13">[14]</ref>. Next, there is a fully-connected layer with 800 leaky rectified linear units <ref type="bibr" target="#b19">[20]</ref> and an output layer with 8 linear units corresponding to the 8 combinations of the 3 available actions (left, right and shot). b) Game Settings: A state was represented by the most recent frame, which was a 60 ? 45 3-channel RGB image. The number of skipped frames is controlled by the skipcount parameter. We experimented with skipcounts of 0-7, 10, 15, 20, 25, 30, 35 and 40. It is important to note that the agent repeats the last decision on the skipped frames. c) Learning Settings: We arbitrarily set the discount factor ? = 0.99, learning rate ? = 0.01, replay memory capacity to 10 000 elements and mini-batch size to 40. The initial = 1.0 starts to decay after 100 000 learning steps, finishing the decay at = 0.1 at 200 000 learning steps.</p><p>Every agent learned for 600 000 steps, each one consisting of performing an action, observing a transition, and updating the network. To monitor the learning progress, 1000 testing episodes were played after each 5000 learning steps. Final controllers were evaluated on 10 000 episodes. The experiment was performed on Intel Core i7-4790k 4GHz with GeForce GTX 970, which handled the neural network. 4) Results: <ref type="figure" target="#fig_7">Figure 7</ref> shows the learning dynamics for the selected skipcounts. It demonstrates that although all the agents improve over time, the skips influence the learning speed, its smoothness, as well as the final performance. When the agent does not skip any frames, the learning is the slowest. Generally, the larger the skipcount, the faster and smoother the learning is. We have also observed that the agents learning with higher skipcounts were less prone to irrational behaviors like staying idle or going the direction opposite to the monster, which results in lower variance on the plots. On the other hand, too large skipcounts make the agent 'clumsy' due to the lack of fine-grained control, which results in suboptimal final scores.</p><p>The detailed results, shown in <ref type="table" target="#tab_0">Table II</ref>, indicate that the optimal skipcount for this scenario is 4 (the "native" column). However, higher values (up to 10) are close to this maximum.</p><p>We have also checked how robust to skipcounts the agents are. For this purpose, we evaluated them using skipcounts different from ones they had been trained with. Most of the agents performed worse than with their "native" skipcounts. The least robust were the agents trained with skipcounts less than 4. Larger skipcounts resulted in more robust agents. Interestingly, for skipcounts greater than or equal to 30, the agents score better on skipcounts lower than the native ones. Our best agent that was trained with skipcount 4 was also the best when executed with skipcount 0.</p><p>It is also worth showing that increasing the skipcount influences the total learning time only slightly. The learning takes longer primarily due to the higher total overhead associated with episode restarts since higher skipcounts result in a greater number of episodes.</p><p>To sum up, the skipcounts in the range of 4-10 provide the best balance between the learning speed and the final performance. The results also indicate that it would be profitable to start learning with high skipcounts to exploit the steepest learning curve and gradually decrease it to fine-tune the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Medikit Collecting Experiment</head><p>The previous experiment was conducted on a simple scenario which was closer to a 2D arcade game rather than a true 3D virtual world. That is why we decided to test if similar deep reinforcement learning methods would work in a more involved scenario requiring substantial spatial reasoning. 1) Scenario: In this scenario, the agent is spawned in a random spot of a maze with an acid surface, which slowly, but constantly, takes away the agent's life (see <ref type="figure" target="#fig_8">Fig. 8</ref>). To survive, the agent needs to collect medikits and avoid blue vials with poison. Items of both types appear in random places during the episode. The agent is allowed to move (forward/backward), and turn (left/right). It scores 1 point for each tick, and it is punished by ?100 points for dying. Thus, it is motivated to survive as long as possible. To facilitate   learning, we also introduced shaping rewards of 100 and ?100 points for collecting a medikit and a vial, respectively. The shaping rewards do not count to the final score but are used during the agent's training helping it to 'understand' its goal. Each episode ends after 2100 ticks (1 minute in real-time) or when the agent dies so 2100 is the maximum achievable score.</p><p>Being idle results in scoring 284 points.</p><p>2) Experimental Setup: The learning procedure was the same as described in Section IV-A2 with the difference that for updating the weights RMSProp <ref type="bibr" target="#b24">[25]</ref> this time.</p><p>a) Neural Network Architecture: The employed network is similar the one used in the previous experiment. The differences are as follows. It involves three convolutional layers with 32 square filters 7, 5, and 3 pixels wide, respectively. The fullyconnected layer uses 1024 leaky rectified linear units and the output layer 16 linear units corresponding to each combination of the 4 available actions.</p><p>b) Game Settings: The game's state was represented by a 120 ? 45 3-channel RGB image, health points and the current tick number (within the episode). Additionally, a kind of memory was implemented by making the agent use 4 last states as the neural network's input. The nonvisual inputs (health, ammo) were fed directly to the first fully-connected layer. Skipcount of 10 was used. c) Learning Settings: We set the discount factor ? = 1, learning rate ? = 0.00001, replay memory capacity to 10 000 elements and mini-batch size to 64. The initial = 1.0 started to decay after 4 000 learning steps, finishing the decay at = 0.1 at 104 000 episodes.</p><p>The agent was set to learn for 1000 000 steps. To monitor the learning progress, 200 testing episodes were played after each 5000 learning steps. The whole learning process, including the testing episodes, lasted 29 hours.</p><p>3) Results: The learning dynamics is shown in <ref type="figure" target="#fig_9">Fig. 9</ref>. It can be observed that the agents fairly quickly learns to get the perfect score from time to time. Its average score, however, improves slowly reaching 1300 at the end of the learning. The trend might, however, suggest that some improvement is still possible given more training time. The plots suggest that even at the end of learning, the agent for some initial states fails to live more than a random player.</p><p>It must, however, be noted that the scenario is not easy and even from a human player, it requires a lot of focus. It is so because the medikits are not abundant enough to allow the bots to waste much time.</p><p>Watching the agent play 5 revealed that it had developed a policy consistent with our expectations. It navigates towards medikits, actively, although not very deftly, avoids the poison vials, and does not push against walls and corners. It also backpedals after reaching a dead end or a poison vial. However, it very often hesitates about choosing a direction, which results in turning left and right alternately on the spot. This quirky behavior is the most probable, direct cause of not fully satisfactory performance.</p><p>Interestingly, the learning dynamics consists of three sudden but ephemeral drops in the average and best score. The reason for such dynamics is unknown and it requires further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>ViZDoom is a Doom-based platform for research in visionbased reinforcement learning. It is easy-to-use, highly flexible, multi-platform, lightweight, and efficient. In contrast to the other popular visual learning environments such as Atari 2600, ViZDoom provides a 3D, semi-realistic, first-person perspective virtual world. ViZDoom's API gives the user full control of the environment. Multiple modes of operation facilitate experimentation with different learning paradigms such as reinforcement learning, apprenticeship learning, learning by demonstration, and, even the 'ordinary', supervised learning. The strength and versatility of environment lie in is customizability via the mechanism of scenarios, which can be conveniently programmed with open-source tools.</p><p>We also demonstrated that visual reinforcement learning is possible in the 3D virtual environment of ViZDoom by performing experiments with deep Q-learning on two scenarios. The results of the simple move-and-shoot scenario, indicate that the speed of the learning system highly depends on the number of frames the agent is allowed to skip during the learning. We have found out that it is profitable to skip from 4 to 10 frames. We used this knowledge in the second, more involved, scenario, in which the agent had to navigate through a hostile maze and collect some items and avoid the others. Although the agent was not able to find a perfect strategy, it learned to navigate the maze surprisingly well exhibiting evidence of a human-like behavior.</p><p>ViZDoom has recently reached a stable 1.0.1 version and has a potential to be extended in many interesting directions. First, we would like to implement a synchronous multiplayer mode, which would be convenient for self-learning in multiplayer settings. Second, bots are now deaf thus, we plan to allow bots to access the sound buffer. Lastly, interesting, supervised learning experiments (e.g., segmentation) could be conducted if ViZDoom automatically labeled objects in the scene.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Doom's first-person perspective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>9 # 12 [ 17 # 18 s 21 # 24 #</head><label>91217182124</label><figDesc>Fig. 2. 1 from vizdoom import * 2 from random import choice 3 from time import sleep, time4 5 game = DoomGame() 6 game.load_config("../config/basic.cfg") 7 game.init() 8 Sample actions. Entries correspond to buttons: 10 # MOVE_LEFT, MOVE_RIGHT, ATTACK 11 actions = [[True, False, False], False, True, False], [False, False, True]] 13 # Loop over 10 episodes. 14 for i in range(10): 15 game.new_episode() 16 while not game.is_episode_finished(): Get the screen buffer and and game variables Perform a random action: 22 action = choice(actions) 23 reward = game.make_action(action) Do something with the reward...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>total reward:", game.get_total_reward()) Python API example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>ViZDoom allows depth buffer access.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>ViZDoom performance. "depth" means generating also the depth buffer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The basic scenario</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Architecture of the convolutional neural network used for the experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Learning dynamics depending on the number of skipped frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Health gathering scenario</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Learning dynamics for health gathering scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I OVERVIEW</head><label>I</label><figDesc>OF 3D FPS GAME ENGINES CONSIDERED.</figDesc><table><row><cell>Features / Game</cell><cell>Doom</cell><cell>Doom 3</cell><cell cols="2">Quake III: Arena Half-Life 2</cell><cell>Unreal</cell><cell>Unreal</cell><cell>Cube</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tournament</cell><cell>Tournament</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2004</cell><cell></cell><cell></cell></row><row><cell>Game Engine</cell><cell cols="2">ZDoom[1] id tech 4</cell><cell>ioquake3</cell><cell>Source</cell><cell>Unreal</cell><cell>Unreal</cell><cell>Cube Engine</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Engine 2</cell><cell>Engine 4</cell><cell></cell></row><row><cell>Release year</cell><cell>1993</cell><cell>2003</cell><cell>1999</cell><cell>2004</cell><cell>2004</cell><cell>not yet</cell><cell>2001</cell></row><row><cell>Open Source</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>License</cell><cell>GPL</cell><cell>GPLv3</cell><cell>GPLv2</cell><cell>Proprietary</cell><cell>Proprietary</cell><cell>Custom</cell><cell>ZLIB</cell></row><row><cell>Language</cell><cell>C++</cell><cell>C++</cell><cell>C</cell><cell>C++</cell><cell>C++</cell><cell>C++</cell><cell>C++</cell></row><row><cell>DirectX</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OpenGL</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Software Render</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Windows</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Linux</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mac OS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Map editor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Screen buffer access</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Scripting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multiplayer mode</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Small resolution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Custom assets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Free original assets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>System requirements</cell><cell>Low</cell><cell>Medium</cell><cell>Low</cell><cell>Medium</cell><cell>Medium</cell><cell>High</cell><cell>Low</cell></row><row><cell>Disk space</cell><cell>40MB</cell><cell>2GB</cell><cell>70MB</cell><cell>4,5GB</cell><cell>6GB</cell><cell>&gt;10GB</cell><cell>35MB</cell></row><row><cell>Code complexity</cell><cell>Medium</cell><cell>High</cell><cell>Medium</cell><cell>-</cell><cell>-</cell><cell>High</cell><cell>Low</cell></row><row><cell>Active community</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Brand recognition</cell><cell>31.5</cell><cell></cell><cell>16.8</cell><cell>18.7</cell><cell>1.0</cell><cell></cell><cell>0.1</cell></row><row><cell>munication).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II AGENTS</head><label>II</label><figDesc>' FINAL PERFORMANCE IN THE FUNCTION OF THE NUMBER OF SKIPPED FRAMES ('NATIVE'). ALL THE AGENTS WERE ALSO TESTED FOR SKIPCOUNTS? {0, 10}.</figDesc><table><row><cell>skipcount</cell><cell>native</cell><cell cols="2">average score ? stdev 0</cell><cell>10</cell><cell>episodes learning time [min]</cell></row><row><cell>0</cell><cell cols="4">51.5 ? 74.9 51.5 ? 74.9 36.0 ? 103.6</cell><cell>6961</cell><cell>91.1</cell></row><row><cell>1</cell><cell cols="4">69.0 ? 34.2 69.2 ? 26.9 39.6 ? 93.9</cell><cell>29 378</cell><cell>93.1</cell></row><row><cell>2</cell><cell cols="4">76.2 ? 15.5 71.8 ? 18.1 47.9 ? 47.6</cell><cell>49 308</cell><cell>91.5</cell></row><row><cell>3</cell><cell cols="4">76.1 ? 14.6 75.1 ? 15.0 44.1 ? 85.4</cell><cell>65 871</cell><cell>93.4</cell></row><row><cell>4</cell><cell cols="2">82.2 ? 9.4</cell><cell cols="2">81.3 ? 11.0 76.5 ? 17.1</cell><cell>104 796</cell><cell>93.9</cell></row><row><cell>5</cell><cell cols="4">81.8 ? 10.2 79.0 ? 13.6 75.2 ? 19.9</cell><cell>119 217</cell><cell>92.5</cell></row><row><cell>6</cell><cell cols="2">81.5 ? 9.6</cell><cell cols="2">78.7 ? 14.8 76.3 ? 16.5</cell><cell>133 952</cell><cell>92</cell></row><row><cell>7</cell><cell cols="2">81.2 ? 9.7</cell><cell cols="2">77.6 ? 15.8 76.9 ? 17.9</cell><cell>143 833</cell><cell>95.2</cell></row><row><cell>10</cell><cell cols="4">80.1 ? 10.5 75.0 ? 17.6 80.1 ? 10.5</cell><cell>171 070</cell><cell>92.8</cell></row><row><cell>15</cell><cell cols="4">74.6 ? 14.5 71.2 ? 16.0 73.5 ? 19.2</cell><cell>185 782</cell><cell>93.6</cell></row><row><cell>20</cell><cell cols="4">74.2 ? 15.0 73.3 ? 14.0 71.4 ? 20.7</cell><cell>240 956</cell><cell>94.8</cell></row><row><cell>25</cell><cell>73 ? 17</cell><cell></cell><cell cols="2">73.6 ? 15.5 71.4 ? 20.8</cell><cell>272 633</cell><cell>96.9</cell></row><row><cell>30</cell><cell cols="4">61.4 ? 31.9 69.7 ? 19.0 68.9 ? 24.2</cell><cell>265 978</cell><cell>95.7</cell></row><row><cell>35</cell><cell cols="4">60.2 ? 32.2 69.5 ? 16.6 65.7 ? 26.1</cell><cell>299 545</cell><cell>96.9</cell></row><row><cell>40</cell><cell cols="4">56.2 ? 39.7 68.4 ? 19.0 68.2 ? 22.8</cell><cell>308 602</cell><cell>98.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">See also https://youtu.be/fKHw3wmT uA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.youtube.com/watch?v=re6hkcTWVUY</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Page</forename><surname>Zdoom Wiki</surname></persName>
		</author>
		<ptr target="http://zdoom.org/wiki/MainPage" />
		<imprint>
			<biblScope unit="page" from="2016" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploratory gradient boosting for reinforcement learning in complex domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<idno>abs/1603.04119</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Purposive behavior acquisition for a real robot by vision-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minoru</forename><surname>Asada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoichi</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukoya</forename><surname>Tawaratsumida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koh</forename><surname>Hosoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Robot Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="163" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A vision-based reinforcement learning for coordination of soccer playing behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minoru</forename><surname>Asada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiji</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoichi</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukoya</forename><surname>Tawaratsumida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koh</forename><surname>Hosoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-94 Workshop on AI and A-life and Entertainment</title>
		<meeting>AAAI-94 Workshop on AI and A-life and Entertainment</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="16" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using a genetic algorithm to tune first-person shooter bots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sushil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Computation, 2004. CEC2004. Congress on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="139" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Intrinsically motivated neuroevolution for vision-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Cuccu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Luciw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Development and Learning (ICDL)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards using first-person shooter computer games as an artificial intelligence testbed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge-Based Intelligent Information and Engineering Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="276" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A hybrid fuzzy ANN system for agent adaptation in a first person shooter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdennour</forename><surname>El Rhalibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madjid</forename><surname>Merabti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Games Technology</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Controlling bots in a First Person Shooter game using genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>A I Esparcia-Alcazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martinez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J J</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia-Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Computation (CEC)</title>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reinforcement learning for a vision based mobile robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Gaskett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2000 IEEE/RSJ International Conference on</title>
		<meeting>2000 IEEE/RSJ International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="409" />
		</imprint>
	</monogr>
	<note>Intelligent Robots and Systems</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An empirical study of machine learning algorithms applied to modeling player behavior in a first person shooter video game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Geisler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A hierarchical First Person Shooter bot using multiple Sarsa(?) reinforcement learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M G</forename><surname>F G Glavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dre-Bot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Games (CGAMES), 2012 17th International Conference on</title>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="148" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive Shooting for Bots in First Person Shooter Games Using Reinforcement Learning. Computational Intelligence and AI in Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M G</forename><surname>F G Glavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="180" to="192" />
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</title>
		<editor>Geoffrey J. Gordon and David B. Dunson</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
	<note>Journal of Machine Learning Research -Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An evaluation of models for predicting opponent positions in first-person shooter video games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hladky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bulitko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence and Games, 2008. CIG &apos;08. IEEE Symposium On</title>
		<imprint>
			<date type="published" when="2008-12" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Believable Bot Navigation via Playback of Human Traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><forename type="middle">V</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Schrum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="151" to="170" />
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evolving deep unsupervised convolutional networks for vision-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on Genetic and evolutionary computation</title>
		<meeting>the 2014 conference on Genetic and evolutionary computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep auto-encoder neural networks in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reinforcement Learning in First Person Shooter Games. Computational Intelligence and AI in Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcpartland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="56" />
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RE-TALIATE: learning winning policies in first-person shooter games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lee-Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?ctor</forename><surname>Mu?oz-Avila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence<address><addrLine>Menlo Park, CA; Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>London</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1801</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Continuous and Reinforcement Learning Methods for First-Person Shooter Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing (JoC)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Evolution of Gamebots for 3D First Person Shooter (FPS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Kee</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ong Jia</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Kim On</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bio-Inspired Computing: Theories and Applications</title>
		<imprint>
			<publisher>BIC-TA</publisher>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
	<note>Sixth International Conference on</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computer game engines for developing first-person virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Trenholme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shamus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Virtual reality</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="181" to="187" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
