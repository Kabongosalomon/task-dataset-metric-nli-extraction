<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OPEN-SET RECOGNITION: A GOOD CLOSED-SET CLASSIFIER IS ALL YOU NEED?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Vaze</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OPEN-SET RECOGNITION: A GOOD CLOSED-SET CLASSIFIER IS ALL YOU NEED?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022 Page: https://www.robots.ox.ac.uk/~vgg/research/osr/</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model. This task is termed open-set recognition (OSR) and has received significant attention in recent years. In this paper, we first demonstrate that the ability of a classifier to make the 'none-of-above' decision is highly correlated with its accuracy on the closed-set classes. We find that this relationship holds across loss objectives and architectures, and further demonstrate the trend both on the standard OSR benchmarks as well as on a large-scale ImageNet evaluation. Second, we use this correlation to boost the performance of a maximum logit score OSR 'baseline' by improving its closed-set accuracy, and with this strong baseline achieve state-of-the-art on a number of OSR benchmarks. Similarly, we boost the performance of the existing state-of-the-art method by improving its closed-set accuracy, but the resulting discrepancy with the strong baseline is marginal. Our third contribution is to present the 'Semantic Shift Benchmark' (SSB), which better respects the task of detecting semantic novelty, in contrast to other forms of distribution shift also considered in related sub-fields, such as out-of-distribution detection. On this new evaluation, we again demonstrate that there is negligible difference between the strong baseline and the existing state-of-the-art. Project</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Given the success of modern deep learning systems on closed-set visual recognition tasks, a natural next challenge is open-set recognition (OSR) <ref type="bibr">(Scheirer et al., 2013)</ref>. In the closed-set setting, a model is tasked with recognizing a set of categories that remain the same during both training and testing phases. In the more realistic open-set setting, a model must not only be able to distinguish between the training classes, but also indicate if an image comes from a class it has not yet encountered.</p><p>The OSR problem was initially formalized in <ref type="bibr">(Scheirer et al., 2013)</ref> and has since inspired a rich line of research <ref type="bibr" target="#b2">(Bendale &amp; Boult, 2016;</ref><ref type="bibr" target="#b6">Chen et al., 2020a;</ref><ref type="bibr">Ge et al., 2017;</ref><ref type="bibr">Neal et al., 2018;</ref><ref type="bibr">Sun et al., 2020;</ref><ref type="bibr">Zhang et al., 2020;</ref><ref type="bibr">Shu et al., 2020)</ref>. The standard baseline for OSR is a model trained with the cross-entropy loss on the known classes. At test time, the maximum value of the softmax probability vector is used to decide if an input belongs to the known classes or not. We henceforth refer to this method as the 'baseline' or 'maximum softmax probability (MSP) baseline'. Most existing literature reports significantly outperforming this OSR baseline on standard benchmarks of re-purposed image recognition datasets, including <ref type="bibr">MNIST (LeCun et al., 2010)</ref> and TinyImageNet <ref type="bibr">(Le &amp; Yang, 2015)</ref>.</p><p>In this paper we reappraise these approaches, by asking whether a well-trained closed-set classifier can perform as well as recent algorithms, and by analyzing the benchmark datasets. To do this, we first investigate the relationship between the closed-set and open-set performance of a classifier (sec. 3). Though one may expect stronger closed-set classifiers to overfit to the training classes <ref type="bibr">(Recht et al., 2019;</ref><ref type="bibr">Zhang et al., 2017)</ref>, and so perform poorly for OSR, we show instead that the closed-set and open-set performance are highly correlated. We show this trend holds across datasets, objectives and model architectures, and further demonstrate the trend on an ImageNet-scale evaluation.  <ref type="figure">Figure 1</ref>: (a) We show that we can push OSR baseline performance to be competitive with or surpass state-of-the-art methods (shown, ARPL + CS <ref type="bibr">(Chen et al., 2021)</ref>). (b) We propose the 'Semantic Shift Benchmark' datasets for OSR, which are larger scale and give precise definitions of what constitutes a 'new class'.</p><p>Secondly, following this observation, we show that the open-set performance of a classifier can be improved by enhancing its closed-set accuracy, tapping the numerous recent advances in image classification <ref type="bibr">(Loshchilov &amp; Hutter, 2017;</ref><ref type="bibr">Szegedy et al., 2016;</ref><ref type="bibr" target="#b10">Cubuk et al., 2020;</ref><ref type="bibr" target="#b1">Bello et al., 2021)</ref>. Specifically, we introduce strategies such as more augmentation, better learning rate schedules and label smoothing, that significantly improve the closed-set performance of the MSP baseline (sec. 4). We also propose the use of the maximum logit score (MLS), rather than normalized softmax probabilities, as an open-set indicator. With these adjustments, we push the baseline to become competitive with or outperform state-of-the-art OSR methods, substantially outperforming the currently reported baseline figures. Notably, we surpass state-of-the-art figures on four of the six OSR benchmark datasets.</p><p>Furthermore, we transfer these improvements to two previous OSR methods, including the current state-of-the-art from <ref type="bibr">(Chen et al., 2021)</ref>. While this does boost its performance, we observe that there is negligible difference with that of the improved 'MLS' baseline (see <ref type="figure">fig. 1a</ref>). This finding is important because it allows us to better assess recent reported progress in the area.</p><p>Finally, we turn to the experimental setting for OSR (sec. 5). Current OSR benchmarks are both small scale and lack a specific definition of what constitutes a 'visual class'. As an alternative, we propose the 'Semantic Shift Benchmark' suite (SSB). We propose the use of fine-grained datasets -including CUB <ref type="bibr">(Wah et al., 2011</ref><ref type="bibr">), Stanford Cars (Krause et al., 2013</ref> and <ref type="bibr">FGVC-Aircraft (Maji et al., 2013)</ref> -which all have clear definitions of a semantic class (see <ref type="figure">fig. 1b</ref>), as well as an ImageNet-scale evaluation based on the full ImageNet database <ref type="bibr">(Ridnik et al., 2021)</ref>. Furthermore, we construct open-set splits with an explicit focus on semantic novelty, which we hope better separates this avenue of research from related machine learning sub-fields such as out-of-distribution <ref type="bibr">(Hendrycks &amp; Gimpel, 2017)</ref> and anomaly detection <ref type="bibr">(Kwon et al., 2020)</ref>. Our proposed splits also offer a better way of quantifying open-set difficulty; we find that different splits lead to a much larger discrepancy in open-set performance than the current measure of open-set difficulty 'openness' <ref type="bibr">(Scheirer et al., 2013)</ref>, which focuses only on the number of open-set classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Open-set recognition. Seminal work in <ref type="bibr">(Scheirer et al., 2013)</ref> formalized the task of open-set recognition, and has inspired a number of subsequent works in the field. <ref type="bibr" target="#b2">(Bendale &amp; Boult, 2016)</ref> introduced the first deep learning approach for OSR, OpenMax, based on the Extreme Value Theory (EVT). GANs have also been used to tackle the task <ref type="bibr">(Ge et al., 2017;</ref><ref type="bibr">Neal et al., 2018)</ref>. <ref type="bibr">OSRCI (Neal et al., 2018)</ref> generates images similar to those in the training set but that do not belong to any of the known classes, and uses the generated images to train an open-set classifier. This work also established the existing OSR benchmark suite. <ref type="bibr">(Kong &amp; Ramanan, 2021)</ref> achieve strong OSR performance by using an adversarially trained discriminator to delineate closed from open-set images, leveraging real open-set images for model selection. Other approaches include reconstruction based methods <ref type="bibr">(Yoshihashi et al., 2019;</ref><ref type="bibr">Oza &amp; Patel, 2019;</ref><ref type="bibr">Sun et al., 2020)</ref> which use poor test-time reconstruction as an open-set indicator, and prototype-based methods <ref type="bibr">(Shu et al., 2020;</ref><ref type="bibr" target="#b6">Chen et al., 2020a;</ref><ref type="bibr" target="#b1">2021)</ref> which represent known classes with learned prototypes, and identify open-set images based on distances to the prototypes.</p><p>State-of-the-art. In this work, we compare against methods which achieve state-of-the-art in the controlled OSR setting (with no extra data for training or model selection, for instance as demonstrated in <ref type="bibr">(Kong &amp; Ramanan, 2021)</ref>). To our knowledge, these methods are ARPL (Adversarial Reciprocal Point Learning) <ref type="bibr" target="#b6">(Chen et al., 2020a;</ref><ref type="bibr" target="#b1">2021)</ref> and <ref type="bibr">OpenHybrid (Zhang et al., 2020)</ref>, which we detail in sec. 3.1 and sec. 4 respectively. In this paper, we show that the baseline can be competitive with or outperform the more complex methods listed above. Finally, we note recent works <ref type="bibr">(Zhou et al., 2021;</ref><ref type="bibr">Miller et al., 2021;</ref><ref type="bibr">Guo et al., 2021)</ref> with which we do not compare as they report lower performance than ARPL and OpenHybrid.</p><p>Related subfields. OSR is also closely related to out-of-distribution (OoD) detection <ref type="bibr">(Hendrycks &amp; Gimpel, 2017;</ref><ref type="bibr">Liang et al., 2018;</ref><ref type="bibr">Hsu et al., 2020)</ref>, novelty detection <ref type="bibr" target="#b0">(Abati et al., 2019;</ref><ref type="bibr">Perera et al., 2019;</ref><ref type="bibr">Tack et al., 2020)</ref>, anomaly detection <ref type="bibr">(Hendrycks et al., 2019;</ref><ref type="bibr">Kwon et al., 2020;</ref><ref type="bibr" target="#b3">Bergman &amp; Hoshen, 2020)</ref> and novel category discovery <ref type="bibr">(Han et al., 2019;</ref><ref type="bibr" target="#b1">2021)</ref>. Amongst these, OoD is perhaps the most widely studied and is similar in nature to OSR. As noted by <ref type="bibr" target="#b11">(Dhamija et al., 2018;</ref><ref type="bibr" target="#b5">Boult et al., 2019)</ref>, OSR is similar to the OoD problem with an additional multi-way classification component between known categories. In fact, there is currently significant overlap in the evaluation datasets between these settings, though cross-setting comparisons are difficult due to different evaluation protocols. Specifically, the OoD setting permits the use of additional data as examples of 'OoD' data during training. <ref type="bibr">(Chen et al., 2021)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CORRELATION BETWEEN CLOSED-SET AND OPEN-SET PERFORMANCE</head><p>One may expect that stronger closed-set classifiers have overfit their learned representations to the closed-set categories, and thus perform poorly for OSR <ref type="bibr">(Recht et al., 2019;</ref><ref type="bibr">Zhang et al., 2017)</ref>. Furthermore, existing literature largely considers the closed and open-set tasks separately, with works generally emphasising good open-set performance despite no degradation in closed-set accuracy <ref type="bibr">(Neal et al., 2018;</ref><ref type="bibr">Zhou et al., 2021;</ref><ref type="bibr">Miller et al., 2021)</ref>. On the contrary, in this section we show that the closed-set and open-set performance of classifiers are strongly correlated. We first demonstrate this for the baseline and a state-of-the-art method on the standard OSR benchmarks (sec. 3.1) and then on a large scale evaluation across a number of model architectures (sec. 3.2).</p><p>Open-set recognition. We formalize the problem of OSR, and highlight its differences from closedset recognition. First, consider a labelled training set for a classifier</p><formula xml:id="formula_0">D train = {(x i , y i )} N i=1 ? X ? C.</formula><p>Here, X is the input space (e.g., images) and C is the set of 'known' classes. In the closed-set scenario, the model is evaluated on a test set in which the labels are also drawn from the same set of classes, i.e.,</p><formula xml:id="formula_1">D test-closed = {(x i , y i )} M i=1 ? X ? C.</formula><p>In the closed-set setting, the model returns a distribution over the known classes as p(y|x). Conversely, in OSR, test images may also come from unseen classes U,</p><formula xml:id="formula_2">giving D test-open = {(x i , y i )} M i=1 ? X ? (C ? U).</formula><p>In the open-set setting, in addition to returning the distribution p(y|x, y ? C) over known classes, the model also returns a score S(y ? C|x) to indicate whether or not the test sample belongs to any of the known classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BASELINE AND STATE-OF-THE-ART ON STANDARD BENCHMARKS</head><p>We first experiment with three representative open-set recognition methods across the standard benchmark datasets in the literature <ref type="bibr">(Neal et al., 2018;</ref><ref type="bibr">Oza &amp; Patel, 2019;</ref><ref type="bibr">Sun et al., 2020;</ref><ref type="bibr" target="#b6">Chen et al., 2020a;</ref><ref type="bibr">Zhang et al., 2020)</ref>. The methods include the standard MSP baseline as well as two variants of ARPL <ref type="bibr">(Chen et al., 2021)</ref>. We use the standard network from the open-set literature <ref type="bibr">(Neal et al., 2018)</ref>, a lightweight model similar to the VGG architecture <ref type="bibr">(Simonyan &amp; Zisserman, 2015)</ref> which we henceforth refer to as 'VGG32' (refer to appendix D for details). The three methods are summarised below, followed by a description of the most commonly used benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods. Maximum Softmax Probability (MSP, baseline):</head><p>The model is trained for closed-set classification using the cross-entropy loss between a one-hot target vector and the softmax output p(y|x) of the classifier. This training strategy, along with the use of the maximum softmax probability  <ref type="bibr">(Chen et al., 2021)</ref>. Foreground points in bold show results averaged across five 'known/unknown' class splits for each method-dataset pair (following standard practise in the OSR literature) while background points, shown feint, indicate results from the underlying individual splits.</p><p>as S(y ? C|x) = max y?C p(y|x), is widely used in both the OSR and OoD literature as a baseline <ref type="bibr">(Hendrycks &amp; Gimpel, 2017)</ref>. ARPL <ref type="bibr">(Chen et al., 2021)</ref>: This method is an extension of the recent RPL (Reciprocal Point Learning) optimization strategy <ref type="bibr" target="#b6">(Chen et al., 2020a)</ref>. Here, the probability that a sample belongs to a class is proportional to its distance from a learned 'reciprocal point' in the feature space. A reciprocal point aims to represent 'otherness' with respect to a class, with the intuition being that open-set examples are different to all known classes. ARPL extends RPL by computing feature distances as the sum of both the Euclidean and cosine distances. In this case, S(y ? C|x) is equal to the maximum distance in feature space between the image and any reciprocal point. ARPL + CS <ref type="bibr">(Chen et al., 2021)</ref> augments ARPL with 'confusing samples': adversarially generated latent points to stand in for 'unseen class' samples. The confusing samples are encouraged to be equidistant from all reciprocal points, with the same open-set scoring rule used as in ARPL. We train both ARPL and ARPL + CS based on the official public implementation <ref type="bibr">(Chen et al., 2021)</ref>. Experimental setup. At test time, the model is fed test images from both known and novel classes, and is tasked with making a binary 'known/unknown' decision on a per-image basis. Following standard practise in the OSR literature, the threshold-free area under the Receiver-Operator curve (AUROC) is used as an evaluation metric. We train with the same hyper-parameters as in <ref type="bibr">(Chen et al., 2021)</ref> and, following standard practise, train on five different splits of closed and open-set classes for each dataset and method combination. When evaluating on existing benchmarks throughout this paper, we use the same data splits as <ref type="bibr">(Chen et al., 2021)</ref>.</p><p>Results. <ref type="figure">Fig. 2</ref> gives the AUROC (open-set performance) against the Top-1 multi-way classification accuracy (closed-set performance). We show the averaged results as well as the individual split results, omitting the CIFAR+10 setting for clarity (as the scatter points are almost coincident with the CIFAR+50 setting). It is clear that there is a positive correlation between the closed-set accuracy and open-set performance: we find a Pearson Product-Moment correlation ? = 0.95 between the accuracy and AUROC, indicating a roughly linear relationship between the two metrics.</p><p>Discussion. To justify our findings theoretically, we look to the model calibration literature <ref type="bibr">(Guo et al., 2017)</ref>. Intuitively, model calibration aims to quantify whether the model 'knows when it doesn't know', in that low confidence predictions are correlated with high error rates. Specifically, assume a classifier, f (x), returns probabilities for each class, making predictions as? = arg max f (x).  Further assume labelled input-output pairs, (x, y) ? X ? C, where C is the label space. Then, the classifier is said to be perfectly calibrated if:</p><formula xml:id="formula_3">P (? = y|f (x) = p) = p ?p ? [0, 1]<label>(1)</label></formula><p>It is further true that if a classifier is trained with a proper scoring rule <ref type="bibr">(Gneiting et al., 2007)</ref> on infinite data, then the classifier will be perfectly calibrated at the loss function's minimum <ref type="bibr">(Minderer et al., 2021)</ref>. Many losses used to train deep networks are proper scoring rules (e.g., the cross-entropy loss). Thus, assuming that generalization error on the test set is correlated with the infinite-data loss value, we would suspect models with lower generalization (test) error to be better calibrated. If we use low-confidence closed-set predictions as an indicator that a test sample belongs to a new semantic class, we would expect stronger models to be better open-set detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LARGE-SCALE EXPERIMENTS AND ARCHITECTURE ABLATION</head><p>So far, we have demonstrated the correlation between closed and open-set performance on a single, lightweight architecture and on small scale datasets -though we highlight that they are the standard existing benchmarks in the OSR literature. Here, we experiment with a range of architectures on a large-scale dataset (ImageNet).</p><p>Methods. We experiment with architectures trained on the standard ImageNet-1K dataset from a number of popular model families, including: VGG (Simonyan &amp; Zisserman, 2015), <ref type="bibr">ResNet (He et al., 2016)</ref> and EfficientNet <ref type="bibr">(Tan &amp; Le, 2019)</ref>. We further include results for the non-convolutional models <ref type="bibr">ViT (Dosovitskiy et al., 2021)</ref> and <ref type="bibr">MLP-Mixer (Tolstikhin et al., 2021;</ref><ref type="bibr">Melas-Kyriazi, 2021)</ref>, which were pre-trained on Imagenet-21K before being fine-tuned on ImageNet-1K. We use the timm library for all model weights (Wightman, 2019).</p><p>Dataset. For large-scale evaluation, we leverage the recently released ImageNet-21K-P <ref type="bibr">(Ridnik et al., 2021)</ref>. This dataset contains a subset of the full ImageNet database, processed and standardized to remove small classes and leaving around 11K object categories. Note that ImageNet-21K-P is a strict superset of ImageNet-1K (ILSVRC12). As such, for models trained on the standard 1000 classes from ImageNet-1K, we select two 1000-category subsets from the disjoint categories in ImageNet-21K-P as the open sets. Differently to existing practise on the standard datasets, our two open-set splits for ImageNet are not randomly sampled, but rather designed to be 'Easy' and 'Hard' based on the semantic similarity of the open-set categories to the training classes. In this way we better capture a model's ability to identify semantic novelty as opposed to low-level distributional shift. This idea and split construction details are expanded upon in sec. 5. For both 'Easy' and 'Hard' splits, we have |C| = 1000 and |U| = 1000.</p><p>Results. <ref type="figure" target="#fig_2">Fig. 3a</ref> shows our open-set results on ImageNet. Once again, we find a positive correlation between closed and open-set performance. In this case we find the linear relationship to be weaker, with ? = 0.88 for the 'Hard' evaluation and ? = 0.63 for the 'Easy'. This is unsurprising given the large discrepancy in architecture styles. In general, we do not find any particular model family to be remarkably better for OSR than others. When looking within a single model family, we find the linear relationship to be substantially strengthened. <ref type="figure" target="#fig_2">Fig. 3b</ref> demonstrates the trend within the ResNet family, with ? = 1.00 and ? = 0.99 for the 'Easy' and 'Hard' OSR splits respectively.</p><p>Discussion. We note that the ViT model appears to buck the OSR trend for both 'Easy' and 'Hard' splits, showing similar findings to <ref type="bibr">(Fort et al., 2021)</ref> for OoD detection. However, in both cases, the ViT model benefitted from being pre-trained on categories from the 'unseen' splits (albeit before fine-tuning on the closed-set classes). Finally, we note the practical utility of our findings in sec. 3. Namely, the fact that the open and closed-set performance are correlated allows OSR to readily improve with the extensive research in standard image recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A GOOD CLOSED-SET CLASSIFIER IS ALL YOU NEED?</head><p>In this section, we demonstrate that we can leverage the correlation established in sec. 3 to improve the performance of the baseline OSR method. Specifically, we improve the closed-set accuracy of the maximum softmax probability (MSP) baseline and, in doing so, make it competitive with or stronger than state-of-the-art open-set models. Specifically, we achieve new state-of-the-art figures on four of the six OSR benchmarks. We find that we can significantly improve the MSP baseline performance by leveraging techniques from the image recognition literature, such as longer training, better augmentations (Cubuk et al., 2020) and label smoothing <ref type="bibr">(Szegedy et al., 2016)</ref>. <ref type="figure" target="#fig_3">Fig. 4</ref> shows how open-set performance of the baseline model increases as we introduce these changes on the Tiny-ImageNet benchmark. For example: longer training (scatter point 7 -scatter point 8); better augmentations (3 -5); and ensembling (8 -9). Full details and a tabular breakdown of the methods used to increase closed-set performance can be found in appendix C.</p><p>We take these improved training strategies and train the VGG32 backbone on the standard benchmark datasets. We train all models for 600 epochs with a batch size of 128, training models on a single NVIDIA Titan X GPU. We do not include ensemble results for fair comparison with previous methods. Full training strategies and implementation details can be found in appendices C and D. We report our results as 'Baseline (MSP+)' in table 1.</p><p>Logit scoring rule. Next, we also change the open-set scoring rule. Previous work has noted that open-set examples tend to have lower feature norms than closed-set ones <ref type="bibr" target="#b11">(Dhamija et al., 2018;</ref><ref type="bibr">Chen et al., 2021)</ref>. As such, we propose the use of the maximum logit score (MLS) for the open-set scoring rule. Logits are the raw outputs of the final linear layer in a deep classifier, before the softmax operation normalizes these such that the outputs can be interpreted as a probability vector summing to one. As the softmax operation normalizes out much of the feature magnitude information present in the logits, we find logits lead to better open-set detection results. We provide a detailed analysis and discussion of this effect in appendix B. We further provide a more general study of the representations learned with cross-entropy models, including visualizations of the learned feature space. We present results of our maximum logit score baseline as 'Baseline (MLS)' in table 1.</p><p>We compare against OpenHybrid (Zhang et al., 2020) and ARPL + CS <ref type="bibr">(Chen et al., 2021)</ref>, which hold state-of-the-art performances on the standard datasets in the controlled setting (with no extra data for training or model selection). We also compare against OSRCI <ref type="bibr">(Neal et al., 2018)</ref>, which established the current OSR benchmark suite. While OSRCI and ARPL + CS have been described in sec. 2 and 3.1 respectively, OpenHybrid tackles the open-set task by training a flow-based density estimator on top of the classifier's feature representation, jointly training both the encoder and density model. In this way, a distribution over the training data log p(x) is learned, which is used to directly provide S(y ? C|x). Comparisons with more methods can be found in appendix E.</p><p>We find that our MLS baseline substantially improves the previously reported baseline figures, with an average absolute increase in AUROC of 15.6% across the datasets. In fact, MLS surpasses the existing state-of-the-art on the SVHN, CIFAR+10, CIFAR+50 and TinyImageNet benchmarks and is, on average, 0.7% better across the entire suite. We also take the OSRCI and ARPL + CS algorithms <ref type="bibr">(Neal et al., 2018;</ref><ref type="bibr">Chen et al., 2021)</ref>, and augment them with our proposed training strategies for a fair comparison, reporting the results under OSRCI+ and (ARPL + CS)+. Specifically, we train them for longer, include label smoothing and use better data augmentations (see appendix D for full details). We also trained OpenHybrid in this controlled setting, but significantly underperformed the reported performance. This is likely because the method was trained for 10k epochs and with a batch size of 1024, which are both 10? larger than those used in these experiments. Note that, despite this, the stronger baseline still outperforms OpenHybrid in a number of cases.</p><p>In almost all cases we are able to boost the open-set performance of OSRCI and ARPL+CS, especially for the former. In the case of (ARPL+CS)+, we achieve new state-of-the-art results on the CIFAR+10 and CIFAR+50 benchmarks, and also report a 4.3% boost on TinyImageNet. However, we note that on average, (ARPL+CS)+ is almost indistinguishable from the improved MLS baseline (with 0.03% difference in average open-set performance).  <ref type="bibr">, 2021)</ref>. We exploit the hierarchical, tree-like semantic structure of the ImageNet database. For instance, the class 'elephant' can be labelled at multiple levels of semantic abstraction ('elephant', 'placental', 'mammal', 'vertebrate', 'animal'). Thus, for each pair of classes between ImageNet-1K and ImageNet-21K-P, we define the semantic distance between two classes as the total path distance between their nodes in the semantic tree. We then approximate the total semantic distance from the ImageNet-21K-P classes to the closed-set by summing distances to all ImageNet-1K classes. Finally, we select 'Easy' and 'Hard' open-set splits by sorting the total distances to the closed-set and selecting two sets of 1000 categories. We note that the larger ImageNet database has been used for OSR research previously <ref type="bibr" target="#b2">(Bendale &amp; Boult, 2016;</ref><ref type="bibr">Kumar et al., 2021;</ref><ref type="bibr">Hendrycks et al., 2021)</ref>. However, we structure explicitly for semantic similarity with ImageNet-1K similarly to concurrent work in <ref type="bibr">(Sariyildiz et al., 2021)</ref>.</p><p>Fine-grained classification datasets. Consider the properties of fine-grained visual categorization (FGVC) datasets. These datasets are defined by an 'entry level' category, such as flowers <ref type="bibr">(Nilsback &amp; Zisserman, 2008)</ref> or birds <ref type="bibr">(Wah et al., 2011)</ref>. Within the dataset, all classes are variants of that single category, defining a single axis of semantic variation, e.g., 'bird species' in the case of birds. Because the axis of variation is well defined, it is reasonable to expect a classifier to learn it given a number of example classes -namely, to learn what bird species are and how they can be distinguished.</p><p>Contrast FGVC datasets with the current OSR benchmarks, such as the CIFAR+10 evaluation. In this case, a model is trained on four CIFAR10 classes such as {airplane, automobile, ship, truck}, all of which could be considered 'entry level', before having to identify images from CIFAR100 classes such as {bicycle, bee, porcupine, baby} as belonging to new classes. In this case, the axis of variation is much less specific, and it is uncertain whether the OSR model is responding to a true semantic signal or simply to low-level distributional shifts in the 'unseen' data. Furthermore, because of the small number of training classes in the current benchmark settings, it is unrealistic for a classifier to learn such high-level class definitions. We give an illustrative example of this in appendix G.</p><p>As a result, we propose three FGVC datasets for OSR evaluation: Caltech-UCSD-Birds <ref type="formula">(</ref>  <ref type="bibr">, 2013)</ref>. These datasets come with labelled attributes (e.g., has_bill_shape::hooked in CUB), which can be used to characterize the differences between classes and thus the degree of semantic shift. We use attributes to construct open-set FGVC class splits which are binned into 'Easy', 'Medium' and 'Hard' classes, with the difficulty depending on the similarity of labelled visual attributes with any of the training classes. We sketch the split-construction process for CUB here, and refer to appendix H for more details on Stanford Cars and FGVC-Aircraft. This information is aggregated for each class, resulting in a matrix M ? [0, 1] C?A , describing the frequency with which each attribute appears in each class. Treating each row in M as a semantic class descriptor, this allows us to compute the semantic similarity of every pair of classes and, given a set of closed-set classes, identify which remaining classes are 'Easy', 'Medium' and 'Hard' (least to most similar) with respect to the closed-set. Examples of 'Easy', 'Medium' and 'Hard' open-set classes, along with their closest class in the closed-set, are shown in <ref type="figure" target="#fig_4">fig. 5</ref> for CUB.</p><p>We note that fine-grained OSR has been demonstrated in <ref type="bibr">(Chen et al., 2021;</ref><ref type="bibr" target="#b6">2020a</ref>) on a dataset of 300 aircraft classes. However, this dataset does not come with labelled attributes, making it harder to construct open-set splits with varying levels of semantic similarity to the training set, which is our focus here. Finally, while prior works have recognised the difficulty of OoD detection for more fine-grained data <ref type="bibr" target="#b4">(Bodesheim et al., 2015;</ref><ref type="bibr">Perera &amp; Patel, 2019;</ref><ref type="bibr">Lee et al., 2018a)</ref>, we propose them for OSR because of their clear definition of a semantic class rather than their increased difficulty. A further discussion of these ideas is presented in appendix G. We provide statistics of the splits from all proposed datasets in table 2, and the splits themselves in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">BENCHMARKING FOR OPEN-SET RECOGNITION</head><p>Evaluation Protocol. For the 'known/unknown' class decision, we report AUROC as is standard practise, as well as accuracy to allow potential gains in open-set performance to be contextualized in the closed-set accuracy of a model. We also report Open-Set Classification Rate (OSCR) <ref type="bibr" target="#b11">(Dhamija et al., 2018)</ref> which measures the trade-off between accuracy and open-set detection rate as a threshold on the confidence of the predicted class is varied. We report results on 'Easy' and 'Hard' splits for all datasets, combining 'Medium' and 'Hard' examples into a single bin when applicable.</p><p>In fine-grained classification, it is standard to pre-train models on ImageNet. This is unsuitable for the proposed fine-grained OSR setting, as ImageNet contains overlapping classes with the proposed datasets. Instead, we pre-train the network on Places (Zhou et al., 2017) using MoCoV2 selfsupervised weights <ref type="bibr" target="#b8">(Chen et al., 2020b;</ref><ref type="bibr">Zhao et al., 2021)</ref>. For the ImageNet benchmark, we can train with labels on the ImageNet-1K dataset and evaluate on the unseen classes. We finetune the ARPL model from a pre-trained ImageNet checkpoint.</p><p>Results. In table 3 we test MLS and ARPL+ <ref type="bibr">(Chen et al., 2021)</ref> using a ResNet50 backbone on the proposed benchmarks (we found ARPL + CS to be prohibitively expensive to train in this setting, see appendix D for details). The results corroborate the trends found in sec. 4: strong closed-set classifiers produce open-set results with good AUROC performance, and the MLS baseline performs comparably to the state-of-the-art method.</p><p>Finally, more careful consideration of the semantics of the open-set classes leads to harder splits significantly reducing OSR performance. This is in contrast to 'openness' <ref type="bibr">(Scheirer et al., 2013)</ref>, the current measure used to assess the difficulty of an OSR problem, dependent only on the ratio of the number of closed to open-set classes. For instance, in the ImageNet case, we find the harder split leads to 5-6% worse AUROC for both methods. We also experimented with randomly subsampling first 1K and then 10K open-set classes, finding that introducing more classes during evaluation only reduced open-set performance by around 0.6% (? 10? less than our proposed splits).  <ref type="figure" target="#fig_6">fig. 6</ref>. We find the correlation between the closed and open-set performance continues to hold with the inclusion of this additional method. We further report the standard deviations of this plot in table 4. It can be seen that, for the same dataset, the standard deviations of all four methods appear to be similar. The standard deviations on the most challenging TinyImageNet benchmark is greater than on the other datasets.</p><p>Finally, we note in <ref type="figure" target="#fig_6">fig. 6</ref> that the trend seems less clear at very high accuracies. This may be because AUROC also becomes very high, making it difficult to identify clear patterns. However, it may also indicate that the relationship between the metrics becomes weaker as closed-set performance saturates.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ANALYSING THE CLOSED-SET AND OPEN-SET CORRELATION</head><p>Here, we aim to understand why improving the closed-set accuracy may lead to increased open-set performance through the MSL baseline. To this end, we train the VGG32 model on the CIFAR10 benchmark setting with the cross-entropy loss. We train the model both with a feature dimension of D = 128 (as is standard for this model) as well as with D = 2 for feature space visualization. We also train without a bias in the linear classifier for more interpretable features and classification boundaries (so class boundaries radiate from the origin of the feature space). Specifically, we train a model to make predictions as? i = softmax(W? ? (x i )), where ? ? (?) is a CNN embedding function (? ? (x) ? R D ) and W ? R C?D is the linear classification matrix. Here C = |C| = 6 and D ? {2, 128}, and we optimise the loss with a one-hot target vector y i and batch size B, as ? 1 B B i=1 y i ? log(? i ). Next, we interrogate the learned embeddings by plotting the mean vector norm of the features from all test images, for both the known and unknown classes, as training proceeds. These are shown in <ref type="figure" target="#fig_7">fig. 7a and fig. 7b</ref> for the models with D = 128 and D = 2 respectively. We also show the average vector norm for the per-class weights in the linear classifiers as dashed lines. Furthermore, snapshots of how these images are embedded for the model with D = 2 are shown in <ref type="figure" target="#fig_7">fig. 7d</ref> to 7f at representative epochs. The plots of the mean feature norms show that, at the start of training, all images are embedded with a similar magnitude. However, as training proceeds, the magnitude of features for the known classes increases substantially more than for the unknown classes.</p><p>To understand this, consider the cross-entropy loss for a single sample in the batch, shown in eq. (2):</p><formula xml:id="formula_4">L i (?, W) = ?? i,c + log( C j=1 exp(? i,j )) = ?w c ? ? ? (x i ) + log( C j=1 exp(w j ? ? ? (x i )))<label>(2)</label></formula><p>where c refers to the correct class index, and w j refers to the classification vector corresponding to the j th class. Empirically, we find that the linear classifier's weights and the feature norms for known classes increase during training, which is justified as increasing both |w c | and |? ? (x i )| reduces the loss value. Note that we observe this despite training with weight decay, which we omit from eq. (2) for clarity. 1 However, for 'hard' or 'uncertain' training examples (for which the classifier's prediction may be incorrect) the model is encouraged to reduce w j ? ? ? (x i ) ?j = c through the second term of eq. (2). While the only way to do this for the D = 2 case is to reduce the feature norm ( <ref type="figure" target="#fig_7">fig. 7b and  fig. 7d</ref> to 7f), we show in <ref type="figure" target="#fig_7">fig. 7a</ref> that this also holds true for the D = 128 case in which D &gt; C. The tendency of deep networks to map 'hard' samples closer to the origin has been noted in <ref type="bibr">(Ranjan et al., 2017)</ref>.</p><p>This suggests that stronger cross-entropy models project features further from the origin, while still ensuring that any 'uncertain' samples have lower feature norms. This, in turn, suggests stronger cross-entropy classifiers would perform better for OSR, with images from novel categories likely to be interpreted as 'uncertain' during evaluation. Our analysis also suggests that cross-entropy training already provides a strong signal and thus a strong baseline for open-set recognition.</p><p>Finally, this motivates us to propose the maximum logit score (MLS) to provide our open-set score, i.e., S(y ? C|x) = max j?C w j ? ? ? (x), rather than the softmax output as in the standard MSP baseline. Normalizing the logits via the softmax operator cancels out the magnitude information of the feature representation, which we have demonstrated is useful for the OSR decision. <ref type="figure" target="#fig_7">Fig. 7c</ref>   We first train the baseline with the same hyper-parameters as in <ref type="bibr">(Chen et al., 2021)</ref>, training for 100 epochs and using a step learning rate schedule, with a basic random crop augmentation strategy. We evaluate using both softmax and logit scoring strategies. It can be seen that using maximum logit scoring gives better open-set performance (AUROC), while softmax scoring appears to be better in terms of OSCR. This is likely due to the fact that softmax normalization cancels the effect of the feature norm, which results in more separable scores that are beneficial to the OSCR calculation.</p><p>Here, we are interested in boosting the open-set performance (AUROC) by improving the closed-set accuracy. Hence, we use the maximum logit for open-set scoring as discussed in appendix B. This already gives an open-set performance of 69.6% AUROC, which is significantly higher than the softmax thresholding baseline reported in almost all of the comparisons in the literature, which report a baseline with 57.7% AUROC. The discrepancy between the reported baseline and our simplest setting is the result of reported figures originating in <ref type="bibr">(Neal et al., 2018)</ref>, wherein all models were trained only for 30 epochs (according to the publicly shared code) while our simplest model is trained for 100 epochs.</p><p>Following this trend, we find that training for longer (200 epochs) and using a better learning rate schedule (cosine annealed schedule (Loshchilov &amp; Hutter, 2017)) significantly enhances both closed-set and open-set performance. We further find that stronger augmentations boost accuracy, where we leverage RandAugment <ref type="bibr" target="#b10">(Cubuk et al., 2020)</ref> to find an optimal strategy. Finally, we find that learning rate warmup and label smoothing <ref type="bibr">(Szegedy et al., 2016)</ref> can together significantly increase accuracy. We select the RandAugment and label smoothing hyper-parameters by maximizing closed-set accuracy on a validation set (randomly sampling 20% of the training set).</p><p>In summary, we find that simply leveraging standard training strategies for image recognition models leads to a significant boost in open-set performance. Specifically, we find that the combination of the above methodologies, including longer training and better augmentations boosts the AUROC to 83.0%. 2) are used after every convolution layer, with dropout used on the input image, and then after the third and sixth layer. Finally, after the ninth layer, the spatial feature is reduced with average pooling to a feature vector with dimensionality D = 128. This is fed to the linear classifier (fully connected layer) to give the output logits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 STANDARD DATASETS</head><p>Here, we describe the experimental setup for our results in sec. 4 of the main paper.</p><p>All models were trained on a single 12GB GPU (mostly a NVIDIA Titan X). When optimizing with the cross-entropy loss, training took between 2 and 6 hours for a single class split, depending on the dataset (for instance, training on TinyImageNet took 2.5 hours). All hyper-parameters were tuned on a validation set which was constructed by holding out a randomly sampled 20% of the closed-set training data from a single split of seen/unseen classes.</p><p>Baselines, MSP+/MLS We trained the VGG32 model with a batch size of 128 for 600 epochs. For each dataset, we train on five splits of 'known/unknown' classes as is standard practise, training each run with the random seed '0'. We use an initial learning rate of 0.1 for all datasets except TinyImageNet, for which we use 0.01. We train with a cosine annealed learning rate, restarting the learning rate to the initial value at epochs 200 and 400. Furthermore, we 'warm up' the learning rate by linearly increasing it from 0 to the 'initial value' at epoch 20.</p><p>We use RandAugment for all experiments, tuning its hyper-parameters on a validation set from a single class split for each dataset. We follow a similar procedure for the label smoothing value s, though we find the optimal value to be s = 0 for all datasets except TinyImageNet, where it helps significantly at s = 0.9.</p><p>(ARPL + CS)+ We use the same experimental procedure for ARPL + CS <ref type="bibr">(Chen et al., 2021)</ref> as for the baselines, again tuning the RandAugment and label smoothing hyperparameters for this method.</p><p>Here, following the original implementation, we find a batch size of 64 and learning rate of 0.001 lead to better performance on TinyImageNet. This method also took significantly longer to train, taking 7.5 hours per class split on TinyImageNet.</p><p>OSRCI+ OSRCI involves multiple stages of training, including first training a GAN to synthesize images similar to the training data, before using generated images as 'open-set' examples to train a (K + 1)-way classifier <ref type="bibr">(Neal et al., 2018)</ref>. As our focus is on the effect of improving classification accuracy on open-set performance, we augment the training of the latter stage of OSRCI. We again train the (K + 1)-way classifier for 600 epochs with a cosine annealed learning rate schedule and RandAugment. For this method, we find that reducing all learning rates by a factor 10 compared to the baselines significantly improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 PROPOSED BENCHMARKS</head><p>Here, we describe the experimental setup for our results in sec. 5 of the main paper.</p><p>ImageNet. For this evaluation, we leverage a ResNet50 model pre-trained with the cross-entropy loss on ImageNet-1K from <ref type="bibr">(Wightman, 2019)</ref>. We evaluate the model directly for our MLS baseline. For ARPL+, we finetune the pre-trained model for 10 epochs with the ARPL optimization strategy.</p><p>FGVC datasets. We use a similar experimental setting for the FGVC datasets as we do for the standard benchmarks. Specifically, for both MLS and ARPL+, we again train for 600 epochs, using a cosine annealed learning rate and learning rate warmup. We also re-tune the RandAugment and label smoothing hyper-parameters on a validation set. Differently, however, we use a ResNet50 backbone with 448 ? 448 image size as is standard in the FGVC literature. We further initialize the network with weights from MoCoV2 training on Places, using an initial learning rate of 0.001 and a batch size of 32. Training for both methods took between one and two days depending on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note:</head><p>We attempted to train ARPL+CS on our proposed datasets but found it computationally infeasible. Specifically, the memory intensive nature of the method meant we could only fit a batch size of 2 on a 12GB GPU. We attempted to scale it up for the FGVC datasets, fitting a batch size of 16 across 4? 24GB GPUs, with training taking a week. However, we found its performance after a week to be slightly lower than ARPL+ in this setting. In table 6, we provide comparisons with more methods, including those using a different backbone architecture, to supplement table 1 from the main paper. The overrall conclusion is the same as in the main paper. Specifically, our improved baseline significantly outperforms reported baseline figures and outperforms state-of-the-art OSR models on a number of standard benchmarks. Training other OSR methods (OSRCI, ARPL + CS <ref type="bibr">(Neal et al., 2018;</ref><ref type="bibr">Chen et al., 2021)</ref>) on top of our improved baseline can boost also their OSR performance. However, the discrepancy between the state-of-the-art and the baseline is now negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E COMPARISONS WITH OTHER DEEP LEARNING BASED OSR METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F OUT-OF-DISTRIBUTION DETECTION RESULTS</head><p>In this section, we run experiments on OoD benchmarks, a separate but related machine learning sub-field to OSR. OoD deals with all forms of distributional shifts, whereas OSR focusses on semantic novelty. Specifically, in the 'multiclass' OoD setting, a model is trained for classification on a given dataset, before being tasked with detecting test samples from other datasets as 'unknown' <ref type="bibr">(Hendrycks &amp; Gimpel, 2017)</ref>. Once again, this task is evaluated as a binary classification ('known'/'unknown') problem. A notable difference with the OSR setting is that OoD models often have access to auxiliary data as examples of 'OoD' during training <ref type="bibr">(Hendrycks et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 CORRELATION BETWEEN CLOSED-SET AND OOD PERFORMANCE</head><p>First, we conduct similar experiments to sec. 3. We evaluate four ResNet models trained on CI-FAR100 on the OoD task, using CIFAR10 for examples of 'OoD'. We show the closed-set and OoD performances of these models are correlated in <ref type="figure" target="#fig_9">fig. 8</ref>, with a Pearson Product-Moment correlation of ? = 0.97. This trend is similar to the one observed in the ImageNet OSR evaluation in <ref type="figure" target="#fig_2">fig. 3b</ref>.  We find that strong closed-set classifiers with our MLS baseline can achieve highly competitive performance on the OoD benchmarks, once again substantially closing the gap between the MSP baseline (Hendrycks &amp; Gimpel, 2017) and state-of-the-art. To solve this problem, and to perform OSR reliably, the model must understand the set of invariances within a single category, as well as a set of 'axes of variation' to distinguish between categories. Specifically, different instances within a single category will have a set of features which can be freely varied without the category label changing. In computer vision, this often refers to characteristics such as pose and lighting, but could also refer to more abstract features such as animal gender or background setting. Meanwhile, the classification system will also have a (possibly abstract) set of axes of variation to which the category label is sensitive.</p><p>In the current OSR benchmarks, with either abstract class definitions or a small number of classes, the set of axes of variation which can distinguish between categories is diverse. In this sense, the problem is ill-posed, with many axes likely being equally valid to distinguish between the training classes, including those based on semantically meaningless low-level features. In contrast, within our proposed fine-grained setting, the set of axes of variation which can distinguish between categories is far more constrained. For instance, in the CUB case, given a training task of classifying 100 bird species, there is little uncertainty as to what the axis of semantic variation could be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H CREATING SPLITS FOR THE SEMANTIC SHIFT BENCHMARK H.1 SPLIT CONSTRUCTION</head><p>In sec. 5 of the main paper, we sketched the process for constructing open-set splits from the CUB dataset. Here, we describe the process in detail for both CUB, Stanford Cars and FGVC-Aircraft, which each have different attribute structures.</p><p>For each FGVC benchmark, we split its classes into two disjoint sets, C and U, containing closed-set and open-set classes respectively. U is further subdivided into disjoint {'Easy', 'Medium', 'Hard'} sets with varying degrees of attribute similarity with any class in C. Specifically, we measure the difficulty of an open-set class by its semantic similarity with its most similar training class (where similarity is defined in terms of attribute overlap).</p><p>In practice, we found the semantic similarity of the 'Medium' and 'Hard' splits of Stanford Cars to the closed-set to be very similar, hence we combine them into a single 'Hard' split.</p><p>CUB. In CUB, each image is labelled for the presence of 312 visual attributes such as has_bill_shape::hooked and has_breast_color::yellow. Note that images from the same class do not all share the same attributes, both because of standard factors such as pose and occlusion, but also because of factors such as the age and gender of the bird.</p><p>This information is summarised on a per-class basis, describing how often each attribute occurs in each class; i.e., a matrix M ? [0, 1] C?A is available, where C = 200 is the total number of classes in CUB and A = 312 is the number of attributes. This allows us to construct a class similarity matrix S ? [0, 1] C?C where S ij = m i ? m j and m i is the L2-normalized i th row of M . Thus, given a set of closed-set classes in C, we can rank all remaining classes (U) according to their maximum similarity with any of the training classes. Finally, we bin the ranked open-set classes into {'Easy', 'Medium', 'Hard'} sets. In practice, we randomly sample 1 million combinations of C, and select the combination which results in the most difficult open-set splits.</p><p>Stanford Cars. Each class name in Stanford Cars follows the format of 'Make'-'Model'-'Type'-'Year'; for instance 'Aston Martin -V8 Vantage -Convertible -2012' is a class. In this case, we create open-set splits of different difficulties based on the similarity between class names. We first create the 'Hard' open-set split by identifying pairs of classes which have the same 'Make', 'Model' and 'Type' but come from different 'Years'. Next, we create the 'Medium' split from class pairs which have the same 'Make' and 'Model' but have different 'Types'. Finally, the 'Easy' split is constructed from pairs which have the same 'Make' but different 'Models'.</p><p>We note that open-set bins of different difficulties in Stanford Cars are the most troublesome to define. This is because the rough hierarchy in the class names may not always correspond to the degree of visual similarity between the classes.</p><p>FGVC-Aircraft. We leverage the hierarchy of class labels in FGVC-Aircraft; each image is labelled with a 'manufacturer' (e.g., 'Airbus' or 'Boeing'), a 'family' (e.g., 'A320' or 'A330') and a 'variant' (e.g.'A330-200' or 'A330-300'). The hierarchy is constructed as a tree, with 'manufacturer' classes at the top level, 'family' classes at the second, and 'variant' classes at the bottom. The standard image classification challenge operates at the variant level, meaning all variant classes are visually distinct with identifiable features. Furthermore, the hierarchy corresponds to visual similarity, i.e there is more inter-class variation between manufacturers than between variants from the same manufacturer. Thus, given the closed-set classes C, we can create an 'Easy' open-set split from variants which do not share a manufacturer with any closed-set class. Meanwhile, 'Medium' open-set classes share a manufacturer with closed-set classes but come from different families, and 'Hard' open-set classes share families with closed-set classes but are different variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 SEMANTIC SHIFT BENCHMARK EXAMPLES</head><p>We include examples of the open-set splits from the SSB's FGVC datasets in <ref type="figure" target="#fig_10">fig. 9</ref> to 11. For each dataset, we show examples of 'Easy' (green/top), 'Medium' (orange/middle) and 'Hard' (red/bottom) classes. In practise, we combine the 'Medium' and 'Hard' splits into a single 'Hard' split for evaluation.</p><p>We show 'Easy' and 'Hard' examples from ImageNet in <ref type="figure">fig. 12</ref>.</p><p>For each difficulty, we show three images from three classes from the open-set (right) and their most similar class in the closed-set (left). We note that 'Hard' open-set classes are far more visually similar to their corresponding closed-set class than 'Easy' open-set classes.</p><p>The Semantic Shift Benchmark can be found here: https://www.robots.ox.ac.uk/~vgg/research/ osr/#ssb_suite.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I AVERAGE PRECISION EVALUATION ON PROPOSED BENCHMARKS</head><p>We report average precision (AP) for the binary 'known/unknown' decision for the proposed benchmark evaluations in table 9. AP is a standard metric in the OoD literature and is better suited for dealing with class imbalance at test time. We note that the 'Hard' FGVC open-set splits (with a small number of classes) report substantially poorer AP than AUROC in absolute terms. We treat open-set examples as 'positive' during evaluation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Datasets.</head><label></label><figDesc>We train the above methods on the standard benchmark datasets for open-set recognition. In all cases, the model is trained on a subset of classes, while other classes are reserved as 'unseen' for evaluation. MNIST (LeCun et al., 2010), SVHN (Netzer et al., 2011), CIFAR10 (Krizhevsky, 2009): These are ten-class datasets, with MNIST and SVHN containing images of hand-written digits and street-view house numbers respectively. Meanwhile, CIFAR10 is a generic object recognition dataset containing natural images from ten diverse classes including animals and vehicles. In these cases, the open-set methods are evaluated by training on six classes, while using the other four classes for testing (|C| = 6; |U| = 4). CIFAR + N (Krizhevsky, 2009): In an extension to the CIFAR10 evaluation protocol, open-set algorithms are benchmarked by training on four classes from CIFAR10, while using N classes from CIFAR100 for evaluation, where N denotes either 10 or 50 classes (|C| = 4; |U| ? {10, 50}). TinyImageNet (Le &amp; Yang, 2015): In the final and most challenging case, exisiting open-set algorithms are evaluated on the TinyImageNet dataset. This dataset contains 200 classes sub-sampled from ImageNet (Russakovsky et al., 2015), with 20 classes used for training and 180 as unknown (|C| = 20; |U| = 180).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Open-set results on a range of architectures on the ImageNet dataset. 'Easy' and 'Hard' OSR splits are constructed from the ImageNet-21K-P dataset. (b) ImageNet open-set results within a single model family (ResNet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Gains in open-set performance as closed-set performance increases on TinyImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Open-set class pairs for CUB. For three difficulties {'Easy' (green/left), 'Medium' (orange/middle), 'Hard' (red/right)}, we show an image from an open-set class (right) and its most similar closed-set class (left). Note that the harder the difficulty, the more visual features (e.g., foot colour or bill shape) the open-set class has in common with the closed-set. Further examples can be found in appendix H.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>CUB) (Wah et al., 2011), Stanford Cars (Krause et al., 2013) FGVC-Aircraft (Maji et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Correlation between open-set and closed-set performances on the standard OSR benchmarks. This plot is similar tofig. 2but includes scatter points for OSRCI(Neal et al., 2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Plots showing how the feature representations and linear classification weights of a deep classifier evolve as training proceeds (CIFAR10 OSR setting). (a), (b) show the average feature norm for seen and unseen classes, as well as the per-class vector norms for the weights in the linear classification head, for models with D = 128 and D = 2 respectively. (c) shows how the open-set performance of the classifier with D = 128 develops as training proceeds, using three different OSR scoring rules. (d), (e), (f) show the feature projections for images from seen and unseen classes at different epochs (indicated by vertical dashed lines in (b)) for the model with D = 2. We show test images from known classes in colour and unknown classes in black. (g) (h) (i) show how classifier weight and feature norms change as a function of weight decay strength (?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Finally, we find that open-set performance can be boosted to 84.0% AUROC by bootstrapping the training data and training K = 5 ensembles. The improvements in open-set performance strongly correlate with the boosts to the closed-set accuracy, with ? = 0.93 between accuracy and AUROC. D IMPLEMENTATION DETAILS D.1 VGG32 ARCHITECTURE This backbone architecture is commonly used in the open-set literature (Neal et al., 2018). The model consists of a simple series of nine 3?3 convolution layers, with downsampling occurring through strided convolutions every third layer. Batch normalization and LeakyRelu (slope of 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>OoD against closed-set performance for four ResNet models trained on CIFAR100, using CIFAR10 as OoD. The plot indicates a similar performance correlation as observed infig. 3b.F.2 OOD PERFORMANCE WITH DIFFERING TYPES OF DISTRIBUTION SHIFTNext, in table 7, we evaluate OoD performance when different datasets are taken as examples of 'OoD' with respect to CIFAR100. Specifically, we compare OSR methods (and an OoD baseline), taking Gaussian Noise, SVHN and CIFAR10 as 'OoD'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Sample classes from closed and open-set splits for the CUB dataset. We show 'Easy' (green/top), 'Medium' (orange/middle) and 'Hard' (red/bottom) classes. Classes on the left (solid outline) are in the closed-set, while classes on the right (dashed outline) are in the open-set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Sample classes from closed and open-set splits for the Stanford Cars dataset. We show 'Easy' (green/top), 'Medium' (orange/middle) and 'Hard' (red/bottom) classes. Classes on the left (solid outline) are in the closed-set, while classes on the right (dashed outline) are in the open-set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Sample classes from closed and open-set splits for the FGVC-Aircraft dataset. We show 'Easy' (green/top), 'Medium' (orange/middle) and 'Hard' (red/bottom) classes. Classes on the left (solid outline) are in the closed-set, while classes on the right (dashed outline) are in the open-set. Sample classes from closed and open-set splits for the ImageNet dataset. We show 'Easy' (green/top) and 'Hard' (red/bottom) classes. Classes on the left (solid outline) are in the closed-set, while classes on the right (dashed outline) are in the open-set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Correlation between closed-set performance (Accuracy) and open-set performance (AUROC). We train three methods on the standard open-set benchmark datasets, including the MSP baseline, ARPL and ARPL + CS</figDesc><table><row><cell>75 80 85 90 95 100 Open Set Performance (AUROC)</cell><cell cols="2">ARPL+CS ARPL MSP Method</cell><cell cols="2">Dataset MNIST SVHN CIFAR10 CIFAR + 50 TinyImageNet</cell><cell>Figure 2:</cell></row><row><cell></cell><cell>70</cell><cell>75 Closed Set Performance (Accuracy) 80 85 90</cell><cell>95</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of our improved baselines (MSP+, MLS) against state-of-the-art methods on the standard OSR benchmark datasets. All results indicate the area under the Receiver-Operator curve (AUROC) averaged over five 'known/unknown' class splits. '+' indicates prior methods augmented with improved closed-set optimization strategies, including:MSP+ (Neal et al.,  2018),OSRCI+ (Neal et al., 2018)  and (ARPL + CS)+(Chen et al., 2021).</figDesc><table><row><cell>Method</cell><cell>MNIST</cell><cell>SVHN</cell><cell>CIFAR10</cell><cell cols="3">CIFAR + 10 CIFAR + 50 TinyImageNet</cell></row><row><cell cols="2">Baseline (MSP) (Neal et al., 2018) 97.8</cell><cell>88.6</cell><cell>67.7</cell><cell>81.6</cell><cell>80.5</cell><cell>57.7</cell></row><row><cell>OSRCI (Neal et al., 2018)</cell><cell>98.8</cell><cell>91.0</cell><cell>69.9</cell><cell>83.8</cell><cell>82.7</cell><cell>58.6</cell></row><row><cell>OpenHybrid (Zhang et al., 2020)</cell><cell>99.5</cell><cell>94.7</cell><cell>95.0</cell><cell>96.2</cell><cell>95.5</cell><cell>79.3</cell></row><row><cell>ARPL + CS (Chen et al., 2021)</cell><cell>99.7</cell><cell>96.7</cell><cell>91.0</cell><cell>97.1</cell><cell>95.1</cell><cell>78.2</cell></row><row><cell>OSRCI+</cell><cell cols="4">98.5 (-0.3) 89.9 (-1.1) 87.2 (+17.3) 91.1 (+7.3)</cell><cell>90.3 (+7.6)</cell><cell>62.6 (+4.0)</cell></row><row><cell>(ARPL + CS)+</cell><cell cols="3">99.2 (-0.5) 96.8 (+0.1) 93.9 (+2.9)</cell><cell>98.1 (+1.0)</cell><cell>96.7 (+1.6)</cell><cell>82.5 (+4.3)</cell></row><row><cell>Baseline (MSP+)</cell><cell cols="6">98.6 (+0.8) 96.0 (+7.4) 90.1 (+22.4) 95.6 (+14.0) 94.0 (+13.5) 82.7 (+25.0)</cell></row><row><cell>Baseline (MLS)</cell><cell cols="6">99.3 (+1.5) 97.1 (+8.5) 93.6 (+25.9) 97.9 (+16.3) 96.5 (+16.0) 83.0 (+25.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Discussion. A number of increasingly sophisticated methods have been proposed for OSR in recent years. Typically, proposed methods have carefully tuned training strategies and hyper-parameters, such as custom learning rate schedules(Zhang et al., 2020), non-standard backbones(Guo et al.,  2021)  and novel data augmentations(Zhou et al., 2021). Meanwhile, the closed-set accuracy of the methods is often unreported. As such, it is difficult to delineate what proportion of the open-set performance gains come from increases in closed-set accuracy. Our findings in this section suggest that many of the gains could equally be realised through the standard baseline. Indeed, in sec. 5, we propose new evaluation protocols and find that once the closed-set accuracy of ARPL and the baseline are made comparable, there is negligible difference in open-set performance. We further experiment on OoD benchmarks in appendix F and report similarly improved baseline performance.5 SEMANTIC SHIFT BENCHMARKCurrent OSR benchmarks have two drawbacks: (1) they all involve small scale datasets; (2) they lack a clear definition of what constitutes a 'semantic class'. The latter is important to delineate the open-set field from other research questions such as out-of-distribution detection(Hendrycks  &amp; Gimpel, 2017)  and anomaly detection(Kwon et al., 2020). Specifically, OSR aims to identify whether a test image is semantically different to the training classes, not whether, for example, the model is uncertain about its prediction or whether there has been a low-level distributional shift.To address these issues, we propose a new suite of evaluation benchmarks. In this section, we first detail a large-scale ImageNet evaluation (introduced in sec. 3.2) before proposing three evaluations on fine-grained datasets which have clear definitions of a semantic class. Differently to previous work, our evaluation settings all aim to explicitly capture the notion of semantic novelty. Finally, we benchmark MLS and ARPL on the new benchmark suite to motivate future research.</figDesc><table /><note>5.1 PROPOSED BENCHMARK DATATSETS ImageNet. We introduce a large-scale evaluation for category shift, with open-set splits based on semantic distances to the training set. Specifically, we designate the original ImageNet-1K classes for the closed-set, and choose open-set classes from the disjoint set of ImageNet-21K-P (Ridnik et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the Semantic Shift Benchmark. We show '#Classes(#Test Images)' for the known classes, and for the 'Easy', 'Medium' and 'Hard' open-set classes.</figDesc><table><row><cell>Dataset</cell><cell>Known</cell><cell>Easy</cell><cell>Medium</cell><cell>Hard</cell></row><row><cell>CUB</cell><cell>100 (2884)</cell><cell>32 (915)</cell><cell cols="2">34 (1004) 34 (991)</cell></row><row><cell>Stanford Cars</cell><cell>98 (3948)</cell><cell>76 (3170)</cell><cell>-</cell><cell>22 (923)</cell></row><row><cell cols="2">FGVC-Aircraft 50 (1668)</cell><cell>20 (667)</cell><cell>17 (565)</cell><cell>13 (433)</cell></row><row><cell>ImageNet</cell><cell cols="3">1000 (50000) 1000 (50000) -</cell><cell>1000 (50000)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>OSR results on the Semantic Shift Benchmark. We measure the closed-set classification accuracy and AUROC on the binary open-set decision. We also report OSCR, which measures the trade-off between open and closed-set performance. OSR results are shown on 'Easy / Hard' splits. Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. In IEEE TPAMI, 2017.Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Learning placeholders for open-set recognition. In CVPR, 2021. A EXPANSION OF FIG. 2 OF THE MAIN PAPER WITH STANDARD DEVIATIONS For completeness, we include another version of fig. 2 which includes OSRCI models (Neal et al., 2018) in</figDesc><table><row><cell>Method</cell><cell></cell><cell>CUB</cell><cell></cell><cell></cell><cell>SCars</cell><cell></cell><cell></cell><cell cols="2">FGVC-Aircraft</cell><cell></cell><cell>ImageNet</cell></row><row><cell></cell><cell>Acc.</cell><cell>AUROC</cell><cell>OSCR</cell><cell>Acc.</cell><cell>AUROC</cell><cell>OSCR</cell><cell>Acc.</cell><cell>AUROC</cell><cell>OSCR</cell><cell>Acc.</cell><cell>AUROC</cell><cell>OSCR</cell></row><row><cell cols="13">ARPL+ 85.9 83.5 / 75.5 76.0 / 69.6 96.9 94.8 / 83.6 92.8 / 82.3 91.5 87.0 / 77.7 83.3 / 74.9 78.2 79.3 / 74.0 66.3 / 63.0</cell></row><row><cell>MLS</cell><cell cols="12">86.2 88.3 / 79.3 79.8 / 73.1 97.1 94.0 / 82.2 92.2 / 81.1 91.7 90.7 / 82.3 86.8 / 79.8 78.8 78.7 / 72.8 67.0 / 63.4</cell></row><row><cell cols="3">6 CONCLUSION</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">In this work we have demonstrated a strong correlation between the closed-set and open-set perfor-</cell></row><row><cell cols="13">mance of models for the task of open-set recognition. Leveraging this finding, we have demonstrated</cell></row><row><cell cols="13">that a well-trained closed-set classifier, using the maximum logit score (MLS) at test-time, can be</cell></row></table><note>competitive with or outperform existing state-of-the-art methods. Though we believe OSR is a critical problem which requires further investigation, our findings give us insufficient evidence to reject our titular question of 'is a good closed-set classifier all you need?'. We have also proposed the 'Semantic Shift Benchmark' suite, which isolates semantic shift from other low-level distributional shifts. Our proposed benchmark suite allows controlled study of semantic novelty, including stratification of the degree of semantic shift.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Standard deviations of our experiments in fig. 2 of the main paper. We report the standard deviations for both the closed-set and open-set performance (accuracy/AUROC) across the five 'known/unknown' class splits.</figDesc><table><row><cell>Method</cell><cell>MNIST</cell><cell>SVHN</cell><cell cols="2">CIFAR10 CIFAR + 50 TinyImageNet</cell></row><row><cell>MSP</cell><cell cols="3">0.20/1.29 0.36/0.55 1.64/1.34 0.79/1.23</cell><cell>4.83/1.36</cell></row><row><cell>OSRCI</cell><cell cols="3">0.22/0.52 0.47/2.97 1.99/1.80 0.63/1.47</cell><cell>3.27/3.02</cell></row><row><cell>ARPL</cell><cell cols="3">0.21/0.77 0.43/0.79 2.10/1.56 0.66/0.44</cell><cell>5.40/1.63</cell></row><row><cell cols="4">ARPL + CS 0.29/1.04 0.51/0.31 1.70/1.68 0.63/0.23</cell><cell>4.40/1.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>shows how the AUROC evolves as training proceeds when both the maximum logit and maximum softmax value are used for OSR scoring. The plot demonstrates that softmax normalization noticeably reduces the model's ability to make the open-set decision. We also show the OSR performance if we use the feature norm as our open-set score (S(y ? C|x) = |? ? (x)|) , showing that this simple indicator can perform remarkably well.C IMPROVING OPEN-SET PERFORMANCE WITH STRONGER CLOSED-SET CLASSIFIERS Here, we describe how we improve the open-set performance of the baseline method in sec. 4 of the main paper, and provide a full breakdown offig. 4. The methods include better learning rate schedules and data augmentations, as well as the use of logits rather than the softmax output for OSR scoring. We document the closed-set and open-set performance on the TinyImageNet dataset (the most challenging of the OSR benchmarks) in table 5. We further include the 'Open Set Classification Rate'(OSCR (Dhamija et al., 2018)) which summarises the trade-off between closed-set accuracy and open-set performance (here, in terms of the False Positive Rate) as the threshold on the open-set score is varied. As demonstrated in sec. 4 of the main paper, the findings of this study generalize well to other datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Breakdown of methods used to improve the closed-set classification accuracy of the baseline method. All experiments were conducted with a VGG32 backbone over five 'known/unknown' splits of the TinyImageNet dataset. The bracketed number with the Cosine scheduler indicates the number of learning rate restarts used during training. We find a Pearson Product-Moment correlation of 0.93 between the closed-set accuracy and the open-set AUROC.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Setting</cell><cell>Closed-Set</cell><cell>Open-Set</cell><cell>Combined</cell></row><row><cell cols="2">Epochs Scheduler</cell><cell>Aug.</cell><cell>Logit Eval Warmup Label Smoothing Ensemble</cell><cell>(Accuracy)</cell><cell>(AUROC)</cell><cell>(OSCR)</cell></row><row><cell>100</cell><cell>Step</cell><cell>RandCrop</cell><cell></cell><cell>64.3</cell><cell>68.9</cell><cell>51.4</cell></row><row><cell>100</cell><cell>Step</cell><cell>RandCrop</cell><cell></cell><cell>64.3</cell><cell>69.6</cell><cell>50.7</cell></row><row><cell>200</cell><cell cols="2">Cosine (0) RandCrop</cell><cell></cell><cell>77.7</cell><cell>74.8</cell><cell>64.3</cell></row><row><cell>200</cell><cell cols="2">Cosine (0) CutOut</cell><cell></cell><cell>77.6</cell><cell>75.4</cell><cell>64.7</cell></row><row><cell>200</cell><cell cols="2">Cosine (0) RandAug</cell><cell></cell><cell>79.8</cell><cell>76.6</cell><cell>67.3</cell></row><row><cell>600</cell><cell cols="2">Cosine (2) RandAug</cell><cell></cell><cell>82.5</cell><cell>78.2</cell><cell>70.3</cell></row><row><cell>600</cell><cell cols="2">Cosine (2) RandAug</cell><cell></cell><cell>82.5</cell><cell>78.4</cell><cell>70.3</cell></row><row><cell>600</cell><cell cols="2">Cosine (2) RandAug</cell><cell></cell><cell>84.2</cell><cell>83.0</cell><cell>74.3</cell></row><row><cell>600</cell><cell cols="2">Cosine (2) RandAug</cell><cell></cell><cell>85.3</cell><cell>84.0</cell><cell>76.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Comparing our improved baseline with other deep learning based OSR methods on the standard benchmark datasets. All results indicate the area under the Receiver-Operator curve (AUROC) as a percentage. We also show the backbone architecture used for each method, showing results with multiple backbones when reported.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>MNIST</cell><cell>SVHN</cell><cell>CIFAR10</cell><cell cols="3">CIFAR + 10 CIFAR + 50 TinyImageNet</cell></row><row><cell>MSP (Neal et al., 2018)</cell><cell>VGG32</cell><cell>97.8</cell><cell>88.6</cell><cell>67.7</cell><cell>81.6</cell><cell>80.5</cell><cell>57.7</cell></row><row><cell cols="2">OpenMax (Bendale &amp; Boult, 2016) VGG32</cell><cell>98.1</cell><cell>89.4</cell><cell>69.5</cell><cell>81.7</cell><cell>79.6</cell><cell>57.6</cell></row><row><cell>G-OpenMax (Ge et al., 2017)</cell><cell>VGG32</cell><cell>98.4</cell><cell>89.6</cell><cell>67.5</cell><cell>82.7</cell><cell>81.9</cell><cell>58.0</cell></row><row><cell>OSRCI (Neal et al., 2018)</cell><cell>VGG32</cell><cell>98.8</cell><cell>91.0</cell><cell>69.9</cell><cell>83.8</cell><cell>82.7</cell><cell>58.6</cell></row><row><cell>CROSR (Yoshihashi et al., 2019)</cell><cell>DHRNet</cell><cell>99.1</cell><cell>89.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.9</cell></row><row><cell>C2AE (Oza &amp; Patel, 2019)</cell><cell>VGG32</cell><cell>98.9</cell><cell>92.2</cell><cell>89.5</cell><cell>95.5</cell><cell>93.7</cell><cell>74.8</cell></row><row><cell>GFROSR (Perera et al., 2020)</cell><cell cols="2">VGG32 / WRN-28-10 -</cell><cell cols="2">93.5 / 95.5 80.7 / 83.1</cell><cell>92.8 / 91.5</cell><cell>92.6 / 91.3</cell><cell>60.8 / 64.7</cell></row><row><cell>CGDL (Sun et al., 2021)</cell><cell>CPGM-AAE</cell><cell>99.5</cell><cell>96.8</cell><cell>95.3</cell><cell>96.5</cell><cell>96.1</cell><cell>77.0</cell></row><row><cell>OpenHybrid (Zhang et al., 2020)</cell><cell>VGG32</cell><cell>99.5</cell><cell>94.7</cell><cell>95.0</cell><cell>96.2</cell><cell>95.5</cell><cell>79.3</cell></row><row><cell>RPL (Chen et al., 2020a)</cell><cell>VGG32 / WRN-40-4</cell><cell cols="3">99.3 / 99.6 95.1 / 96.8 86.1 / 90.1</cell><cell>85.6 / 97.6</cell><cell>85.0 / 96.8</cell><cell>70.2 / 80.9</cell></row><row><cell>PROSER (Zhou et al., 2021)</cell><cell>WRN-28-10</cell><cell>-</cell><cell>94.3</cell><cell>89.1</cell><cell>96.0</cell><cell>85.3</cell><cell>69.3</cell></row><row><cell>ARPL (Chen et al., 2021)</cell><cell>VGG32</cell><cell>99.6</cell><cell>96.3</cell><cell>90.1</cell><cell>96.5</cell><cell>94.3</cell><cell>76.2</cell></row><row><cell>ARPL + CS (Chen et al., 2021)</cell><cell>VGG32</cell><cell>99.7</cell><cell>96.7</cell><cell>91.0</cell><cell>97.1</cell><cell>95.1</cell><cell>78.2</cell></row><row><cell>OSRCI+</cell><cell>VGG32</cell><cell cols="4">98.5 (-0.3) 89.9 (-1.1) 87.2 (+17.3) 91.1 (+7.3)</cell><cell>90.3 (+7.6)</cell><cell>62.6 (+4.0)</cell></row><row><cell>(ARPL + CS)+</cell><cell>VGG32</cell><cell cols="3">99.2 (-0.5) 96.8 (+0.1) 93.9 (+2.9)</cell><cell>98.1 (+1.0)</cell><cell>96.7 (+1.6)</cell><cell>82.5 (+4.3)</cell></row><row><cell>Baseline (MSP+)</cell><cell>VGG32</cell><cell cols="6">98.6 (+0.8) 96.0 (+7.4) 90.1 (+22.4) 95.6 (+14.0) 94.0 (+13.5) 82.7 (+25.0)</cell></row><row><cell>Baseline (MLS)</cell><cell>VGG32</cell><cell cols="6">99.3 (+1.5) 97.1 (+8.5) 93.6 (+25.9) 97.9 (+16.3) 96.5 (+16.0) 83.0 (+25.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Results on out-of-distribution detection benchmarks. We evaluate two MLS models: one represents a model which we train ourselves; the second represents a strong pre-trained model from(Lim et al., 2019).As a strong baseline from the OoD literature, we report results from Outlier Exposure (O.E.)(Hendrycks et al., 2019), which encourages the classifier to predict a uniform distribution when fed auxiliary 'OoD' images from 80 Million Tiny Images(Torralba et al., 2008). We also report results from OpenHybrid (Zhang et al., 2020) which reports a CIFAR100 ? CIFAR10 result. Furthermore, we train ARPL+CS and MLS in this setting, training a ResNet50 for 200 epochs. As a final experiment, we take a strong model pre-trained on CIFAR100 from(Lim et al., 2019)  and evaluate it on the OoD benchmarks. Our results show that, while OpenHybrid performs strongly on the CIFAR100 ? CIFAR10 experiment, the two MLS models outperform the O.E baseline on this evaluation despite not having seen extra data during training.F.3 EVALUATION ON OOD BENCHMARKSFinally, we run our MLS method on the standard OoD benchmark suite. Specifically, we take models trained on CIFAR10 and CIFAR100, and evaluate them when Places365(Zhou et al., 2017), Textures<ref type="bibr" target="#b9">(Cimpoi et al., 2014)</ref>,LSUN-Crop (Yu et al., 2015),LSUN-Resize (Yu et al., 2015), iSUN(Xu  et al., 2015)  andSVHN (Netzer et al., 2011)  are used in turn as 'OoD' datasets. We take well-trained WideResNet-40 models (trained with Fast Auto-Augment on CIFAR10 and CIFAR100 from(Lim  et al., 2019)) and run our MLS baseline on top. We compare against state-of-the-art OoD methods which do not use extra data for fine-tuning, and report our results in table 8. We report average AUROC across the six OoD datasets.</figDesc><table><row><cell></cell><cell>Outlier Exposure (Hendrycks et al., 2019)</cell><cell>OpenHybrid (Zhang et al., 2020)</cell><cell cols="2">ARPL+CS MLS</cell><cell>MLS (Lim et al., 2019)</cell></row><row><cell>CIFAR100 ? Gaussian Noise</cell><cell>95.7</cell><cell>-</cell><cell>67.6</cell><cell>73.5</cell><cell>78.9</cell></row><row><cell>CIFAR100 ? SVHN</cell><cell>86.9</cell><cell>-</cell><cell>77.9</cell><cell>83.3</cell><cell>88.9</cell></row><row><cell>CIFAR100 ? CIFAR10</cell><cell>75.7</cell><cell>85.6</cell><cell>73.0</cell><cell>77.7</cell><cell>83.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Results of our strong baseline on the full OoD benchmark suite. We take strong WideResNet-40 models from(Lim et al., 2019)  and run our MLS baseline on top. Models are trained on CIFAR10 and CIFAR100 as 'in-distribution' and we report AUROC averaged across six OoD datasets. All compared figures are taken from(Du et al., 2022)  andLiu et al. (2020).Discussion. Our results show that strong closed-set classifiers can also perform well in the OoD setting, even compared to very recent methods such as Virtual Outlier Synthesis(VOS, (Du et al.,  2022)). In fact, in some cases, we find the MLS baseline exceeds state-of-the-art for this task.Interestingly, the MLS baseline performs best with in the 'near-OoD' case (e.g. SVHN and CIFAR10 as 'OoD' in table 7, i.e. in the more similar settings to OSR). In fact, the MLS models trained on CIFAR100 are worse at detecting Gaussian Noise than CIFAR10 images as 'OoD'. We present this peculiar finding as evidence that the OoD and OSR research questions may have different, and possibly orthogonal, solutions. We hope that benchmarks which can isolate semantic novelty from low-level distributional shifts, such as the Semantic Shift Benchmark from sec. 5, can facilitate more controlled OSR and OoD research.G DISCUSSION: UNDERSTANDING SYSTEMS OF CATEGORIZATION Before one can establish if an image belongs to a new class, one must first understand what constitutes a single class, or how the system of categorization is constructed. To illustrate this, consider a classifier trained on instances of two household pets: {Labrador (dog), British Shorthair (cat)}. Now consider an open-world setting in which the model must be able to distinguish previously unseen objects, perhaps: {Poodle (dog), Sphynx (cat)}. In this case, understanding the categorization system is essential to making the open-set decision. Does the classification system delineate individual animal species? In this case, both 'Poodle' and 'Sphynx' should be identified as 'open-set' examples. Or does it instead simply separate 'cats' from 'dogs'? In which case neither object belongs to the open-set.</figDesc><table><row><cell>Method</cell><cell cols="2">CIFAR10 CIFAR100</cell></row><row><cell>MSP (Hendrycks &amp; Gimpel, 2017)</cell><cell>90.9</cell><cell>75.5</cell></row><row><cell>ODIN (Liang et al., 2018)</cell><cell>91.1</cell><cell>77.4</cell></row><row><cell>Energy Score (Liu et al., 2020)</cell><cell>91.9</cell><cell>79.6</cell></row><row><cell>Mahanabolis (Lee et al., 2018b)</cell><cell>93.3</cell><cell>84.1</cell></row><row><cell>VOS (Du et al., 2022)</cell><cell>94.1</cell><cell>-</cell></row><row><cell>MLS (Ours)</cell><cell>95.1</cell><cell>80.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Average Precision (AP) results on the proposed benchmark datasets for 'Easy' / 'Medium' / 'Hard' splits. / 58.2 / 47.2 69.2 / 58.2 / 39.6 76.6 / -/ 68.6</figDesc><table><row><cell></cell><cell>CUB</cell><cell>FGVC-Aircraft</cell><cell>ImageNet</cell></row><row><cell cols="4">ARPL+ 59.9 / 53.3 / 45.3 66.9 / 58.9 / 34.4 78.2 / -/ 71.2</cell></row><row><cell>MLS</cell><cell>67.1</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Ablations for different weight decay values are shown infig. 7gto 7j (we use ? = 1e ? 4 in this paper).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to thank Andrew Brown for many interesting discussions on this work. This research is funded by a Facebook AI Research Scholarship, a Royal Society Research Professorship, and the EPSRC Programme Grant VisualAI EP/T028572/1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>Open-set recognition is of immediate relevance to the safe and ethical deployment of machine learning models. In real-world settings, it is unrealistic to expect that all categories of interest to the user will be represented in the training set. For instance, in an autonomous driving scenario, forcing the model to identify every object as an instance of a training category could lead it to make unsafe decisions.</p><p>When considering potential negative societal impacts of this work, we identify the possibility that OSR research may lead to complacent consideration of the training data. As we have demonstrated, OSR models are far from perfect and cannot be exclusively relied upon in practical deployment. As such, it remains of critical importance to carefully curate training data and ensure its distribution is representative of the target task.</p><p>Finally, we comment on the dataset privacy considerations for the existing and proposed benchmarks. All datasets are licensed for academic/non-commercial research. However, CIFAR, TinyImageNet and ImageNet contain some personal data for which consent was likely not obtained. The proposed FGVC datasets have the added benefit of containing no personal information.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07579</idno>
		<title level="m">Jonathon Shlens, and Barret Zoph. Revisiting resnets: Improved training and scaling strategies</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Local novelty detection in multi-class recognition problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bodesheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Freytag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning and the unknown: Surveying steps toward open world recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><forename type="middle">Raj</forename><surname>Dhamija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>G?nther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henrydoss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning open set network with discriminative reciprocal points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial reciprocal points learning for open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reducing network agnostophobia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Akshay Raj Dhamija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>G?nther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
