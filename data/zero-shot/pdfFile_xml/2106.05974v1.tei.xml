<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling Vision with Sparse Mixture of Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><forename type="middle">Neumann</forename><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Rodolphe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenatton</forename><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><forename type="middle">Andr?</forename><surname>Susano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinto</forename><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Daniel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keysers</forename><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brain</forename><surname>Neil</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houlsby</forename><surname>Google Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scaling Vision with Sparse Mixture of Experts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are "dense", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-ofthe-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.</p><p>We first describe MoEs and sparse MoEs. We then present how we apply this methodology to vision, before explaining our design choices for the routing algorithm and the implementation of V-MoEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conditional Computation with MoEs</head><p>Conditional computation aims at activating different subsets of a network for different inputs <ref type="bibr" target="#b4">[5]</ref>. A mixture-of-experts model is a specific instantiation whereby different model "experts" are responsible for different regions of the input space <ref type="bibr" target="#b30">[31]</ref>.</p><p>We follow the setting of [54], who present for deep learning a mixture of experts layer with E experts as MoE(x) = ? E i=1 g(x) i e i (x) where x ? R D is the input to the layer, e i ? R D ? R D is the function computed by expert i, and g ? R D ? R E is the "routing" function which prescribes the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning historically shows that increasing network capacity and dataset size generally improves performance. In computer vision, large models pre-trained on large datasets often achieve the state of the art <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref>. This approach has had even more success in Natural Language Processing (NLP), where large pre-trained models are ubiquitous, and perform very well on many tasks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b17">18]</ref>. Text Transformers <ref type="bibr" target="#b60">[61]</ref> are the largest models to date, some with over 100B parameters <ref type="bibr" target="#b8">[9]</ref>. However, training and serving such models is expensive <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b45">46]</ref>. This is partially because these deep networks are typically "dense"-every example is processed using every parameter -thus, scale comes at high computational cost. In contrast, conditional computation <ref type="bibr" target="#b4">[5]</ref> aims to increase model capacity while keeping the training and inference cost roughly constant by applying only a subset of parameters to each example. In NLP, sparse Mixture of Experts (MoEs) are gaining popularity <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22]</ref>, enabling training and inference with fewer resources while unlocking trillion parameter models.</p><p>In this work, we explore conditional computation for vision at scale. We introduce the Vision MoE (V-MoE), a sparse variant of the recent Vision Transformer (ViT) architecture <ref type="bibr" target="#b19">[20]</ref> for image classification. The V-MoE replaces a subset of the dense feedforward layers in ViT with sparse MoE layers, where each image patch is "routed" to a subset of "experts" (MLPs). Due to unique failure modes and non-differentiability, routing in deep sparse models is challenging. We explore various design choices, and present an effective recipe for the pre-training and transfer of V-MoE, notably outperforming their dense counterparts. We further show that V-MoE models are remarkably flexible. The performance vs. inference-cost trade-off of already trained models can be smoothly adjusted during inference by modulating the sparsity level with respect to the input and/or the model weights.</p><p>With V-MoE, we can scale to model sizes of 15B parameters, the largest vision models to date. We match the performance of state-of-the-art dense models, while requiring fewer time to train. <ref type="figure">Figure 1</ref>: Overview of the architecture. V-MoE is composed of L ViT blocks. In some, we replace the MLP with a sparsely activated mixture of MLPs. Each MLP (the expert) is stored on a separate device, and processes a fixed number of tokens. The communication of these tokens between devices is shown in this example, which depicts the case when k = 1 expert is selected per token. Here each expert uses a capacity ratio C = 4 3 : the sparse MoE layer receives 12 tokens per device, but each expert has capacity for 16 ( 16?1 12 = 4 3 ; see Section 2.4). Non-expert components of V-MoE such as routers, attention layers and normal MLP blocks are replicated identically across devices.</p><p>Alternatively, V-MoE can match the cost of ViT while achieving better performance. To help control this tradeoff, we propose Batch Prioritized Routing, a routing algorithm that repurposes model sparsity to skip the computation of some patches, reducing compute on uninformative image regions.</p><p>We summarize our main contributions as follows: Vision models at scale. We present the Vision Mixture of Experts, a distributed sparsely-activated Transformer model for vision. We train models with up to 24 MoE layers, 32 experts per layer, and almost 15B parameters. We show that these models can be stably trained, seamlessly used for transfer, and successfully fine-tuned with as few as 1 000 datapoints. Moreover, our largest model achieves 90.35% test accuracy on ImageNet when fine-tuned. Performance and inference. We show that V-MoEs strongly outperform their dense counterparts on upstream, few-shot and full fine-tuning metrics in absolute terms. Moreover, at inference time, the V-MoE models can be adjusted to either (i) match the performance of the largest dense model while using as little as half of the amount of compute, or actual runtime, or (ii) significantly outperform it at the same cost. Batch Prioritized Routing. We propose a new priority-based routing algorithm that allows V-MoEs to discard the least useful patches. Thus, we devote less compute to each image. In particular, we show V-MoEs match the performance of the dense models while saving 20% of the training FLOPs. Analysis. We provide some visualization of the routing decisions, revealing patterns and conclusions which helped motivate design decisions and may further improve understanding in the field. input-conditioned weight for the experts. Both e i and g are parameterized by neural networks. As defined, this is still a dense network. However, if g is sparse, i.e., restricted to assign only k ? E non-zero weights, then unused experts need not be computed. This unlocks super-linear scaling of the number of model parameters with respect to inference and training compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MoEs for Vision</head><p>We explore the application of sparsity to vision in the context of the Vision Transformer (ViT) <ref type="bibr" target="#b19">[20]</ref>. ViT has been shown to scale well in the transfer learning setting, attaining better accuracies than CNNs with less pre-training compute. ViT processes images as a sequence of patches. An input image is first divided into a grid of equal-sized patches. These are linearly projected to the Transformer's <ref type="bibr" target="#b60">[61]</ref> hidden size. After adding positional embeddings, the patch embeddings (tokens) are processed by a Transformer, which consists predominately of alternating self-attention and MLP layers.</p><p>The MLPs have two layers and a GeLU <ref type="bibr" target="#b28">[29]</ref> non-linearity: MLP(x) = W 2 ? gelu (W 1 x). For Vision MoE, we replace a subset of these with MoE layers, where each expert is an MLP; see <ref type="figure">Figure 1</ref>. The experts have the same architecture e i (x) = MLP ?i (x) but with different weights ? i = (W i 1 , W i 2 ). This follows a similar design pattern as the M4 machine translation model <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Routing</head><p>For each MoE layer in V-MoE, we use the routing function g(x) = TOP k (softmax (Wx + )), where TOP k is an operation that sets all elements of the vector to zero except the elements with the largest k values, and is sampled independently ? N (0, 1 E 2 ) entry-wise. In practice, we use k = 1 or k = 2. In the context of the Vision Transformer, x is a representation of an image token at some layer of the network. Therefore, V-MoE routes patch representations, not entire images.</p><p>The difference between previous formulations <ref type="bibr" target="#b53">[54]</ref> is that we apply TOP k after the softmax over experts weights, instead of before. This allows us to train with k = 1 (otherwise gradients with respect to routings are zero almost everywhere) and also performs better for k &gt; 1 (see Appendix A).</p><p>Finally, we add a small amount of noise with standard deviation 1 E to the activations Wx, which we find improves performance. We empirically found this performed well but that the setup was robust to this parameter. The noise typically altered routing decisions ?15% of the time in earlier layers, and ?2-3% in deeper layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Expert's Buffer Capacity</head><p>During training, sparse models may favor only a small set of experts <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b51">52]</ref>. This common failure mode can cause two problems. First, statistical inefficiency: in the limit of collapse to a single expert, the model is no more powerful than a dense model. Second, computational inefficiency: imbalanced assignment of items to experts may lead to a poor hardware utilization.</p><p>To combat imbalance and simplify our implementation, we fix the buffer capacity of each expert (i.e. the number of tokens that each expert processes), and train our model with auxiliary losses that encourage load balancing. This is essentially the same approach as followed by <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22]</ref>. In our case, we use slight variants of two of the auxiliary losses proposed in <ref type="bibr" target="#b53">[54]</ref>, as described in Appendix A.</p><p>We define the buffer capacity of an expert (B e ) as a function of the number of images in the batch (N ), the number of tokens per image (P ), the number of selected experts per token (k), the total number of experts (E), and the capacity ratio (C): B e = round kN P C E . If the router assigns more than B e tokens to a given expert, only B e of them are processed. The remaining tokens are not entirely 'lost' as their information is preserved by residual connections (the top diagram of <ref type="figure">Figure 1</ref>). Also, if k &gt; 1, several experts try to process each token. Tokens are never fully discarded. If an expert is assigned fewer than B e tokens, the rest of its buffer is zero-padded.</p><p>We use the capacity ratio to adjust the capacity of the experts. With C &gt; 1, a slack capacity is added to account for a potential routing imbalance. This is typically useful for fine-tuning when the new data might come from a very different distribution than during upstream training. With C &lt; 1, the router is forced to ignore some assignments. In Section 4 we propose a new algorithm that takes advantage of setting C ? 1 to discard the least useful tokens and save compute during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Transfer Learning</head><p>In this section, we first present training different variants of V-MoE on a large dataset (Section 3.2) in order to be used for Transfer Learning afterwards. The ability to easily adapt our massive models to new tasks, using a small amount of data from the new task, is extremely valuable: it allows to amortize the cost of pre-training across multiple tasks. We consider two different approaches to Transfer Learning: linear few-shot learning on fixed representations and full fine-tuning of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Models</head><p>We build V-MoE on different variants of ViT <ref type="bibr" target="#b19">[20]</ref>: ViT-S(mall), ViT-B(ase), ViT-L(arge) and ViT-H(uge), the hyperparameters of which are described in Appendix B.5. There are three additional major design decisions that affect the cost (and potentially the quality) of our model:</p><p>Number of MoE layers. Following <ref type="bibr" target="#b38">[39]</ref>, we place the MoEs on every other layer (we refer to these as V-MoE Every-2). In addition, we experimented with using fewer MoE layers, by placing them on the last-n even blocks (thus we dub these V-MoE Last-n). In Appendix E.1 we observe that, although using fewer MoE layers decreases the number of parameters of the model, it has typically little impact on quality and can speed-up the models significantly, since less communication overhead is incurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of selected experts k:</head><p>The cost of our model does not depend on the total number of experts but the number of selected ones per token. Concurrent works in NLP fix k = 1 <ref type="bibr" target="#b21">[22]</ref> or k = 2 <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b38">39]</ref>. In our case, we use by default k = 2 (see <ref type="figure" target="#fig_0">Figure 10</ref> in Appendix B for the exploration of different values of k), while we found the total number of experts E = 32 to be the sweet spot in our setting.</p><p>Buffer capacity C: As mentioned in Section 2.4, we use a fixed buffer capacity. While this is typically regarded as a downside or engineering difficulty to implement these models, we can adjust the capacity ratio to control different trade-offs. We can intentionally set it to a low ratio to save compute, using Batch Prioritized Routing (see <ref type="bibr">Section 4)</ref>. During upstream training, we set C = 1.05 by default to give a small amount of slack without increasing the cost noticeably.</p><p>Note that for a given trained model, the latter two-k and C-can be adjusted without further training, whereas the positioning and quantity of expert layers is effectively fixed to match pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data</head><p>We pre-train our models on JFT-300M <ref type="bibr" target="#b56">[57]</ref>, a semi-automatically noisy-labeled dataset. It has ? 305M training and 50 000 validation images, organised in a hierarchy of 18 291 classes (average 1.89 labels per image). We deduplicate it with respect to all our validation/test sets as in previous efforts <ref type="bibr" target="#b35">[36]</ref>. <ref type="bibr" target="#b1">2</ref> Our few-shot experiments on ImageNet (i.e. ILSVRC2012) use only 1, 5, or 10 shots per class to adapt the upstream model, evaluating the resulting model on the validation set.</p><p>We also fine-tuned the pre-trained models on the full training set (ca. 1M images). We report performance in a similar regime for four other datasets in Appendix B.5. Lastly, we explore the ability to fine-tune our large models in the low-data regime by evaluating them on the Visual Task Adaptation Benchmark (VTAB) <ref type="bibr" target="#b68">[69]</ref>, a diverse suite of 19 tasks with only 1 000 data points per task. As well as natural image classification, VTAB includes specialized tasks (e.g. medical or satellite imagery) and structured tasks (e.g. counting or assessing rotation/distance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Upstream results</head><p>JFT is a multilabel dataset, so we measure model performance via precision@1 (see Appendix B.6 for details). Note that as in previous works <ref type="bibr" target="#b19">[20]</ref>, hyperparameters were tuned for transfer performance, and JFT precision could be improved at the expense of downstream tasks e.g. by reducing weight decay. <ref type="figure" target="#fig_1">Figure 2a</ref> shows the quality of different V-MoE and ViT variants with respect to total training compute and time. It shows models that select k = 2 experts and place MoEs in the last n even blocks (n = 5 for V-MoE-H, n = 2 otherwise), but the best results are achieved by V-MoE-H/14 Every-2 (see <ref type="table" target="#tab_3">Table 2</ref>, 14 is the patch size). See Appendix B.5 for results of all models. <ref type="bibr" target="#b1">2</ref> We also checked the effect of deduplication with respect to the ImageNet training set, showing negligible (within noise) impact on few-shot results (only 1-shot worsened, see <ref type="table" target="#tab_10">Table 9</ref>).       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Linear few-shot results</head><p>We evaluate the quality of the representations learned using few-shot linear transfer. Given training examples from the new dataset {(X, Y ) i }, we use the pre-trained model M to extract a fixed representation M(x i ) of each image. We fit a linear regression model mapping M(x i ) to the one-hot encoding of the target labels Y i , following <ref type="bibr" target="#b19">[20]</ref> (see <ref type="bibr" target="#b26">[27,</ref><ref type="bibr">Chapter 5]</ref> for background).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2b</head><p>shows that the upstream gains are preserved under 5-shot ImageNet evaluation, considering both compute and time; in other words, the quality of the representations learned by V-MoE also outperforms ViT models when looking at a new task. <ref type="table" target="#tab_3">Table 2</ref> further shows the results on {1, 10}-shot for some selected models, and the full detailed results are available in Appendix B.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Full fine-tuning results</head><p>The typically most performant approach for Transfer Learning <ref type="bibr" target="#b18">[19]</ref> consists of replacing the upstream classification head with a new task-specific one and fine-tuning the whole model. Though one may expect that massive models like V-MoEs require special handling for fine-tuning, we broadly follow the standard fine-tuning protocol for Vision Transformers. We use the auxiliary loss during fine-tuning as well, although we observe that it is often not needed in this step, as the router is already well trained. We explore the two sets of tasks considered therein: No patch discarded. Full data. We follow the setup of <ref type="bibr" target="#b19">[20]</ref>, except that we apply a dropout rate of 0.1 on the expert MLPs (as done in <ref type="bibr" target="#b21">[22]</ref>), and we halve the number of fine-tuning steps for all datasets other than ImageNet. <ref type="figure" target="#fig_3">Figure 3</ref> shows the results on ImageNet (averaged over three runs). Here, V-MoE also performs better than dense counterparts, though we suspect the fine-tuning protocol could be further improved and tailored to the sparse models. See <ref type="table" target="#tab_9">Table 8</ref> for all details, including results on other datasets.</p><formula xml:id="formula_0">C = 0.9 C = 0.7 C = 0.5 C = 0.3 C = 0.1</formula><p>Low-data regime. On the VTAB benchmark, we use a similar setup and hyperparameter budget as <ref type="bibr" target="#b19">[20]</ref> (but fine-tune with half the schedule length). <ref type="table" target="#tab_2">Table 1</ref> shows that, while performance is similar for V-MoE-H/14, experts provide significant gains at the ViT-L/16 level, indicating that despite the large size of these models, they can still be fine-tuned with small amounts of data and no further tricks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Scaling up V-MoE</head><p>Finally, we test how well V-MoE can scale vision models to a very large number of parameters, while continuing to improve performance. For this, we increase the size of the model and use a larger pre-training dataset: JFT-3B is a larger version of JFT-300M, it contains almost 3B images and is noisily annotated with 30k classes. Inspired by <ref type="bibr" target="#b67">[68]</ref>, we apply the changes detailed in Appendix B.3, and train a 48-block V-MoE model, with every-2 expert placement (32 experts and k = 2), resulting in a model with 14.7B parameters, which we denote by V-MoE-15B.</p><p>We successfully train V-MoE-15B, which is, as far as we are aware, the largest vision model to date. It has an impressive accuracy of 82.78% on 5-shot ImageNet and 90.35% when fully finetuned, as shown in Appendix B.5, which also includes more details about the model. Training this model required 16.8k TPUv3-core-days. To contextualize this result, the current state of the art on ImageNet is Meta Pseudo-Labelling (MPL) <ref type="bibr" target="#b48">[49]</ref>. MPL trains an EfficientNet-based model on unlabelled JFT-300M using ImageNet pseudo-labelling, achieving 90.2% while requiring 22.5k TPUv3-core-days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Skipping Tokens with Batch Prioritized Routing</head><p>We present a new routing algorithm that allows the model to prioritize important tokens (corresp. patches). By simultaneously reducing the capacity of each expert, we can discard the least useful tokens. Intuitively, not every patch is equally important to classify a given image, e.g., most background patches can be dropped to let the model only focus on the ones with the relevant entities. ViT-H/14</p><p>Inference GigaFLOPs/image Even for large C's BPR outperforms vanilla; at low C the difference is stark. BPR is competitive with dense by processing only 15-30% of the tokens.</p><formula xml:id="formula_1">ViT V-MoE V-MoE (BPR) ViT V-MoE V-MoE (BPR)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">From Vanilla Routing to Batch Prioritized Routing</head><p>With the notation from Section 2, the routing function g is applied row-wise to a batch of inputs X ? R N ?P ?D . A batch contains N images composed of P tokens each; each row of X corresponds to the D-dimensional representation of a particular token of an image. Accordingly, g(X) t,i ? R denotes the routing weight for the t-th token and the i-th expert.</p><p>In all routing algorithms considered, for i &lt; j, every TOP-i assignment has priority over any TOP-j assignment. The router first tries to dispatch all i th expert choices before assigning any j th choice 3 .</p><p>Given the TOP-i position, the default-or vanilla-routing, as used in <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22]</ref>, assigns tokens to experts as follows. It sequentially goes over the rows of g(X) and assigns each token to its TOP-i expert when the expert's buffer is not full. As a result, priority is given to tokens depending on the rank of their corresponding row. While images in a batch are randomly ordered, tokens within an image follow a pre-defined fixed order. The algorithm is detailed in Algorithm 1 of Appendix C.</p><p>Batch Prioritized Routing (BPR). To favour the "most important" tokens, we propose to compute a priority score s(x) on each token, and sort g(X) accordingly before proceeding with the allocation. We sort tokens based on their maximum routing weight, formally s(X) t = max i g(X) t,i . The sum of TOP-k weights, i.e. s(X) t = ? i g(X) t,i , worked equally well. These two simple approaches outperformed other options we explored, e.g., directly parameterising and learning the function s.</p><p>We reuse the router outputs as a proxy for the priority of allocation. Our experiments show this preserves the performant predictive behaviour of the model, even though the router outputs primarily encode how well tokens and experts can be paired, not the token's "importance" for the final classification task. <ref type="figure" target="#fig_4">Figure 4</ref> visualizes token prioritisation with Batch Prioritized Routing for increasingly small capacities. Since all tokens across all images in the batch X compete with each other, different images may receive different amounts of compute. We summarize BPR in Algorithm 2, in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Skip tokens with low capacity C</head><p>Batch Prioritized Routing opens the door to reducing the buffer size by smartly selecting which tokens to favor. This can have a dramatic impact in the computational cost of the overall sparse model. We discuss now inference and training results with C defined in Section 2.4 in the regime C ? 1. At inference time. Prioritized routing is agnostic to how the model was originally trained. <ref type="figure">Figure 6</ref> shows the effect of reducing compute at inference time by using BPR versus vanilla routing, on a V-MoE-H/14 model trained using vanilla routing. The difference in performance between both methods is remarkable -especially for C ? 0.5, where the model truly starts fully dropping tokens, as k = 2. Also, BPR allows the model to be competitive with the dense one even at quite low capacities. As shown in <ref type="figure">Figure 5</ref> for V-MoE-L/16 and V-MoE-H/14, Batch Prioritized Routing and low C allow V-MoE to smoothly trade-off performance and FLOPS at inference time, quite a unique model feature.</p><p>More concretely, <ref type="table" target="#tab_2">Table 10</ref> shows V-MoE models can beat the dense VIT-H performance by using less than half the FLOPs and less than 60% of the runtime. Conversely, we can match the inference FLOPs cost and preserve a one-point accuracy gain in ImageNet/5shot and almost three-point in JFT precision at one <ref type="table" target="#tab_2">(Table 11</ref>). Dense models generally require less runtime for the same amount of FLOPs due to the data transfer involved in the V-MoE implementation.</p><p>At training time. Batch Prioritized Routing can also be leveraged during training. In Appendix C we show how expert models with max-weight routing can match the dense performance while saving around 20% of the total training FLOPs, and strongly outperform vanilla with a similar FLOP budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Analysis</head><p>Although large-scale sparse MoEs have led to strong performance <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b53">54]</ref>, little is known and understood about how the internals of those complex models work. We argue that such exploratory experiments can inform the design of new algorithms. In this section, we provide the first such analysis at this scale, which guided the development of the algorithms presented in the paper.</p><p>Specialized experts. Intuitively, routers should learn to distribute images across experts based on their similarity. For instance, if the model had three experts, and the task mainly involved three categories-say animals, cars, and buildings-one would expect an expert to specialize in each of those. We test this intuition, with some obvious caveats: (a) experts are placed at several network depths, (b) k experts are combined, and (c) routing happens at the token rather than the image level. <ref type="figure" target="#fig_7">Figure 7</ref> illustrates how many images of a given ImageNet class use each expert. The plots were produced by running a fine-tuned V-MoE-H Every-2 model. Interestingly, we saw similar patterns with the upstream model without fine-tuning. Experts specialize in discriminating between small sets of classes (those primarily routed through the expert). In earlier MoE layers we do not observe this. Experts may instead focus on aspects common to all classes (background, basic shapes, colours) -for example, <ref type="figure" target="#fig_3">Figure 30</ref> (Appendix E) shows correlations with patch location in earlier layers.</p><p>The value of routers. After training a sparse MoE, it is natural to study the usefulness of the learned routers, in the light of several pitfalls. For example, the routers may just act as a load balancer if experts end up learning very similar functions, or the routers may simply choose poor assignments.</p><p>In Appendix E.1, we replace, after training, one router at a time with a uniformly random router. The models are robust to early routing changes while more sensitive to the decisions in the last layers.</p><p>Routing weights distributions. We analyse the router outputs in Appendix E.3, and observe the distribution of selected weights varies wildly across different mixture of experts layers.</p><p>Changing k at inference time. We have observed expert models are remarkably flexible. Somewhat surprisingly, sparse models are fairly robust to mismatches between their training and inference configurations. In Appendix E.4, we explore the effect of training with some original value of k while applying the model at inference time with a different k ? ? k. This can be handy to control (decrease or increase) the amount of FLOPs per input in a particular production system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Conditional Computation. To grow the number of model parameters without proportionally increasing the computational cost, conditional computation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12]</ref> only activates some relevant parts of the model in an input-dependent fashion, like in decision trees <ref type="bibr" target="#b6">[7]</ref>. In deep learning, the activation of portions of the model can use stochastic neurons <ref type="bibr" target="#b5">[6]</ref> or reinforcement learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Mixture of Experts. MoEs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b65">66]</ref> combine the outputs of sub-models known as experts via a router in an input-dependent way. MoEs have successfully used this form of conditional computation in a range of applications <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b66">67</ref>]. An input can select either all experts <ref type="bibr" target="#b20">[21]</ref> or only a sparse mixture thereof as in recent massive language models <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>MoEs for Language. MoEs have recently scaled language models up to trillions of parameters. Our approach is inspired by <ref type="bibr" target="#b53">[54]</ref> who proposed a top-k gating in LSTMs, with auxiliary losses ensuring the expert balance <ref type="bibr" target="#b25">[26]</ref>. <ref type="bibr" target="#b38">[39]</ref> further scaled up this approach for transformers, showing strong gains for neural machine translation. With over one trillion parameters and one expert per input, <ref type="bibr" target="#b21">[22]</ref> sped up pre-training compared to a dense baseline <ref type="bibr" target="#b49">[50]</ref> while showing gains thanks to transfer and distillation. <ref type="bibr" target="#b39">[40]</ref> alternatively enforced a balanced routing by solving a linear assignment problem.</p><p>MoEs for Vision. For computer vision, previous work on MoEs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b63">64]</ref> focused on architectures whose scale is considerably smaller than that of both language models and our model. In DeepMoE <ref type="bibr" target="#b62">[63]</ref>, the "experts" are the channels of convolutional layers that are adaptively selected by a multi-headed sparse gate. This is similar to <ref type="bibr" target="#b63">[64]</ref> where the kernels of convolutional layers are activated on a per-example basis. Other approaches use shallow MoEs, learning a single router, either disjointly <ref type="bibr" target="#b24">[25]</ref> or jointly <ref type="bibr" target="#b1">[2]</ref>, together with CNNs playing the role of experts. <ref type="bibr" target="#b0">[1]</ref> further have a cost-aware procedure to bias the assignments of inputs across the experts. Unlike shallow MoEs, we operate with up to several tens of routing decisions per token along the depth of the model. Scaling up routing depth was marked as a major challenge in <ref type="bibr" target="#b50">[51]</ref>, which we successfully tackle in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have employed sparse conditional computation to train some of the largest vision models to date, showing significant improvements in representation learning and transfer learning. Alongside V-MoE, we have proposed Batch Prioritized Routing, which allows successful repurposing of model sparsity to introduce sparsity with respect to the inputs. This can be done without further adapting the model, allowing the re-use of trained models with sparse conditional computation.</p><p>This has interesting connotations for recent work in NLP using sparse models; recent analysis shows model sparsity is the most promising way to reduce model CO 2 emissions <ref type="bibr" target="#b45">[46]</ref> and that 90% of the footprint stems from inference costs -we present an algorithm which takes the most efficient models and makes them even more efficient without any further model adaptation.</p><p>This is just the beginning of conditional computation at scale for vision; extensions include scaling up the expert count, reducing dependance on data and improving transfer of the representations produced by sparse models. Directions relating to heteregonous expert architectures and conditional variable-length routes should also be fruitful. We expect increasing importance of sparse model scaling, especially in data rich domains such as large scale multimodal or video modelling.  </p><formula xml:id="formula_2">w 1 w 2 w 3 x 1 0.9 0.5 0.1 Expert 1 x 2 0.1 0.5 0.9 Expert 3 x 3 0.9 0.5 0.1 Expert 1 x 4 0.1 0.5 0.9 Expert 3 ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further details about the Vision Mixture of Experts</head><p>In this section, we provide additional details about the definition of V-MoE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Ablation on the modification of the routing function</head><p>Our formulation is similar to that in <ref type="bibr" target="#b53">[54]</ref>, except that we apply the "top k" operation after normalization of the experts weights, i.e. TOP k and softmax are applied in reverse order.</p><p>We choose this ordering because the original formulation from <ref type="bibr" target="#b53">[54]</ref> cannot be trained easily in the case of k = 1; it would lead to zero gradient with respect to x and W almost everywhere. Moreover, even for k &gt; 1, we found our alternative formulation to perform better (see <ref type="table" target="#tab_4">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Description of the load balancing losses</head><p>We describe below the regularizers that we use to enforce a balanced usage of the experts. Those regularizers present slight modifications with respect to their original definitions in <ref type="bibr" target="#b53">[54]</ref>.</p><p>Importance Loss. We incentivize a balanced usage of experts via an importance loss. The importance of expert i for a batch of images X is defined as the normalized routing weight corresponding to expert i summed over images:</p><formula xml:id="formula_3">Imp i (X) ?= x?X softmax(W x) i ,<label>(1)</label></formula><p>where W is the layer-specific weight matrix for the router. We use the squared coefficient of variation of the importance distribution over experts, Imp(X)</p><formula xml:id="formula_4">?= {Imp i (X)} E i=1 :</formula><p>L Imp (X) = std(Imp(X)) mean(Imp(X)) 2 ? var(Imp(X)).</p><p>[54] proposed a similar loss, while in their case token x contributed to the importance of expert i in Equation <ref type="formula" target="#formula_3">(1)</ref> only if i was indeed selected for x. We observed some modest empirical benefits thanks to Equation <ref type="formula" target="#formula_5">(2)</ref>.</p><p>Load Loss. The importance loss seeks to guarantee that all experts have on average similar output routing weights. Unfortunately, it is not difficult to think of routing configurations where these weights are balanced overall, but, still, some small subset of experts get all the assignments (see <ref type="table" target="#tab_5">Table 4</ref>).</p><p>Ideally, we would like to also explicitly balance the number of assignments. This quantity is discrete; therefore it is not differentiable, and we need to rely on a proxy. Following the proposal in <ref type="bibr" target="#b53">[54]</ref>, for each expert i and token x, we compute the probability of i being selected -i.e., being among the top-k-for x if we were to re-sample only the noise for expert i. For simplicity, we slightly modify the definition in <ref type="bibr" target="#b53">[54]</ref>. For each token x, we define the score threshold above which experts were selected; this is simply the k-th maximum score:</p><formula xml:id="formula_6">threshold k (x) ?= max k-th (W x + ) ,<label>(3)</label></formula><p>where was the noise vector originally sampled during the forward pass. Then, for each expert i we compute the probability of i being above the threshold if we were to only re-sample its noise:</p><formula xml:id="formula_7">p i (x) ?= P((W x) i + new ? threshold k (x)) = P( new ? threshold k (x) ? (W x) i ).<label>(4)</label></formula><p>The probability is defined over new ? N (0, ? 2 ), with ? = 1 E. The load for expert i over batch X is:</p><formula xml:id="formula_8">load i (X) = x?X p i (x).<label>(5)</label></formula><p>Finally, the load loss corresponds to the squared coefficient of variation of the load distribution:</p><formula xml:id="formula_9">L load (X) = std(load(X)) mean(load(X)) 2 , load(X) ?= {load i (X)} E i=1 .<label>(6)</label></formula><p>Final Auxiliary Loss. The final auxiliary loss is just the average over both:</p><formula xml:id="formula_10">L aux (X) = 1 2 L imp (X) + 1 2 L load (X).<label>(7)</label></formula><p>The overall loss is: L(X) = L classification (X) + ? L aux (X), for some hyperparameter ? &gt; 0. We set ? = 0.01 in all our experiments, observing that this choice was robust and not sensitive.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Upstream hyperparameters</head><p>We present the architectural details for the upstream models in <ref type="table" target="#tab_9">Table 8</ref> (embedding size-equivalently referred to as hidden size, MLP dimension, number of Transformer blocks, etc.). <ref type="table" target="#tab_7">Table 6</ref> shows the training hyper-parameters for our main models. We use the original setup for each ViT model <ref type="bibr" target="#b19">[20]</ref>. However, ViT-S was not formally introduced in <ref type="bibr" target="#b19">[20]</ref>, and our parameters for ViT-S (dense and sparse) do not match DeiT-Small introduced in [59]. There are many changes to typical dense models which can be applied alongside model sparsity in order to scale models up. In order to scale the base architecture to which we add sparse mixture of expert layers, we make the following changes based on [68]:</p><p>? Low precision: We use bfloat16 instead of float32 to store the gradient moving average.</p><p>? Learning-rate decay: We replace the linear schedule with an inverse square root schedule (rsqrt). ? Weight decay: We apply weight decay to the kernel weights in the model with value 0.03 (while biases are not regularized), except for the head kernel where we apply a stronger regularization of 3.0. ? Model head: We replace the token head <ref type="bibr" target="#b19">[20]</ref>-where the first token is selected-with a new self-attention based head that also includes an additional MLP <ref type="bibr" target="#b67">[68]</ref>. <ref type="table" target="#tab_8">Table 7</ref> shows the hyperparameters used for finetuning. As discussed, they are broadly identical to those used in the Vision Transformer <ref type="bibr" target="#b19">[20]</ref>, though with half the schedule length. We also apply expert dropout of 0.1 on the expert MLPs (as suggested in <ref type="bibr" target="#b21">[22]</ref>); this did not make a significant difference, typically marginally reducing or improving performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Fine-tuning hyperparameters</head><p>We finetuned the V-MoE-15B model on ImageNet at resolution 560x560 for 30 000 steps (i.e., about 6 epochs) with base learning rate 0.006. We used debiased Polyak averaging similar to <ref type="bibr" target="#b19">[20]</ref> with momentum 0.999999.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Results and details for all models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Computing Precision-at-1 on JFT</head><p>JFT is multi-label, and it contains a hierarchy of classes. However, for computing precision at one, we ignore this hierarchy: given predictions on an image, we just look at whether the class with highest predicted probability is indeed one of the true labels for the image. <ref type="table" target="#tab_10">Table 9</ref> shows the effect of Imagenet deduplication on the training data for fewshot with V-MoE-S/32. Overall, we do not observe a consistent and significant effect after de-duplicating the data. The variance across seeds is notable and-except in the case of IN/1shot-de-duplicated models can outperform (and underperform) the original ones on few-shot evaluation.    In order to test the effect of removing some images in the training set that are "close" to some ImageNet ones, we trained three V-MoE-S/32 models -with different seeds-on the de-duplicated dataset, and compare their few-shot performance as shown below. The variance in the results is considerable. The original model dominates on 1-shot, while two out of the three seeds outperform the original model on 5-, 10-, and 25-shot. The de-duplicated dataset contained more images overall, but we limited the training set to the original size (around 305M) and trained for the same epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 Training data deduplication</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dedup               <ref type="figure">Figure 5</ref> for a zoomed-in version on the largest models (versus inference FLOPs).  Result: complete assignment of patches to experts (with some potential dropping) initialize empty buffers with capacity B e for all experts e (see Section 2); for i = 1, . . . , k do for patch p = 1, . . . , N do e, w = Router(TOP ? i position, patch p); if e is not full then add patch p to processing buffer of expert e with weight w; else skip i-th expert assignment for patch p; end end end Algorithm 2: Batch Prioritized Routing Allocation Result: complete assignment of patches to experts (with some potential dropping) initialize empty buffers with capacity B e for all experts e (see Section 2); for patch p = 1, . . . , N do s(p) = ComputeScore(patch p, Router(?)); end patch orderingp = SortPatches(scores s, decreasing = True); for i = 1, . . . , k do for patch p = (1), . . . , (N ) according top do e, w = Router(TOP ? i position, patch p); if e is not full then add patch p to processing buffer of expert e with weight w; else skip i-th expert assignment for patch p; end end end</p><p>We explored a few scoring functions, and concluded that sorting according to the maximum routing weight for each patch p works really well-formally, s(p) = max e w e,p , where w e,p is the output of the routing function g for patch p and expert e (see Section 4.1). We experimented with the sum of all the TOP-k weights too (rather than just the TOP-1), leading to similar results. Moreover, we tried to directly learn a scoring function. In this case, the router would output E weights per patch (one per expert, jointly normalized by a softmax function) together with the score s(p) -one per patch. We explored a couple of scoring functions (linear + sigmoid, etc), to conclude that the maximum routing weight is quite a good baseline and hard to beat.</p><p>A natural extension of this algorithm consists in sorting at the patch-expert assignment level, rather than at the global patch level. The main difference with Algorithm 2 is that the sorting then looks at (patch p, TOP?i expert for p) scores for 1 ? i ? k. For example, assume k = 2 and we have two patches, p 1 and p 2 . Suppose p 1 selects experts (e 11 , e 12 ) with routing weights (0.7, 0.2), while p 2 selects (e 21 , e 22 ) with weights (0.5, 0.4). Under Algorithm 2 the order in which patch-expert assignments would be attempted is: (p 1 , e 11 ), (p 2 , e 21 ), (p 1 , e 12 ), (p 2 , e 22 ). If we use sorting at the patch-expert level, however, we would end up with: (p 1 , e 11 ), (p 2 , e 21 ), (p 2 , e 22 ), (p 1 , e 12 ). The latter could make more sense as the second assignment for p 2 could be more relevant than the second assignment for p 1 given their weights. We have not empirically tried this approach, however.</p><p>For completeness, we also report another related algorithm we did actually experiment with. We call it skip-patch. In this case, we first set a hyper-parameter S ? (0, 1). We will process a fraction S of the patches, and directly skip the remaining 1 ? S fraction. As before, we rank the N patches according to some scoring function s(?). Then, we directly discard the bottom (1 ? S)% of the patches, and proceed like in Algorithm 2 over the selected M = SN patches. Algorithm 3 formally describes the idea. Going back to our previous example with two patches, if we set S = 0.5 there, we will discard p 2 altogether, and just process: (p 1 , e 11 ), (p 1 , e 12 ). Note that S and C are two different parameters, and it makes sense to adjust C given S to avoid an excessive FLOPs waste.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3: Skip-Patch Routing Allocation</head><p>Result: complete assignment of patches to experts (with some enforced dropping) let S ? (0, 1); initialize empty buffers with capacity B e for all experts e (see Section 2); for patch p = 1, . . . , N do s(p) = ComputeScore(patch p, Router(?)); end patch orderingp = SortPatches(scores s, decreasing = True); patch orderingp = KeepPatches(TOP ? M, M = SN,p); for i = 1, . . . , k do for patch p = (1), . . . , (M ) according top do e, w = Router(TOP ? i position, patch p); if e is not full then add patch p to processing buffer of expert e with weight w; else skip i-th expert assignment for patch p; end end end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Applied during Inference</head><p>An appealing property of the algorithms introduced in the previous section is that they are agnostic to how the model was originally trained. Indeed, we first show the effect of reducing compute at inference time by using Batch Prioritized Routing, Algorithm 2, on models trained using Algorithm 1. Note the model parameters are identical in both cases, including the router parameters -we are only applying the model at inference, no further learning is involved-, but we apply different routing strategies. Overall, we observe that discarding patches at random (as Algorithm 1 effectively does) leads to a steep loss of performance when we only keep a small percentage of the patches, as one could expect. On the other hand, if we process the "right" patches -via Algorithm 2-the performance is surprisingly robust as long as we keep up to around 20% of the patches. <ref type="figure" target="#fig_28">Figure 15</ref> shows the inference performance as a function of C for the main every-2 expert models with k = 2, under Algorithm 2. We observe performance decreases slowly and smoothly as we constrain more and more the amount of patches experts can process.</p><p>Next we compare the inference performance of Algorithms 1 and 2. Results for V-MoE-H/14 are presented in <ref type="figure" target="#fig_29">Figure 16</ref>, V-MoE-L/16 in <ref type="figure" target="#fig_7">Figure 17</ref>, V-MoE-B/16 in <ref type="figure" target="#fig_10">Figure 18</ref>, and V-MoE-S/32 in <ref type="figure" target="#fig_13">Figure 19</ref>. In all cases we see the same clear trend. By definition of Algorithms 1 and 2, when k = 2, if C ? 0.5, then every patch has a decent change of getting its TOP-1 expert processed if routing is balanced. Therefore, the most interesting regime here is C &lt; 0.5. In that case, we see an enormous gap in performance between Algorithms 1 and 2, showing that choosing the right patches really pays off. Moreover, in most cases, using 15% of the patches (C = 0.15) is enough to match the upstream performance of the dense model. For the few-shot representations, between 20% and 30% of the patches is usually enough.</p><p>Overall, we consider the flexibility provided by Algorithm 2 to be quite a remarkable property of expert models. Once trained, they allow for a smooth trade-off between performance and compute, with no further training or adjustment needed. This can be certainly useful in a practical setting where the use-case may determine the available resources and constraints at hand.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Applied during Training</head><p>The previous subsection explored applying priority routing during inference to a pre-trained model. A natural extension consist in directly training a model with Algorithm 2 from scratch. By forcing experts to work with a small buffer or capacity ratio (i.e. C &lt;&lt; 1), we can save substantial training FLOPs while hopefully still get decent performance improvements with respect to dense models.</p><p>We show results for three models: V-MoE-S/32, V-MoE-B/32, and V-MoE-L/32. For completeness, we compare Algorithms 1 and 2. In all cases we see strong improvements when training with Algorithm 2. When we use full capacity (C ? 1.0), however, we expect both algorithms to behave in a fairly similar fashion, as no dropping is needed as long as routing is reasonably balanced. Finally, <ref type="figure" target="#fig_1">Figures 24 and 25</ref> presents the results for VIT-L/32 with k = 1 and k = 2. Remarkably, between 70 and 75% of the training FLOPs are enough to mimic the upstream dense performance. Note that, when k = 2, the lowest capacity (C = 0.1) already outperforms the dense upstream precision. The expert model is also able to deliver identical few-shot performance while saving more than 20% of the training FLOPs.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Applied during Fine-tuning</head><p>We also investigate the effect of using the max-routing algorithm in fine-tuning. We consider V-MoE-S/32 models pre-trained at various capacities both with and without Batch Prioritized Routing. We fine tune them on ImageNet to see the effect of priority routing during downstream fine-tuning and inference. This is shown in <ref type="figure" target="#fig_1">Figure 26</ref>.  There are a few conclusions that can be garnered:</p><p>? Downstream fine-tuning results are significantly impacted by capacity, with accuracy reducing from 77.4% to 68.6% by reducing capacity to 0.1. ? Batch Prioritized Routing can recover some of this performance drop; if it is applied during pre-training and fine-tuning, accuracy increases to 71.8% at the same capacity. ? It is more important to retain high capacity during fine-tuning than while pre-training.</p><p>For example, with priority routing applied both at downstream and upstream, C = 1.05 during pre-training with C = 0.1 during fine-tuning has accuracy 71.7%, but the inverse is significantly better with accuracy 74.1%. In both cases, priority routing is key to ameliorating the effect of low capacity during fine-tuning and pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Examples of patch dropping</head><p>No patch discarded. C = 0.9 C = 0.7 C = 0.5 C = 0.3 C = 0.1</p><p>No patch discarded. C = 0.9 C = 0.7 C = 0.5 C = 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Model Analysis</head><p>Several previous works have proposed deep models based on mixture of experts; most of them have also presented promising results. Unfortunately, despite the current excitement regarding this set of techniques, little is indeed known about how these complex models internally work. Exploratory experiments that shed light into the mechanics of routers and expert specialization could inform new algorithms. We try to provide the first such analysis here, which we actually found useful to develop some of the algorithms presented in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 The value of routers</head><p>The most natural question to ask after training a sparse model is whether the learned routers are doing something useful. There are several potential ways things could go wrong. For example, the router could just become a load balancer if experts end up implementing very similar functions. Alternatively, the router may simply choose sub-optimal assignments. As a first test, we replace one router at a time with a uniformly random router. For this, we take a pre-trained model -in particular, a V-MoE-L/16 with k = 2-, and re-evaluate its upstream and few-shot performance when perturbing the routers. <ref type="figure" target="#fig_1">Figure 27</ref> contains the results. In red, we show the original performance for the pre-trained model -that is, when applying all the learned routers. We also show the impact of replacing each router independently and in isolation with a uniformly random router -the layer ID is shown in the x-axis. In particular, the new router samples the weights in a white Gaussian fashion, so every pair of experts is equally likely to be the TOP-k choice for any given input. We also tried to randomly permute the output weights -so to avoid a distributional shift in applied routing weights-and it worsened results.</p><p>Overall, we observe that the last two layers -21 and 23-provide an essential routing for the upstream model to work well (validation precision at 1 in JFT). We have seen a similar pattern in other models. Interestingly, the previous to last MoE layer (21-th in this case) is the one where getting the routing right is the most important. The model is robust to mis-routing at most intermediate layers-layer 9 is an exception here. This observation motivated us into trying to train sparse models with MoE layers only at the very end-21 and 23, for example-with excellent results (and computational savings).  After analyzing the results in <ref type="figure" target="#fig_1">Figure 27</ref>, a natural follow up question is whether the model is robust to compounded mis-routing? We answer this question by looking at what happens when we replace a number of consecutive MoE layers with uniformly random routers. <ref type="figure" target="#fig_1">Figure 28</ref> shows the outcome. We start from the bottom MoE layer, and for every MoE layer i in the network, we evaluate the model where routers in 1 to i layers (both included) act randomly. Unfortunately, in this case, performance drops quickly as one would expect. Tokens are following random walks (if we ignore capacity issues) up to some point, and then using the correct remaining routers. If the random walk is long enough, the performance is severely degraded. We conclude the token paths in a trained model are far from random or meaningless.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Specialized experts</head><p>In <ref type="figure" target="#fig_1">Figure 29</ref> we show results for a massive model with 24 MoE layers, each of them with 32 experts. After training the model on JFT and fine-tuning it on ImageNet, we did forward passes (up to the pre-logits) with ImageNet images. Each plot corresponds to one MoE layer, in increasing order. The  x-axis corresponds to the 32 experts per layer, and the y-axis are the 1000 ImageNet classes (in different adjusted orders per plot; i.e., class 5 in layers i and j are generally different for i ? j). For each pair (expert e, class i) we show the average routing weight for the patches corresponding to all images with class i for that particular expert e. Intuitively, this is a proxy for how much images of class i activate and use expert e. <ref type="figure" target="#fig_1">Figure 29</ref> shows strong expert-class correlations for the last few layers. In other words, it seems experts specialize in discriminating between a small set of classes (those primarily routed through the expert). In the initial MoE layers, however, we do not observe such correlation, and we conclude the experts may focus on different aspects of the patches that may be common to all classes (background, basic shapes, etc.).</p><p>To further investigate the logic behind the first layers of experts, <ref type="figure" target="#fig_3">Figure 30</ref> shows the correlation between selected experts and the patch id or location. The model partitions each image in the same number of patches -say if the patch size is 14x14, and images are 224x224x3, then there are 256 patches (sometimes we add an additional learnable token). We add a positional embedding to each patch that helps the model track the relative ordering. In this case, we see that for the first few MoE layers, the routers tend to distribute patches to experts according to their patch id. One simple explanation could be that patches in similar positions usually share visual characteristics that one expert learns to handle -say, image corners, backgrounds, or the center of the images with objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Routing weights distribution</head><p>Most of the key model hyper-parameters, like the number of experts that process each patch k or the expert buffer capacity ratio C that controls the amount of patch dropping, can be adjusted layer-wise. For example, if we do not see expert specialization in lower layers, we could simply set k = 1 there to avoid wasting compute. It may however be useful to increase k in the last few layers to allow for composability of concepts, like when we try to identify several objects. <ref type="figure" target="#fig_3">Figure 31</ref> shows the TOP-1 and TOP-2 weight distribution for an sparse model with k = 2. Two main conclusions can be drawn. First, in lower layers, both choices seem to have a similar magnitude -thus, both indeed contribute to the combined representation. Moreover, the weights are usually low in this layers -note 1 E ? 0.03 is the minimum weight the top selected expert can be assigned-, which one may interpret as the router being somewhat indifferent among experts. Second, the trend clearly changes when patches travel towards the end of the network. In particular, the TOP-1 and TOP-2 weight distributions strongly diverge, with the former approaching 1.0 and the latter approaching 0. This means the intrinsic k at the top of the network is closer to 1 (than the actual k = 2). The composability that we mentioned before may not be indeed needed at the patch level, as patches are quite small for large networks (here, 14x14), and it may be difficult to identify several concepts. Nonetheless, some tail of the distributions shown in <ref type="figure" target="#fig_3">Figure 31</ref> still uses both experts in the last layers.</p><p>We would like to remark that each image is subject to a large number of routing decisions through its patches. Concretely, <ref type="figure" target="#fig_1">Figure 32</ref> shows how most images use -on aggregate by pooling over all their patches-most of the experts in every layer. This motivated our efforts to try to save compute by discarding, or not processing, patches that are not useful for the final classification. We cover this in detail in Section 4.    <ref type="figure" target="#fig_3">Figure 31</ref>: Routing weight distribution for TOP-1 and TOP-2 selected experts. We show the distribution over the TOP-1 (green) and TOP-2 (red) weights for a V-MoE-H/14 model fine-tuned on ImageNet. Note for any given patch these weights do not need to add to one -and in fact they will not-, as we apply the softmax before the TOP-k selection.  In this case, every image has 730 patches. Even though most experts are selected at least once -that is what we plot here-, we expect some of the experts to be selected way more often by the patches of an image, and with a higher average weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Changing k at inference</head><p>We now explore a remarkable aspect of expert models: their flexibility. Somewhat surprisingly, we have observed sparse models to be fairly robust to mismatches between the training and inference configurations. In this section, we explore the effect of training with some original k while applying the model at inference time with a different k ? ? k. This can be handy to control (decrease or increase) the amount of FLOPs per input in a particular production system. <ref type="figure" target="#fig_3">Figure 33</ref> is based on a V-MoE-S/32 model trained with k = 1. We evaluate the upstream and few-shot metrics at inference time for a range of new k ? s. Note we do not perform any further training in any case, and the model parameters (including the router) are identical in all cases. The only difference is the number of experts we apply to each input, the amount of the network we activate. In red we show the original model's performance, and in blue the new ones. Finally, for each k ? , in yellow, we show the performance of a V-MoE-S/32 model trained originally with k = k ? -which, as we expected, increases in k ? . We see that increasing the value of k from its original value (k = 1) at inference time by one or two units actually significantly improves performance, both upstream and few-shot. However, at some point, if the new k ? is too large, the performance starts suffering -probably as the model is not prepared for the new distribution of total output routing weights applied in the linear combination, and sub-optimal experts for a given input start contributing to its representation.  The trends are somewhat similar. By applying k ? = 3 or k ? = 4 we obtain modest improvements, whereas by decreasing k to k ? = 1 we obtain a performance very similar to the performance of a model trained directly with k = 1, especially for few-shot. This is interesting, as we can devote more FLOPs for training by setting k = 2 upfront, while deferring the choice of inference k without losing potential performance. We explored these ideas further in Section 4. Also, the drop in performance for large values of k is less severe in this case, probably due to the fact that the trained model was used to combine several different experts (not the case for <ref type="figure" target="#fig_3">Figure 33</ref>).</p><p>Finally, in <ref type="figure" target="#fig_3">Figure 35</ref> we present the case where the upstream model was trained with k = 5. This is an expensive model to train, and we see we can change the inference value of k from k ? = 3 to k ? = 7 with results that are similar to their optimal value, if we had trained with those values in the first place. At this point the model is stable enough to deal with large values of k, but it suffers way more when we set k ? = 1, as the model is not used to picking a single expert and -we suspect-the TOP-1 expert may not carry so much importance or weight for this model where five experts were selected per input while training. Of course, it may not just be a matter of routing weight distribution. The expert themselves may be quite different when training with k = 1 -say, more self-containedand with k = 5 -perhaps more team-players.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Changing k during fine-tuning</head><p>We also consider the effect of adjusting the number of selected experts during fine-tuning and inference. We consider the aforementioned V-MoE-S/32 models, with 32 experts, pre-trained with k = {1, ..., 9} experts. These models are then fine-tuned with varied k. We show the result of this in <ref type="figure" target="#fig_3">Figure 36</ref>. As one may expect given our previous results, generally increasing k improves performance. Regardless of upstream k, generally accuracy improves from increasing k during fine-tuning. Similarly, increasing k during pre-training improves performance downstream.</p><p>Conversely, when k = 1 downstream, all models fail to improve from pre-training with higher upstream k. Models pre-trained with k &gt; 1 seemingly learn to combine expert outputs, in that they do not generalize as well to selecting a single expert downstream, and lose the benefits of pre-training with larger k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6 Pre-training with less data</head><p>We have shown that the standard recipe of pre-training with large datasets allows use of powerful sparse models on downstream vision tasks where less data is available. The question naturally arises: do these models require large amounts of data upstream? We present here some initial explorations in this direction.</p><p>Training on JFT300M with less data. We first train a V-MoE-L/32 on subsets of JFT300M. This was also done for dense models in <ref type="bibr" target="#b19">[20]</ref>, and in <ref type="figure" target="#fig_3">Figure 37</ref> we compare directly to their results. V-MoE seems initially fairly robust to reduced data, but after reducing to 9M pre-training samples (3% of the dataset), it becomes slightly preferable to instead train a dense model.</p><p>Training on ImageNet21k. ImageNet21k <ref type="bibr" target="#b15">[16]</ref> is a large public dataset with approximately 14M images and 21k classes. Previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref> have successfully pre-trained on it to achieve strong results in downstream tasks. In particular, dense ViT models trained on ImageNet21k perform reasonably well. With the exception of ViT-S, where V-MoE immediately outperforms the dense counterpart, applying sparse scaling generally harmed performance. We observed overfitting, both  The effect of varying the amount of pre-training data. We compare the performance of V-MoE-L/32 and VIT-L/32 for increasing data sizes. In particular, we take subsets of JFT-300M with 9M, 30M, 90M, and 300M datapoints -note the full dataset contains around 305M datapoints. Given that we train with smaller datasizes, we decided to use 8 experts rather than 32 (every-2). At the lowest data size (9M, around 3% of the original), the MoE model is not able to leverage its extra-capacity. For the remaining ones, starting at 30M (around 10% of the original dataset), it does.</p><p>in the sense of reducing validation accuracy on the pre-training dataset, but also in reduced transfer performance as training continued. As an initial attempt at tackling this, we used RandAugment <ref type="bibr" target="#b13">[14]</ref> with N = 2 transformations of magnitude M = 10. This is shown in <ref type="figure" target="#fig_3">Figure 38</ref>. Interestingly, RandAug typically helps expert models while harming dense models. With this applied, for each architecture, there is an expert model which outperforms the dense baseline. This is far from a complete exploration; it indicates that these models can work with smaller data sources, and the key to their efficacy likely lies in more careful considerations of data augmentation and regularisation. We expect recent bodies of work exploring this for dense transformers <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b59">60]</ref> to be useful here, and that works in data efficient vision transformers <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b64">65]</ref> to also further unlock the potential of V-MoE with less pre-training data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>JFT-300M Precision@1 and ImageNet 5-shot accuracy. Colors represent different ViT variants, markers represent either standard ?ViT or ?V-MoEs on the last n even blocks. The lines represent the Pareto frontier of ViT (dashed) and V-MoE (solid) variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>ImageNet Fine-Tuning Accuracy. Colors represent different VIT variants, markers represent either standard ?ViT or ?V-MoEs on the last n even blocks. Lines show the Pareto frontier of VIT (dashed) and V-MoE (solid).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>White patches are discarded tokens in the first layer of experts, for different capacities, using Batch Prioritized Routing (Section 4.1) with a V-MoE-H/14. See Appendix D for more examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Reducing compute with priority routing. Performance vs. inference FLOPs for large models. V-MoEs with the original vanilla routing are represented by ?, while ? shows V-MoEs where BPR and a mix of C ? {0.6, 0.7, 0.8} and k ? {1, 2} are used to reduce compute. ViT models shown as x. Priority routing works where vanilla fails. Performance vs. inference capacity ratio for a V-MoE-H/14 model with k = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Deeper routing decisions correlate with image classes. We show 4 MoE layers of a V-MoE-H/14. The x-axis corresponds to the 32 experts in a layer. The y-axis are the 1 000 ImageNet classes; orderings for both axes are different across plots. For each pair (expert e, class c) we show the average routing weight for the tokens corresponding to all images with class c for that particular expert e.Figure 29includes all the remaining layers; see Appendix E.2 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Performance on (a) JFT-300M, (b) ImageNet 5-shot and (c) fine-tuning on full ImageNet achieved by different models as a function of the total training time (TPUv3-core-days). Colors represent different VIT variants, markers represent either standard ?ViT or ?V-MoEs on the last n even blocks. The lines represent the Pareto frontier of VIT (dashed) and V-MoE (solid) variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Total training FLOPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Total training runtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Upstream performance of sparse and dense models. The x-axis in (a) shows the total FLOPs required during training, while (b) represents the total training time for identical hardware.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Upstream, few-shot and training FLOPs as a function of k for every-2 V-MoE-S/32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>(a) ImageNet 1-shot (total training FLOPs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>ImageNet 1-shot (total training runtime).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 11 :</head><label>11</label><figDesc>ImageNet/1shot performance of sparse and dense models. The x-axis in (a) shows the total FLOPs required during training, while (b) represents the total training time for identical hardware.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>ImageNet 5-shot (total training FLOPs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>ImageNet 5-shot (total training runtime).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 12 :</head><label>12</label><figDesc>ImageNet/5shot performance of sparse and dense models. The x-axis in (a) shows the total FLOPs required during training, while (b) represents the total training time for identical hardware.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>(a) ImageNet 10-shot (total training FLOPs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>ImageNet 10-shot (total training runtime).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 13 :</head><label>13</label><figDesc>ImageNet/10shot performance of sparse and dense models. The x-axis in (a) shows the total FLOPs required during training, while (b) represents the total training time for identical hardware.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 14 :</head><label>14</label><figDesc>Reducing compute with priority routing. Performance vs. inference FLOPs and runtime for all models. V-MoEs with the original vanilla routing are represented by ?, while ? shows V-MoEs where BPR and a mix of C ? {0.6, 0.7, 0.8} and k ? {1, 2} are used to reduce compute. ViT models shown as x. See</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 15 :</head><label>15</label><figDesc>Inference performance for various every-2 V-MoE models with k = 2 for different capacities. We show Batch Prioritized Routing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 16 :</head><label>16</label><figDesc>Inference performance for every-2 V-MoE-H/14 model with k = 2 for different capacities. We show Batch Prioritized Routing versus vanilla routing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 17 :</head><label>17</label><figDesc>Inference performance for every-2 V-MoE-L/16 model with k = 2 for different capacities. We show Batch Prioritized Routing versus vanilla routing. -B/16 (BPR) V-MoE-B/16 (Vanilla)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 18 :</head><label>18</label><figDesc>Inference performance for every-2 V-MoE-B/16 model with k = 2 for different capacities. We show Batch Prioritized Routing versus vanilla routing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 19 :</head><label>19</label><figDesc>Inference performance for every-2 V-MoE-S/32 model with k = 2 for different capacities. We show Batch Prioritized Routing versus vanilla routing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figures 20 and 21</head><label>21</label><figDesc>show V-MoE-S/32 with k = 1 and k = 2 respectively. We are able to match the dense upstream performance with around 80% of the training FLOPs in both cases. Also, around 85 and 80% of the training FLOPs are enough to match the few-shot evaluation performance in each case. Overall, we can save 20% of the FLOPs while training a small model like V-MoE-S/32. Figures 22 and 23 show V-MoE-B/32 with k = 1 and k = 2 respectively. Again, with at most 80% of the training FLOPs the expert models match the upstream performance of its dense counterpart. Also, we can save around 10% of the training FLOPs while keeping or improving the few-shot representation quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 20 :</head><label>20</label><figDesc>Training with Batch Prioritized Routing. Model: V-MoE-S/32, k = 1. Mean over 4 seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 21 :Figure 22 :Figure 23 :Figure 24 :</head><label>21222324</label><figDesc>Training with Batch Prioritized Routing. Model: V-MoE-S/32, k = 2. Mean over 4 seeds. Training with Batch Prioritized Routing. Model: V-MoE-B/32, k = 1. Mean over 4 seeds. Training with Batch Prioritized Routing. Model: V-MoE-B/32, k = 2. Mean over 4 seeds. Training with Batch Prioritized Routing. Model: V-MoE-L/32, k = 1. Mean over 4 seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>32 0Figure 25 :</head><label>3225</label><figDesc>MoE-L/32 (k=2) Vanilla V-MoE-L/32 (k=2) Dense VIT-L/Training with Batch Prioritized Routing. Model: V-MoE-L/32, k = 2. Mean over 4 seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 26 :</head><label>26</label><figDesc>Fine-tuning with Batch Prioritized Routing. Model: V-MoE-S/32, k = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 27 :</head><label>27</label><figDesc>Replace one layer at a time by a random router for V-MoE-L/16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Figure 28 :</head><label>28</label><figDesc>Replace all layers up to a given one by random routers for V-MoE-L/16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Figure 30 :</head><label>30</label><figDesc>Average weight for selected experts per patch position on a every-2 V-MoE-H/14 fine-tuned model. The x-axis corresponds to the 32 experts in a layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Figure 32 :</head><label>32</label><figDesc>Number of selected experts per image (after pooling selection from all patches). We show the distribution of total number of used experts per layer per image for a V-MoE-H/14 model fine-tuned on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>Figure 33 :</head><label>33</label><figDesc>Original V-MoE-S/32 every-2 model was trained with k = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>Figure 34</head><label>34</label><figDesc>shows the case where the original model is a V-MoE-S/32 with k = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>Figure 34 :Figure 35 :</head><label>3435</label><figDesc>Original V-MoE-S/32 every-2 model was trained with k = 2. Original V-MoE-S/32 every-2 model was trained with k = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head>Figure 36 :Figure 37 :</head><label>3637</label><figDesc>Varying k (number of selected experts) at fine-tuning/inference times for V-MoE-S/32 models pre-trained with different values of k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>Figure 38 :</head><label>38</label><figDesc>Performance of ImageNet-21k pre-trained models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>VTAB.</figDesc><table><row><cell>Scores and 95%</cell></row><row><cell>confidence intervals</cell></row><row><cell>for ViT and V-MoE.</cell></row></table><note>Expert models provide notable gains across all model sizes, for only a mild increase in FLOPs, establishing a new Pareto frontier (gray lines). Alternatively, we can match or improve performance of ViT models at lower cost (e.g. V-MoE-L/16 improves upon ViT-H/14). Similar conclusions hold for training time, which includes communication overhead of dispatching data across devices.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Main V-MoE &amp; VIT models;Table 8shows results for additional models and datasets.</figDesc><table><row><cell>Model</cell><cell cols="8">Params JFT prec@1 IN/1shot IN/5shot IN/10shot IN/Fine-t. ExaFLOPs TPUv3-days</cell></row><row><cell>VIT-H/14</cell><cell>656M</cell><cell>56.68</cell><cell>62.34</cell><cell>76.95</cell><cell>79.02</cell><cell>88.08</cell><cell>4.27k</cell><cell>2.38k</cell></row><row><cell>V-MoE-L/16, Every-2</cell><cell>3.4B</cell><cell>57.65</cell><cell>62.41</cell><cell>77.10</cell><cell>79.01</cell><cell>87.41</cell><cell>2.17k</cell><cell>1.20k</cell></row><row><cell>V-MoE-H/14, Last-5</cell><cell>2.7B</cell><cell>60.12</cell><cell>62.95</cell><cell>78.08</cell><cell>80.10</cell><cell>88.23</cell><cell>4.75k</cell><cell>2.73k</cell></row><row><cell>V-MoE-H/14, Every-2</cell><cell>7.2B</cell><cell>60.62</cell><cell>63.38</cell><cell>78.21</cell><cell>80.33</cell><cell>88.36</cell><cell>5.79k</cell><cell>3.47k</cell></row><row><cell>V-MoE-15B, Every-2</cell><cell>14.7B</cell><cell>-</cell><cell>68.66</cell><cell>82.78</cell><cell>84.29</cell><cell>90.35</cell><cell>33.9k</cell><cell>16.8k</cell></row><row><cell>NFNet-F4+ [8]</cell><cell>527M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.20</cell><cell>-</cell><cell>1.86k</cell></row><row><cell>MPL [49]</cell><cell>480M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.20</cell><cell>-</cell><cell>22.5k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of routing functions.</figDesc><table><row><cell>Model</cell><cell cols="7">Routing Function Proposed in K prec@1 ImageNet/1shot ImageNet/5shot ImageNet/10shot</cell></row><row><cell cols="2">VIT-S/32 TOP-K(softmax)</cell><cell>This work</cell><cell>2</cell><cell>34.15</cell><cell>38.42</cell><cell>53.11</cell><cell>56.06</cell></row><row><cell cols="2">VIT-S/32 softmax(TOP-K)</cell><cell>[54]</cell><cell>2</cell><cell>33.75</cell><cell>35.59</cell><cell>50.21</cell><cell>53.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Simple example (k = 1) where average weights are balanced, but Expert 2 is never selected.</figDesc><table><row><cell>Token Expert 1 Expert 2 Expert 3 Selected Expert</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Finetuning datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Num examples Num classes</cell></row><row><cell>CIFAR10 [37]</cell><cell>50 000</cell><cell>10</cell></row><row><cell>CIFAR100 [37]</cell><cell>50 000</cell><cell>100</cell></row><row><cell>Oxford Flowers 102 [44]</cell><cell>1 020</cell><cell>102</cell></row><row><cell>Oxford-IIT Pet [45]</cell><cell>3 680</cell><cell>37</cell></row><row><cell cols="2">ImageNet (ILSVRC2012 [16]) 1 281 167</cell><cell>1 000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameter values for upstream training on JFT. Weight decay of 0.1 indicates that this value is applied to all model parameters (including biases), while (0.03, 3) indicates that 0.03 is used for the kernels and 3 for the classification head. Additional fine-tuning datasets Alongside finetuning on ImageNet (ILSVRC2012<ref type="bibr" target="#b15">[16]</ref>), we also train on four other datasets shown inTable 5. For the Visual Task Adaptation Benchmark (VTAB<ref type="bibr" target="#b69">[70]</ref>), we finetune on 19 datasets with 1 000 datapoints per class. We refer interested readers to the original work by Zhai et al.<ref type="bibr" target="#b69">[70]</ref> for more details, but in brief, the benchmark consists of 3 task categories: Remote Sensing Image Scene Classification (RESISC -<ref type="bibr" target="#b10">[11]</ref>) These are datasets of images which were captured with specialised (medical, satellite etc) photographic equipment. Kitti (Vehicle distance prediction<ref type="bibr" target="#b23">[24]</ref>) ? dSprites (pixel location &amp; orientation prediction -<ref type="bibr" target="#b41">[42]</ref>) These assess understanding of scene structure in some way, predominately from synthetic environments. Example tasks include 3D depth estimation and counting.</figDesc><table><row><cell>Variant</cell><cell cols="5">JFT-300M Epochs Optimizer Base LR LR decay Weight Decay</cell></row><row><cell>S/32</cell><cell>5</cell><cell>Adam</cell><cell>1 ? 10 ?3</cell><cell>linear</cell><cell>0.1</cell></row><row><cell>B/16,32</cell><cell>7</cell><cell>Adam</cell><cell>8 ? 10 ?4</cell><cell>linear</cell><cell>0.1</cell></row><row><cell>L/32</cell><cell>7</cell><cell>Adam</cell><cell>6 ? 10 ?4</cell><cell>linear</cell><cell>0.1</cell></row><row><cell>L/16</cell><cell>{7,14}</cell><cell>Adam</cell><cell>4 ? 10 ?4</cell><cell>linear</cell><cell>0.1</cell></row><row><cell>H/14</cell><cell>14</cell><cell>Adam</cell><cell>3 ? 10 ?4</cell><cell>linear</cell><cell>0.1</cell></row><row><cell>V-MoE-15B</cell><cell cols="2">-Adafactor</cell><cell>8 ? 10 ?4</cell><cell>rsqrt a</cell><cell>(0.03, 3)</cell></row></table><note>a A linear learning rate cooldown is applied at the end of training.B Transfer Experiment DetailsB.1? Natural tasks CalTech101 [41] ? CIFAR100 [37] ? Street View House Numbers (SVHN -[43]) ? Describable Textures (DTD -[13]) ? Oxford Flowers [44] ? Oxford Pets [45] These tasks contain 'classical' natural real-world images obtained with a camera.? Specialised tasks EuroSAT [28] ? Diabetic Retinopothy [35] PatchCamelyon [62] ?? Structured datasets DeepMind Lab (Object distance prediction -[69]) ? SmallNOrb (Azimuth &amp; Elevation prediction -[38] CLEVR (Counting &amp; Distance prediction [33] ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameter values for fine-tuning on different datasets.</figDesc><table><row><cell>Dataset</cell><cell>Steps</cell><cell cols="2">Base LR Expert Dropout</cell></row><row><cell>ImageNet</cell><cell>10 000</cell><cell>{0.003, 0.01, 0.03, 0.06}</cell><cell>0.1</cell></row><row><cell>CIFAR10</cell><cell cols="2">2 500 {0.001, 0.003, 0.01, 0.03}</cell><cell>0.1</cell></row><row><cell>CIFAR100</cell><cell cols="2">2 500 {0.001, 0.003, 0.01, 0.03}</cell><cell>0.1</cell></row><row><cell>Oxford-IIIT Pets</cell><cell cols="2">250 {0.001, 0.003, 0.01, 0.03}</cell><cell>0.1</cell></row><row><cell>Oxford Flowers-102</cell><cell cols="2">250 {0.001, 0.003, 0.01, 0.03}</cell><cell>0.1</cell></row><row><cell>VTAB (19 tasks)</cell><cell>1 250</cell><cell>0.001</cell><cell>0.1</cell></row><row><cell cols="3">B.3 Model modifications for scaling to V-MoE-15B</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Upstream, few-shot and downstream performance for dense and sparse models. Architectural details and training costs also provided. All V-MoE models have E = 32 experts and were trained with C = 1.05. We specify the number of selected experts per token (k), the number of JFT-300M epochs, the number of Transformer blocks (L), the number of attention heads (H), the patch embedding size (D), the hidden size of the MLP, the total number of parameters, the JFT-300M Precision@1 (%), the ImageNet 1, 5 and 10-shot accuracy (%), the fine-tuning accuracy (%) on ImageNet (INet/Ft.), CIFAR10, CIFAR100, Oxford-IIIT Pets, and Oxford Flowers-102; the total training time on a single core of a TPUv3, and the total training compute (in exaFLOPs).</figDesc><table><row><cell>Name</cell><cell cols="5">k Epochs Blocks Heads Embed. MLP</cell><cell cols="12">Params JFT-300M INet/1s INet/5s INet/10s INet/Ft. CIFAR10 CIFAR100 Pets Flowers TPUv3-days ExaFLOPs</cell></row><row><cell>ViT-S/32</cell><cell>-</cell><cell>5</cell><cell>8</cell><cell>8</cell><cell>512 2048</cell><cell>36.5M</cell><cell>29.05</cell><cell>29.37</cell><cell>43.21</cell><cell>46.38</cell><cell>73.73</cell><cell>97.95</cell><cell cols="2">87.20 91.03</cell><cell>96.78</cell><cell>7.22</cell><cell>12.27</cell></row><row><cell>V-MoE-S/32, Last 2</cell><cell>1</cell><cell>5</cell><cell>8</cell><cell>8</cell><cell>512 2048</cell><cell>166.7M</cell><cell>30.93</cell><cell>30.65</cell><cell>46.06</cell><cell>49.47</cell><cell>76.32</cell><cell>98.05</cell><cell cols="2">87.93 92.62</cell><cell>95.88</cell><cell>10.83</cell><cell>12.50</cell></row><row><cell>V-MoE-S/32, Last 2</cell><cell>2</cell><cell>5</cell><cell>8</cell><cell>8</cell><cell>512 2048</cell><cell>166.7M</cell><cell>33.26</cell><cell>35.49</cell><cell>50.90</cell><cell>54.16</cell><cell>77.10</cell><cell>98.19</cell><cell cols="2">88.86 93.20</cell><cell>96.50</cell><cell>12.40</cell><cell>14.40</cell></row><row><cell cols="2">V-MoE-S/32, Every 2 2</cell><cell>5</cell><cell>8</cell><cell>8</cell><cell>512 2048</cell><cell>296.9M</cell><cell>34.00</cell><cell>37.53</cell><cell>51.75</cell><cell>54.97</cell><cell>77.08</cell><cell>98.23</cell><cell cols="2">88.50 94.02</cell><cell>97.86</cell><cell>17.60</cell><cell>16.53</cell></row><row><cell>V-MoE-S/32, Last 2</cell><cell>5</cell><cell>5</cell><cell>8</cell><cell>8</cell><cell>512 2048</cell><cell>166.7M</cell><cell>35.49</cell><cell>38.77</cell><cell>53.60</cell><cell>56.94</cell><cell>77.59</cell><cell>98.25</cell><cell cols="2">89.25 93.26</cell><cell>97.31</cell><cell>18.49</cell><cell>20.44</cell></row><row><cell>ViT-B/32</cell><cell>-</cell><cell>7</cell><cell>12</cell><cell>12</cell><cell>768 3072</cell><cell>102.1M</cell><cell>39.31</cell><cell>40.58</cell><cell>56.37</cell><cell>59.63</cell><cell>80.73</cell><cell>98.61</cell><cell cols="2">90.49 93.40</cell><cell>99.27</cell><cell>27.62</cell><cell>56.08</cell></row><row><cell>V-MoE-B/32, Last 2</cell><cell>1</cell><cell>7</cell><cell>12</cell><cell>12</cell><cell>768 3072</cell><cell>395.0M</cell><cell>41.41</cell><cell>44.49</cell><cell>60.14</cell><cell>63.63</cell><cell>81.70</cell><cell>98.88</cell><cell cols="2">91.28 94.85</cell><cell>99.21</cell><cell>30.59</cell><cell>56.41</cell></row><row><cell>V-MoE-B/32, Last 2</cell><cell>2</cell><cell>7</cell><cell>12</cell><cell>12</cell><cell>768 3072</cell><cell>395.0M</cell><cell>43.17</cell><cell>48.04</cell><cell>62.45</cell><cell>65.72</cell><cell>82.60</cell><cell>98.67</cell><cell cols="2">91.47 95.25</cell><cell>99.21</cell><cell>36.80</cell><cell>62.75</cell></row><row><cell cols="2">V-MoE-B/32, Every 2 2</cell><cell>7</cell><cell>12</cell><cell>12</cell><cell>768 3072</cell><cell>980.6M</cell><cell>43.37</cell><cell>47.57</cell><cell>62.88</cell><cell>65.94</cell><cell>82.21</cell><cell>98.89</cell><cell cols="2">91.73 95.39</cell><cell>99.60</cell><cell>54.88</cell><cell>76.09</cell></row><row><cell>V-MoE-B/32, Last 2</cell><cell>5</cell><cell>7</cell><cell>12</cell><cell>12</cell><cell>768 3072</cell><cell>395.0M</cell><cell>43.94</cell><cell>49.07</cell><cell>63.33</cell><cell>66.68</cell><cell>82.72</cell><cell>98.87</cell><cell cols="2">91.46 95.07</cell><cell>99.24</cell><cell>49.11</cell><cell>81.75</cell></row><row><cell>ViT-L/32</cell><cell>-</cell><cell>7</cell><cell>24</cell><cell>16</cell><cell>1024 4096</cell><cell>325.3M</cell><cell>46.98</cell><cell>50.95</cell><cell>66.64</cell><cell>69.77</cell><cell>84.37</cell><cell>99.19</cell><cell cols="2">92.52 95.83</cell><cell>99.45</cell><cell>97.30</cell><cell>196.13</cell></row><row><cell>V-MoE-L/32, Last 2</cell><cell>2</cell><cell>7</cell><cell>24</cell><cell>16</cell><cell>1024 4096</cell><cell>845.8M</cell><cell>49.68</cell><cell>54.52</cell><cell>69.90</cell><cell>72.80</cell><cell>85.04</cell><cell>99.24</cell><cell cols="2">92.50 96.34</cell><cell>99.08</cell><cell>110.65</cell><cell>207.94</cell></row><row><cell>ViT-B/16</cell><cell>-</cell><cell>7</cell><cell>12</cell><cell>12</cell><cell>768 3072</cell><cell>100.5M</cell><cell>44.58</cell><cell>48.21</cell><cell>63.50</cell><cell>66.94</cell><cell>84.15</cell><cell>99.00</cell><cell cols="2">91.87 95.80</cell><cell>99.56</cell><cell>95.04</cell><cell>224.45</cell></row><row><cell>V-MoE-B/16, Last 2</cell><cell>1</cell><cell>7</cell><cell>12</cell><cell>12</cell><cell>768 3072</cell><cell>393.3M</cell><cell>47.21</cell><cell>51.98</cell><cell>67.94</cell><cell>70.93</cell><cell>84.71</cell><cell>99.09</cell><cell cols="2">92.37 96.40</cell><cell>99.57</cell><cell>106.95</cell><cell>225.78</cell></row><row><cell>V-MoE-B/16, Last 2</cell><cell>2</cell><cell>7</cell><cell>12</cell><cell>12</cell><cell>768 3072</cell><cell>393.3M</cell><cell>48.31</cell><cell>54.92</cell><cell>68.84</cell><cell>71.81</cell><cell>85.39</cell><cell>99.21</cell><cell cols="2">92.78 96.56</cell><cell>99.63</cell><cell>130.86</cell><cell>250.70</cell></row><row><cell cols="2">V-MoE-L/32, Every 2 2</cell><cell>7</cell><cell>24</cell><cell>16</cell><cell cols="2">1024 4096 3448.2M</cell><cell>49.31</cell><cell>53.61</cell><cell>69.21</cell><cell>72.02</cell><cell>84.81</cell><cell>99.18</cell><cell cols="2">93.02 96.32</cell><cell>99.33</cell><cell>165.51</cell><cell>267.10</cell></row><row><cell cols="2">V-MoE-B/16, Every 2 2</cell><cell>7</cell><cell>12</cell><cell>12</cell><cell>768 3072</cell><cell>979.0M</cell><cell>49.31</cell><cell>55.45</cell><cell>69.60</cell><cell>72.50</cell><cell>85.26</cell><cell>99.16</cell><cell cols="2">92.76 96.74</cell><cell>99.20</cell><cell>201.40</cell><cell>303.24</cell></row><row><cell>ViT-L/16</cell><cell>-</cell><cell>14</cell><cell>24</cell><cell>16</cell><cell>1024 4096</cell><cell>323.1M</cell><cell>53.40</cell><cell>60.25</cell><cell>74.36</cell><cell>76.62</cell><cell>87.12</cell><cell>99.33</cell><cell cols="2">93.93 97.12</cell><cell>99.63</cell><cell>651.26</cell><cell>1572.92</cell></row><row><cell>V-MoE-L/16, Last 2</cell><cell>1</cell><cell>14</cell><cell>24</cell><cell>16</cell><cell>1024 4096</cell><cell>843.6M</cell><cell>55.80</cell><cell>60.53</cell><cell>75.81</cell><cell>78.00</cell><cell>87.47</cell><cell>99.39</cell><cell cols="2">94.39 97.09</cell><cell>99.39</cell><cell>698.14</cell><cell>1577.40</cell></row><row><cell>V-MoE-L/16, Last 2</cell><cell>2</cell><cell>14</cell><cell>24</cell><cell>16</cell><cell>1024 4096</cell><cell>843.6M</cell><cell>56.76</cell><cell>61.46</cell><cell>76.53</cell><cell>78.64</cell><cell>87.54</cell><cell>99.29</cell><cell cols="2">94.19 97.37</cell><cell>99.58</cell><cell>761.27</cell><cell>1666.10</cell></row><row><cell cols="2">V-MoE-L/16, Every 2 2</cell><cell>14</cell><cell>24</cell><cell>16</cell><cell cols="2">1024 4096 3446.0M</cell><cell>57.65</cell><cell>62.41</cell><cell>77.10</cell><cell>79.01</cell><cell>87.41</cell><cell>99.48</cell><cell cols="2">94.64 97.55</cell><cell>99.38</cell><cell>1205.99</cell><cell>2177.14</cell></row><row><cell>ViT-H/14</cell><cell>-</cell><cell>14</cell><cell>32</cell><cell>16</cell><cell>1280 5120</cell><cell>655.8M</cell><cell>56.68</cell><cell>62.34</cell><cell>76.95</cell><cell>79.02</cell><cell>88.08</cell><cell>99.50</cell><cell cols="2">94.71 97.11</cell><cell>99.71</cell><cell>2387.99</cell><cell>4276.42</cell></row><row><cell>V-MoE-H/14, Last 5</cell><cell>2</cell><cell>14</cell><cell>32</cell><cell>16</cell><cell cols="2">1280 5120 2688.6M</cell><cell>60.12</cell><cell>62.95</cell><cell>78.08</cell><cell>80.10</cell><cell>88.23</cell><cell>99.53</cell><cell cols="2">94.86 97.17</cell><cell>99.67</cell><cell>2735.70</cell><cell>4750.73</cell></row><row><cell cols="2">V-MoE-H/14, Every 2 2</cell><cell>14</cell><cell>32</cell><cell>16</cell><cell cols="2">1280 5120 7160.8M</cell><cell>60.62</cell><cell>63.38</cell><cell>78.21</cell><cell>80.33</cell><cell>88.36</cell><cell>99.58</cell><cell cols="2">94.91 97.45</cell><cell>99.68</cell><cell>3477.18</cell><cell>5795.35</cell></row><row><cell>V-MoE-15B</cell><cell>2</cell><cell>-</cell><cell>48</cell><cell>16</cell><cell cols="2">1408 6400 14705.1M</cell><cell>-</cell><cell>68.66</cell><cell>82.78</cell><cell>84.29</cell><cell>90.35</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>16775.50</cell><cell>33943.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Effect of ImageNet deduplication on the training data for fewshot with V-MoE-S/32.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Time and FLOPs unmatched inference results for JFT prec@1 and ImageNet 5shot.</figDesc><table><row><cell>Model</cell><cell cols="6">Experts Routing JFT prec@1 INet/5shot Time[%] FLOPs[%]</cell></row><row><cell>VIT-H/14</cell><cell>-</cell><cell>-</cell><cell>56.68</cell><cell>76.95</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>VIT-L/16</cell><cell>-</cell><cell>-</cell><cell>53.40</cell><cell>74.36</cell><cell>27.58</cell><cell>36.83</cell></row><row><cell>V-MoE-L/16</cell><cell>Last-2</cell><cell>Vanilla</cell><cell>56.76</cell><cell>76.53</cell><cell>32.56</cell><cell>39.02</cell></row><row><cell cols="3">V-MoE-L/16 Every-2 Vanilla</cell><cell>57.64</cell><cell>77.10</cell><cell>57.40</cell><cell>49.95</cell></row><row><cell>V-MoE-H/14</cell><cell>Last-5</cell><cell>Vanilla</cell><cell>60.12</cell><cell>78.08</cell><cell>120.22</cell><cell>111.12</cell></row><row><cell cols="3">V-MoE-H/14 Every-2 Vanilla</cell><cell>60.62</cell><cell>78.21</cell><cell>164.89</cell><cell>135.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>FLOPs matched inference results with Batch Prioritized Routing, lower C, and reduced k.</figDesc><table><row><cell>Model</cell><cell cols="2">Experts At Inference</cell><cell>C</cell><cell cols="4">JFT prec@1 INet/5shot Time[%] FLOPs[%]</cell></row><row><cell>VIT-H/14</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.68</cell><cell>76.95</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>V-MoE-H/14</cell><cell>Last-5</cell><cell cols="2">k=2 ? k=1 1.05</cell><cell>58.60</cell><cell>77.87</cell><cell>111.57</cell><cell>100.26</cell></row><row><cell>V-MoE-H/14</cell><cell>Last-5</cell><cell cols="2">k=2 ? k=1 1.25</cell><cell>59.21</cell><cell>77.59</cell><cell>113.67</cell><cell>102.53</cell></row><row><cell>V-MoE-H/14</cell><cell>Last-5</cell><cell>k=2</cell><cell>0.5</cell><cell>58.61</cell><cell>77.92</cell><cell>118.14</cell><cell>100.02</cell></row><row><cell>V-MoE-H/14</cell><cell>Last-5</cell><cell>k=2</cell><cell>0.6</cell><cell>59.42</cell><cell>78.05</cell><cell>121.68</cell><cell>102.30</cell></row><row><cell cols="4">V-MoE-H/14 Every-2 k=2 ? k=1 1.05</cell><cell>59.46</cell><cell>77.82</cell><cell>134.87</cell><cell>100.07</cell></row><row><cell cols="2">V-MoE-H/14 Every-2</cell><cell>k=2</cell><cell>0.5</cell><cell>59.44</cell><cell>77.70</cell><cell>155.83</cell><cell>100.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Average weight for selected experts per class. We show the 16 MoE layers of an every-2 V-MoE-H/14. The x-axis corresponds to the 32 experts in a layer. The y-axis are the 1000 ImageNet classes; orderings for both axes are different across plots. For each pair (expert e, class i) we show the average routing weight for the patches corresponding to all images with class i for that particular expert e.</figDesc><table><row><cell>1 200 400 600 800 1000 image class 1 200 400 600 800 1000 image class 1 200 400 600 800 1000 image class 1 200 400 600 800 1000 image class 150 300 450 600 730 patch id 1 150 300 450 600 730 patch id 1 150 300 450 600 patch id 1 5 10 15 20 25 MoE Layer 1 MoE Layer 9 MoE Layer 17 1 5 10 15 20 25 expert id MoE Layer 25 MoE Layer 1 MoE Layer 9 MoE Layer 17 expert id 150 300 450 600 730 patch id Figure 29: 1 730 1 MoE Layer 25</cell><cell>32 32</cell><cell>MoE Layer 3 MoE Layer 11 MoE Layer 19 1 5 10 15 20 25 expert id MoE Layer 27 MoE Layer 3 MoE Layer 11 MoE Layer 19 1 5 10 15 20 25 expert id MoE Layer 27</cell><cell>32 32</cell><cell>MoE Layer 5 MoE Layer 13 MoE Layer 21 1 5 10 15 20 25 expert id MoE Layer 29 MoE Layer 5 MoE Layer 13 MoE Layer 21 1 5 10 15 20 25 expert id MoE Layer 29</cell><cell>32 32</cell><cell>MoE Layer 7 MoE Layer 15 MoE Layer 23 1 5 10 15 20 25 expert id MoE Layer 31 MoE Layer 7 MoE Layer 15 MoE Layer 23 1 5 10 15 20 25 expert id MoE Layer 31</cell><cell>32 32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>The y-axis are the 730 patches in ImageNet images with 14x14 patch size, at (384, 384, 3) resolution; orderings for the x-axis are different across plots. For each pair (expert e, patch-id i) we show the average routing weight for all the patches with patch-id i that were assigned to that particular expert e.</figDesc><table><row><cell></cell><cell></cell><cell>MoE Layer 1</cell><cell></cell><cell></cell><cell></cell><cell>MoE Layer 3</cell><cell></cell><cell></cell><cell></cell><cell>MoE Layer 5</cell><cell></cell><cell></cell><cell></cell><cell>MoE Layer 7</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell></row><row><cell>density</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.00</cell><cell>0.25</cell><cell>0.50 MoE Layer 9</cell><cell>0.75</cell><cell>1.00</cell><cell>0.00</cell><cell>0.25 MoE Layer 11 0.50</cell><cell>0.75</cell><cell>1.00</cell><cell>0.00</cell><cell>0.25 MoE Layer 13 0.50</cell><cell>0.75</cell><cell>1.00</cell><cell>0.00</cell><cell>0.25 MoE Layer 15 0.50</cell><cell>0.75</cell><cell>1.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell></row><row><cell>density</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">0.00 0.25 0.50 0.75 1.00 MoE Layer 17</cell><cell cols="4">0.00 0.25 0.50 0.75 1.00 MoE Layer 19</cell><cell cols="4">0.00 0.25 0.50 0.75 1.00 MoE Layer 21</cell><cell cols="4">0.00 0.25 0.50 0.75 1.00 MoE Layer 23</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell></row><row><cell>density</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">0.00 0.25 0.50 0.75 1.00 MoE Layer 25</cell><cell cols="4">0.00 0.25 0.50 0.75 1.00 MoE Layer 27</cell><cell cols="4">0.00 0.25 0.50 0.75 1.00 MoE Layer 29</cell><cell cols="4">0.00 0.25 0.50 0.75 1.00 MoE Layer 31</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell><cell></cell><cell></cell><cell cols="2">TOP-1 TOP-2</cell></row><row><cell>density</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">0.00 0.25 0.50 0.75 1.00 routing weight</cell><cell cols="4">0.00 0.25 0.50 0.75 1.00 routing weight</cell><cell cols="4">0.00 0.25 0.50 0.75 1.00 routing weight</cell><cell cols="4">0.00 0.25 0.50 0.75 1.00 routing weight</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>. 9 % 7 6 . 2 % 7 6 . 6 % 7 6 . 5 % 7 6 . 9 % 7 6 . 9 % 7 7 . 1 % 7 7 . 1 % 7 7 . 1 %7 4 . 6 % 7 7 . 2 % 7 8 . 1 % 7 8 . 2 % 7 8 . 0 % 7 8 . 3 % 7 8 . 0 % 7 8 . 4 % 7 8 . 0 %</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Finetuned ImageNet accuracy</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 3 4 5 6 7 Upstream # selected</cell><cell cols="3">7 4 . 7 % 7 7 . 7 % 7 8 . 4 % 7 8 . 8 % 7 8 . 9 % 7 9 . 1 % 7 9 . 2 % 7 9 . 2 % 7 9 . 1 % 7 4 . 6 % 7 7 . 3 % 7 8 . 2 % 7 8 . 8 % 7 8 . 9 % 7 8 . 9 % 7 9 . 0 % 7 9 . 1 % 7 9 . 2 % 7 4 . 7 % 7 7 . 5 % 7 8 . 5 % 7 9 . 0 % 7 9 . 3 % 7 9 . 4 % 7 9 . 2 % 7 9 . 5 % 7 9 . 6 % 7 4 . 5 % 7 7 . 4 % 7 7 . 8 % 7 8 . 3 % 7 8 . 3 % 7 8 . 6 % 7 8 . 5 % 7 8 . 4 % 7 8 . 5 % 7 4 7 4 . 9 % 7 7 . 5 % 7 8 . 2 % 7 8 . 5 % 7 8 . 6 % 7 8 . 1 % 7 8 . 7 % 7 8 . 3 % 7 8 . 7 %</cell><cell>76.0% 77.0% 78.0% 79.0%</cell></row><row><cell>8</cell><cell>7 4 . 3 %</cell><cell cols="2">7 7 . 3 % 7 8 . 1 % 7 8 . 7 % 7 9 . 1 % 7 9 . 2 % 7 9 . 2 % 7 9 . 3 % 7 9 . 5 %</cell><cell>75.0%</cell></row><row><cell>9</cell><cell>7 4 . 2 %</cell><cell>7 7 . 3 %</cell><cell>7 8 . 2 % 7 8 . 9 % 7 9 . 2 % 7 9 . 0 % 7 8 . 8 % 7 9 . 5 % 7 9 . 3 %</cell></row><row><cell></cell><cell cols="3">1 2 3 4 5 6 7 8 9 Downstream # selected</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A token may however successfully assign all its TOP-k choices while another may not allocate a single one. This can happen for instance if the latter selects very popular experts that run out of capacity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank Alex Kolesnikov, Lucas Beyer and Xiaohua Zhai for providing continuous help and details about scaling ViT models; Alexey Dosovitskiy, who provided some of the pre-trained ViT models; Ilya Tolstikhin, who suggested placing experts only in the last layers; Josip Djolonga for his early review of the manuscript; Dmitry Lepikhin for providing details about the original GShard implementation; Barret Zoph and Liam Fedus for insightful comments and feedback; James Bradbury, Blake Hechtman and the rest of JAX and TPU team who helped us running our models efficiently, and many others from Google Brain for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biased mixtures of experts: Enabling computer vision inference under data transfer limitations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andreopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7656" to="7667" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Network of experts for large-scale image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<title level="m">ViViT: A video vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06297</idno>
		<title level="m">Conditional computation in neural networks for faster models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning of representations: Looking forward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Statistical Language and Speech Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<title level="m">High-performance large-scale image recognition without normalization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved learning algorithms for mixture of experts in multiclass classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1229" to="1252" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.7362</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Low-rank approximations for conditional feedforward computation in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4461</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0510</idno>
		<title level="m">Deep sequential neural network</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning factored representations in a deep mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4314</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-cue pedestrian detection and tracking from a moving vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Munder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="59" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hard mixtures of experts for large scale weakly supervised vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining predictors: comparison of five meta machine learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="91" to="105" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference, and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2217" to="2226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A patient-adaptable ECG beat classifier using a mixture of experts approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palreddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Tompkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="891" to="900" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Token labeling: Training a 85</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10858</idno>
	</analytic>
	<monogr>
		<title level="m">5% top-1 accuracy vision transformer with 56m parameters on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Kaggle diabetic retinopathy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyepacs</forename><surname>Kaggle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Big transfer (BiT): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GShard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Base layers: Simplifying training of large, sparse models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16716</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. Computer Vision and Pattern Recognition Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<title level="m">dSprites: Disentanglement testing sprites dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conf. on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-M</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Using mixture of expert models to gain insights into semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pavlitskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hubschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schlicht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10580</idno>
		<title level="m">Meta pseudo labels</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Diversity and depth in per-example routing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Routing networks and the challenges of modular and compositional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klinger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12774</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Routing networks: Adaptive selection of non-linear functions for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01239</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human motion from Bayesian mixtures of experts. A probabilistic discriminative approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci., Univ. Toronto, Tech. Rep. CSRG</title>
		<imprint>
			<biblScope unit="volume">502</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02243</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to perceive the world as articulated: an approach for hierarchical learning in sensory-motor systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nolfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="1131" to="1141" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rotation equivariant CNNs for digital pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Linmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep mixture of experts via shallow embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04971</idno>
		<title level="m">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Twenty years of mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Yuksel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Gader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1177" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Time series prediction using mixtures of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Zeevi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Adler</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
