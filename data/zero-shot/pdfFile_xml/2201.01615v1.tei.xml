<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lawin Transformer: Improving Semantic Segmentation Transformer with Multi-Scale Representations via Large Window Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Yan</surname></persName>
							<email>yanhaotian@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Intelligent System Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhang</surname></persName>
							<email>zhangchuang@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Intelligent System Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
							<email>wuming@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Intelligent System Lab</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lawin Transformer: Improving Semantic Segmentation Transformer with Multi-Scale Representations via Large Window Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-scale representations are crucial for semantic segmentation. The community has witnessed the flourish of semantic segmentation convolutional neural networks (CNN) exploiting multi-scale contextual information. Motivated by that the vision transformer (ViT) is powerful in image classification, some semantic segmentation ViTs are recently proposed, most of them attaining impressive results but at a cost of computational economy. In this paper, we succeed in introducing multi-scale representations into semantic segmentation ViT via window attention mechanism and further improves the performance and efficiency. To this end, we introduce large window attention which allows the local window to query a larger area of context window at only a little computation overhead. By regulating the ratio of the context area to the query area, we enable the large window attention to capture the contextual information at multiple scales. Moreover, the framework of spatial pyramid pooling is adopted to collaborate with the large window attention, which presents a novel decoder named large window attention spatial pyramid pooling (LawinASPP) for semantic segmentation ViT. Our resulting ViT, Lawin Transformer, is composed of an efficient hierachical vision transformer (HVT) as encoder and a LawinASPP as decoder. The empirical results demonstrate that Lawin Transformer offers an improved efficiency compared to the existing method. Lawin Transformer further sets new state-of-the-art performance on Cityscapes (84.4% mIoU), ADE20K (56.2% mIoU) and COCO-Stuff datasets. The code will be released at https://github.com/yan-hao-tian/lawin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is one of the most significant dense prediction tasks in computer vision. With the prosperity of deep convolutional neural network (CNN) in this field, the CNN-based semantic segmentation pipeline gains more and more popularity in a wide range of practical applications such as self-driving cars, medical imaging analysis and remote sensing imagery interpretation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Having scrutinized the famed semantic segmentation CNN, we note that a series of work largely focus on exploiting multi-scale representations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>, which plays a vital role of understanding the context prior at multiple scales. To incorporate the rich contextual information, most of these works apply filters or pooling operations, such as atrous convolution <ref type="bibr" target="#b46">[47]</ref> and adaptive pooling, to the spatial pyramid pooling (SPP) module <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Since the impressive performance of Vision Transformer (ViT) on image classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref>, there are some efforts to resolve semantic segmentation with pure transformer models, still outperforming the previous semantic segmentation CNN by a large margin <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b51">52]</ref>. However, it takes a very high computation cost to implement these semantic segmentation ViTs, especially when the input image is large. In order to tackle this issue, the method purely based on the hierarchical vision transformer (HVT) has emerged with saving much computational budget. Swin Transformer is one of the most representative HVTs achieving state-of-the-art results on many vision tasks <ref type="bibr" target="#b29">[30]</ref>, whilst it employs a heavy decoder <ref type="bibr" target="#b41">[42]</ref> to classify pixels. Seg-Former refines the design of both encoder and decoder, resulting a very efficient semantic segmentation ViT <ref type="bibr" target="#b42">[43]</ref>. But it is problematic that SegFormer solely relies on increasing the model capacity of encoder to progressively improve performance, which has a potentially lower efficiency ceiling.</p><p>Through the above analysis, we think one major problem for current semantic segmentation ViT is lack of multi-scale contextual information, thus impairing the performance and efficiency. To overcome the limitation, we present a novel window attention mechanism named large window attention. In large window attention, the uniformly split patch queries the context patch covering a much larger region as illustrated in <ref type="figure" target="#fig_0">Fig 1,</ref> whereas the patch in local window attention merely queries itself. On the other hand, considering that the attention would become computationally prohibitive with the enlargement of context patch, we devise a simple yet effective strategy to alleviate the dilemma of large context. Specifically, we first pool the large context patch to spatial dimension of the corresponding query patch in order to preserve the original computational complexity. Then we enable the multi-head mechanism in large window attention and set the number of head strictly equal to the square of the downsampling ratio R while pooling the context, mainly for recovering the discarded dependencies between query and context. Finally, inspired by token-mixing MLP in MLP-Mixer <ref type="bibr" target="#b36">[37]</ref>, we apply R 2 position-mixing operations on the R 2 subspaces of head respectively, strengthening the spatially representational power of multi-head attention. Therefore, the patch in our proposed large window attention can capture contextual information at any scales, merely yielding a little computational overhead caused by position-mixing operations. Coupled with large window attention with different ratios R, a SPP module evolves into a large window attention spatial pyramid pooling (Law-inASPP), which one can employ like ASPP (Atrous Spatial Pyramid Pooling) <ref type="bibr" target="#b8">[9]</ref> and PPM (Pyramid Pooling Module) <ref type="bibr" target="#b49">[50]</ref> to exploit multi-scale representations for semantic segmentation.</p><p>We extend the efficient HVT to Lawin Transformer by placing the LawinASPP at the top of HVT, which introduces the multi-scale representations into the semantic segmentation ViT. The performance and efficiency of Lawin Transformer is evaluated on Cityscapes <ref type="bibr" target="#b16">[17]</ref>, ADE20K <ref type="bibr" target="#b52">[53]</ref> and COCO-Stuff <ref type="bibr" target="#b3">[4]</ref> datasets. We conduct extensive experiments to compare Lawin Transformer with existing HVTbased semantic segmentation method <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43</ref>]. An improved efficiency of Lawin Transformer is proved by that Lawin Transformer spends less computational resource in attaining the better performance. Besides, our experiments show that Lawin Transformer outperforms other state-ofthe-art methods consistently on these benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Segmentation</head><p>Semantic segmentation models based on fully convolutional neural network (FCN) <ref type="bibr" target="#b30">[31]</ref> are the most promising ways to accomplish the pixel-level classification. Towards precise scene understanding, consecutive improvements have been developed for semantic segmentation CNN in many aspects. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> mitigates the boundary information shortage of high-level feature. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref> are proposed to enlarge the receptive field of model. The spatial pyramid pooling (SPP) module has proved to be effective in exploiting multi-scale representations, which gathers scene clues from local context to global context <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b49">50]</ref>. An alternative line of work utilizes the variants of selfattention mechanism to model dependencies among representations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Vision Transformer</head><p>Transformer has revolutionized neural language processing and proven extremely successful in computer vision. ViT <ref type="bibr" target="#b18">[19]</ref> is the first end-to-end Vision Transformer for image classification by projecting the input image into a token sequence and attach it to a class token. DeiT <ref type="bibr" target="#b37">[38]</ref> improves the data efficiency of training ViT with a token distillation pipeline. Apart from the sequence-to-sequence structure, the efficiency of PVT <ref type="bibr" target="#b38">[39]</ref> and Swin Transformer <ref type="bibr" target="#b29">[30]</ref> sparks much interests in exploring the Hierarchical Vision Transformer (HVT) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref>. ViT is also extended to solve the low-level tasks and dense prediction problems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>. Specially, concurrent semantic segmentation methods driven by ViT presents impressive performance. SETR <ref type="bibr" target="#b51">[52]</ref> deploys the ViT as an encoder and upsamples the output patch embedding to classify pixels. Swin Transformer extends itself to a semantic segmentation ViT by connecting a UperNet <ref type="bibr" target="#b41">[42]</ref>. Segmenter <ref type="bibr" target="#b35">[36]</ref> depends on the ViT/DeiT as backbone and propose a mask Transformer decoder. Segformer <ref type="bibr" target="#b42">[43]</ref> shows a simple, efficient yet powerful design of encoder and decoder for semantic segmentation. MaskFormer <ref type="bibr" target="#b10">[11]</ref> reformulates the semantic segmentation as a mask classification problem, having much fewer FLOPs and parameters compared to Swin-UperNet. In this paper, we take a new step towards a more efficient design of semantic segmentation ViT, by introducing multi-scale representations into the HVT. <ref type="bibr" target="#b36">[37]</ref> is a novel neural network much simpler than ViT. Similar to ViT, MLP-Mixer first adopts a linear projection to obtain a token sequence like ViT. The sharp dinstinction is that MLP-Mixer is entirely based on multi-layer perceptrons (MLP), because it replaces the selfattention in transformer layer with the token-mixing MLP. Token-Mixing MLP acts along the channel dimension, mixing the token (position) to learn spatial representations. In our proposed large window attention, token-mixing MLP is applied to the pooled context patch, which we call positionmixing to boost the spatial representations of multi-head attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">MLP-Mixer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP-Mixer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this part, we first briefly introduce multi-head attention and token-mixing MLP. Then we elaborate large window attention and describe the architecture of LawinASPP. Finally, the overall structure of Lawin Transformer is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>Multi-head attention is the core of Transformer layer. In the Hierarchical Vision Transformer (HVT), the operation of multi-head attention is limited to local uniformly split window, which is called local window attention. Assuming the input is a 2D feature map denoted as x 2d ? R C ?H ?W , we can formulate the action of window attention as:</p><formula xml:id="formula_0">x 2d = Reshape h, HW P 2 , C h , P , P (x 2d ) ,<label>(1)</label></formula><formula xml:id="formula_1">x 2d = Reshape (C , H , W ) (MHA (x 2d )) + x 2d ,<label>(2)</label></formula><p>where h is the head number and P is the spatial size of windows, and MHA () is the Multi-Head Attention (MHA) mechanism. The basic operation of MHA can be described as:</p><formula xml:id="formula_2">A = softmax (W q x 2d )(W k x 2d ) T ? D h (W v x 2d ) , (3) MHA = concat [A 1 ; A 2 ; ...; A h ] W mha ,<label>(4)</label></formula><p>where W q , W k and W v ? R C ?D h are the learned linear transformations and W mhsa ? R D?C is the learned weights that aggregates multiple attention values. D h is typically set to D/h and D is the embedding dimension. Token-mixing MLP is the core of MLP-Mixer which can aggregate spatial information, by allowing the spatial position to communicate each other. Given the input 2D feature map x 2d ? R C ?H ?W , the operation of token-mixing MLP can be formulated as:</p><formula xml:id="formula_3">x 2d = Reshape (C , HW ) (x 2d ) , (5) x 2d = Reshape (C , H , W ) (MLP (x 2d )) + x 2d , (6) MLP (x 2d ) = W 2 ? (W 1 x 2d ) ,<label>(7)</label></formula><p>where W 1 ? R HW ?D mlp and W 2 ? R D mlp ?HW are both learned linear transformations, and ? is the activation function providing non-linearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Large Window Attention</head><p>Similar with window attention mentioned in section 3.1, large window attention splits the entire feature map uniformly into several patches. Conversely, when large window attention sliding over the image, the current patch is allowed to query a larger area. For simplicity, we denote the query patch as Q ? R P 2 ?C and the queried large context patch as C ? R R 2 ?P 2 ?C , where R is the ratio of the context patch size to the query patch size, and P 2 is the area of patch. Because the computational complexity of attention is  <ref type="figure">Figure 3</ref>. The overall structure of Lawin Transformer. The image is fed into the encoder part,which is a MiT. Then the features from the last three stages are aggregated and fed into the decoder part, which is a LawinASPP. Finally the resulted feature is enhanced with low-level information by the first-stage feature of encoder. "MLP" denotes the multi-layer perceptron. "CAT" denotes concatnating the features. "Lawin" denotes large window attention. "R" denotes the ratio of the size of context patch to query patch.</p><p>O P 2 , when the spatial size of C is increased by R times, the computational complexity increases to O R 2 P 2 . Under this circumstance, the computation of attention is not limited to the P ? P local patch, and even unaffordable if ratio R or input resolution is very large. To preserve the original computational complexity, we pool C to an abstract tensor with a downsampling ratio of R, reducing the spatial size of context patch back to (P, P ). However, there are certain drawbacks associated with such an easy process. The downsampling of context patch inevitably discards the abundant dependencies between Q and C especially as R is large. To mitigate the inattention, we naturally adopt the multi-head mechanism and let the number of head strictly equal to R 2 , thereby formulating the attention matrix from P 2 , P 2 to R 2 , P 2 , P 2 . It is notable that the number of head has no impact on the computational complexity.</p><p>There has been researches revealing that, with certain techniques regularizing the head subspace, multi-head attention can learn desired diverse representations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. Considering that the spatial information becomes abstract after downsampling, we intend to strengthen the spatially representational power of multi-head attention. Motivated by that in MLP-Mixer the token-mixing MLP is complementary to channel-mixing MLP for gathering spatial knowledge, we define a set of head-specific position-mixing MLP = {MLP 1 , MLP 2 , ..., MLP h }. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, every head of the pooled context patch is pushed into its corresponding token(position)-mixing MLP, and spatial positions within the same head communicate each other in an identical behavior. We term the resulting context as position-mixed context patch and denote it as C P , which is calculated by:</p><formula xml:id="formula_4">C = Reshape h, C /h, P 2 (? (C)) ,<label>(8)</label></formula><formula xml:id="formula_5">C h = MLP h ? h +? h ,<label>(9)</label></formula><formula xml:id="formula_6">C P = Reshape C, P 2 (concat [C 1 ; C 2 ; ...; C h ]) ,<label>(10)</label></formula><p>where? h denotes the h-th head of? and MLP h ? R P 2 ?P 2 is the h-th transformation strengthening the spatial representations for the h-th head, and ? denotes the average pooling operation. With the position-mixed context C P , we can reformulate the Eq. (3) and Eq. (4) as follows:</p><formula xml:id="formula_7">A = softmax (W q Q h ) W k C P h T ? D h W v C P h , (11) MHA = concat [A 1 ; A 2 ; ...; A h ] W mha .<label>(12)</label></formula><p>One primary concern is on the overhead of MLP, so we list the computational complexity of local window attention and large window attention:</p><formula xml:id="formula_8">? (Lowin) = 4(HW )C 2 + 2(HW )P 2 C ,<label>(13)</label></formula><formula xml:id="formula_9">? (Lawin) = 4(HW )C 2 + 3(HW )P 2 C ,<label>(14)</label></formula><p>where H and W are the height and width of entire image respectively, and P is the size of local window. Since P 2 , usually set to 7 or 8, is much smaller than C in high-level features, the extra expense induced by MLP is reasonably neglectable. It is admirable that the computational complexity of large window attention is independent of the ratio R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">LawinASPP</head><p>To capture multi-scale representations, we adopt the architecture of spatial pyramid pooling (SPP) to collaborate with large window attention and get the novel SPP module called LawinASPP. LawinASPP consists of 5 parallel branches including one shortcut connection, three large window attentions with R = <ref type="figure" target="#fig_1">(2, 4, 8)</ref> and an image pooling branch. As shown in <ref type="figure">Fig. 3</ref>, branches of large window attention provide three hierarchies of receptive fields for the local window. Following the previous literature on window attention mechanism <ref type="bibr" target="#b29">[30]</ref>, we set the patch size of local window to 8, thus the provided receptive fields are of <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">64)</ref>. The image pooling branch uses a global pooling layer to obtain the globally contextual information and push it into a linear transformation followed by a bilinearly upsampling opeartion to match the feature dimension. The short path copies the input feature and paste it when all contextual information is output. All resulting features are first concatenated, and a learned linear transformation performs dimensionality reduction for generating the final segmentation map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Lawin Transformer</head><p>Having investigated the advanced HVTs, we select the MiT and Swin-Transformer as the encoder of Lawin Transformer. MiT is designed for serving as encoder of Seg-Former <ref type="bibr" target="#b42">[43]</ref> which is a simple, efficient yet powerful semantic segmentation ViT. Swin-Transformer <ref type="bibr" target="#b29">[30]</ref> is an extremely successful HVT built upon local window attention. Prior to applying LawinASPP, we concatenate the multilevel features with output stride = (8, 16, 32) by resizing them to the size of feature with output stride = 8 and use a linear layer to transform the concatenation. The resulting transformed feature with output stride = 8 is fed into the LawinASPP and then we obtain the feature with multi-scale contextual information. In the state-of-the-art ViT for semantic segmentation, the feature for final prediction of segmentation logits is always derived from 4-level features of encoder. We hence employ the first-level feature with output stride = 4 to compensate low-level information. The output of LawinASPP is upsampled to the size of a quarter of input image, then fused with the first-level feature by a linear layer. Finally, the segmentation logits are predicted on the low-level-enhanced feature. More details are illustrated in <ref type="figure">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Expriments</head><p>Datasets: We conduct experiments on three public datasets including Cityscapes <ref type="bibr" target="#b16">[17]</ref>, ADE20K <ref type="bibr" target="#b52">[53]</ref> and COCO-Stuff <ref type="bibr" target="#b3">[4]</ref>. Cityscapes is an urban scene parsing dataset containing 5,000 fine-annotated images captured from 50 cities with 19 semantic classes. There are 2,975 images divided into training set, 500 images divided into validation set and 1,525 images divided into testing set. ADE20K is one of the most challenging datasets in semantic segmentation. It consists of a training set of 20,210 images with 150 categories, a testing set of 3,352 images and a validation set of 2,000 images. COCO-Stuff is also a very challenging benchmark consists of 164k images with 172 semantic classes. The training set contains 118k images, and the test-dev dataset contains 20k images and the validation set contains of 5k images Implementation Details: Our experiment protocols are exactly the same as those of <ref type="bibr" target="#b42">[43]</ref>. Specially, we use the publicly available ImageNet1K-pretrained MiT <ref type="bibr" target="#b42">[43]</ref> as the encoder of Lawin Transformer. All experiments in this section are implemented based on MMSegmentation <ref type="bibr" target="#b14">[15]</ref> codebase on a server with 8 Tesla V100. When doing the ablation study, we choose MiT-B3 as encoder and train all models for 80k iterations. Unless specified, all results are achieved by single-scale inference. Note that all results of other methods are obtained by ours training the official code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with SegFormer</head><p>To demonstrate the improved efficiency of Lawin Transformer, we compare it with SegFormer <ref type="bibr" target="#b42">[43]</ref>. Both of them is built upon window attention and takes MiT as encoder. To enable fairness, we reimplement SegFormer in our environment. <ref type="table">Table 1</ref> shows the comparison on parameters, FLOPs and mIoU. Apparently, across all variants of MiT (B0?B5), Lawin Transformer trumps Seg-Former on mIoU and FLOPs at a little extra parameters. When the light-weight MiT-B0 and MiT-B1 serve as encoder, Lawin Transformer can improve the performance with an adorable saving of computation cost. For example, Lawin-B0 uses much less FLOPs (by 3.1) to obtain a gain of 0.5% mIoU on COCO-Stuff dataset, and a gain of 0.8% on ADE20K dataset. Moreover, we observe that in some cases, Lawin Transformer can bridge the performance gap caused by the model capacity of encoder. For instance, SegFormer-B3 performs worse than SegFormer-B4 on all three datasets. But if replacing the original decoder with LawinASPP, the resulted Lawin-B3 outperforms SegFormer-B4 by 0.6% mIoU and yields a computation saving of 34G FLOPs on ADE20K, even using much less parameters. Also, on Cityscapes, Lawin-B4 improves over SegFormer-B5 by 0.4% with nearly a third less computation cost, Lawin-B3 improves over SegFormer-B5 by 0.2% with nearly a half less computation cost and parameters. These empirical results suggest that semantic segmentation ViT could encounter a performance bottleneck as the capacity of encoder continues to increase. In contrast with simply enlarging the encoder, LawinASPP presents a promising and efficient way to overcome the bottleneck by capturing the rich contextual information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with UperNet and MaskFormer</head><p>To further show the efficiency, we replace MiT with Swin-Transformer <ref type="bibr" target="#b29">[30]</ref> and compare the Swin-Lawin Transformer with Swin-UperNet and MaskFormer on ADE20K as shown in table 2. From table 2, we have following observations. Firstly, compared with Swin-UperNet, Swin-Lawin improves the performance largely and saves a great deal of computation cost. In particular, Lawin Transformer with Swin-B can outperform UperNet with Swin-L at nearly a quarter of its computation cost. Secondly, compared with Swin-MaskFormer, Swin-Lawin consistently uses less FLOPs and parameters across all variants of Swin-Transformer. Finally, through a closer look at performance, we find that Swin-Lawin performs worse than MaskFormer when the capacity of encoder is small (Swin-T?S). However, as the capacity of encoder increases (Swin-B?L), Swin-Lawin outperforms MaskFormer. It can be seen that with the increase of capacity, the performance gain created by Swin-Lawin compared with Swin-Uper also become larger. We infer that the short path branch and the low-level information in Lawin Transformer have very important roles in the final prediction (Sec 4.3.3 discusses the contribution of different hierarchies in Lawin Transformer), which both arise from the multi-level feature of backbone directly. So the more powerful the encoder part, the greater the performance gain from Lawin Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Spatial Pyramid Pooling</head><p>Thanks to the spatial pyramid pooling (SPP) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref> architecture in LawinASPP, Lawin Transformer captures multiscale representations with large window attention in an efficient manner. To study the impact of large window attention and SPP architecture on performance, we choose some representative methods relying on SPP including PPM (Pyramid Pooling Module) <ref type="bibr" target="#b49">[50]</ref>, ASPP (Atrous Spatial Pyramid Pooling) <ref type="bibr" target="#b8">[9]</ref> and SEP-ASPP (Depthwise Separable Atrous Spatial Pyramid Pooling) <ref type="bibr" target="#b9">[10]</ref>. The sharp distinction between LawinASPP with these alternatives is the basic pool-  <ref type="table">Table 5</ref>. Results of Lawin-B3 on ADE20k with context patch of different spatial sizes. "Size" means the spatial size of pooled context.</p><p>ing operator. PPM uses pyramid adaptive pooling to capture contextual information at different scales. ASPP uses the atrous convolution to extract multi-scale features. SEP-ASPP uses the depthwise separable atrous convolution <ref type="bibr" target="#b12">[13]</ref> in the place of atrous convolution for the sake of efficiency. <ref type="table">Table 3</ref> shows parameters, FLOPs and mIoU when MiT-B3 combined with different SPP-based module, which are tested on ADE20K. PPM and SEP-ASPP are impressively computational economical, even using less FLOPs than LawinASPP. However there is a considerable performance gap between them and LawinASPP (2.1% for PPM, 1.7% for SEP-ASPP). ASPP achieves a slightly higher performance than SEP-ASPP, but spends the most computational resources. Through these competitions, LawinASPP proves to be the preferred module to introduce mutli-scale representations into the semantic segmentation ViT, which is mainly attributed to large window attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Key Component in Large Window Attention</head><p>Pooling and Multi-Head: Pooling the large context patch with a downsampling ratio R and increasing the head number of MHA to R 2 purpose to reducing the computational complexity and recovering the discarded dependencies respectively. To verify the strategy, we implement the first group experiments shown in table 4. We first test the performance when the large context patch keeps the spatial size without any downsampling. However, the required memory of this setting is unaffordable so we do a little tweak as illustrated in <ref type="figure">Fig. 4</ref>, setting the size of query patch equal to context patch. The performance of this simple implementation is lower than the standard implementation by 1.3%. If the context patch is pooled to the same size of query patch, the performance degrades severely, only achieving a 47.3% mIoU, owing to the sparsity of attention. Enabling multihead mechanism can bring an improvement of 0.6%, but falling behind the standard and even the simple implementation. This group comparison shows that large window attention with the pooled context patch actually suffers inadequate dependencies, and the multi-head mechanism can alleviate it marginally.</p><p>Position-Mixing and Channel-Mixing: In large window attention, we innovatively employ the position-mixing operation to strengthen the spatially representational power of multi-head attention. In MLP-Mixer <ref type="bibr" target="#b36">[37]</ref>, the channelmixing MLP is applied to learn knowledge of feature channels. That MLP-Mixer uses both kinds of MLPs prompts our investigation of channel-mixing. We enhance the communication along feature channels within each head by replacing the token-mixing MLP with channel-mixing MLP. The context patch is downsampled and the multi-head mechansim is enabled. The second group of results listed in <ref type="table" target="#tab_2">table 4</ref> shows that the channel-mixing MLP boosts the representation of multi-head attention and provides an appreciable performance improvement of 1.2%, but not powerful as token-mixing MLP (2%). Furthermore, we make a combination of token-mixing MLP with channel-mixing MLP, like a block in MLP-Mixer, to transform each head subspace along two dimensions, which attains a competitive result of 49.4% mIoU but worse than isolated positionmixing (49.9%). With these observations, we argue that the position-mixing operation is more useful than channelmixing for recovering the dependencies of spatial downsampling operation. Spatial Size of Context: That large window attention pools the context patch to the same spatial size of query patch keeps the balance between efficiency and performance. We are interested in the consequence of disturbing the balance.</p><p>To be specific, we evaluate the performance in following situations that the context patch is pooled to the spatial size of two times query patch and the context patch is pooled to the half size of query patch. The former sacrifices the computational economy and might be advantageous to performance, and the latter saves more computation cost and might be harmful to performance. It can be found in table 5 that no apparent performance is obtained in the former case. When the context patch is pooled to a smaller size, the mIoU drops 0.8% and only saves a little computation cost of 3.7G. Pooling the context patch to the size of query patch is a sensible choice keeping the balance well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Branch in LawinASPP</head><p>As illustrated in <ref type="figure">Fig. 3</ref>, LawinASPP aggregates features derived from five branches to gather rich contextual information at multiple scales. Following the aggregation, the firstlevel feature comes to enhance it with low-level information via an auxiliary branch. We here study the efficacy of the six branches in LawinASPP. In  <ref type="bibr" target="#b51">[52]</ref> ViT-L -45.8 SegFormer <ref type="bibr" target="#b42">[43]</ref> MiT-B5 112 46.7 Lawin</p><p>MiT-B5 94 47.5 <ref type="table">Table 9</ref>. Performance Comparion on COCO-Stuff. The backbone with superscript indicates that it is pretrained on ImageNet22K. R = 8 respectively. The image pooling branch yields an improvement of 0.6% so the global contextual information is an essential hierarchy of LawinASPP. The short path is also indispensable to LawinASPP in that the biggest performance gain of (1.0%) is from this branch. We surprisingly observe that adding the auxiliary branch leads to an improvement of 0.8%, which manifests the importance of low-level information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comprasion with State-of-the-Art</head><p>Finally, we compare our results with existing approaches on the ADE20K, Cityscapes and COCO-Stuff datasets. <ref type="table" target="#tab_4">Table 7</ref> shows the results of state-of-the-art methods on Cityscapes dataset. The first group contains the CNNbased semantic segmentation method and the second group contains the ViT-based semantic segmentation method. If not specified, the crop size of input image is 768/769 ? 768/769. To boost the performance of Lawin Transformer, we use MiT-B5 and Swin-L as the encoder. Lawin Transformer with Swin-L achieves the best performance on Cityscapes. <ref type="table">Table 8</ref> reports the performance of state-of-the-art methods on ADE20K dataset. The results are still grouped into two parts consists of CNN-based methods and ViT-based methods. If not specified, the crop size of input image is 512 ? 512. Lawin Transformer with Swin-L outperforms all other methods. Lawin Transformer with MiT-B5 uses the least FLOPs (159 GFLOPs) and achieves an excellent performance (53.0% mIoU). <ref type="table">Table 9</ref> lists some results of state-of-the-art methods on COCO-stuff. Since there are few paper reporting the performance on COCO-Stuff, we just list the result of representative CNN-based methods. Lawin-B5 obtains the best mIoU of 47.5% and also uses the least FLOPs of 94G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we develop an efficient semantic segmentation transformer called Lawin Transformer. The decoder part of Lawin Transformer is capable of capturing rich contextual information at multiple scales, which is established on our proposed large window attention. Compared to the existing efficient semantic segmentation Transformer, Lawin Transformer can achieve higher performance with less computational expense. Finally, we conduct experiments on Cityscapes, ADE20K and COCO-Stuff dataset, yielding state-of-the-art results on these benchmarks. We hope Lawin Transformer will inspire the creativity of semantic segmentation ViT in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Difference between LawinASPP and ASPP. In ASPP, atrous convolution with different dilation rates captures representations at multple scalees. In contrast, LawinASPP replaces atrous convolution with our proposed large window attention. The red window represents the query area. The yellow, orange and purple windows represent the context area with different spatial sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>A large window attention. The red patch Q is the query patch and the purple patch C is the context patch. The context is reshaped and fed into token-mixing MLPs. The output context C P is named position-mixed context. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of Swin-Lawin Transformer with Mask-Former and Swin-UperNet on ADE20K. The method marked with ? takes cropped input of 640 ? 640. The method marked with indicates that its encoder is pretrained on ImageNet22k.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell>ADE20K</cell><cell></cell><cell>Cityscapes</cell><cell cols="2">COCO-Stuff</cell></row><row><cell>Method</cell><cell cols="8">Params(M)? FLOPs(G)? mIoU(SS/MS)? FLOPs(G)? mIoU(SS/MS)? FLOPs(G)? mIoU(SS)?</cell></row><row><cell cols="2">SegFormer-B0</cell><cell>3.8</cell><cell>8.4</cell><cell>38.1 / 38.6</cell><cell>125.5</cell><cell>76.5 / 78.2</cell><cell>8.4</cell><cell>35.7</cell></row><row><cell>Lawin-B0</cell><cell></cell><cell>4.1</cell><cell>5.3</cell><cell>38.9 / 39.6</cell><cell>99.3</cell><cell>77.2 / 78.7</cell><cell>5.3</cell><cell>36.2</cell></row><row><cell cols="2">SegFormer-B1</cell><cell>13.7</cell><cell>15.9</cell><cell>41.7 / 42.8</cell><cell>243.7</cell><cell>78.5 / 80.0</cell><cell>15.9</cell><cell>40.2</cell></row><row><cell>Lawin-B1</cell><cell></cell><cell>14.1</cell><cell>12.7</cell><cell>42.1 / 43.1</cell><cell>217.5</cell><cell>79.0 / 80.4</cell><cell>12.7</cell><cell>40.5</cell></row><row><cell cols="2">SegFormer-B2</cell><cell>27.5</cell><cell>62.4</cell><cell>46.5 / 47.5</cell><cell>717.1</cell><cell>81.0 / 82.2</cell><cell>62.4</cell><cell>44.5</cell></row><row><cell>Lawin-B2</cell><cell></cell><cell>29.7</cell><cell>45.0</cell><cell>47.8 / 48.8</cell><cell>562.8</cell><cell>81.7 / 82.7</cell><cell>45.0</cell><cell>45.2</cell></row><row><cell cols="2">SegFormer-B3</cell><cell>47.3</cell><cell>79.0</cell><cell>48.7 / 49.2</cell><cell>962.9</cell><cell>81.7 / 83.3</cell><cell>79.0</cell><cell>45.4</cell></row><row><cell>Lawin-B3</cell><cell></cell><cell>49.5</cell><cell>61.7</cell><cell>50.3 / 51.1</cell><cell>808.6</cell><cell>82.5 / 83.7</cell><cell>61.7</cell><cell>46.6</cell></row><row><cell cols="2">SegFormer-B4</cell><cell>64.1</cell><cell>95.7</cell><cell>49.6 / 50.4</cell><cell>1240.6</cell><cell>82.2 / 83.6</cell><cell>95.7</cell><cell>46.4</cell></row><row><cell>Lawin-B4</cell><cell></cell><cell>66.3</cell><cell>78.2</cell><cell>50.7 / 51.4</cell><cell>1086.2</cell><cell>82.7 / 83.8</cell><cell>78.2</cell><cell>47.3</cell></row><row><cell cols="2">SegFormer-B5</cell><cell>84.7</cell><cell>183.3</cell><cell>50.7 / 51.2</cell><cell>1460.4</cell><cell>82.3 / 83.7</cell><cell>111.6</cell><cell>46.7</cell></row><row><cell>Lawin-B5</cell><cell></cell><cell>86.9</cell><cell>159.1</cell><cell>52.3 / 53.0</cell><cell>1306.4</cell><cell>82.8 / 83.9</cell><cell>94.2</cell><cell>47.5</cell></row><row><cell></cell><cell></cell><cell cols="5">Table 1. Comparison of SegFormer with Lawin Transformer.</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">FLOPs(G)? Params(M)? mIoU(SS/MS)?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Uper-T</cell><cell>236.1</cell><cell>59.9</cell><cell></cell><cell>44.5 / 45.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask-T</cell><cell>59.0</cell><cell>41.8</cell><cell></cell><cell>46.7 / 48.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lawin-T</cell><cell>48.9</cell><cell>34.5</cell><cell></cell><cell>45.3 / 46.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Uper-S</cell><cell>259.3</cell><cell>81.3</cell><cell></cell><cell>47.7 / 49.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask-S</cell><cell>79.0</cell><cell>63.1</cell><cell></cell><cell>49.8 / 51.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lawin-S</cell><cell>72.0</cell><cell>55.9</cell><cell></cell><cell>48.7 / 50.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Uper  ? -B</cell><cell>470.4</cell><cell>121.4</cell><cell></cell><cell>51.6 / 53.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask  ? -B</cell><cell>195.0</cell><cell>101.9</cell><cell></cell><cell>52.7 / 53.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lawin  ? -B</cell><cell>172.9</cell><cell>94.5</cell><cell></cell><cell>53.0 / 54.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Uper  ? -L</cell><cell>646.4</cell><cell>233.4</cell><cell></cell><cell>52.7 / 54.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask  ? -L</cell><cell>375.0</cell><cell>212.0</cell><cell></cell><cell>54.1 / 55.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lawin  ? -L</cell><cell>350.6</cell><cell>201.2</cell><cell></cell><cell>54.7 / 56.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">FLOPs(G) Params(M) mIoU</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSP [50]</cell><cell></cell><cell>48.2</cell><cell>49.8</cell><cell>47.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASPP [9]</cell><cell></cell><cell>82.9</cell><cell>57.0</cell><cell>48.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SEP-ASPP [10]</cell><cell>57.2</cell><cell>50.7</cell><cell>48.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LawinASPP</cell><cell>61.7</cell><cell>49.5</cell><cell>49.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Table 3. Results of different SPP modules when coupled with MiT-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>B3 on ADE20K.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Results of Lawin-B3 with a variety settings of the ratio, head and MLP type on ADE20K.</figDesc><table><row><cell>Ratio</cell><cell>Head</cell><cell cols="4">C-Mixing P-Mixing mIoU</cell></row><row><cell>(1,1,1)</cell><cell>(1,1,1)</cell><cell></cell><cell></cell><cell></cell><cell>48.6</cell></row><row><cell>(2,4,8)</cell><cell>(1,1,1)</cell><cell></cell><cell></cell><cell></cell><cell>47.3</cell></row><row><cell cols="2">(2,4,8) (4,16,64)</cell><cell></cell><cell></cell><cell></cell><cell>47.9</cell></row><row><cell cols="2">(2,4,8) (4,16,64)</cell><cell></cell><cell></cell><cell></cell><cell>49.1</cell></row><row><cell cols="2">(2,4,8) (4,16,64)</cell><cell></cell><cell></cell><cell></cell><cell>49.9</cell></row><row><cell cols="2">(2,4,8) (4,16,64)</cell><cell></cell><cell></cell><cell></cell><cell>49.4</cell></row><row><cell></cell><cell></cell><cell>32</cell><cell>16</cell><cell></cell></row><row><cell cols="2">64</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Figure 4. A simple implementation of LawinASPP. The area of</cell></row><row><cell cols="5">both query patch and context patch is set to (64,32,16)</cell></row><row><cell>Pooling Ratio</cell><cell cols="2">Head</cell><cell cols="3">Size FLOPs(G) mIoU</cell></row><row><cell>(1,2,4)</cell><cell cols="2">(1,4,16)</cell><cell>16</cell><cell>74.6</cell><cell>49.9</cell></row><row><cell>(2,4,8)</cell><cell cols="2">(4,16,64)</cell><cell>8</cell><cell>61.7</cell><cell>49.9</cell></row><row><cell>(4,8,16)</cell><cell cols="2">(16,64,256)</cell><cell>4</cell><cell>58.0</cell><cell>49.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Performance Comparison on Cityscapes. The backbone marked with indicates that it is pretrained on ImageNet22K. The method marked with ? takes cropped input of 1024 ? 1024.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">FLOPs(G)? MS(%)?</cell></row><row><cell>PSPNet [50]</cell><cell>ResNet101</cell><cell>2049</cell><cell>80.0</cell></row><row><cell>GCNet [5]</cell><cell>ResNet101</cell><cell>2203</cell><cell>80.7</cell></row><row><cell>PSANet [51]</cell><cell>ResNet101</cell><cell>2178</cell><cell>80.9</cell></row><row><cell>NonLocal [40]</cell><cell>ResNet101</cell><cell>2224</cell><cell>80.9</cell></row><row><cell>DeeplabV3 [9]</cell><cell>ResNet101</cell><cell>2781</cell><cell>80.8</cell></row><row><cell>CCNet [26]</cell><cell>ResNet101</cell><cell>2225</cell><cell>80.7</cell></row><row><cell>DANet [21]</cell><cell>ResNet101</cell><cell>2221</cell><cell>82.0</cell></row><row><cell>DNL [45]</cell><cell>ResNet101</cell><cell>2224</cell><cell>80.7</cell></row><row><cell>OCNet [49]</cell><cell>ResNet101</cell><cell>1820</cell><cell>81.6</cell></row><row><cell cols="2">DeeplabV3+ [10] ResNet101</cell><cell>2032</cell><cell>82.2</cell></row><row><cell>SETR-PUP [52]</cell><cell>ViT-L</cell><cell>-</cell><cell>82.2</cell></row><row><cell>Segmenter [36]</cell><cell>VIT-L</cell><cell>-</cell><cell>81.3</cell></row><row><cell>SegFormer [43]</cell><cell>MiT-B5</cell><cell>1460</cell><cell>83.5</cell></row><row><cell>SegFormer  ?</cell><cell>MiT-B5</cell><cell>1460</cell><cell>83.7</cell></row><row><cell>Lawin</cell><cell>MiT-B5</cell><cell>1306</cell><cell>83.7</cell></row><row><cell>Lawin  ?</cell><cell>MiT-B5</cell><cell>1306</cell><cell>83.9</cell></row><row><cell>Lawin</cell><cell>Swin-L</cell><cell>1797</cell><cell>84.2</cell></row><row><cell>Lawin  ?</cell><cell>Swin-L</cell><cell>1797</cell><cell>84.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table 6 ,Table 8 .</head><label>68</label><figDesc>we report the results when different branches are absent. For the branches of large window attention, the performance drops 0.4%, 0.5% and 0.5% for removing the branch with R = 2, R = 4 and Performance Comparison on ADE20K. The backbone marked with indicates that it is pretrained on ImageNet22K. The method marked with ? takes cropped input of 640 ? 640.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">FLOPs(G)? MS(%)?</cell></row><row><cell>PSPNet [50]</cell><cell>ResNet101</cell><cell>256</cell><cell>45.4</cell></row><row><cell>GCNet [5]</cell><cell>ResNet101</cell><cell>276</cell><cell>45.2</cell></row><row><cell>PSANet [51]</cell><cell>ResNet101</cell><cell>272</cell><cell>45.4</cell></row><row><cell>NonLocal [40]</cell><cell>ResNet101</cell><cell>278</cell><cell>45.8</cell></row><row><cell>DeeplabV3 [9]</cell><cell>ResNet101</cell><cell>348</cell><cell>46.7</cell></row><row><cell>CCNet [26]</cell><cell>ResNet101</cell><cell>278</cell><cell>45.0</cell></row><row><cell>DANet [21]</cell><cell>ResNet101</cell><cell>278</cell><cell>45.0</cell></row><row><cell>DNL [45]</cell><cell>ResNet101</cell><cell>278</cell><cell>45.8</cell></row><row><cell>OCNet [49]</cell><cell>ResNet101</cell><cell>227</cell><cell>45.4</cell></row><row><cell>DeeplabV3+ [10]</cell><cell>ResNet101</cell><cell>255</cell><cell>46.4</cell></row><row><cell>OCRNet [48]</cell><cell>HRNetW48</cell><cell>165</cell><cell>44.9</cell></row><row><cell>SETR-MLA  ? [52]</cell><cell>ViT-L</cell><cell>-</cell><cell>50.3</cell></row><row><cell>Segmenter  ? [36]</cell><cell>ViT-L</cell><cell>-</cell><cell>53.6</cell></row><row><cell>SegFormer  ? [43]</cell><cell>MiT-B5</cell><cell>183</cell><cell>51.2</cell></row><row><cell>Lawin  ?</cell><cell>MiT-B5</cell><cell>159</cell><cell>53.0</cell></row><row><cell>Swin-Uper  ? [30]</cell><cell>Swin-L</cell><cell>646</cell><cell>54.1</cell></row><row><cell>MaskFormer  ? [11]</cell><cell>Swin-L</cell><cell>375</cell><cell>55.6</cell></row><row><cell>Lawin  ?</cell><cell>Swin-L</cell><cell>351</cell><cell>56.2</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">FLOPs(G)? mIoU(%)?</cell></row><row><cell>NonLocal [40]</cell><cell>ResNet101</cell><cell>278</cell><cell>37.9</cell></row><row><cell cols="2">DeeplabV3+ [10] ResNet101</cell><cell>255</cell><cell>38.4</cell></row><row><cell>OCRNet [48]</cell><cell>HRNetW48</cell><cell>165</cell><cell>42.3</cell></row><row><cell>SETR-MLA</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Md Amirul Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3751" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Twins: Revisiting spatial attention design in vision trans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13840</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">formers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<idno>2020. 5</idno>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03584</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697,2021.4</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR. OpenReview.net</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01136</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic multiscale filters for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3562" to="3572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive pyramid context network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7519" to="7528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxue</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10180</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to detect roads in high-resolution aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep semantic segmentation for automated driving: Taxonomy, roadmap and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Elkerdawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Yogamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 20th international conference on intelligent transportation systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segmenter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05633</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Contnet: Why not use convolution and transformer at the same time?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13497</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12416" to="12425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI 16</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Psanet: Pointwise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
