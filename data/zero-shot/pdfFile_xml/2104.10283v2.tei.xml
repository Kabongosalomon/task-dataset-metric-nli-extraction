<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphVQA: Language-Guided Graph Neural Networks for Scene Graph Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Liang</surname></persName>
							<email>wxliang@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhao</forename><surname>Jiang</surname></persName>
							<email>jiangyh@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphVQA: Language-Guided Graph Neural Networks for Scene Graph Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Images are more than a collection of objects or attributes -they represent a web of relationships among interconnected objects. Scene Graph has emerged as a new modality for a structured graphical representation of images. Scene Graph encodes objects as nodes connected via pairwise relations as edges. To support question answering on scene graphs, we propose GraphVQA, a language-guided graph neural network framework that translates and executes a natural language question as multiple iterations of message passing among graph nodes. We explore the design space of GraphVQA framework, and discuss the trade-off of different design choices. Our experiments on GQA dataset show that GraphVQA outperforms the state-of-the-art model by a large margin (88.43% vs. 94.78%). Our code is available at https://github. com/codexxxl/GraphVQA . 2017. A simple neural network module for relational reasoning. In NIPS, pages 4967-4976.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Images are more than a collection of objects or attributes. Each image represents a web of relationships among interconnected objects. Towards formalizing a representation for images, Visual Genome <ref type="bibr" target="#b12">(Krishna et al., 2017a)</ref> defined scene graphs, a structured formal graphical representation of an image that is similar to the form widely used in knowledge base representations. As shown in <ref type="figure">Figure 1</ref>, scene graph encodes objects (e.g., girl, burger) as nodes connected via pairwise relationships (e.g., holding) as edges. Scene graphs have been introduced for image retrieval <ref type="bibr" target="#b10">(Johnson et al., 2015)</ref>, image generation <ref type="bibr" target="#b9">(Johnson et al., 2018)</ref>, image captioning <ref type="bibr" target="#b2">(Anderson et al., 2016)</ref>, understanding instructional videos <ref type="bibr" target="#b6">(Huang et al., 2018)</ref>, and situational role classification <ref type="bibr" target="#b15">(Li et al., 2017)</ref>.</p><p>To support question answering on scene graphs, we propose GraphVQA, a language-guided graph 1 Equal Contribution. Authors listed in alphabetical order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: Question</head><p>What is the red object left of the girl that is holding a hamburger?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: Image (Represented as Scene Graph)</head><p>Step 1: Scene Graph Reasoning Figure 1: Scene Graph: Scene graph encodes objects (e.g., girl, burger) as nodes connected via pairwise relationships (e.g., holding) as edges. GraphVQA Framework: Our core insight is to translates and executes a natural language question as multiple iterations of message passing among graph nodes (e.g., hamburger -&gt; small girl -&gt; red tray). The final state after message passing represents the answer (e.g., tray).</p><p>neural network framework for Scene Graph Question Answering(Scene Graph QA). Our core insight is to translate a natural language question into multiple iterations of message passing among graph nodes. <ref type="figure">Figure 1</ref> shows an example question "What is the red object left of the girl that is holding a hamburger". This question can be naturally answered by the following iterations of message passing "hamburger ? small girl ? red tray". The final state after message passing represents the answer (e.g., tray), and the intermediate states reflect the model's reasoning. Each message passing iteration is accomplished by a graph neural network (GNN) layer. We explore various message passing designs in GraphVQA, and discuss the trade-off of different design choices. Scene Graph QA is closely related to Visual Question Answering (VQA). Although there are many research efforts in scene graph generation, Scene Graph QA remains relatively underexplored. Sporadic attempts in scene graph based VQA <ref type="bibr" target="#b4">(Hu et al., 2019;</ref><ref type="bibr" target="#b14">Li et al., 2019;</ref><ref type="bibr">Santoro et al., 2017)</ref> mostly propose various attention mechanisms designed primarily for fully-connected graphs, thereby failing to model and capture the important structural information of the scene graphs.</p><p>We evaluate GraphVQA on GQA dataset <ref type="bibr" target="#b7">(Hudson and Manning, 2019a)</ref>.</p><p>We found that GraphVQA with de facto GNNs can outperform the state-of-the-art model by a large margin (88.43% vs. 94.78%). We discuss additional related work in appendix A. Our results suggest the importance of incorporating recent advances from graph machine learning into our community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Machine Learning with Graphs</head><p>Modeling graphical data has historically been challenging for the machine learning community. Traditionally, methods have relied on Laplacian regularization through label propagation, manifold regularization or learning embeddings. Today's de facto choice is graph neural network (GNN), which is a operator on local neighborhoods of nodes.</p><p>GNNs follow the message passing scheme. The high level idea is to update each node's feature using its local neighborhoods of nodes. Specifically, node i's representation at l-th layer h </p><formula xml:id="formula_0">N i = AGG j?N i ? (l) (h (l?1) i , h (l?1) j , e ji ) (1) h (l) i = ? (l) (h (l?1) i , h (l) N i )<label>(2)</label></formula><p>where e ji denotes the feature of edge from node j to node i, h</p><p>N i denotes aggregated neighborhood information, ? (l) and ? (l) denotes differentiable functions such as MLPs, and AGG denotes aggregation functions such as mean or sum pooling.</p><p>3 GraphVQA Framework </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Parsing Module</head><p>Question Parsing Module uses a sequence-tosequence transformer architecture to translate the question [q 1 , . . . , q Q ] into a sequence of instruction vectors [i (1) , . . . , i (M ) ] with a fixed M .</p><p>[i (1) , . . . , i (M ) ] = Seq2Seq(q 1 , . . . , q Q ) (3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scene Graph Encoding Module</head><p>Scene Graph Encoding Module first initializes node featuresX = [x 1 , ...,x N ] with the word embeddings of the object name and attributes, and edge features E with the word embedding of edge type. We then obtain contextualized node features X by:</p><formula xml:id="formula_2">x i = ?( 1 |N i | j?N i (W enc [x j ; e ij ]))<label>(4)</label></formula><p>where ? denotes the activation function, e ij denotes the feature of the edge that connects node i and node j, and X = [x 1 , x 2 , ..., x N ] denotes the contextualized node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Reasoning Module</head><p>Graph Reasoning Module is the core of GraphVQA framework. Graph Reasoning Module executes the M instruction vectors step-by-step, with N graph neural network layers. One major difference between our Graph Reasoning Module and standard GNN is that, we want the message passing in layer L conditioned on the L th instruction vector. Inspired by language model type condition <ref type="bibr" target="#b17">(Liang et al., 2020b)</ref>, we adopt a general design that is compatible with any graph neural network design: Before running the L th GNN layer, we concatenate the L th instruction vector to every node and edge feature from the previous layer. Specifically,</p><formula xml:id="formula_3">h (L?1) i = [h (L?1) i ; i (L) ] (5) e (L?1) ij = [e (L?1) ij ; i (L) ]<label>(6)</label></formula><p>where?</p><formula xml:id="formula_4">(L?1) i and? (L?1) ij</formula><p>denotes the node feature and edge feature as inputs to the L th GNN layer. Next, we introduce three standard GNNs that we have explored, starting from the simplest one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Graph Convolution Networks (GCN)</head><p>GCN (Kipf and Welling, 2017) treats neighborhood nodes as equally important sources of information, and simply averages the transformed features of neighborhood nodes. </p><formula xml:id="formula_5">h (L) i = ?( 1 |N i | j?N i (W (L) GCN? (L?1) j )) (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Graph Isomorphism Network (GINE)</head><p>GIN (Xu et al., 2019) is provably as powerful as the Weisfeiler-Lehman graph isomorphism test. GINE <ref type="bibr" target="#b5">(Hu et al., 2020)</ref> augments GIN by also considering edge features during the message passing:</p><formula xml:id="formula_6">h (L) i =?((1 + )? (L?1) i + j?N (i) ?(? (L?1) j +? (L?1) j,i ))</formula><p>where ? denotes expressive functions such as MLPs, and is a scale factor for the emphasis of the central node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Graph Attention Network (GAT)</head><p>Different from GIN and GINE, <ref type="bibr">GAT (Veli?kovi? et al., 2018)</ref> learns to use attention mechanism to weight neighbour nodes differently. Intuitively, GAT fits more naturally with our Scene Graph QA task, since we want to emphasis different neighbor nodes given different instruction vectors. Specifically, the attention score ? (L) ij for message passing from node j to node i at L th layer is calculated as:</p><formula xml:id="formula_7">? (L) ij =Softmax N i (MLP(? (L?1) i ,? (L?1) j ,? (L?1) ij )) (8)</formula><p>where Softmax N i is a normalization to ensure that the attention scores from one node to its neighbor nodes sum to 1. After calculating the attention scores, we calculate each node's new representation as a weighted average from its neighbour nodes.</p><formula xml:id="formula_8">h (L) i = ?( j?N i ? (L) ij W (L) GAT? (L?1) j )<label>(9)</label></formula><p>where ? denotes the activation function. Similar to transformer models, we use multiple attention heads in practice. In addition, many modern deep learning tool-kits can be incorporated into GNNs, such as batch normalization, dropout, gating mechanism, and residual connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answering Module</head><p>After executing the Graph Reasoning module, we obtain the final states of all graph nodes after M iterations of message passing [h</p><formula xml:id="formula_9">(M ) 1 , ..., h (M ) N ].</formula><p>We first summarize the final states after message passing, and then predict the answer token with the question summary vector q:</p><formula xml:id="formula_10">h = Aggregate([h (M ) 1 , h (M ) 2 , ..., h (M ) N ]) (10) y = Softmax(MLP(h, q))<label>(11)</label></formula><p>where y is the predicted answer. We note that GraphVQA does not require any explicit supervision on how to solve the question step-by-step, and we only supervise on the final answer prediction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and Metrics</head><p>We evaluate three instantiations of GraphVQA: GraphVQA-GCN, GraphVQA-GINE, GraphVQA-GAT. We compare with the state-of-the-art model LCGN <ref type="bibr" target="#b4">(Hu et al., 2019)</ref>. We discuss LCGN in appendix B.3. We also compare with a simple GCN without instruction vector concatenation discussed in ? 3.3 to study the importance of language guidance. We report the standard evaluation metrics defined in Hudson and Manning (2019a) such as accuracy and consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The first take-away message is that GraphVQA outperforms the state-of-the-art approach LCGN, even with the simplest GraphVQA-GCN. Besides, GraphVQA-GAT outperforms LCGN by a large margin (88.43% vs. 94.78% accuracy), highlighting the benefits of incorporating recent advances from graph machine learning. The second take-away message is that conditioning on instruction vectors is important. Removing such conditioning drops performance (GCN vs. GraphVQA-GCN, 85.7% vs. 90.18%). The third take-away message is that attention mechanism is important for Scene Graph QA, as GraphVQA-GAT also outperforms both GraphVQA-GCN and GraphVQA-GINE by a large margin (94.78% vs. 90.38%), even though GINE is provably more expressive than GAT (Xu et al., 2019). Analysis <ref type="figure" target="#fig_4">Figure 3</ref> shows the accuracy breakdown on question semantic types. We found that GraphVQA-GAT achieves significantly higher accuracy in relationship questions (95.53%). This shows the strength in the attention mechanism in modeling the relationships in scene graphs. <ref type="figure" target="#fig_5">Figure 4</ref> shows the accuracy breakdown on question word count. As expected, longer questions are harder to answer by all models. In addition, we found that as questions become longer, the accuracy GraphVQA-GAT deteriorates drops than other methods, showing that GraphVQA-GAT is better at answering long questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present GraphVQA to support question answering on scene graphs. GraphVQA translates and executes a natural language question as multiple iterations of message using graph neural networks. We explore the design space of GraphVQA framework, and found that GraphVQA-GAT (Graph Attention Network) is the best design. GraphVQA-GAT outperforms the state-of-the-art model by a large margin (88.43% vs. 94.78%). Our results suggest the potential benefits of revisiting existed Vision + Language multimodal models from the perspective of graph machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Visual Question Answering</head><p>VQA requires an interplay of visual perception with reasoning about the question semantics grounded in perception. The predominant approach to visual question answering (VQA) relies on encoding the image and question with a "black-box" neural encoder, where each image is usually represented as a bag of object features, where each feature describes the local appearance within a bounding box detected by the object detection backbone. However, representing images as collections of objects fails to capture relationships which are crucial for visual question answering. Recent study has further demonstrated some unsettling behaviours of those models: they tend to ignore important question terms <ref type="bibr" target="#b21">(Mudrakarta et al., 2018)</ref>, look at wrong image regions <ref type="bibr" target="#b3">(Das et al., 2016)</ref>, or undesirably adhere to superficial or even potentially misleading statistical associations . In addition, it has been shown that recent advances are primarily driven by perception improvements (e.g. object detection) rather than reasoning <ref type="bibr" target="#b1">(Amizadeh et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Scene Graph Question Answering</head><p>Although there are many research efforts in scene graph generation, using scene graphs for visual question answering remains relatively underexplored (Hudson and Manning, 2019b; Hu et al., 2019; <ref type="bibr" target="#b14">Li et al., 2019;</ref><ref type="bibr" target="#b16">Liang et al., 2020a)</ref>. Hudson and Manning (2019b) propose a task-specific graph traversal framework with neural networks. The framework requires specifying the detail ontology of the dataset (e.g., color: red, blue,...; material: wooden, metallic), and thus is not directly generalizable. Other attempts in graph based VQA <ref type="bibr" target="#b4">(Hu et al., 2019;</ref><ref type="bibr" target="#b14">Li et al., 2019)</ref> mostly explore attention mechanism on fully-connected graphs, thereby failing to capture the important structural information of the scene graphs. <ref type="figure">Figure 6</ref> provides another set of accuracy breakdown result on question structural types. We found that GraphVQA-GAT achieves the best for all types of questions except for the verify types. Specifically, GraphVQA-GAT outperforms significantly than other methods on answering queries, comparing among objects and making choices. This intuitively matches the principle of attention mechanism and again shows its advantages in modeling structural information in scene graphs.   When ? is just a single layer MLP, the corresponding GIN/GINE structure will be very similar to the GCN structure. Since in Section 4 we implemented ? as a single layer MLP, the performance of GraphVQA-GCN and GraphVQA-GINE stays at very similar stage. As GIN and GINE are now very popular as basic components for large-scale graph neural network design, one may ask if using ? with more powerful expression ability will help the performance. The short answer is no. We provide a simple ablation study on different choice of ?, using a two layer MLP-style network with (FC, ReLU, FC, ReLU, BN) structure. <ref type="table" target="#tab_6">Table 4</ref> shows that the result of GraphVQA-GINE-2 degrades to the worst. One possible reason is that the scale for each scen graph is generally small, therefore the expression ability might already be enough for a single layer MLP, and use a more complex ? may leads to harder optimization problems, and thus leads to a downgrade of the performance. Such guess could possibly be further investigated and evaluated in our future work. In addition, the scene graph-based VQA as in this work might offer an opportunity for further accelerating the real world image-based applications <ref type="bibr" target="#b18">(Liang and Zou, 2020)</ref>. Exploring such deployment benefits is another direction of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Additional Performances Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Brief Introduction of LCGN</head><p>Language-Conditioned Graph Networks (LCGN) <ref type="bibr" target="#b4">(Hu et al., 2019)</ref>  </p><formula xml:id="formula_11">j,i = Softmax((W 3xt,i ) T ((W 4xt,j ) ? (W 5 c t ))) (13) The messages m (t)</formula><p>i,j , are then computed as:</p><formula xml:id="formula_12">m (t) j,i = w (t) j,i ((W 6xt,j ) ? (W 7 c t ))<label>(14)</label></formula><p>Finally, LCGN aggregates the neighborhood message information to update the context local representation x ctx,i,t .  Note that the graph neural structure of LCGN can be regarded as a variant of recurrently-used single standard GAT layer, but with more self-designed learnable parameters. The main difference between LCGN's and other proposed graph neural structure is that the output node and edge features will be recurrently fed into the same layer again for each reasoning step, leading to a RNN-style network structure, instead of a sequential-style network. Moreover, our LCGN implementation is a variant of original LCGN, including a few improvements. Firstly, we use a transformer encoder and decoder to obtain instruction vectors instead of Bi-LSTM <ref type="bibr" target="#b20">(Liang et al., 2020d)</ref>. Secondly, we incorporate the true scene graph relations as edges instead of densely connected edges. Thirdly, edge attributes are also used in the generation of initial node features.</p><formula xml:id="formula_13">x ctx,i,t = W 8 [x ctx,i,t?1 ; N j=1 m (t) j,i ]<label>(15)</label></formula><p>C Implementation Details C.1 Data Pre-processing</p><p>The edges in the original scene graphs are directed. This means in most of the cases where we only have one directed edge connecting two nodes in the graph, the messages can only flow through one direction. However, this does not make sense in the natural way of human reasoning. For example, an relation of "A is to the left of B" should obviously entail an opposite relation of "B is to the right of A". Therefore, in order to enhance the connectivity of our graphs, we introduce a synthetic symmetric edge for every non-paired edge, making it pointing reversely to the source node. And in order to encode this reversed relationship, we negate the original edge's feature vector and use it as the representation of our synthetic symmetric edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Additional Dataset Information</head><p>These scene graphs are generated from 113k images on COCO and Flicker using the Visual Genome Scene Graph <ref type="bibr" target="#b13">(Krishna et al., 2017b)</ref> annotations. Specifically, each node in the GQA scene graph is representing an object, such as a person, a window, or an apple. Along with the positional information of bounding box, each object is also annotated with 1-3 different attributes. These attributes are the adjectives used to describe associated objects. For examples, there can be color attributes like "white", size attributes like "large", and action attributes like "standing". Attributes are important sources of information beyond the coarse-grained object classes <ref type="bibr" target="#b19">(Liang et al., 2020c)</ref>. Each edge in the scene graph denotes relation between two connected objects. These relations can be action verbs, spatial prepositions, and comparatives, such as "wearing", "below", and "taller".</p><p>We use the official split of the GQA dataset. We use two files "val_sceneGraphs.json" and "train_sceneGraphs.json" directly obtained on the GQA website as our raw dataset. Since each image (graph) is independent, GQA splits the dataset by individual graphs with rough split percentages of train/validation: 88%/12%. In the table 2, we summarize the statistics that we collected from the dataset. We did not report the statistics of the test set since the scene graphs in the test set is not publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Training details</head><p>We train the models using the Adam optimization method, with a learning rate of 10 ?4 , a batch size of 256, and a learning rate drop(divide by 10) each 90 epochs. We train all models for 100 epochs. Both hidden states and word embedding vectors have a dimension size of 300, the latter being initialized using GloVe <ref type="bibr">(Pennington et al., 2014)</ref>.The instruction vectors have a dimension size of 512. All results reported are for a single-model settings (i.e., without ensembling). We use cross validation for hyper-parameter tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>shows an overview of four modules in GraphVQA: (1) Question Parsing Module translates the question to M instruction vectors. (2) Scene Graph Encoding Module initializes node features X and edge features E with word embeddings. (3) Graph Reasoning Module performs message passing with graph neural networks for each instruction vector. (4) Answering Module summarizes the final state after message passing and predicts the answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Semantics of the GraphVQA Framework. (1) Question Parsing Module translates the question to M instruction vectors. (2) Scene Graph Encoding Module initializes node features X and edge features E with word embeddings. (3) Graph Reasoning Module perform message passing with graph neural networks for each instruction vector. (4) Answering Module summarizes the final state after message passing and predicts the answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy breakdown on question semantic types. GraphVQA-GAT achieves significantly higher accuracy in relationship questions (95.53%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Accuracy breakdown on question word count. Num denotes the number of questions of each length. GraphVQA-GAT shows significant better performance for long question answering tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Structure of 2 Layer Graph Neural Network Figure 6: Accuracy breakdown on question structural types. GraphVQA-GAT achieves significantly higher accuracy in all types except for verify.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation Results on GQA. All numbers are in percentages. The lower the better for distribution.</figDesc><table><row><cell>4 Experiments</cell></row><row><cell>Setup We evaluate our GraphVQA framework</cell></row><row><cell>on the GQA dataset (Hudson and Manning, 2019a)</cell></row><row><cell>which contains 110K scene graphs, 1.5M questions,</cell></row><row><cell>and over 1000 different answer tokens. We use</cell></row></table><note>the official train/validation split of GQA. Since the scene graphs of the test set are not publicly available, we use validation split as test set. We set the number of instructions M = 5. More dataset and training details are included in Appendix C.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Scene Graphs Statistics of the GQA Dataset</figDesc><table><row><cell>Level of Classification Structural Semantic</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Typical types of questions</figDesc><table><row><cell>B.2 Expressive Ability Analysis of</cell></row><row><cell>GraphVQA-GINE</cell></row><row><cell>As mentioned in Section 3.3.1 and Section 3.3.2,</cell></row></table><note>a expressive function ? is used in GINE layer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, . . . , i M ], LCGN uses a single layer attention to convert them into context representations [c 1 , . . . , c M ]. Then, given a set of node representations [x loc,1 , . . . , x loc,n ] , LCGN first randomly initialize another set of context representations [x ctx,1 , . . . , x ctx,n ], and then use them to concate-</figDesc><table><row><cell></cell><cell cols="2">updates node representations re-</cell></row><row><cell cols="3">currently using the same single layer graph neu-</cell></row><row><cell cols="3">ral network. Given a set of instruction vectors</cell></row><row><cell cols="3">[i 1 nate with node representations to form initial local</cell></row><row><cell cols="2">features, i.e,</cell></row><row><cell cols="3">x t,i = [x loc,i , x ctx,i,t?1 , W 1 x loc,i ? W 2 x ctx,i,t?1 ]</cell></row><row><cell></cell><cell></cell><cell>(12)</cell></row><row><cell cols="3">With the assumption that all nodes are connected,</cell></row><row><cell cols="2">LCGN computes the edge weights w</cell><cell>(t) j,i for each</cell></row><row><cell cols="2">node pair (i,j), i.e,</cell></row><row><cell>w</cell><cell>(t)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation Study Results for 2 layer GraphVQA-GINE. All numbers are in percentages. The lower the better for distribution.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to sincerely thank NAACL-HLT 2021 MAI-Workshop program committee for their review efforts and helpful feedback. We would also like to extend our gratitude to Jingjing Tian and the teaching staff of Stanford CS 224W for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the behavior of visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neuro-symbolic visual reasoning: Disentangling &quot;visual&quot; from &quot;reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Amizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhito</forename><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="279" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SPICE: semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (5)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="932" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language-conditioned graph networks for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10293" to="10302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding &quot;it&quot;: Weakly-supervised referenceaware visual grounding in instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><forename type="middle">M</forename><surname>Dery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5948" to="5957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GQA: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning by abstraction: The neural state machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5901" to="5914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-016-0981-7</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relation-aware graph attention network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10312" to="10321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">LRTA: A transparent neural-symbolic reasoning framework with modular supervision for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><forename type="middle">N</forename><surname>Reganti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Govind</forename><surname>Thattai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?khan</forename><surname>T?r</surname></persName>
		</author>
		<idno>abs/2011.10731</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MOSS: end-to-end dialog system framework with modular supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8327" to="8335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural group testing to accelerate deep learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AL-ICE: active learning with contrastive natural language explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4380" to="4391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond user self-reported likert scale ratings: A comparison model for automatic dialog evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1363" to="1374" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Did the model understand the question?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Pramod Kaushik Mudrakarta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukund</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedar</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhamdhere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1896" to="1906" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
