<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Traffic signal prediction on transportation networks using spatio-temporal correlations on graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semin</forename><surname>Kwak</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolas</forename><surname>Geroliminis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
						</author>
						<title level="a" type="main">Traffic signal prediction on transportation networks using spatio-temporal correlations on graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Multivariate time series forecasting</term>
					<term>Bayesian inference</term>
					<term>heat diffusion model</term>
					<term>dynamic linear model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multivariate time series forecasting poses challenges as the variables are intertwined in time and space, like in the case of traffic signals. Defining signals on graphs relaxes such complexities by representing the evolution of signals over a space using relevant graph kernels such as the heat diffusion kernel. However, this kernel alone does not fully capture the actual dynamics of the data as it only relies on the graph structure. The gap can be filled by combining the graph kernel representation with data-driven models that utilize historical data. This paper proposes a traffic propagation model that merges multiple heat diffusion kernels into a data-driven prediction model to forecast traffic signals. We optimize the model parameters using Bayesian inference to minimize the prediction errors and, consequently, determine the mixing ratio of the two approaches. Such mixing ratio strongly depends on training data size and data anomalies, which typically correspond to the peak hours for traffic data. The proposed model demonstrates prediction accuracy comparable to that of the state-of-the-art deep neural networks with lower computational effort. It notably achieves excellent performance for long-term prediction through the inheritance of periodicity modeling in data-driven models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M ULTIVARIATE time-series prediction is an important task since many real-life problems can be modeled within this framework, such as weather forecasting <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, traffic prediction <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b18">[19]</ref>, power consumption forecasting <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and others <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. In transportation sensor networks, output signals from neighboring sensors may be similar or vastly different, as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref> and <ref type="bibr">(b)</ref>. Therefore, in this example, sensor A's signal can be utilized to predict sensor B's as the two signals are well correlated. However, the signal of sensor C is not correlated with that of sensor B, so it may not contribute to the prediction; Sensor C is located after an intersection, and most traffic demands flow in another direction in the intersection, therefore, the sensor rarely suffers congestion. Naturally freeway congestion (expressed with a sharp decrease in the average speed of vehicles) is initiated at a bottleneck location such as an onramp merging area with high entrance flow or an incident location. Then, it propagates backwards with a finite speed, which is 3 to 4 times smaller than the speed of traffic. <ref type="figure" target="#fig_0">Fig. 1(c)</ref> shows an example of congestion propagation in I-280 and I-880 freeways in California. Note that there is a drastic decrease in the speed at a location (sensor B) and a time (around 3 pm) that propagates through the traffic stream (this is called a shockwave). Once demand for travel decreases congestion disappears by following the opposite trend during the offset of congestion with a forward moving wave. Note that this propagation speed is not constant and depends on the concentration or density of vehicles (with units of veh/km) on the two sides of the shockwave. There are various theories in transportation science to describe the mechanisms of stopand-go phenomena inspired by fluid and heat diffusion models (see <ref type="bibr" target="#b23">[24]</ref> for an overview).</p><p>Due to complex spatio-temporal correlation, the choice of model greatly influences the predictive performance. For small-scale sensor networks, such correlations can be estimated directly from historical data <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b9">[10]</ref>. The vector Auto Regression (AR) is a representative model for multivariate time series forecasting <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. In this model, regression parameters, or correlations between sensors, are estimated solely using historical data. In our previous work <ref type="bibr" target="#b7">[8]</ref>, we implemented a predictor that explicitly expresses the periodicity of traffic signals with temporally localized vector AR model. However, these data-driven models are not suitable for multivariate time series prediction with a large number of variables because the number of correlations to be estimated increases exponentially compared with the number of sensors, which causes incompleteness of the estimator (or overfitting).</p><p>Recently, many studies have prioritized the correlations among sensors by defining signals on graphs <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b18">[19]</ref>. In particular, in transportation networks, the physical travel distance between sensors is a critical a priori information, the closer the sensors are in space, the higher the correlation <ref type="bibr" target="#b24">[25]</ref>. Utilizing this information, the authors had extracted the signal's spatial features through the heat propagation kernel (or convolutional filter) and passed it to temporal blocks for forecasting, such as recurrent neural network (RNN) <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref> and temporal convolutional layer (TCN) <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref>. By introducing this prior information to complex deep neural networks, they achieved state-of-the-art performance in traffic prediction.</p><p>However, the two predictors (with and without graphs) each have their own drawbacks. In the former case, to the best of our knowledge, all studies, which currently show the best performance, construct predictors based on deep neural networks. Therefore, these models require expensive tuning processes of many hyperparameters and relatively long training due to numerical optimization processes. In the latter case, on the other hand, it can be inefficient concerning the prediction accuracy, especially for large networks when the structural information becomes important.</p><p>This paper proposes a new model that combines the advantages of different frameworks by implanting the sensors' structural information into the existing data-driven model <ref type="bibr" target="#b7">[8]</ref>, inheriting the periodicity modeling for the traffic signal. In most studies, the periodicity of the traffic signal is taken as the input feature of the predictor, such as an encoded vector (a) Sensor locations of PEMS-BAY network. The distance between two consecutive sensors in a freeway is 0.6 mile in average. that represents the time of the day or the day of the week, but the study <ref type="bibr" target="#b7">[8]</ref> instead induces the periodicity of the signal more clearly by making the model itself different for each time. Each model has a matrix, which should be estimated by historical data, representing the correlation between signals at two consecutive time intervals. As the size of the network is proportional to the size of the matrix, a larger network can lead to overfitting. In this paper, we resolve the overfitting problem by approximating this matrix to the one derived from data-independent graph topological information, therefore, we estimate only the remainder by data. In detail, we transform the graph topological information into heat diffusion kernels, which is introduced in <ref type="bibr" target="#b25">[26]</ref>, and approximate the matrix to a combination of the heat diffusion kernels. In the process, we introduce some hyper-parameters. For example, one determines which of the prior or historical datasets is more reliable. Most of the existing studies estimate hyper-parameters through exhaustive search as a cross-validation method using a validation set, but we estimate hyper-parameters directly from data by utilizing Bayesian inference <ref type="bibr" target="#b26">[27]</ref>. As a result, the estimation process is relatively fast as most parameter estimation is performed by analytic calculations except a few ones requiring a numerical optimization process. Besides, our model is strongly interpretable. For example, through the hyper-parameter, it can be seen that during the peak period, traffic prediction is relatively more dependent on data than structural information compared to the non-peak period. Also, most importantly, predictors based on this model showed comparable performance with a much shorter learning time than state-of-the-art models. Especially, the proposed model shows great long-term prediction performance as the model captures well the periodicity of traffic signals. Since the proposed model requires a minimal number of hyper-parameter tuning, it might be applied to other daily periodic graph signal prediction problems easily (e.g., weather forecasting, daily energy consumption prediction). Here we summarize contributions of the work:</p><p>? We propose a novel traffic prediction method that successfully integrate graph structural information to the existing data-driven model <ref type="bibr" target="#b7">[8]</ref>. Hyper-parameters are learned directly from data through Bayesian inference rather than by exhaustive search. ? Therefore, the training time required for inference is minimal. The trained model is straightforward to analyze, unlike other deep neural network-based models. ? It shows prediction performance comparable with deep learning methods especially for long-term prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATA MODEL</head><p>In this section, we describe a mathematical model that represents a relationship between traffic signals that are different in time. First, we define traffic signals on a graph and introduce an existing prediction model <ref type="bibr" target="#b7">[8]</ref> using this signals. Then, we suggest a model extending the previous one that is applicable for large scale networks by exploiting graph information. The diagonal matrix whose diagonal elements are from the vector a diag(A)</p><p>The vector whose elements are the diagonal components of the matrix A I Identity matrix 1</p><p>All one vector</p><formula xml:id="formula_0">e A limn?? I + 1 n A n = ? n=0 1 n! A n [A] i,j</formula><p>The element of i-th row and j-th column of the matrix</p><formula xml:id="formula_1">A [A] i,:</formula><p>The slice of i-th row of the matrix A |A|</p><p>The determinant of the matrix A |S|</p><p>The cardinality of the set S N (?, ? 2 )</p><p>A Gaussian distribution which has the probability density</p><formula xml:id="formula_2">function f (x) = 1 ? ? 2? exp ? 1 2 x?? ? 2 N (?, ?) A multivariate Gaussian distribution which has the probability density function f (x) = 1 ? (2?) N |?| exp ? 1 2 (x ? ?) T ? ?1 (x ? ?) N (M, ? 2 ) i,j N ([M] i,j , ? 2 ) N (M, ?) i N ([M] i,: , ?)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph signal</head><p>We start with modeling a transportation network using a graph. We define an undirected graph G = (V, E); V is a set of nodes where each v ? V denotes a node (sensor) on the graph; E is a set of edges where each of the edges connects two nodes. We define a signal on the nodes of the graph with a traffic feature, in this paper, for instance, speed, which is expressed as a vector x d t ? R N of a day d and time t, where the constant N is the number of nodes. Therefore, the vector x d t represents a snapshot of speeds at a particular time and day. Especially, we express the day index on the vector representation to exploit the periodicity of traffic signals later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic linear model (DLM)</head><p>In our previous study <ref type="bibr" target="#b7">[8]</ref>, we defined a state equation of traffic in a small-scale transportation network (a path graph) as temporally localized linear models as follows:</p><formula xml:id="formula_3">x d t+1 = H t x d t + n d t , ?t ? [0, T ? 1] .<label>(1)</label></formula><p>We called this model the Dynamic linear model (DLM). The first time index (t = 0) corresponds to the beginning of a day (midnight in our work), and the last index (t = T ?1) refers to the end of the day. Each entry of the noise vector n d t ? R N is assumed to be an independent and identically distributed (i.i.d.) random variable, which follows a Gaussian distribution N (0, ? ?1 t ). Here the precision parameter ? t explains how precisely a data pair (x d t , x d t+1 ) fits to the model. The transition matrix H t represents the linear relationship between traffic signals x d t and x d t+1 . The most important motivation behind this model is that the propagation of traffic features over time occurs periodically on a daily basis. Consequently, we modeled that the transition matrix H t as a time-variant matrix that contains temporally localized (only between two consecutive traffic features) spatiotemporal correlations of every sensor pair regardless of the day of the week, noting that the transition matrix does not have the day index. In other words, we assumed the correlations are identical both for weekends and weekdays <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the work <ref type="bibr" target="#b7">[8]</ref>, the transition matrix is estimated by maximizing the likelihood (note that we ignore some parameters such as the regularization parameter and the forgetting factor introduced in the work for the brevity) as follows:</p><formula xml:id="formula_4">H t = argmax Ht f (X t+1 |X t , H t , ? t ) = X t+1 X T t (X t X T t ) ?1 , (2) where the collection of the m-past signals X t = x 0 t x 1 t ? ? ? x m?1 t</formula><p>. Therefore, the optimal transition matrix is solely determined by the historical data X t and X t+1 . From Eq. (2) we see that the matrix X t X T t can be an illconditioned matrix when N is large. In other words, the transition matrixH t can be overfitted by data. In the following subsection, we suggest a method to avoid this problem by utilizing graph topological information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DLM with graph topological information</head><p>In this subsection, we suggest a way to avoid the overfitting problem approximating the transition matrix to a heat diffusion matrix. To achieve this goal, we first define a weight matrix that contains all edge weights between node v i and v j using a Gaussian kernel weighting function with a threshold constant ?:</p><formula xml:id="formula_5">[W] i,j = e ? dist 2 (i,j) ? 2 , if dist(i, j) ? ? 0, otherwise.<label>(3)</label></formula><p>The function dist(i, j) denotes the shortest travel distance on G between the node v i and v j :</p><formula xml:id="formula_6">dist(i, j) = min{dist(v i ? v j ), dist(v j ? v i )},<label>(4)</label></formula><p>where the function dist(v i ? v j ) represents the shortest travel distance from node v i to node v j . As the graph G is undirected, the weight matrix is a symmetric matrix, i.e., W T = W.</p><p>The constants ? and ? are the kernel width and the distance threshold. If the kernel width is large, the correlation of a pair of nodes is strong (close to one) even though the shortest travel distance between the two nodes is large. On the other hand, the smaller the threshold is, the sparser the weight matrix is.</p><p>The graph heat diffusion model <ref type="bibr" target="#b25">[26]</ref> explains how each vertex propagates its heat to its neighbors on the graph over time. As congestion evolves from one location to its neighbor over time, we can express the change of traffic features by the heat diffusion model, especially for short-term traffic changes since the total traffic volume of a network is well preserved for the short-term in general.</p><p>The kernel on graphs that supports the heat diffusion model is introduced by <ref type="bibr" target="#b25">[26]</ref>:</p><formula xml:id="formula_7">H G (? ) = e ?? L(G) ,<label>(5)</label></formula><p>where the constant ? denotes the diffusion period and the matrix L(G) is the Laplacian of a graph G. The matrix is defined as</p><formula xml:id="formula_8">L(G) = diag(W1) ? W.<label>(6)</label></formula><p>By definition, two extreme heat diffusion kernels of a connected graph G are:</p><formula xml:id="formula_9">H G (? ) = I, when ? ? 0, 1 N 11 T , when ? ? ?,<label>(7)</label></formula><p>where 1 is the vector whose elements are all one. Therefore, with the heat diffusion kernel, we can describe the diffusion of a traffic signal through the graph G as follows:</p><formula xml:id="formula_10">x d t+1 (? ) = H G (? )x d t .<label>(8)</label></formula><p>We call the vectorx d t+1 (? ) the internally diffused signals from x d t by the diffusion period ? on the graph G over one incremental time step.</p><p>Here, we define a convex combination of the heat diffusion kernels of K different predetermined diffusion periods with a set T = {? (0) , ? <ref type="bibr" target="#b0">(1)</ref> , ? ? ? , ? (K?1) } 1 as</p><formula xml:id="formula_11">H G (T ) = ? ?T ? (? ) H G (? ),<label>(9)</label></formula><p>where ? ?T ? (? ) = 1. The mixture retains the property that the total input volume is preserved through the diffusion process as shown in Appendix A, i.e.,</p><formula xml:id="formula_12">1 T H G (T )x d t = 1 T x d t .</formula><p>We embed heat diffusion kernels into DLM to exploit topological information of the transportation network. The key idea is to express the transition matrix as a small variant from a mixture of diffusion kernels. We decompose the transition matrix into the time-variant internal diffusion and residual as follows:</p><formula xml:id="formula_13">H t = H G t (T ) + residual<label>(10)</label></formula><p>so that the internal diffusion matrix H G t (T ) preserves the total traffic volume over time, i.e.,</p><formula xml:id="formula_14">1 T H G t (T )x d t = 1 T x d t .</formula><p>Here, the time dependent internal transition matrix can be safely defined as in Eq. (9) by substituting the time-invariant parameter ? (? ) for the time-variant one ? (? ) t because of the volume conservation property. The internal diffusion matrix represents how the current signal x d t diffuses through the transportation network (endogenous) whereas the residual represents how much the traffic situation is getting better or worse in the next time step based on the current signal (exogenous).</p><p>With this interpretation, we model the prior distribution of the transition matrix as:</p><formula xml:id="formula_15">f (H t |? t , ? t , G) = N H G t (T ), ? ?1 t ,<label>(11)</label></formula><p>where the precision parameter ? t represents how precisely the diffusion matrix explains the transition matrix and</p><formula xml:id="formula_16">? t = {? (? ) t |? ? T }.</formula><p>The decomposition allows us to utilize data more efficiently during the estimation process later. In Eq. (1), the transition matrix is a variable to be estimated from the data. Since the dimension of this matrix is N 2 , an increase in the number of <ref type="bibr" target="#b0">1</ref> We predetermine the set T with two diffusion periods ? 0 and ?? that correspond to each extreme case in Eq. <ref type="formula" target="#formula_9">(7)</ref>, respectively. In practice, we set ? 0 as the biggest one that satisfies H G (? ) ? I 2 &lt; and ?? as the smallest one that satisfies H G (? ) ? 1/N 11 T 2 &lt; with a predefined set ? ? linspace(-10,10,0.1), where the set contains evenly spaced (0.1) numbers from ?10 to 10. After that, we define T = logspace(? 0 , ??, K), where the function returns K evenly spaced numbers on a log scale from ? 0 to ??. Set T = logspace(? 0 , ? ? , K) <ref type="bibr">3:</ref> Define L(G) by Eq. <ref type="formula" target="#formula_8">(6)</ref> 4:</p><formula xml:id="formula_17">Define the function H G (? ) = e ?? L(G) 5: for t ? [0, T ? 2] do 6:</formula><p>Infer? t ,? t and? t by solving <ref type="formula" target="#formula_7">(25)   7:</ref> Infer? t by Eq. <ref type="formula" target="#formula_3">(19)</ref> 8: end for 9: return? t , ?t 10: end function sensors causes the estimation of more elements, which results in an overfitting problem. This is the biggest impediment to extending DLM to large networks. Still, if the structural information is set as a priori through Eq. (11), the problem can be effectively avoided even if the number of sensors increases. Assuming the graph G and the period set T are predefined, the internal diffusion matrix only depends on the parameters ? (? ) t . By setting the number of diffusion periods to be much smaller than that of sensors i.e., |T | N , we can describe the major part of the transition matrix by the internal diffusion matrix with a few parameters when the sampling interval (the time difference of two consecutive time indices) is relatively short, with likely preservation of the traffic volumes, i.e.,</p><formula xml:id="formula_18">1 T x d t+1 ? 1 T x d t .</formula><p>Consequently, we only need to exploit data to infer the parameters ? (? ) t and the residual part whose norm is small with the decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PREDICTION AND INFERENCE</head><p>This section describes how to estimate modeling parameters and predict graph signals by using the model. Both the estimation and the prediction were performed by maximizing the posterior distribution of each variable. Especially for hyperparameters, we utilize Bayesian inference to estimate them instead of exhaustive search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inference of the transition matrix</head><p>We infer the transition matrix by maximizing its posterior distribution:</p><formula xml:id="formula_19">H t = argmax Ht f (H t |X t , X t+1 , ? t , ? t , ? t , G),<label>(12)</label></formula><p>which is proportional to the product of the prior and the likelihood by Bayes' rule:</p><formula xml:id="formula_20">Posterior dist. ? f (H t |? t , ? t , G)f (X t+1 |X t , H t , ? t ). (13)</formula><p>Maximizing the posterior distribution can be interpreted as balancing between the prior and likelihood of the transition matrix. For example, if there is no topological information about sensors, the transition matrix should be inferred by considering the training dataset only. In this case, we can set the prior distribution as a uniform distribution, meaning that there is no strong preference for a particular value of the transition matrix; the most probable transition matrix becomes the maximum likelihood solution, which is Eq. <ref type="formula" target="#formula_23">(2)</ref>:</p><formula xml:id="formula_21">H t |No topological info. :=H t = argmax Ht f (X t+1 |X t , H t , ? t ) = X t+1 X T t (X t X T t ) ?1 .<label>(14)</label></formula><p>On the other hand, if we do not have any measurements, the most probable transition matrix should be the maximizer of the prior distribution:</p><formula xml:id="formula_22">H t |No measurements = argmax Ht f (H t |? t , ? t , G) = H G t (T ).<label>(15)</label></formula><p>Since we use both prior and data measurements, the actual optimal transition matrix becomes a combination of these two. According to the dynamic linear model, the likelihood <ref type="bibr" target="#b15">16)</ref> and the prior</p><formula xml:id="formula_23">f (X t+1 |H t , X t , ? t ) ? e ? 1 2 tr{?t(Xt+1?HtXt)(Xt+1?HtXt) T }<label>(</label></formula><formula xml:id="formula_24">f (H t |? t , ? t , G) ? e ? 1 2 tr{?t(Ht?H G t (T ))(Ht?H G t (T )) T } .<label>(17)</label></formula><p>Therefore, by Eq. <ref type="formula" target="#formula_3">(13)</ref>, <ref type="formula" target="#formula_3">(16)</ref> and <ref type="formula" target="#formula_3">(17)</ref>,</p><formula xml:id="formula_25">f (H t |X t+1 , X t , ? t , ? t , ? t , G) ? e ? 1 2 ?ttr{(Xt+1?HtXt)(Xt+1?HtXt) T } ? e ? 1 2 ?ttr{(Ht?H G t (T ))(Ht?H G t (T )) T } ? e ? 1 2 tr{(Ht??t)(?tXtX T t +?tI)(Ht??t) T } ,<label>(18)</label></formula><p>wher?</p><formula xml:id="formula_26">H t = (H t ? t U t ? t + H G t (T )? t U t )(? t ? t + ? t I) ?1 U T t =H t ? t U t ? t (? t ? t + ? t I) ?1 U T t + H G t (T )? t U t (? t ? t + ? t I) ?1 U T t ,<label>(19)</label></formula><p>with the eigendecomposition of</p><formula xml:id="formula_27">X t X T t = U t ? t U T t . There- fore, f (H t |X t+1 , X t , ? t , ? t , ? t , G)</formula><p>is a multivariate Gaussian distribution with mean? t and the covariance of each row;</p><formula xml:id="formula_28">(? t X t X T t + ? t I) ?1 .</formula><p>Here, we measure how much each part contributes to the transition matrix</p><formula xml:id="formula_29">c data t = w data t w data t + w prior t , c prior t = w prior t w data t + w prior t<label>(20)</label></formula><p>by defining the weight of each part</p><formula xml:id="formula_30">w data t = ? t U t ? t (? t ? t + ? t I) ?1 U T t F , w prior t = ? t U t (? t ? t + ? t I) ?1 U T t F .<label>(21)</label></formula><p>Note that these weights depend on the precision parameters ? t and ? t . If the data precision parameter ? t is relatively large compared to ? t , then c data t &gt; c prior , meaning that the contribution of data measurements is larger than that of the prior information.</p><p>Algorithm 2 Prediction of traffic features (h-steps ahead)</p><formula xml:id="formula_31">function PREDICTION(x d t , h,? t , ? ? ? ,? t+h?1 ) Set p = x d t for i ? [0, h ? 1] do Set p =? t+i p end for x t+h|t = p return x t+h|t end function</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inference of other parameters</head><p>For the next step, we infer parameters ? t , ? t , and ? t . Similar to inferring the most probable transition matrix, we infer the most probable ? t , ? t , and ? t by maximizing the following posterior distribution:</p><formula xml:id="formula_32">? t ,? t ,? t = argmax ?t,?t,?t f (? t , ? t , ? t |X t+1 , X t ).<label>(22)</label></formula><p>Setting the prior distribution f (? t , ? t , ? t ) as a uniform distribution based on the assumption that there is no preference for a certain value for these parameters before inferring, the objective changes to maximize evidence</p><formula xml:id="formula_33">f (X t+1 |X t , ? t , ? t , ? t ) [27] since f (? t , ? t , ? t |X t+1 , X t ) ? f (X t+1 |X t , ? t , ? t , ? t )f (? t , ? t , ? t ) ? f (X t+1 |X t , ? t , ? t , ? t ).<label>(23)</label></formula><p>In Appendix B, we show that the evidence is</p><formula xml:id="formula_34">f (X t+1 |X t , ? t , ? t , ? t ) = f (X t+1 |X t , H t , ? t )f (H t |? t , ? t )dH t = N (H G t (T )X t , ? ?1 t I + ? ?1 t X T t X t ).<label>(24)</label></formula><p>Therefore, we infer the most probable hyper-parameters by maximizing the log-evidence with a quasi-newton method (L-BFGS-B <ref type="bibr" target="#b27">[28]</ref>):</p><formula xml:id="formula_35">maximize ?t,?t,?t log N (H G t (T )X t , ? ?1 t I + ? ?1 t X T t X t ) subject to 0 ? ? (? ) t ? 1 ?? ? T , 0 &lt; ? t , 0 &lt; ? t , ? ?T ? (? ) t = 1.<label>(25)</label></formula><p>Algorithm 1 summarizes the inference processes. We emphasize that parameter inference through evidence maximization prevents overfitting of the transition matrix to either data measurements or prior information. In Eq. (24) we calculate the evidence by marginalizing the transition matrix. In other words, we set the transition matrix as a random variable instead of fixing it as a representative value, e.g., maximum likelihood estimator. Noting that these parameters determine the contributions of measurements and priors when the transition matrix is estimated in Eq. <ref type="bibr" target="#b18">(19)</ref>, the marginalization process automatically penalizes the transition matrix to avoid the extreme cases <ref type="bibr" target="#b26">[27]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Prediction of traffic features</head><p>Prediction of traffic features is performed by extracting and exploiting as much information as possible from measurements and prior knowledge. Mathematically, we can express a traffic signal that we want to predict as a random variable since the signal defined in the future is entirely unknown. In this paper, therefore, we try to infer the probability density function of the signal</p><formula xml:id="formula_36">x d t+h f (x d t+h |x d t , x d t?1 ? ? ? , G),<label>(26)</label></formula><p>where the time indices t and t + h represent respectively the current time and the future time index (h-steps ahead) that we want to predict. In the expression, the probability density function is conditioned by the signals {x d t , x d t?1 , ? ? ? } and the graph G that represents a set of measurements and prior structural information, respectively.</p><p>In reality, it is common to limit the number of measurements to a fixed-sized one in a training set. In addition to the training set that contains measurements apart from the day to be predicted, it is crucial to keep measurements just before t, as the temporal correlation is strong when the time difference is small. As a result, we estimate the density function that is conditioned by a training set, the p-most recent measurements, and the graph G:</p><formula xml:id="formula_37">f (x d t+h |x d t , x d t?1 , ? ? ? , x d t?(p?1) , X 0:T ?1 , G),<label>(27)</label></formula><p>where the training set X 0:T ?1 contains signals from t = 0 to t = T ? 1 of multiple days d ? [0, m ? 1]. The dynamic linear model further simplifies the distribution (27) as follows</p><formula xml:id="formula_38">f (x d t+h |x d t , X t:t+h , G)<label>(28)</label></formula><p>because of the temporal locality of the model. We define a predictor x d t+h|t at the time step t for the horizon h as the maximizer of the probability density function In other words, we define the predictor x d t+h|t as the most probable x d t+h based on the current measurement vector x d t , the training set X t:t+h , and the graph G.</p><formula xml:id="formula_39">x d t+h|t := argmax x d t+h f (x d t+h |x d t , X t:t+h , G).<label>(29)</label></formula><formula xml:id="formula_40">Proposition 1. f (x d t+h |x d t , X t:t+h , G) is a Gaussian distri- bution that has the mean vector? t+h?1 ? ? ?? t x d t assuming f (H t |X t , X t+1 , ? t , ? t , ? t , G) = ?(H t ?? t ),</formula><p>where the Dirac delta function ?(x) = 1 when x = 0 and ?(x) = 0, otherwise. The most probable transition? t is the maximizer of the posterior distribution f (H t |?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. See Appendix C.</head><p>Since the mean value of a Gaussian distribution maximizes the distribution, the optimal predictor is</p><formula xml:id="formula_41">x d t+h|t =? t+h?1 ? ? ?? t x t :=? t+h?1?t x d t .<label>(30)</label></formula><p>Therefore, the most probable signal x d t+h is the successive propagation of the current measurement vector x d t through the most probable transition matrices that coincides with a straightforward computation with Eq. (1) ignoring the noise term. Therefore, the prediction for any horizon is just a matrix multiplication. Algorithm 2 summarizes this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings</head><p>The proposed method was evaluated on different transportation networks. <ref type="figure" target="#fig_2">Figure 2</ref> shows the networks (G 1 and G 2 ) consisting of respectively 288 and 357 sensors with multiple freeways that are connected through ramps. They experience significant levels of congestion in the morning and evening peaks at various locations. These networks connect many origins and destinations with complex demand profiles, creating propagation of congestion that is different in duration, size, and time of occurrence. The PEMS-BAY dataset was also used as a benchmark to compare with other state-of-theart models <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b16">[17]</ref>. This data set consists of data measured from 325 sensors ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>) on the freeways of San Francisco Bay area. The training and test dataset were constructed in the same way as <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b16">[17]</ref> to achieve a fair comparison. The sampling interval of each dataset is 5 minutes by default, and in the following subsection, it is downsampled to 10 and 15 minutes, respectively, for a specific experiment. Both datasets of networks G 1 and G 2 contain 209 days of speed data, and each of those is divided into a training set and a test set at an 8:2 ratio by default. Another ratio is applied in Section IV-B for a specific experiment.</p><formula xml:id="formula_42">237E 17N 85N 87N 101N 280N 680N 880N 17S 85S 87S 101S 280S 680S 880S 237W 237E</formula><note type="other">17N 85N</note><p>We used the root mean square error (RMSE) as an error metric to measure the accuracy of prediction since the solution in Eq. (30) is also the optimal under the minimum mean squares error (MMSE) sense <ref type="bibr" target="#b7">[8]</ref>. The RMSE of a method with the prediction horizon h is defined as</p><formula xml:id="formula_43">RMSE(h, method) = mean(x method t+h|t ? x t+h ) 2 ,<label>(31)</label></formula><p>where the mean value is evaluated over all t in the test set. For prediction horizons, we set from 5 minutes to 120 minutes every 5 minutes. In our previous work <ref type="bibr" target="#b7">[8]</ref>, on a freeway with a total length of about 60 miles (similar to the longest path of the networks considered here), the actual travel time is about 70 minutes under usual congestion. In the most severe congestion, the maximum travel time is about 100 minutes, and accordingly, we set the maximum prediction horizon to 120 minutes.</p><p>All datasets were normalized using the mean and standard deviation of each sensor in the training set. For a reference, we defined a baseline method that predicts future traffic features assuming that the current traffic does not change over time, i.e., x baseline x+h|t = x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of network prior</head><p>In this section, we show how network prior information contributes to predictive performance. Our model generalizes the DLM <ref type="bibr" target="#b7">[8]</ref> to extend the model for a more extensive sensor network using the sensor's topology structure. When the DLM is simply used in an extensive network without topology structure information, an overfitting problem can occur. We introduce the three following setups to evaluate how well the proposed model utilizes the topology structure avoiding the overfitting problem, 1) a single DLM for the entire sensor network (without topological information), 2) separate DLMs (K = 5) for each freeway (blockdiagonal DLM), 3) and the proposed model that is a single DLM (K = 5) with topological information. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, the proposed model shows the best performance, followed by block-diagonal DLM and single DLM without topological information. The proposed model induces the sensor's topological information through heat diffusion kernels to give weights to each element of this transition matrix and focus on estimating more essential components, resulting in it as a sparse matrix, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. As a result, it shows excellent performance in long-term prediction by effectively estimating off-diagonal elements (correlation between signals of sensors installed on different freeways) while avoiding the overfitting problem. In the case of the model with a single DLM, all elements of this matrix are estimated using historical data, while in the case of the model with separate DLMs, only the block diagonal elements are estimated (red shaded area). Therefore, since the former one needs to estimate a much larger number of elements from the data than the latter, an overfitting problem may occur. In contrast, in the separate DLMs, the historical data cannot be fully utilized due to the lack of association between sensors belonging to different freeways. In particular, this insufficiency causes degradation of long-term predictions as congestion propagates slowly from one freeway to others.</p><p>The low prediction error is obtained only when the topological information is optimally implanted into the DLM. Bayesian inference in our model is the key component to support this process, as it optimally estimates various parameters that characterize the mixing ratio between data and prior, which respectively correspond to DLM and topological information. We set up the following experiment to find test the effectiveness of this estimation method: 1) the model with measurements (Eq. (2)), 2) the model with topological information <ref type="figure" target="#fig_0">(Eq. (15)</ref>), 3) and the model with both topological information and measurements <ref type="figure" target="#fig_0">(Eq. (19)</ref>). For all the above models, we set three different cases that are characterized by different sizes of the training sets with the same test set.    show the prediction accuracy of each case. Interestingly, the model with measurements produced smaller errors when the size of the training set is smaller. The reason is that each training set period is close to that of the test set with respect to time, which means larger training sets contain measurement that are far from those in the test sets. This may distort the inference process as traffic measurements have seasonal patterns. On the other hand, the model using only the topological information showed poor performance in predicting the far future because mixture kernels do not represent well the change in traffic conditions due to the volume preservation characteristic. The model with both topological information and measurements showed the best performance and similar outputs regardless of the size of the training set. It shows that Bayesian inference estimates parameters ? t and ? t in Eq. <ref type="bibr" target="#b18">(19)</ref> optimally, extracting maximal information both from data and prior. As the size of the training set increases, the data contribution increases since the larger training set can generalize measurements more easily. Another important aspect from the results is that the data contribution increases during peak periods such as morning and evening peaks since the traffic volume is most likely not preserved during these periods (therefore, it is difficult to explain it only with diffusion processes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of different diffusion periods</head><p>We evaluated the proposed method with different diffusion processes (short, long, and mixture of both) in order to examine how the model of Eq. (9) performs in different settings. The transition matrix? t was set from Eq. (19) with three different diffusion priors:</p><formula xml:id="formula_44">1) H G t (T ) = H G (? 0 ) = lim ? ?0 e ?? L(G) (short diffusion kernel; identity mapping), 2) H G t (T ) = H G (? ? ) = lim ? ?? e ?? L(G) (long diffusion kernel; averaging), 3) and H G t (T ) = ? (?0) t H G (? 0 ) + ? (??) t</formula><p>H G (? ? ) (mixture of short and long diffusion kernels).</p><p>We also set three different cases that are characterized by different sampling intervals (T s ), 5, 10, and 15 minutes. The sampling interval indicates the time duration that corresponds to the one-time incremental (the difference between t + 1 and t). The sampling interval is related to the diffusion period ? as a diffusion kernel expresses how traffic signals diffuse through a graph within a sampling interval. <ref type="figure" target="#fig_9">Figures 6(a)</ref>-(c) show the prediction accuracy of each diffusion prior on the transportation network G 1 with the three different sampling intervals. The predictor with the long diffusion process showed relatively poor performance compared to the baseline method for small prediction horizons, but it was improved when prediction horizons become larger. On the other hand, the one with the short diffusion process showed relatively good performance compared to the baseline method for all prediction horizons; however, it had insufficient performance for large prediction horizons compared to the one with the long diffusion process. The mixture model takes advantage of the two extreme cases, significantly improving the performance for both small and large prediction horizons. Specifically, around 50 minutes prediction horizon in <ref type="figure" target="#fig_9">Fig. 6(a)</ref>, the performance of the mixture model is noticeably better than the others, meaning that a mixture of poor predictors can produce a good performance.</p><p>We emphasize that the distribution of the diffusion processes (? t ) was determined optimally by Bayesian inference. in the mixture model that corresponds to the short and long diffusion processes, respectively. Although the short diffusion process dominates the whole process, as shown in the figures, the small portion of the long diffusion process contributes to the improvement. More importantly, the ratio becomes smaller when the sampling interval increases. It shows that Bayesian inference performs well in optimally determining parameters, since the performance of the mixture model stays similar when the sampling interval is changed.</p><p>We also emphasize that the ratio depends on time. For example, during the early morning, the diffusion kernel with long diffusion period (? ? ) contributes more to the prediction performance although short diffusion (identity mapping) seems to be a more reasonable choice as there are few changes in traffic during that time. However, if the signal values are relatively uniform (in the case of a traffic signal at early morning), taking an average can remove noise while minimizing signal distortion as x t+1 ? x t (identity) ? 1 N 11 T x t (averaging; robust to noise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with state-of-the-art technologies</head><p>We compare the proposed method with other methods using a benchmark dataset: PEMS-BAY dataset <ref type="bibr" target="#b10">[11]</ref>. For a fair comparison, we use the same settings which are defined in <ref type="bibr" target="#b10">[11]</ref> (also same in <ref type="bibr" target="#b18">[19]</ref>) <ref type="bibr" target="#b1">2</ref> . The models used for the comparison are as follows.</p><p>1) FC-LSTM (Fully Connected Long Short-Term Memory): This model has been used as a representative reference for time-sequence modeling in deep learning <ref type="bibr" target="#b28">[29]</ref>. In general, the LSTM module extracts correlations of signals farther apart in time than the RNN structure. However, this model's disadvantage is that spatial correlations can only be expected to learn directly from data as there is no separate module for extracting spatial relationships of signals. The RMSE score for PEMS-BAY dataset is retrieved from <ref type="bibr" target="#b10">[11]</ref>.</p><p>2) STGCN: Yu et al. <ref type="bibr" target="#b14">[15]</ref> extracted spatial features with Graph Convolutional Neural Network (CNN) utilizing spectral graph convolution in graph theory. After that, they attached Gated CNN block to extract temporal features.</p><p>3) DCRNN (Diffusion Convolution Recurrent Neural Network): Li et al. <ref type="bibr" target="#b10">[11]</ref> constructed a successful predictor by extracting the signal's spatial features from the underlying graph structure by diffusion convolutional layers. Compared to STGCN, they designed the filter in the spatial domain directly rather than the graph spectral domain. The authors combine this diffusion module to Gated Recurrent Unit (GRU) which is a Recurrent Neural Network (RNN) variant. 4) Graph WaveNet: Xu et al. <ref type="bibr" target="#b18">[19]</ref> improved DCRNN by using dilated 1D convolution (also called WaveNet) to  <ref type="table" target="#tab_1">Table II</ref> shows the RMSE of each model and our proposed method. We confirm that the performance of the proposed method reaches that of state-of-the-art methods based on a complex deep learning architecture. It even performs better for long-term prediction as we model based on DLM that explicitly expresses the daily periodicity of traffic signals. For example, the RMSEs of our proposed method for 90 and 120 min horizons are respectively 4.70 and 5.26, while these are 5.26 and 6.02 with the pre-trained DCRNN model. <ref type="bibr" target="#b2">3</ref> Our proposed method requires lower computational effort compared to the others. Also, it infers the majority of the parameters (N 2 ) analytically by Eq. <ref type="bibr" target="#b18">(19)</ref>. The method only requires numerical computation when it solves the optimization problem (25) to infer K + 2 parameters, which has O(K 2 ) complexity, where K 2 is noticeably smaller than N 2 . Note that the hyperparameters are optimally estimated by solving the optimization problem (25) rather than the cross-validation method. As hyperparameter tuning is an expensive task, it can be a major advantage of the proposed method.</p><p>On the other hand, all state-of-the-art methods require heavy numerical computations to train a large number of parameters as they are based on deep-neural-net architectures. Our method successfully infers all parameters at the time scale of minutes with CPU computations, which is noticeably shorter than other DNN based methods with GPU computations as shown in <ref type="table" target="#tab_1">Table III</ref> (note that the DNN based methods required from 50 epochs to 100 epochs to converge).</p><p>Another advantage of our model compared to the deeplearning-based architectures is that only a small number of parameters need to be decided heuristically. This can provide easy scalability to apply our model to other traffic datasets or datasets with similar properties to traffic data (daily periodicity). For example, in our model, the parameters to be determined before training are the threshold constant ?, the kernel width ? to build a proper graph, and the number of diffusion processes K to determine how many diffusion processes should be mixed. We empirically choose the constants ? and ? such that the corresponding graph G is a k-vertexconnected graph with a small number k. For the number of diffusion processes K, we set K = 5 for the PEMS-BAY dataset but the prediction performance is not sensitive to the parameter (?0.01 minutes changes of the RMSE score from K = 3 to K = 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a method for predicting traffic signals in transportation sensor networks. We successfully integrated topological information of the sensor network into a data-driven model by assuming that the parameters in the model are supported by the mixture of diffusion kernels with uncertainties. We exploited the Bayesian inference to optimally determine the parameters that characterize the distribution of diffusion processes and the importance of measurements against prior information. The importance varies with time, and we discover that the data are relatively more important, especially for the peak period. Most importantly, the proposed method reached accurate prediction at the level of state-of-the-art methods with less computational effort. It particularly shows excellent performance in long-term predictions by exploiting DLM's periodicity modeling. Our method can be applicable for predicting graph signals exhibiting daily patterns such as weather or energy consumption. For future works, we may improve the short-term prediction performance if we give more valuable prior information (e.g., graph structure more suitable for prediction; currently, it only depends on topology), or if it is possible to derive all inference processes (especially the marginalization steps in Eq. (40) and (35)) with a non-linear model overcoming the limitation of linear models. eigen-decomposition of the matrix be</p><formula xml:id="formula_45">L(G) = VDV T ,<label>(32)</label></formula><p>where the orthonormal matrix V and the diagonal matrix D contain eigenvectors and corresponding eigenvalues, respectively. Since the orthonormal matrix V contains the eigenvector 1 ? N 1, 1 Txd t+1 (? )</p><formula xml:id="formula_46">(8) = 1 T H G (? )x d t (5) = 1 T e ?? L(G) x d t = 1 T Ve ?? D V T x d t = N ? N 1 ? N 1 T x d t = 1 T x d t .<label>(33)</label></formula><p>Therefore,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Inference of parameters 1: function INFERENCE(W, K, X 1:T ) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Transportation sensor networks (District 7 area in California) that are used for evaluating the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>min] Root mean square error (RMSE)[mph]    Prediction accuracy (RMSE) for the three different models on the PEMS-BAY dataset. Each model represents respectively a single DLM (without topological information), separate multiple DLMs for each freeway, and the proposed model (a DLM with topological information).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The heatmap of the elements in an estimated transition matrix Ht of the proposed model. Darker colors represent larger absolute values. The sensors are grouped by freeways and ordered from upstream to downstream within each freeway. Each axis shows the name of the freeways. The sensors' correlations within the same freeway are represented as red-shaded areas (block-diagonal elements of the matrix). The separate multiple DLMs only use block diagonal elements in the matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>73 days for training/test set; 8:2 ratio) Accuracy of the prediction and the data contribution for different training-test set ratio. The baseline method predicts future traffic features assuming that the current traffic does not change over time, i.e., x baseline x+h|t = xt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figures 5 (</head><label>5</label><figDesc>a)-(c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figures 5 (</head><label>5</label><figDesc>d)-(f) show the data contribution which is defined in Eq. (20) of the mixture model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>min] Root mean square error (RMSE) [mph] (a) Prediction accuracy (Ts =5 min.) min] Root mean square error (RMSE) [mph] (b) Prediction accuracy (Ts =10 min.) min] Root mean square error (RMSE) [mph](c) Prediction accuracy (Ts =15 min.) Accuracy of the prediction and ratio of the short and long diffusion processes for the same test set with different time intervals. The baseline method predicts future traffic features assuming that the current traffic does not change over time, i.e., x baseline x+h|t = xt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figures 6(d)-(f) show the ratio of the coefficients ? (?0) t and ? (??) t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>NOTATIONS AND DEFINITIONS USED IN THIS ARTICLE.</figDesc><table><row><cell>R m</cell><cell>m-dimensional Euclidean space</cell></row><row><cell>a, a, A</cell><cell>Scalar, vector, matrix</cell></row><row><cell>diag(a)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II RMSE</head><label>II</label><figDesc>OF DIFFERENT METHODS FOR PEMS-BAY DATASET. Pan et al. [30] introduced graph attention network to extract spatial features. They utilize RNN architecture to extract temporal features.</figDesc><table><row><cell>Horizon</cell><cell cols="3">15 min 30 min 60 min</cell></row><row><cell>FC-LSTM [29]</cell><cell>4.19</cell><cell>4.55</cell><cell>4.96</cell></row><row><cell>DCRNN [11]</cell><cell>2.95</cell><cell>3.97</cell><cell>4.74</cell></row><row><cell>STGCN [21]</cell><cell>2.96</cell><cell>4.27</cell><cell>5.69</cell></row><row><cell>Graph WaveNet [19]</cell><cell>2.74</cell><cell>3.70</cell><cell>4.52</cell></row><row><cell>ST-MetaNet [30]</cell><cell>2.90</cell><cell>4.02</cell><cell>5.06</cell></row><row><cell>Proposed</cell><cell>2.90</cell><cell>3.77</cell><cell>4.44</cell></row><row><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="4">COMPUTATION COSTS FOR TRAINING ON THE PEMS-BAY DATASET</cell></row><row><cell>Model</cell><cell></cell><cell>Training(s)</cell><cell></cell></row><row><cell>DCRNN [11]</cell><cell cols="2">750 (per epoch)</cell><cell></cell></row><row><cell cols="3">Graph WaveNet [19] 580 (per epoch)</cell><cell></cell></row><row><cell>Proposed</cell><cell></cell><cell>760 (total)</cell><cell></cell></row><row><cell cols="4">extract temporal features in terms of computation time and</cell></row><row><cell>performance.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5) ST-MetaNet:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our code is available at: https://github.com/semink/lsdlm/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">As GraphWaveNet predicts all the horizons at once (not recursive), we could not use the pre-trained model for the longer horizons. As a result, we choose DCRNN which shows the second-best result on 60 min horizon.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(34)</p><p>B. Evidence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Posterior of x t+h</head><p>When h = 1,</p><p>where? t+l?2?t =? t+l?2?t+l?3 ? ? ?? t . By the chain rule,</p><p>(40) Applying matrix inversion lemma, Eq. (40) becomes f (x t+l |x t , X t+l , ? ? ? , x t ) = N (? t+l?1?t x t , R t+l?1 ). (42) Finally x t+h|t =? t+h?1 ? ? ?? t x t .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multivariate temporal convolutional network: A deep neural networks approach for multivariate time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">876</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sembnet: A semantic bayesian network for multivariate prediction of meteorological time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="192" to="201" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A combined multivariate model for wind power prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Conversion and Management</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="361" to="373" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiple convolutional neural networks for multivariate time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="page" from="107" to="119" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dsanet: Dual self-attention network for multivariate time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on information and knowledge management</title>
		<meeting>the 28th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2129" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predictions of freeway traffic speeds and volumes using vector autoregressive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Al-Deek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="72" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multivariate shortterm traffic flow forecasting using bayesian vector autoregressive moving average model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Travel time prediction for congested freeways with a dynamic linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Geroliminis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lasso vector autoregression structures for very shortterm wind power forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavalcante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Bessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Browell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wind Energy</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="657" to="675" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High dimensional forecasting via interpretable vector autoregression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wilms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Matteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">166</biblScope>
			<biblScope unit="page" from="1" to="52" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Henrickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gated residual recurrent graph neural networks for traffic prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="485" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial-temporal graph attention networks: A deep learning approach for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="166" to="246" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3634" to="3640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">T-gcn: A temporal graph convolutional network for traffic prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph wavenet for deep spatial-temporal graph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1907" to="1913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Connecting the dots: Multivariate time series forecasting with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="753" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph wavelet neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multivariate time series forecasting via attention-based encoderdecoder framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">388</biblScope>
			<biblScope unit="page" from="269" to="279" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep multi-view spatial-temporal network for taxi demand prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An end-to-end adaptive input selection with dynamic weights for forecasting multivariate time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Amarbayasgalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Batbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Ryu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="99" to="099" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Univariate versus multivariate time series forecasting: An application to international tourism demand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Preez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Witt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="435" to="451" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Traffic and related self-driven manyparticle systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of modern physics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1067</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diffusion kernels on graphs and other discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on machine learning</title>
		<meeting>the 19th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="315" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="447" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A limited memory algorithm for bound constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Urban traffic prediction from spatio-temporal data using deep meta learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1720" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Volume conservation of mixture of heat diffusion By definition (in Eq</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>)), the graph Laplacian L(G) has an eigenvector 1</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">? N 1 with the corresponding eigenvalue 0</title>
		<imprint/>
	</monogr>
	<note>Let an</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
