<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">M4Depth: Monocular depth estimation for autonomous vehicles in unseen environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Fonder</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Ernst</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Van</forename><surname>Droogenbroeck</surname></persName>
						</author>
						<title level="a" type="main">M4Depth: Monocular depth estimation for autonomous vehicles in unseen environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Depth estimation</term>
					<term>deep learning</term>
					<term>unmanned vehi- cles</term>
					<term>disparity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating the distance to objects is crucial for autonomous vehicles when using depth sensors is not possible. In this case, the distance has to be estimated from on-board mounted RGB cameras, which is a complex task especially in environments such as natural outdoor landscapes. In this paper, we present a new method named M4Depth for depth estimation. First, we establish a bijective relationship between depth and the visual disparity of two consecutive frames and show how to exploit it to perform motion-invariant pixel-wise depth estimation. Then, we detail M4Depth which is based on a pyramidal convolutional neural network architecture where each level refines an input disparity map estimate by using two customized cost volumes. We use these cost volumes to leverage the visual spatio-temporal constraints imposed by motion and to make the network robust for varied scenes. We benchmarked our approach both in test and generalization modes on public datasets featuring synthetic camera trajectories recorded in a wide variety of outdoor scenes. Results show that our network outperforms the state of the art on these datasets, while also performing well on a standard depth estimation benchmark. The code of our method is publicly available at https://github.com/michael-fonder/M4Depth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>E ESTIMATING accurate dense depth maps is an essential task for the planning of unmanned vehicle trajectories <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. To the best of our knowledge, the only reliable longrange distance sensors suitable for outdoor are heavy, bulky and power consuming. Consequently, they can hardly be used on vehicles such as drones where weight, size, and power availability are constrained. Distances between objects and the camera therefore need to be inferred instead of being measured. This is possible when an on-board RGB camera mounted on a vehicle is led to see the same objects from multiple viewpoints.</p><p>Small and lightweight unmanned vehicles are perfect devices to reach places otherwise barely accessible and, hence, are often use to venture off road in environments where direct cues about distance, such as object structures, are unknown. Natural landscapes are examples of such environments whose elements (vegetation, ground and relief) do not exhibit normalized structures or patterns. Datasets suitable to study the task of depth estimation in environments with little structure -we call them "unstructured environments" in the following-were <ref type="figure">Figure 1</ref>. State-of-the-art depth estimation methods such as DPT <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> or ManyDepth <ref type="bibr" target="#b4">[5]</ref> struggle to produce accurate estimates in cluttered and natural environments. Our method, called M4Depth, outperforms existing methods in these instances and generalizes well to unknown environments.</p><p>proposed only recently (see <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>). As a result, existing methods for outdoor depth estimation are optimized for older benchmarks targeting autonomous driving applications, on datasets featuring constrained trajectories and environments. Indeed, the motion of a car on a road is strongly constrained, and urban environments contain many objects with a specific structure, such as cars, roads, signs, buildings, etc. Since structure and semantics of elements in a scene likely provide direct cues about depth, it is uncertain if existing methods perform well in environments where such cues are not available.</p><p>In this paper, we address the challenging task of estimating depth in unstructured environments. The formulation of this task and the related work are presented in Sections II and III, respectively. Then, in Section IV, we describe a dedicated depth estimation method based on a deep neural network. To perform well when inferring depth for specific environments or in generalization, we use both the image sequence and the camera motion, which enables one to derive depth from pixel displacement estimates. In Section V, we detail our experimental setup which is aimed at evaluating methods on unstructured environments and in generalization, present results, and discuss our method. Section VI concludes the paper.</p><p>Our contributions can be summarized as follows: arXiv:2105.09847v3 [cs.CV] 1 Jul 2022 1) We extend the notion of visual disparity to camera baselines featuring six degrees of freedom <ref type="bibr">(6 DoF)</ref> transformations, and present customized cost volumes for this disparity. 2) We present a novel real-time and lightweight multi-level architecture based on these cost volumes to perform endto-end depth estimation on video streams acquired in unstructured environments. 3) It is shown that M4Depth, is state of the art on the Mid-Air dataset <ref type="bibr" target="#b5">[6]</ref>, that it has good performances on the KITTI dataset <ref type="bibr" target="#b7">[8]</ref>, that it outperforms existing methods in a generalization setup on the TartanAir dataset <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM STATEMENT</head><p>First, we present the technicalities of the problem we want to solve. We consider a camera rigidly attached to a vehicle moving within an unknown static environment. The intrinsic parameters of the camera are supposed to be known and constant. We introduce the following components and notations:</p><p>? I t is an RGB image of size H ? W recorded by the camera at time step t. Images have the following properties: 1) motion blur and rolling shutter artifacts are negligible; 2) the camera focal length f is known and constant during a flight; 3) camera shutter speed and gain are unknown, and can change over time. ? T t is the transformation matrix encoding the motion of optical center of the camera from time step t?1 to t. This matrix is assumed to be known, which is realistic when the camera motion is monitored such as with drones. ? z ij,t is the z coordinate (in meters) of the point recorded by the pixel at coordinates (i, j) of the frame I t with respect to the camera coordinate system.</p><p>Using these notations, a depth map d t is an array of z ij,t values with ij ? {1, . . . , W } ? {1, . . . , H}.</p><p>We denote by h t the complete series of image frames and camera motions up to time step t. We define a set D of functions D that are able to estimate a depth map d from h t , that isd t = D(h t ), such that D ? D, with h t = [I 0 , [I 1 , T 1 ], ... , [I t , T t ]]. Our objective is to find function D in this set that best estimates d t .</p><p>Since collision avoidance is essential for autonomous vehicle applications, errors in the estimate for closer objects should have a higher impact than errors occurring for objects in the background of the scene. This is taken care of by constructing a dedicated loss function for training and by minimizing the error relatively to the distance of the object. During testing, we will use the set of performance metrics defined by Eigen et al. to better grasp the behavior of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>Related works are presented according to four different categories.</p><p>Depth from a single image. Estimating depth from a single RGB image is an old and well-established principle. If the first methods were fully handcrafted <ref type="bibr" target="#b9">[9]</ref>, the growth of machine learning and the development of CNNs has impacted on the field of depth estimation through the introduction of new methods such as Monodepth <ref type="bibr" target="#b10">[10]</ref>, Monodepth2 <ref type="bibr" target="#b12">[11]</ref>, or the method proposed by Pogi et al. <ref type="bibr" target="#b14">[12]</ref> that have led to massive improvements in the quality of the depth estimates. Methods based on vision transformer networks, such as DPT <ref type="bibr" target="#b2">[3]</ref>, push the performance even further and are currently the state of the art in the field.</p><p>Recent surveys <ref type="bibr" target="#b15">[13]</ref>- <ref type="bibr" target="#b17">[15]</ref> present summarized descriptions and comparisons of single image depth estimation methods. The main observation made in these surveys is that estimating depth from a single picture remains difficult, especially for autonomous vehicle applications. Since the problem is illposed, networks have to heavily rely on priors to compute a suitable proposal. Such dependency on priors leads to a lack of robustness and generalization. Therefore, methods of this family need to be fine-tuned for every new scenario or environment encountered in order to produce good estimates. Despite their massive parameter count, transformers are no exception to these observations, as illustrated in <ref type="figure">Fig. 1</ref>.</p><p>Depth from an image sequence. Methods exist that include recurrence in networks to make use of temporal information for improving estimates <ref type="bibr" target="#b18">[16]</ref>- <ref type="bibr" target="#b23">[20]</ref>. They are mainly adaptations of existing architectures, by adding or modifying specific layers. As such, these methods do not make direct use of motion information.</p><p>This issue has been thoroughly addressed by Watson et al. <ref type="bibr" target="#b4">[5]</ref>. Their method, named ManyDepth, includes a model for motion and uses, among other things, a cost volume built with the plane-sweeping method <ref type="bibr" target="#b24">[21]</ref>, <ref type="bibr" target="#b25">[22]</ref>. In this method, both depth and motion are learned in an unsupervised fashion. By ignoring the real camera motion, methods that work only with sequences are unable to estimate the proper scale for depth without relying on any prior knowledge about the structure of the scene. Even worse, nothing prevents the scale estimated for the outputs to drift over the sequence. This is problematic for autonomous vehicles moving in unstructured environments.</p><p>Use of motion information. When motion is used by depth estimation methods, it is mostly exploited to build a loss function for self-supervised training <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b19">[17]</ref>, <ref type="bibr" target="#b20">[18]</ref>, <ref type="bibr" target="#b26">[23]</ref>, <ref type="bibr" target="#b27">[24]</ref>. In these cases, motion and depth are learnt by two independent networks in an unsupervised fashion and depth is still estimated without any clue about motion. As the core of these depth estimation networks does not change compared to methods simply working on sequences, their estimations suffer from the same issues as the ones produced by methods that do not use motion.</p><p>One notable exception is the idea proposed by Luo et al. <ref type="bibr" target="#b28">[25]</ref>. Their method uses an self-supervised loss based on motion estimation to fine-tune the network at test time. It achieves outstanding performance, but at the cost of a large computational burden. Furthermore, this method cannot estimate depth before the whole image sequence is available, meaning that it 3D reconstruction. Structure from motion (SfM) and multiview stereo (MVS) are two research fields that have developed in parallel with depth estimation. The idea is to reconstruct 3D shapes from a set of RGB images that capture the scene from different points of view under specific hypothesis (MVS requires known camera poses, SfM does not). Reconstruction is achieved by explicitly expressing the relative camera position between the images of the set. Approaches for performing this task are varied <ref type="bibr" target="#b29">[26]</ref> and are, by their nature, often unsuitable for real-time depth estimation. However, some are adaptable for depth estimation on sequences <ref type="bibr" target="#b30">[27]</ref>- <ref type="bibr" target="#b32">[29]</ref>, while others are specifically designed to work on image sequences in real time <ref type="bibr" target="#b33">[30]</ref>, <ref type="bibr" target="#b34">[31]</ref>.</p><p>The approach proposed by Duzcceker et al. <ref type="bibr" target="#b33">[30]</ref> and the method called DeepV2D by Teed et al. <ref type="bibr" target="#b34">[31]</ref> are similar. They both propose a three-stage network. Their stages are an image-encoding network followed by the computation of a cost volume that is finally processed by a depth estimation network. The purpose of the cost volume consists of providing the costs for matching a point in an image with a series of candidates in another image. The cost volume of both methods is built by a plane-sweeping method <ref type="bibr" target="#b24">[21]</ref>, <ref type="bibr" target="#b25">[22]</ref>.</p><p>Our baseline. Based on this related work, we have selected a representative set of existing methods for which the training code is available, as given in <ref type="table" target="#tab_0">Table I</ref>; they constitute the baseline for our test bench. In this table, we indicate for each method, respectively, the nature of its supervision mode, if it is based on a single or multiple frames, if it is recurrent, how it deals with the camera pose, and if weights for the KITTI are provided by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DESCRIPTION OF M4DEPTH</head><p>Inspired by the previous works, we decided to base M4Depth on a multi-level architecture trainable in an end-to-end fashion that relies on cost volumes. The key novelty of M4Depth is the construction of a visual disparity map for 6 DoF motion, that is converted into a depth map by using motion information. This is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deriving depth from visual disparity</head><p>In stereo vision, the relative pose between the cameras is fixed and depth is estimated by calculating the disparity between the two camera views. In multi-view approaches, the objective is either to calibrate the cameras or to reconstruct a point cloud of the scene. In our setup, the camera moves along a path known thanks to the on-board inertial measurement unit. Since we only know the relative motion between two camera locations which is not fixed, we have to define an appropriate notion of visual disparity to be able to estimate a dense depth map from the pixelwise frame-to-frame pixel displacement vectors. By doing so, the network only has to estimate the visual disparity to compute depth.</p><p>Our notion of visual disparity, new to the best of our knowledge, denoted by ? is established as follows. The transformation matrix T t formalizing the known physical camera motion with 6 DoF between consecutive frames of the video stream can be decomposed as a rotation matrix R t and a 3D translation vector t t . Using the classical pinhole camera model, a point P in space seen by the camera at two different time instants t and t ? 1, and projected at coordinates (i t , j t ) in the current frame t is linked to its previous coordinates (i t?1 , j t?1 ) in frame at time t?1 by the motion T t as follows</p><formula xml:id="formula_0">z it?1jt?1 ? ? i t?1 j t?1 1 ? ? = K ? ? R t z itjt ? ? i t /f x j t /f y 1 ? ? + t t ? ? , (1) where z itjt = d t (i t , j t )</formula><p>is the depth of the point P at time t, and K is a camera calibration matrix, which is identical at times t and t ? 1. It is reasonable to simplify the expression of the 3 ? 3 K matrix to</p><formula xml:id="formula_1">K = diag(f x , f y , 1) ,<label>(2)</label></formula><p>with f x and f y being the focal lengths along the x and y axes respectively. This assumes that the coordinates (i, j) are expressed directly with respect to the principal point of the sensor (c x , c y ) and that the skew parameter is negligible.</p><p>Before defining our visual disparity, we rewrite Equ. (1) as</p><formula xml:id="formula_2">z it?1jt?1 ? ? i t?1 j t?1 1 ? ? = z V z itjt ? ? i V j V 1 ? ? + ? ? f x t x f y t y t z ? ? ,<label>(3)</label></formula><p>with</p><formula xml:id="formula_3">[z V i V , z V j V , z V ] T = K R [i t /f x , j t /f y , 1] T .<label>(4)</label></formula><p>From this equation, we can see that (i V , j V ) are the coordinates of the point P in the plane of a virtual camera V whose origin is the same as the camera at time t but with the orientation of the camera at time t ? 1.</p><p>We now define our pixelwise visual disparity ? itjt as the Euclidean norm</p><formula xml:id="formula_4">? itjt = ? 2 it + ? 2 jt<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">? it ? jt = i t?1 ? i V j t?1 ? j V .</formula><p>After reorganization, using Equ. (3) and simplification, we get</p><formula xml:id="formula_6">? it ? jt = 1 z itjt z V + t z f x t x ? t z i V f y t y ? t z j V .<label>(6)</label></formula><p>By taking into account the physics of a scene and the camera motion of an autonomous vehicle, it can be shown that z itjt z V +t z should rarely be negative. As a result, the disparity ? itjt can be computed as follows</p><formula xml:id="formula_7">? itjt = (f x t x ? t z i V ) 2 + (f y t y ? t z j V ) 2 z itjt z V + t z .<label>(7)</label></formula><p>This expression links the disparity for a pixel to the depth of the corresponding point in space. Since disparity can be estimated from the RGB content of two consecutive images, we have a mean to estimate the depth by inverting the equation, yielding</p><formula xml:id="formula_8">z itjt = (f x t x ? t z i V ) 2 + (f y t y ? t z j V ) 2 ? itjt z V ? t z z V .<label>(8)</label></formula><p>In practice, there are different ways to estimate ? itjt , and in our method, we build various proposals for ? itjt and let the network use them to compute the best estimate. Note that, once a disparity map ? t has been estimated, the (i t?1 , j t?1 ) coordinates are given by a function ?, parametrized as follows</p><formula xml:id="formula_9">(i t?1 , j t?1 ) = ?(i t , j t , T t , ? t ) .<label>(9)</label></formula><p>These (i t?1 , j t?1 ) coordinates are defined on a continuous space instead of a discrete grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Definition of the network</head><p>To find the disparity, we could resort to the point triangulation process, which is by nature an iterative process. Instead of simply iterating on a full network as proposed in <ref type="bibr" target="#b31">[28]</ref> or bypassing the iteration by proposing a full range of candidates as in <ref type="bibr" target="#b33">[30]</ref>, <ref type="bibr" target="#b34">[31]</ref>, we approach the iterative process as a multiscale pyramidal network, as PWC-Net <ref type="bibr" target="#b36">[32]</ref>. By doing so, the iterative process is embedded in the architecture itself. This is an adaptation of the U-Net encoder-decoder architecture with skip connections <ref type="bibr" target="#b37">[33]</ref>, where each level l of the decoder has to produce an estimate for the desired output, that is a disparity map in our case. In the decoder, the estimate produced at one level is forwarded to feed the next level to be refined. The levels of this type of architecture are generic and can be stacked at will to obtain the desired network depth.</p><p>Our architecture, illustrated in <ref type="figure">Fig. 2</ref>, uses the same standard encoder network as PWC-Net <ref type="bibr" target="#b36">[32]</ref> with the only exception that we add a Domain-Invariant Normalization layer (DINL) <ref type="bibr" target="#b38">[34]</ref> after the first convolutional layer. We use it to increase the robustness of the network to varied colors, contrasts and luminosity conditions without increasing the number of convolutional filters.</p><p>At each level L of the decoder, a small convolutional subnetwork is in charge of refining the disparity map. We named it the disparity refiner. Its inputs are the upscaled disparity estimate made by the previous decoder level in the architecture and a series of preprocessed data generated by a preprocessing unit.</p><p>The preprocessing unit is illustrated in <ref type="figure">Fig. 3</ref>. It is made of fixed operations and has no learnable parameters. Its purpose is to prepare the input for the next disparity refiner. In short, the preprocessor has two main purposes. First, it adapts the vectors of the feature maps produced by the encoders to make the network robust to unknown visual inputs. For that, it uses this data alongside camera motion to build two distinct cost volumes, the Disparity Sweeping Cost Volume (DSCV) and the Spatial Neighborhood Cost Volume (SNCV). Second, it recomputes the disparity estimate obtained for the previous time by adjusting it to the camera motion. These data are then concatenated and forwarded to the disparity refiner.</p><p>Let us now detail these operations and motivate their use.</p><p>Split and normalize layers. The use of leaky ReLU activation units in the encoder can lead to a feature maps containing plenty of small values. While classification or segmentation networks rely on the raw value of each entry in a feature vector, our network relies on the relative differences between neighboring feature vectors through the use of cost volumes.</p><p>To get good generalization properties, this relative difference should remain significant in all situations. The split and normalize layers ensure that this is the case.</p><p>The split layer subdivides feature vectors in K sub-vectors to be processed in parallel in subsequent layers. It gives the network the ability to decouple the relative importance of specific features within a same vector by assigning them to different sub-vectors.</p><p>The normalize layer normalizes the features of a same subvector and therefore levels the difference in magnitude of different sub-vectors. This is beneficial for the disparity refiner layers as this normalization leads the outputs of the cost volumes to span to a known pre-defined range. It also allows a full use of the information embedded in sub-vectors whose magnitude is very small because of the leaky ReLU activation units.</p><p>Recompute layer. The disparities estimated by the network are specific to the motion occurring between two given frames. By using the set of equations developed in Section IV-A and if the camera motion is known, it is possible to compute the disparities that should be observed at a given time step from a previous disparity estimate. The purpose of the recompute layer is to update the disparities estimated for the previous frame to provide an hint in the form of a first estimate of the disparities for the current frame.</p><p>Spatial Neighborhood Cost Volume (SNCV). This cost volume is computed from a single feature map f and is a form of spatial autocorrelation. Each pixel of the cost volume is assigned the costs of matching the feature vector located at the same location in the feature map with the neighboring feature vectors located within a given range r of the considered location  <ref type="figure">Figure 2</ref>. Architecture overview of M4Depth (with three levels here), fed by two consecutive frames and the camera motion. Each disparity refiner produces a disparity estimate and learnable disparity features. All convolutions are followed by a leaky ReLU activation unit <ref type="bibr" target="#b39">[35]</ref>, except for the ones producing a disparity estimate. To ease the convergence, disparities are encoded in the log-space.  <ref type="figure">Figure 3</ref>. Details of the operations performed by our preprocessing units. They do not feature any learnable parameters. The split layer subdivides feature vectors in K sub-vectors to be processed in parallel in subsequent steps. We give the value of K that we use for an architecture with six levels.</p><formula xml:id="formula_10">SNCV r (f )(i, j) = [ cost (f (i, j), f (i + p, j + q)) ? p, q ? {?r, . . . , r} ] ,<label>(10)</label></formula><p>where the cost of matching two vectors x 1 and x 2 of dimension N is defined as their correlation <ref type="bibr" target="#b40">[36]</ref>, <ref type="bibr" target="#b41">[37]</ref> cost(x 1 ,</p><formula xml:id="formula_11">x 2 ) = 1 N x T 1 x 2 .<label>(11)</label></formula><p>The SNCV gives an indication about the two-dimensional spatial structure of the scene captured by the features of the encoder. By design, it is impossible to recover the feature vectors that led to a given cost value. Network parameters trained with this cost metric will therefore be invariant to changes in the input feature vectors if they lead to the same cost value. This can help to obtain a robust and generalizable depth estimation network, which was not achievable by forwarding the feature map directly.</p><p>Disparity Sweeping Cost Volume (DSCV). This cost volume is computed from two consecutive feature maps f t?1 and f t , and a disparity map estimate? t (see left of <ref type="figure">Fig. 3</ref>). For each pixel, the cost volume assigns the cost of matching the feature vector located at the same place in f t with the corresponding reprojected feature vectors from f t?1 according to</p><formula xml:id="formula_12">DSCV ? (f t , f t?1 ,? t )(i, j) = [cost (f t (i, j), f reproj (i, j, ? ? )) ? ? ? ? {??, . . . , ?}] .<label>(12)</label></formula><p>In that expression, the feature map f t?1 is reprojected for a range ? of disparities equally distributed around a given estimate, that is</p><formula xml:id="formula_13">f reproj (i, j, ? ? ) = f t?1 (? (i, j, T t , max ( ,? t + ? ? ))) ,<label>(13)</label></formula><p>where ? is given by Equ. <ref type="bibr" target="#b9">(9)</ref>, and &gt; 0. In that expression, f reproj is interpolated from f t?1 since ? returns real coordinates , and max ( ,? t + ? ? ) ensures the positiveness of the disparity used for computing the reprojection. Each vector element of the cost volume corresponds to one given disparity correction with respect to the provided estimate. Browsing through a range of disparities for each pixel creates a series of candidates for the corresponding reprojected point. By searching for the reprojected candidate that is the most similar to the visual information observed at time step t, it is possible to assess which disparity is the most likely to be associated to each pixel.</p><p>Since we are using a pyramidal architecture, a range ? at the L-th level is equivalent to a range ?2 L in the original picture. As opposed to methods directly using depth for building a cost volume, this property helps with scanning a larger range of disparities while keeping a pixelwise precision in the final picture without having to check a large number of candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss function definition</head><p>Since the levels of our architecture are stackable at will, the architecture can have any depth. We now detail our loss function for a network that is made of M levels.</p><p>As in previous works <ref type="bibr" target="#b42">[38]</ref>- <ref type="bibr" target="#b44">[40]</ref>, we use a multi-scale loss function. For each frame and each level, we compute the L 1 distance on the logarithm of the depths resulting from the conversion of disparity estimates using Equ. <ref type="bibr" target="#b7">(8)</ref>. The logarithm leads to a scale invariant loss function <ref type="bibr" target="#b46">[41]</ref> and the use of an L 1 distance is motivated by its good convergence properties <ref type="bibr" target="#b47">[42]</ref>. Since intermediate depth maps have a lower resolution, ground truths are resized by bilinear interpolation to match the dimensions of the estimates. The resulting terms are aggregated through a weighted sum, yielding</p><formula xml:id="formula_14">L t = 1 HW M l=1 zij ?d l t 2 l+1 |log(z ij ) ? log(? ij )| .<label>(14)</label></formula><p>The total loss for the sequence is defined as the average of the loss computed for each of its time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we present three experiments to analyze the performance of our M4Depth method. For each of them, we detail the chosen dataset, the training (if appropriate), and discuss the results. Performance metrics are taken from Eigen et al. <ref type="bibr" target="#b46">[41]</ref> for depth maps limited to 80 m.</p><p>In order to compare our method with its parent, PWC-Net <ref type="bibr" target="#b36">[32]</ref>, we perform the same experiments for both M4Depth and PWC-Net. As PWC-Net is an optical flow network, we use Equ.</p><p>(1) to get the frame-to-frame optical flow from depth and motion for training the network. During testing, we estimate depth from the optical flow by first computing our visual disparity from the estimated optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unstructured environments</head><p>For our first experiment, we compare the performance of our method with the ones of the state of the art on a dataset featuring unstructured environments.</p><p>Mid-Air dataset. For this experiment, we use Mid-Air <ref type="bibr" target="#b5">[6]</ref>. This synthetic dataset consists of a set of drone trajectories recorded in large, unstructured and static environments under varied weather and lighting conditions. All trajectories are recorded in different places of the environments, which means that there is little overlap between the visual content of two individual trajectories. This allows one to build a test set whose content is not present in the training set while belonging to the same data distribution. In addition, Mid-Air meets all the assumptions of our problem statement (see Section II), which makes it a perfect choice.</p><p>The first performance reported on Mid-Air for depth estimation was provided recently by Miclea et al. <ref type="bibr" target="#b48">[43]</ref>. The authors of this paper do, however, not provide the details required to reproduce their train and test splits and their results. As a result, we have to define our own splits.</p><p>The dataset features 192 trajectories. We select one in three to create our test set, which is more varied than the small test set suggested in the original paper <ref type="bibr" target="#b5">[6]</ref>. The frame rate is subsampled by a factor four (from 25 to 6.25 fps) to increase the apparent motion between two frames. For all our experiments, images and depth maps are resized to a size of 384 ? 384 pixels. We use bilinear interpolation to resize color images and the nearest neighbor method for depth maps.</p><p>Training. We use the He initialization <ref type="bibr" target="#b49">[44]</ref> for our variables and the Adam optimizer <ref type="bibr" target="#b50">[45]</ref> for the training itself. We keep the default parameters proposed by both of them. The learning rate is set to 10 ?4 . We trained our network with six levels. All our trainings are performed on sequences of four time steps and with a batch size of three sequences. The network is trained on a GPU with 16 GB of VRAM for 220 k iterations. After each epoch we compute the performance of the network on the validation set of the KITTI dataset to avoid any overfitting, and keep a copy of the best set of weights to be used for the tests after the training.</p><p>A series of data augmentation steps are performed on each sequence during the training to boost the robustness of our trained network to visual novelties. More precisely, we apply the same random brightness, contrast, hue, and saturation change to all the RGB pictures of a sequence and the colors of a sequence are inverted with a 50% probability. Finally, we randomly rotate the data of the sequence by a multiple of 90 degrees around the z-axis of the camera when training on Mid-Air. With these settings, a training requires approximately 30 hours.</p><p>Because of the lack of reproducible performances reported on Mid-Air, we had to train a selection of state-of-the-art methods drawn in <ref type="table" target="#tab_0">Table I</ref> to build a baseline. The training details for the chosen methods are given in the supplementary material. We could not guarantee to get the best performance out of DeepV2D <ref type="bibr" target="#b34">[31]</ref> because of the importance of its hyperparameters and the excessive duration of its training time. We, therefore, decided to discard it for this experiment.</p><p>Results. The results are reported in <ref type="table" target="#tab_0">Table II. In this table and</ref> following tables, the best score for a metric is highlighted in bold and the second best is underlined.</p><p>Globally, it appears that M4Depth outperforms the baseline methods. However, it slightly underperforms on the relative performance metrics when compared to PWC-Net. This observation, compared with the excellent performances on other metrics, indicates that our network tends to overestimate depth more often than other methods. A qualitative comparison of the outputs of the different methods is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. From this figure, we observe that although M4Depth lacks of details in areas with sharp depth transitions, it recovers depth details more accurately than baseline methods, even for challenging scene elements such as forests or unstructured terrain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Standard depth estimation benchmark</head><p>The purpose of the second experiment is to assess the performance on a standard depth estimation benchmark.</p><p>KITTI dataset <ref type="bibr" target="#b7">[8]</ref>. Most real datasets that provide RGB+D and motion data focus on cars driving in partially dynamic urban environments <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b51">[46]</ref>, <ref type="bibr" target="#b52">[47]</ref>. In this field, KITTI is the  reference benchmark dataset when evaluating the performance of a depth estimation method. KITTI is not fully compliant with our problem statement: it has incomplete depth maps, there are some moving objects, and the camera has only three degrees of freedom, etc. Despite that, it is a good practical choice for doing tests on real data.</p><p>We use the dataset split proposed by Eigen et al. <ref type="bibr" target="#b46">[41]</ref>. The camera pose is estimated by a combined GPS-inertial unit and is therefore subject to measurement imperfections. Since a few samples were recorded in urban canyons where poor GPS reception induced erratic pose estimates, and as our method requires reliable pose estimates, we discarded these problematic samples from the splits. Additionally, we also subsampled the frame rate by a factor of two (from 10 to 5 fps) to roughly match the one of our Mid-Air sets. Finally, images were resized to 256 ? 768 pixels.</p><p>Training. For tests on KITTI, we reuse the weights of the network with 6 levels trained on Mid-Air and fine-tune them for 20 k additional iterations on a 50 ? 50% mix of KITTI and Mid-Air samples. The fine-tuning is required to train our network to deal with large areas with poor textures and frame-to-frame illumination changes as these characteristics are not present in Mid-Air. As the ground-truth depth maps for KITTI were generated from Lidar measurements, they are sparse and fine details are missing in the ground truths. Shortcomings created by these imperfections can be mitigated by fine-tuning on both datasets. During the fine-tuning, we also perform random color augmentation on the sequences. With these settings, the fine-tuning requires three hours.</p><p>Results. The performance of M4Depth with six levels on the KITTI dataset is reported in <ref type="table" target="#tab_0">Table III</ref>. We observe that M4Depth has similar performances to current state-of-the-art methods. As expected, instances with dynamic elements or poor GPS localization lead to degraded performances. These results however prove that M4Depth also works with real data despite their imperfections. An overview of the outputs of our method on KITTI is shown in <ref type="figure" target="#fig_6">Fig. 8</ref>. M4Depth appears to preserve fine details, and to estimated depth reliably even in areas with less texture or for glossy objects such as cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalization</head><p>In this last experiment, we want to evaluate the generalization capabilities of all the methods. For this, we want to use static scenes that are semantically close to either the Mid-Air dataset (natural unstructured environments) or the KITTI dataset (urban environments), and test the performance of the method trained on Mid-Air (respectively KITTI) on the selected unstructured (respectively urban) scenes without any fine-tuning.</p><p>TartanAir dataset <ref type="bibr" target="#b6">[7]</ref>. For this experiment, we use TartanAir. It is a synthetic dataset consisting of trajectories recorded by a free-flying camera in a series of different environment scenes. With each scene being relatively small in size, there is a lot of overlap in the visual content recorded for different trajectories within a same scene. As such, assembling clearly separated train and test sets drawn from the same data distribution is not possible. Despite this drawback, the diversity of the scenes makes TartanAir an interesting choice for testing the generalization capabilities of methods.</p><p>For the generalization test from the Mid-Air dataset, we select the "Gascola" and "Season forest winter" scenes of TartanAir and use the weights trained for the baseline. For the one from the KITTI dataset, we select the "Neighborhood" and "Old Town" scenes and use the pre-trained weights released by the authors of the methods.</p><p>We resized the images of this dataset to 384 ? 576 pixels and subsampled the frame rate by a factor of two. Additionally, some scenes appeared to have large underexposed areas where there is no color information in RGB frames. Having large pitch-black areas in an RGB image is unrealistic in practice as cameras dynamically adapt their shutter speed depending on the exposure of the scene. To prevent the errors made by depth estimation methods in these areas from dominating the performance analysis, we discarded all the pixels for which the color in the RGB image with a value equal to zero.</p><p>Results. As we want to focus only on the generalization performance for depth estimation, we bypass the pose estimation network for ManyDepth and DeepV2D, and use the ground-truth motion to generate the depth maps with these methods. Also, the depth maps produced by baseline methods are not guaranteed to be at the correct scale. To alleviate this issue in performance tests, we apply a median scaling to the depth maps of baseline methods. The results of our experiments are reported in <ref type="table" target="#tab_0">Table IV</ref>. Overall M4Depth outperforms the other methods with a significant margin both for structured and unstructured environments. As on Mid-Air, PWC-Net slightly outperforms M4Depth on some relative metrics, but not for both sequences. It is worth noting that the hierarchy of the performances has completely changed between the test on KITTI and the one in generalization as our method outperforms DeepV2D <ref type="bibr" target="#b34">[31]</ref> on the latter. These results therefore show the better generalization capability of M4Depth when compared to state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion on the architecture</head><p>Ablation study. We report the average performance over four trainings for ablated versions of our architecture in <ref type="table" target="#tab_5">Table V</ref>.</p><p>The results show that the SNCV is the block that leads to the best performance boost. This highlights the benefits of giving some spatial information to the disparity refiners. The other blocks contribute to improve either test or generalization performances, but not both at the same time. As expected, the main contributors to generalization performances are the DINL and the normalization layer.</p><p>Increasing the number of levels in the architecture improves the performances. It should be noted, however, that the network tends to overfit the training dataset, therefore leading to worse generalization performance if the network gets too deep.</p><p>Overall, this ablation study shows that a compromise between performance on the training dataset and performance in generalization has to be made.</p><p>Limitations. With our approach, large areas with no repetitive textures are prone to poor depth estimates. The feature matching performed by our cost volumes matching can indeed become unstable if large areas share exactly the same features. This can therefore lead to bad depth estimates.</p><p>We mitigate this issue by using a multi-scale network and by including an SNCV at each of its levels, but these solutions do not make our network completely immune to this issue.</p><p>Inference speed. Our network has 4.5 million parameters and requires up to 500 MB of GPU memory to run with six levels. At inference time on Mid-Air, an NVidia Tesla V100 GPU needs 17 ms to process a single frame for a raw TensorFlow implementation. This corresponds to 59 frames per second which is roughly twenty-times faster than DeepV2D, the best performing method on KITTI. Such inference speed is compatible with the real-time constraints required for robotic applications.</p><p>Interpretation of the results. As opposed to other methods, our network is designed to exclusively use the relative difference between feature vectors rather than relying on the raw semantic cues, i.e., the raw value of the feature vectors, to estimate depth. All reference methods, even the ones based on cost volumes, forward the feature maps generated by their encoder directly to their depth estimation subnetwork. Doing so gives networks the ability to use semantic cues directly to estimate depth. This ability is only valuable for instances where the set of features possibly encountered can be encoded by the network and associated to a specific depth.</p><p>Our experiments show that reference methods perform wellbetter than M4Depth for some-on KITTI, the dataset with constrained and structured scenes. However, they fall behind in unstructured environments when the link between semantic cues and depth is weak, and in generalization when semantic cues are different from the reference. This tends to imply that baseline networks rely on the raw feature values to derive depth.</p><p>All these observations lead us to believe that severing the direct link between the encoder and the decoder of the architecture while proposing relevant substitute data through the preprocessing unit is the key factor that allows M4Depth to perform so well overall in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we address the challenging task of estimating depth from RGB image sequences acquired in unknown environments by a camera moving with six degrees of freedom. For this, we first define a notion of visual disparity for generic camera motion, which is central for our method M4Depth, and show how it can be used to estimate depth. Then, we present new cost volumes designed to boost the performance of the underlying deep neural network of our method.</p><p>Three series of experiments were performed on synthetic datasets as well as on the KITTI dataset that features real data. They show that M4Depth is superior to the baseline <ref type="table" target="#tab_0">Table II  PERFORMANCE COMPARISON ON OUR TEST SET OF MID-AIR. HERE, A 6-LEVEL VERSION OF M4DEPTH IS COMPARED TO THE BASELINE METHODS.  SCORES CORRESPOND TO THE BEST PERFORMANCE OBTAINED OUT OF FIVE INDIVIDUAL NETWORK TRAININGS. THE BEST SCORE FOR A METRIC IS   HIGHLIGHTED IN BOLD AND THE SECOND BEST IS</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test size (a) Performance for the generalization test on two unstructured environments from TartanAir, that are gascola (G) and season forest winter (W). Scores were generated by using the same network weights as the ones used to report the performance on Mid-Air in <ref type="table" target="#tab_0">Table II</ref>. (b) Performance for the generalization test on two structured environments from TartanAir, that are neighborhood (N) and old town (OT). Scores were generated by using the same network weights as the ones used to report the performance on KITTI in <ref type="table" target="#tab_0">Table III</ref>. both in unstructured environments and in generalization while also performing well on the standard KITTI benchmark, which shows its superiority for autonomous vehicles that would need to venture off road. In addition to being motion-and featureinvariant, our method is lightweight, runs in real time, and can be trained in an end-to-end fashion.</p><formula xml:id="formula_15">Abs Rel ? SQ Rel ? RMSE ? RMSE log ? ? &lt; 1.25 ? ? &lt; 1.25 2 ? ? &lt; 1.25 3 ? G W G W G W G W G W G W G W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Our further works on M4Depth will, among others, focus on the determination of its own uncertainty on depth estimates at inference time. Such an addition would provide a great advantage over other methods that do not offer this capability.</p><p>To help the community to reproduce our results, we made the code of our method publicly available at https://github.com/ michael-fonder/M4Depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A GEOMETRY OF THE SETUP (COMPLEMENT TO SECTION 4.1 DERIVING DEPTH FROM VISUAL DISPARITY)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Camera model</head><p>In the pinhole camera model, the camera is represented by a sensor plane and a focal point, which is the optical center of the camera, taken as the origin (see <ref type="figure" target="#fig_4">Fig. 6</ref>). The focal point is located somewhere along the principal axis that intersects the sensor plane perpendicularly on its central point. The distance separating the focal point from the sensor plane is the focal length; it is expressed as a multiple of a sensor pixel width. In its simple expression, the pinhole model of a camera is characterized by five intrinsic parameters:</p><p>? f x and f y , the focal lengths along the x and y axes, respectively; ? s, the skew factor of a pixel, and ? (c x , c y ), the coordinates of the principal point on the camera sensor.</p><p>These parameters can be regrouped in a matrix K, called the projection matrix, as follows</p><formula xml:id="formula_16">K = ? ? f x s c x 0 f y c y 0 0 1 ? ? .<label>(15)</label></formula><p>The pixel coordinates (i, j), in the camera plane, of the projection of a point located at (x, y, z) in the 3-D scene are obtained by using the camera intrinsic matrix K and the rightangle theorem as follows:</p><formula xml:id="formula_17">(i, j) = ? z , ? z with ? ? ? ? ? ? z 1 ? ? ? ? = K 0 0 1 ? ? ? ? x y z 1 ? ? ? ? .<label>(16)</label></formula><p>When the camera moves, the 3-D coordinates of a point can be expressed not with respect to the current frame but with respect to the coordinates system of the first frame. In this system, if we express the current camera position by a vector p of size 3 and its orientation by a 3 ? 3 rotation matrix R, then, the position of a point (X, Y, Z) in space can be obtained by the following transformation</p><formula xml:id="formula_18">? ? ? ? X Y Z 1 ? ? ? ? = R p 0 1 T ? ? ? ? x y z 1 ? ? ? ? ,<label>(17)</label></formula><p>where T denotes the matrix transpose operator. In this equation, T is called the transformation matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detailed derivation of the visual disparity</head><p>The definition of the (new) visual disparity notion involves two distinct poses of the same camera. We denote these poses by C 1 and C 2 . The C 2 pose is expressed relative to the pose of C 1 and it is encoded by the transformation matrix T 2 with:</p><formula xml:id="formula_19">T 2 = R t 0 1 .<label>(18)</label></formula><p>Let us assume that some visual information is visible from two different camera points of view, C 1 and C 2 (see <ref type="figure" target="#fig_5">Fig. 7</ref>). A point P seen by C 1 and projected on the sensor plane in (i 1 , j 1 ) is projected at a different location on the sensor of C 2 , that is (i 2 , j 2 ). For rigid camera motion, the camera intrinsic parameters are the same for C 1 and C 2 . Therefore, we simplify the notations by expressing the coordinates (i, j) relative to the principal point (c x , c y ). With this definition and by assuming that the skew parameter s is negligible, which is common, the expression of the camera intrinsic matrix K simplifies into</p><formula xml:id="formula_20">K = diag(f x , f y , 1) .<label>(19)</label></formula><p>To define our notion of visual disparity, we first need to find the relation that links (i 1 , j 1 ) to (i 2 , j 2 ). From Equ. <ref type="bibr" target="#b18">(16)</ref>, it can be seen that recovering the full 3D coordinates of a point whose projection coordinates (i, j) and depth z are known is trivial if the intrinsic matrix is known (which is a common hypothesis in computer vision as long as the camera is not zooming). Assuming that P is located at a depth z 2 of C 2 , its 3D coordinates with respect to C 2 are given by</p><formula xml:id="formula_21">P C2 = ? ? i 2 /f x j 2 /f y 1 ? ? z 2 ,<label>(20)</label></formula><p>These coordinates are expressed with respect to the C 2 referential. Their expression in C 1 is given by</p><formula xml:id="formula_22">P C1 = [R|t] P C2 1 = RP C2 + t .<label>(21)</label></formula><p>Computing the projection coordinates (i 1 , j 1 ) in C 1 for P is obtained by applying the camera projection equation (Equ. <ref type="bibr" target="#b18">(16)</ref>), that is</p><formula xml:id="formula_23">z 1 ? ? i 1 j 1 1 ? ? = KP C1 .<label>(22)</label></formula><p>By combining Equ. <ref type="bibr" target="#b23">(20)</ref>, <ref type="bibr" target="#b24">(21)</ref>, and <ref type="formula" target="#formula_1">(22)</ref>, we have</p><formula xml:id="formula_24">z 1 ? ? i 1 j 1 1 ? ? = K ? ? R z 2 ? ? i 2 /f x j 2 /f y 1 ? ? + t ? ? .<label>(23)</label></formula><p>Equation <ref type="bibr" target="#b26">23</ref> gives us the relationship between the two coordinates (i 1 , j 1 ) and (i 2 , j 2 ) for a given location in the 3D space. This can be rewritten by introducing an intermediate</p><formula xml:id="formula_25">point (i V , j V ), as z 1 ? ? i 1 j 1 1 ? ? = z 2 K R ? ? i 2 /f x j 2 /f y 1 ? ? + K ? ? t x t y t z ? ? = z 2 z R ? ? i V j V 1 ? ? + ? ? f x t x f y t y t z ? ? ,<label>(24)</label></formula><formula xml:id="formula_26">with ? ? z V i V z V j V z V ? ? = K R ? ? i 2 /f x j 2 /f y 1 ? ? .<label>(25)</label></formula><p>From this equation, we see that (i V , j V ) are the coordinates of point P in the plane of a virtual camera C V whose origin is the same as camera C 2 but with the orientation of camera C 1 . The plane of the virtual camera is thus parallel to that of camera 1. We now define our disparity ? as the Euclidean norm</p><formula xml:id="formula_27">? ij = ? 2 i + ? 2 j ,<label>(26)</label></formula><p>where</p><formula xml:id="formula_28">? i ? j = i 1 ? i V j 1 ? j V .<label>(27)</label></formula><p>We insert the expression of Equ. <ref type="bibr" target="#b30">(27)</ref> in Equ. <ref type="bibr" target="#b27">(24)</ref>, that is</p><formula xml:id="formula_29">z 1 ? ? i V + ? i j V + ? j 1 ? ? = z 2 z V ? ? i V j V 1 ? ? + ? ? f x t x f y t y t z ? ? .<label>(28)</label></formula><p>It follows that</p><formula xml:id="formula_30">z 1 = z 2 z V + t z ,<label>(29)</label></formula><p>and</p><formula xml:id="formula_31">? i ? j = 1 z 2 z V + t z f x t x ? t z i V f y t y ? t z j V .<label>(30)</label></formula><p>The disparity ? ij then becomes</p><formula xml:id="formula_32">? ij = (f x t x ? t z i V ) 2 + (f y t y ? t z j V ) 2 |z 2 z V + t z | .<label>(31)</label></formula><p>It is calculable for any point P whose projection coordinates (i 2 , j 2 ) are within the image frame of C 2 and when the camera motion is known, which is our setup. The only difficulty is the indetermination of the sign of z 2 z V + t z . Theoretically, z 2 z V + t z can be negative, which is achievable, physically, only if point P is located between the camera planes of C 1 and C V . This situation is highly specific in practice, and should rarely occur in the context of autonomous vehicles applications because it is never advisable to move backwards, nor to move close to an object. From the implementation point of view of our method, since such a point P is not visible to C 1 , it is discarded during the calculation of the cost volumes because it does not have any correspondence in C 1 . As a result, we use the following variant of Equ. <ref type="bibr" target="#b34">(31)</ref> in practice</p><formula xml:id="formula_33">? ij = (f x t x ? t z i V ) 2 + (f y t y ? t z j V ) 2 z 2 z V + t z .<label>(32)</label></formula><p>This last equation is useful because it links the disparity that should be observed for a pixel and the depth of the corresponding point in space. Since disparity can be observed and estimated from the two images, we have a mean to estimate the depth by inverting the equation, which yields</p><formula xml:id="formula_34">z 2 = (f x t x ? t z i V ) 2 + (f y t y ? t z j V ) 2 ? ij z V ? t z z V .<label>(33)</label></formula><p>In summary, Equ. (31) and <ref type="bibr" target="#b37">(33)</ref> give the relationships to convert a depth map to a disparity map and vice-versa for given pixel sensor coordinates and camera motion. These relations are valid only if point P is static. In practice, in our architecture, we respectively use the notations C t?1 and C t instead of C 1 and C 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B ADDITIONAL RESULTS (COMPLEMENT TO SECTION 5 EXPERIMENTS)</head><p>In this section, we provide an additional overview and analysis on the depth map estimates produced by our network with six levels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. KITTI</head><p>In <ref type="figure" target="#fig_6">Fig. 8</ref>, we show several outputs produced by M4Depth on the test set of KITTI when trained on Mid-Air and fine-tuned on KITTI. Despite being trained in a supervised fashion on sparse data, our network manages to make accurate predictions on the whole image.</p><p>The first column shows that the network can deal with reflective surfaces, and the second that the depth of textureless areas can also be correctly estimated. However, these surfaces remain challenging for the network and are not always handled as well as shown in these examples.</p><p>In our problem statement, we made the hypothesis that environments are static. The two rightmost samples show the behavior of M4Depth when this hypothesis is not met. The depth estimated for mobile objects is either largely under-or overestimated depending on the relative motion perceived by the camera. As we use a pyramidal architecture, the reduction of the spatial dimension in the deeper layers of the architecture can lead to depth estimation artifacts that bleed around mobile objects, as seen in the third column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generalization</head><p>Some samples of our generalization experiments on unstructured and urban scenes of the TartanAir dataset are shown in <ref type="figure" target="#fig_7">Fig. 9 and 10</ref> respectively.</p><p>The quality of these results confirms the good scores obtained on performance metrics. Objects in the scene are properly identified, correctly detoured, and their depth is accurately estimated.</p><p>Some general observations on the weaknesses of M4Depth can also be made from these outputs. First, the network cannot resolve all the details when the scene is too cluttered. This is especially visible in forest environments where tree branches overlap. Second, there are sometimes issues with sky recognition. This is however to be expected as our network mostly relies on perceived frame-to-frame pixel displacement to produce estimates. Finally, small and isolated structures such as cables are not always detected (see the outputs on urban scenes). In this section, we provide the training details to reproduce the results of all the methods of our baseline for the Mid-Air dataset. These details complement the code made available on a GitHub repository. In the paper, we have chosen five methods for our baseline, namely: Monodepth <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b53">[48]</ref>, Monodepth2 <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b54">[49]</ref>, ST-CLSTM <ref type="bibr" target="#b23">[20]</ref>, <ref type="bibr" target="#b55">[50]</ref>, the method of Wang et al. <ref type="bibr" target="#b20">[18]</ref>, <ref type="bibr" target="#b56">[51]</ref>, ManyDepth <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b57">[52]</ref>, and PWCDC-Net <ref type="bibr" target="#b36">[32]</ref>.</p><p>To get a baseline that is true to the work of the authors and is coherent between different methods, we proceeded as follows. We kept the original default parameter values of each method. Next, we adjusted the batch size so that every learning step contained around 18 frames, and we trained each network five times. As some method have specific input pipelines, we adjusted the training epoch count of each method to guarantee that a network sees every training sample at least 50 times during its training. After a first round of training, it appeared that some methods did not converge. This has led us to adapt the training setup for these methods in order to obtain a representative performance.</p><p>The training of PWCDC-Net <ref type="bibr" target="#b36">[32]</ref> had to be done differently as it is an optical flow network. To make it work, we had to convert depth maps to optical flow maps by using Equ. <ref type="bibr" target="#b26">(23)</ref>. It also required more steps during the training to reach a steady-state on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth M4Depth</head><p>(a) Gascola scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB image Ground truth M4Depth</head><p>(b) Season forest (winter) scene. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB image Ground truth M4Depth</head><p>(a) Neighborhood scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB image Ground truth M4Depth</head><p>(b) Old town scene. The values reported for the baseline methods correspond to the best results obtained out of five runs. The most important parameters of the baseline setup are given in <ref type="table" target="#tab_0">Table VI</ref>. We kept all other parameters unchanged to a large extent. However, adjustments were necessary for some methods as explained hereafter.</p><p>? With the proposed setup, Monodepth failed to produce any output for three trainings out of the five. However, the length of the training sequence had to be downsized from 10 to 8 frames to accommodate our internal pipeline constraints. ? Finally, for Manydepth, we had to select the encoder architecture; we chose the ResNet-50 encoder.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ST-CLSTM<ref type="bibr" target="#b23">[20]</ref> (g) Monodepth<ref type="bibr" target="#b10">[10]</ref> (h) Monodepth2<ref type="bibr" target="#b12">[11]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of the depth maps estimated by M4Depth and baseline methods. M4Depth recovers depth details more accurately than baseline methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Comparison of depth maps estimated by M4Depth on the KITTI dataset (Row 2) with the corresponding interpolated ground truth (Row 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>A diagram of the pinhole camera model with axes and notations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Illustration of moving the camera from position C 1 to C 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Comparison of depth estimates produced by M4Depth on the KITTI test set (Row 2) with the corresponding interpolated ground truth (Row 3). The two rightmost samples show artifacts produced by the network around elements of the scene that are dynamic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Samples of depth maps produced in generalization on unstructured scenes of the TartanAir dataset by M4Depth with six levels trained on Mid-Air. Black areas in the ground truths correspond to pixels with no color information in the RGB image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Samples of depth maps produced in generalization on urban scenes of the TartanAir dataset by M4Depth with six levels trained on Mid-Air and fine-tuned on KITTI. Black areas in the ground truths correspond to pixels with no color information in the RGB image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>?</head><label></label><figDesc>The lower epoch count for Monodepth2 is due to the fact that the training pipeline sees each sample three times during a single epoch. ? Performances obtained with the default learning rate for the ST-CLSTM method were extremely poor. We obtained better results by reducing it to 10 ?4 . ? The code written by Wang et al. worked as expected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I MAIN</head><label>I</label><figDesc>CHARACTERISTICS OF A SELECTION OF DEPTH ESTIMATION METHODS USED FOR COMPARISON IN THIS PAPER.</figDesc><table><row><cell>Method</cell><cell>Supervision</cell><cell>Multi-frame</cell><cell>Recurrent</cell><cell>Cam. pose</cell><cell>Pre-trained on KITTI</cell></row><row><cell>Monodepth [10]</cell><cell>Self-sup.</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Available</cell></row><row><cell>Monodepth2 [11]</cell><cell>Self-sup.</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Available</cell></row><row><cell>ST-CLSTM [20]</cell><cell>Self-sup.</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>Not available</cell></row><row><cell>Wang [18]</cell><cell>Self-sup.</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>Not available</cell></row><row><cell>ManyDepth [5]</cell><cell>Self-sup.</cell><cell>Yes</cell><cell>No</cell><cell>Self-est.</cell><cell>Available</cell></row><row><cell>DeepV2D [31]</cell><cell>Supervised</cell><cell>Yes</cell><cell>No</cell><cell>Self-est.</cell><cell>Available</cell></row><row><cell>M4Depth (ours)</cell><cell>Supervised</cell><cell>Yes</cell><cell>Yes</cell><cell>Given</cell><cell>N/A</cell></row><row><cell cols="6">only operates in an offline mode and makes it inappropriate</cell></row><row><cell cols="4">for autonomous vehicle applications.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>UNDERLINED. BEST OF 5 TRAININGS) ON THE KITTI DATASET . THE SCORES REPORTED FOR REFERENCE METHODS ARE THE ONES PUBLISHED BY THEIR RESPECTIVE AUTHORS.</figDesc><table><row><cell>Method</cell><cell>Test size</cell><cell>Abs Rel ?</cell><cell>SQ Rel ?</cell><cell>RMSE ?</cell><cell>RMSE log ?</cell><cell>? &lt; 1.25 ?</cell><cell>? &lt; 1.25 2 ?</cell><cell>? &lt; 1.25 3 ?</cell></row><row><cell>Monodepth [10]</cell><cell>384 ? 384</cell><cell>0.314</cell><cell>8.713</cell><cell>13.595</cell><cell>0.438</cell><cell>0.678</cell><cell>0.828</cell><cell>0.895</cell></row><row><cell>Monodepth2 [11]</cell><cell>384 ? 384</cell><cell>0.394</cell><cell>5.366</cell><cell>12.351</cell><cell>0.462</cell><cell>0.610</cell><cell>0.751</cell><cell>0.833</cell></row><row><cell>ST-CLSTM [20]</cell><cell>384 ? 384</cell><cell>0.404</cell><cell>6.390</cell><cell>13.685</cell><cell>0.438</cell><cell>0.751</cell><cell>0.865</cell><cell>0.911</cell></row><row><cell>Wang [18]</cell><cell>384 ? 384</cell><cell>0.241</cell><cell>5.532</cell><cell>12.599</cell><cell>0.362</cell><cell>0.648</cell><cell>0.831</cell><cell>0.911</cell></row><row><cell>ManyDepth [5]</cell><cell>384 ? 384</cell><cell>0.203</cell><cell>3.549</cell><cell>10.919</cell><cell>0.327</cell><cell>0.723</cell><cell>0.876</cell><cell>0.933</cell></row><row><cell>PWCDC-Net [32]</cell><cell>384 ? 384</cell><cell>0.095</cell><cell>2.087</cell><cell>8.351</cell><cell>0.215</cell><cell>0.887</cell><cell>0.938</cell><cell>0.962</cell></row><row><cell>M4Depth-d6 (Ours)</cell><cell>384 ? 384</cell><cell>0.105</cell><cell>3.454</cell><cell>7.043</cell><cell>0.186</cell><cell>0.919</cell><cell>0.953</cell><cell>0.969</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table III</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PERFORMANCE OF M4DEPTH (Method Test size</cell><cell>Abs Rel ?</cell><cell>SQ Rel ?</cell><cell>RMSE ?</cell><cell>RMSE log ?</cell><cell>? &lt; 1.25 ?</cell><cell>? &lt; 1.25 2 ?</cell><cell>? &lt; 1.25 3 ?</cell></row><row><cell>Monodepth [10]</cell><cell>256 ? 512</cell><cell>0.114</cell><cell>0.898</cell><cell>4.935</cell><cell>0.206</cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell></row><row><cell>Monodepth2 [11]</cell><cell>320 ? 1024</cell><cell>0.106</cell><cell>0.806</cell><cell>4.630</cell><cell>0.193</cell><cell>0.876</cell><cell>0.958</cell><cell>0.980</cell></row><row><cell>ST-CLSTM [20]</cell><cell>375 ? 1240</cell><cell>0.104</cell><cell>N/A</cell><cell>4.139</cell><cell>0.131</cell><cell>0.833</cell><cell>0.967</cell><cell>0.988</cell></row><row><cell>ManyDepth [5]</cell><cell>320 ? 1024</cell><cell>0.087</cell><cell>0.685</cell><cell>4.142</cell><cell>0.167</cell><cell>0.920</cell><cell>0.968</cell><cell>0.983</cell></row><row><cell>Wang [18]</cell><cell>128 ? 416</cell><cell>0.077</cell><cell>0.205</cell><cell>1.698</cell><cell>0.110</cell><cell>0.941</cell><cell>0.990</cell><cell>0.998</cell></row><row><cell>DeepV2D [31]</cell><cell>300 ? 1088</cell><cell>0.037</cell><cell>0.174</cell><cell>2.005</cell><cell>0.074</cell><cell>0.977</cell><cell>0.993</cell><cell>0.997</cell></row><row><cell>PWCDC-Net [32]</cell><cell>256 ? 768</cell><cell>0.152</cell><cell>2.015</cell><cell>5.883</cell><cell>0.251</cell><cell>0.828</cell><cell>0.920</cell><cell>0.956</cell></row><row><cell>M4Depth-d6 (Ours)</cell><cell>256 ? 768</cell><cell>0.095</cell><cell>0.7084</cell><cell>3.515</cell><cell>0.146</cell><cell>0.898</cell><cell>0.962</cell><cell>0.982</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table IV</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">PERFORMANCE COMPARISON IN GENERALIZATION ON THE TARTANAIR DATASET.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table V PERFORMANCE</head><label>V</label><figDesc>OF M4DEPTH (TRAINED ON MID-AIR, AVERAGED OVER 4 RUNS) FOR VARIOUS ARCHITECTURE DEPTHS AND ABLATIONS (ON A NETWORK WITH 6 LEVELS), AND FOR A FULL ARCHITECTURE WITH 2, 4, AND 6 LEVELS, WHEN TESTED ON MID-AIR (MA) AS WELL AS IN GENERALIZATION ON THE OLD TOWN SCENE (OT) OF TARTANAIR.</figDesc><table><row><cell>Ablation</cell><cell>Abs Rel ? MA OT</cell><cell cols="2">SQ Rel ? MA OT</cell><cell cols="2">RMSE ? MA OT</cell><cell>RMSE log ? MA OT</cell><cell>? &lt; 1.25 ? MA OT</cell><cell>? &lt; 1.25 2 ? MA OT</cell><cell>? &lt; 1.25 3 ? MA OT</cell></row><row><cell>SNCV</cell><cell>0.118 0.609</cell><cell cols="2">4.392 26.870</cell><cell>7.730</cell><cell>9.469</cell><cell>0.203 0.608</cell><cell>0.912 0.738</cell><cell>0.947 0.807</cell><cell>0.965 0.846</cell></row><row><cell>Normalize</cell><cell>0.099 0.583</cell><cell cols="2">3.179 23.495</cell><cell>7.032</cell><cell>9.019</cell><cell>0.185 0.496</cell><cell>0.920 0.770</cell><cell>0.955 0.842</cell><cell>0.971 0.880</cell></row><row><cell>DINL</cell><cell>0.104 0.521</cell><cell cols="2">3.480 19.378</cell><cell>7.182</cell><cell>8.826</cell><cell>0.189 0.536</cell><cell>0.915 0.763</cell><cell>0.952 0.833</cell><cell>0.969 0.872</cell></row><row><cell>f l?1 ?,t</cell><cell>0.113 0.435</cell><cell cols="2">4.007 14.445</cell><cell>7.382</cell><cell>8.732</cell><cell>0.196 0.517</cell><cell>0.916 0.771</cell><cell>0.950 0.840</cell><cell>0.967 0.878</cell></row><row><cell>Split</cell><cell>0.113 0.366</cell><cell>4.074</cell><cell>9.937</cell><cell>7.424</cell><cell>7.900</cell><cell>0.196 0.478</cell><cell>0.914 0.762</cell><cell>0.949 0.834</cell><cell>0.966 0.873</cell></row><row><cell>? l t?1</cell><cell>0.107 0.435</cell><cell cols="2">3.482 15.071</cell><cell>7.201</cell><cell>8.836</cell><cell>0.197 0.437</cell><cell>0.911 0.788</cell><cell>0.949 0.863</cell><cell>0.966 0.900</cell></row><row><cell>M4Depth-d2</cell><cell>0.108 0.660</cell><cell cols="2">3.164 30.091</cell><cell cols="2">8.141 13.743</cell><cell>0.230 0.618</cell><cell>0.903 0.742</cell><cell>0.943 0.820</cell><cell>0.962 0.861</cell></row><row><cell>M4Depth-d4</cell><cell>0.114 0.330</cell><cell cols="2">4.124 12.569</cell><cell>7.405</cell><cell>8.391</cell><cell>0.196 0.399</cell><cell>0.916 0.809</cell><cell>0.951 0.882</cell><cell>0.967 0.917</cell></row><row><cell>M4Depth-d6</cell><cell>0.109 0.434</cell><cell cols="2">3.724 14.087</cell><cell>7.169</cell><cell>8.875</cell><cell>0.190 0.494</cell><cell>0.917 0.778</cell><cell>0.952 0.848</cell><cell>0.968 0.885</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table VI MAIN</head><label>VI</label><figDesc>PARAMETERS USED TO TRAIN BASELINE METHODS. THE ASTERISK DENOTES DEFAULT PARAMETERS OF METHODS SUGGESTED BY THEIR RESPECTIVE AUTHORS.</figDesc><table><row><cell>Method</cell><cell>Train epoch count</cell><cell>Batch size</cell><cell>Sequence length</cell></row><row><cell>Monodepth [10], [48]</cell><cell>50</cell><cell>18</cell><cell>1*</cell></row><row><cell>Monodepth2 [11], [49]</cell><cell>17</cell><cell>6</cell><cell>3*</cell></row><row><cell>ST-CLSTM [20], [50]</cell><cell>50</cell><cell>3</cell><cell>5*</cell></row><row><cell>Wang et al. [18], [51]</cell><cell>50</cell><cell>3</cell><cell>8*</cell></row><row><cell>Manydepth [5], [52]</cell><cell>25</cell><cell>6</cell><cell>3*</cell></row><row><cell>PWCDC-Net [32]</cell><cell>100</cell><cell>8</cell><cell>2*</cell></row><row><cell></cell><cell>APPENDIX C</cell><cell></cell><cell></cell></row><row><cell cols="4">MID-AIR BASELINE METHODS TRAINING DETAILS</cell></row><row><cell cols="4">(COMPLEMENT TO SECTION 5.1 UNSTRUCTURED</cell></row><row><cell></cell><cell cols="2">ENVIRONMENTS)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stereo vision and laser odometry for autonomous helicopters in GPS-denied indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Achtelik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prentice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.819082</idno>
		<ptr target="https://doi.org/10.1117/12.819082" />
	</analytic>
	<monogr>
		<title level="m">Unmanned Systems Technology XI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7332</biblScope>
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenkin</surname></persName>
		</author>
		<title level="m">Computational Principles of Mobile Robotics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.01196</idno>
		<ptr target="https://doi.org/10.1109/iccv48922.2021.01196" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="12" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.3019967</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2020.3019967" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1623" to="1637" />
			<date type="published" when="2022-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The temporal opportunist: Self-supervised multi-frame monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR46437.2021.00122</idno>
		<ptr target="https://doi.org/10.1109/CVPR46437.2021.00122" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Nashville, Tennessee, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="1164" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mid-air: A multi-modal dataset for extremely low altitude drone flights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2019.00081</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2019.00081" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting><address><addrLine>UAVision, Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="553" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TartanAir: A dataset to push the limits of visual SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS45743.2020.9341801</idno>
		<ptr target="https://doi.org/10.1109/IROS45743.2020.9341801" />
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting><address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="4909" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Providence, Rhode Island, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2012.6248074</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2012.6248074" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6602" to="6611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2017.699</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.699" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="3827" to="3837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2019.00393</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00393" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards real-time unsupervised monocular depth estimation on CPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2018.8593814</idno>
		<ptr target="https://doi.org/10.1109/IROS.2018.8593814" />
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="5848" to="5854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning for monocular depth estimation: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2020.12.089</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2020.12.089" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">438</biblScope>
			<biblScope unit="page" from="14" to="33" />
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monocular depth estimation based on deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiaogang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wenjing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peiyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CAC51589.2020.9327548</idno>
		<ptr target="https://doi.org/10.1109/CAC51589.2020.9327548" />
	</analytic>
	<monogr>
		<title level="m">Chinese Automation Congress (CAC)</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="2436" to="2440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monocular depth estimation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11431-020-1582-8</idno>
		<ptr target="https://doi.org/10.1007/s11431-020-1582-8" />
	</analytic>
	<monogr>
		<title level="j">Science China Technological Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1612" to="1627" />
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DepthNet: A recurrent neural network architecture for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2018.00066</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2018.00066" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting><address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Don&apos;t forget the past: Recurrent depth estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/LRA.2020.3017478</idno>
		<ptr target="https://doi.org/10.1109/LRA.2020.3017478" />
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6813" to="6820" />
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent neural network for (un-)supervised learning of monocular video visual odometry and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00570</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00570" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="5550" to="5559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatial correspondence with generative adversarial network:learning depth from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="7494" to="7504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2019.00759</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00759" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting temporal consistency for real-time video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ya</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00181</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00181" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="1725" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A space-sweep approach to true multi-image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.1996.517097</idno>
		<ptr target="https://doi.org/10.1109/CVPR.1996.517097" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-06" />
			<biblScope unit="page" from="358" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time plane-sweeping stereo with multiple sweeping directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2007.383245</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2007.383245" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Every pixel counts ++: Joint learning of geometry and motion with 3D holistic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2930258</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2930258" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2624" to="2641" />
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3D geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00594</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00594" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Consistent video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<idno type="DOI">10.1145/3386569.3392377</idno>
		<ptr target="https://doi.org/10.1145/3386569.3392377" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="71" />
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>?zye?il</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Voroninski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
		<idno type="DOI">10.1017/S096249291700006X</idno>
		<ptr target="https://doi.org/10.1017/S096249291700006X" />
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="305" to="364" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cascade cost volume for high-resolution multi-view stereo and stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00257</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00257" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="2495" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DeMoN: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.596</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.596" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="5622" to="5631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MVSNet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01237-3_47</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01237-3_47" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV), ser</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11212</biblScope>
			<biblScope unit="page" from="785" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">DeepVideoMVS: Multi-view stereo on video with recurrent spatio-temporal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?z?eker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Speciale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2012.02177</idno>
		<idno>abs/2012.02177</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2012.02177" />
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">DeepV2D: Video to depth with differentiable structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1812.04605</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/arXiv.1812.04605</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1812.04605" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00931</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00931" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention (MICCAI), ser. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain-invariant stereo matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58536-5_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58536-5_25" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV), ser</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12347</biblScope>
			<biblScope unit="page" from="420" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1505.00853</idno>
		<idno>abs/1505.00853</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1505.00853" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.316</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.316" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.615</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.615" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="5807" to="5815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structure-aware residual pyramid network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting><address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-08" />
			<biblScope unit="page" from="694" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pyramid-structured depth MAP super-resolution based on deep dense-residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2019.2944646</idno>
		<ptr target="https://doi.org/10.1109/LSP.2019.2944646" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1723" to="1727" />
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-scale residual pyramid attention network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition (ICPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Italy</forename><surname>Milan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR48806.2021.9412670</idno>
		<ptr target="https://doi.org/10.1109/ICPR48806.2021.9412670" />
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="5137" to="5144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On regression losses for deep depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trouv?-Peloux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Champagnat</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2018.8451312</idno>
		<ptr target="https://doi.org/10.1109/ICIP.2018.8451312" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="2915" to="2919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with improved long-range accuracy for UAV environment perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-C</forename><surname>Miclea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2021.3060513</idno>
		<ptr target="https://doi.org/10.1109/TGRS.2021.3060513" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geosciences and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.123" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.352</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.352" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of RGB-D SLAM systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2012.6385773</idno>
		<ptr target="https://doi.org/10.1109/IROS.2012.6385773" />
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting><address><addrLine>Vilamoura, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-10" />
			<biblScope unit="page" from="573" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<ptr target="https://github.com/mrharicot/monodepth" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Digging into self-supervised monocular depth prediction</title>
		<ptr target="https://github.com/nianticlabs/monodepth2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Exploiting temporal consistency for real-time video depth estimation -unofficial implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weihao</surname></persName>
		</author>
		<ptr target="https://github.com/weihaox/ST-CLSTM" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Recurrent neural network for (un-)supervised learning of monocular videovisual odometry and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<ptr target="https://github.com/wrlife/RNN_depth_pose" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The temporal opportunist: Self-supervised multi-frame monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<ptr target="https://github.com/nianticlabs/manydepth" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
