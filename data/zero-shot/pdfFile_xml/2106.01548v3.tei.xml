<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WHEN VISION TRANSFORMERS OUTPERFORM RESNETS WITHOUT PRE-TRAINING OR STRONG DATA AUGMENTATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
							<email>chohsieh@cs.ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
							<email>bgong@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WHEN VISION TRANSFORMERS OUTPERFORM RESNETS WITHOUT PRE-TRAINING OR STRONG DATA AUGMENTATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformers (ViTs) and MLPs signal further efforts on replacing handwired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpnessaware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3% and +11.0% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. Model checkpoints are available at https://github.com/google-research/vision_transformer. * Work done as a student researcher at Google.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Transformers <ref type="bibr" target="#b56">(Vaswani et al., 2017)</ref> have become the de-facto model of choice in natural language processing (NLP) <ref type="bibr" target="#b19">(Devlin et al., 2018;</ref><ref type="bibr" target="#b44">Radford et al., 2018)</ref>. In computer vision, there has recently been a surge of interest in end-to-end Transformers <ref type="bibr" target="#b55">Touvron et al., 2021b;</ref><ref type="bibr" target="#b38">Liu et al., 2021b;</ref><ref type="bibr" target="#b22">Fan et al., 2021;</ref><ref type="bibr" target="#b1">Arnab et al., 2021;</ref><ref type="bibr" target="#b4">Bertasius et al., 2021;</ref><ref type="bibr" target="#b0">Akbari et al., 2021)</ref> and MLPs <ref type="bibr" target="#b53">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b54">Touvron et al., 2021a;</ref><ref type="bibr" target="#b37">Liu et al., 2021a;</ref><ref type="bibr" target="#b40">Melas-Kyriazi, 2021)</ref>, prompting the efforts to replace hand-wired features or inductive biases with general-purpose neural architectures powered by data-driven training. We envision these efforts may lead to a unified knowledge base that produces versatile representations for different data modalities, simplifying the inference and deployment of deep learning models in various application scenarios.</p><p>Despite the appealing potential of moving toward general-purpose neural architectures, the lack of convolution-like inductive biases also challenges the training of vision Transformers (ViTs) and MLPs. When trained on ImageNet <ref type="bibr" target="#b18">(Deng et al., 2009)</ref> with the conventional Inception-style data preprocessing <ref type="bibr" target="#b51">(Szegedy et al., 2016)</ref>, Transformers "yield modest accuracies of a few percentage points below ResNets of comparable size" . To boost the performance, existing works resort to large-scale pre-training <ref type="bibr" target="#b1">Arnab et al., 2021;</ref><ref type="bibr" target="#b0">Akbari et al., 2021)</ref> and repeated strong data augmentations <ref type="bibr" target="#b55">(Touvron et al., 2021b)</ref>, resulting in excessive demands of data, computing, and sophisticated tuning of many hyperparameters. For instance, Dosovitskiy et al.  pre-train ViTs using 304M labeled images, and <ref type="bibr" target="#b55">Touvron et al. (2021b)</ref> repeatedly stack four strong image augmentations.</p><p>In this paper, we show ViTs can outperform ResNets <ref type="bibr" target="#b24">(He et al., 2016)</ref> of even bigger sizes in both accuracy and various forms of robustness by using a principled optimizer, without the need for largescale pre-training or strong data augmentations. MLP-Mixers <ref type="bibr" target="#b53">(Tolstikhin et al., 2021)</ref> also become on par with ResNets.</p><p>We first study the architectures fully trained on ImageNet from the lens of loss landscapes and draw the following findings. First, visualization and Hessian matrices of the loss landscapes reveal that Transformers and MLP-Mixers converge at extremely sharp local minima, whose largest principal curvatures are almost an order of magnitude bigger than ResNets'. Such effect accumulates when the gradients backpropagate from the last layer to the first, and the initial embedding layer suffers the largest eigenvalue of the corresponding sub-diagonal Hessian. Second, the networks all have very small training errors, and MLP-Mixers are more prone to overfitting than ViTs of more parameters (because of the difference in self-attention). Third, ViTs and MLP-Mixers have worse "trainabilities" than ResNets following the neural tangent kernel analyses <ref type="bibr" target="#b59">(Xiao et al., 2020)</ref>.</p><p>Therefore, we need improved learning algorithms to prevent the convergence to a sharp local minimum when it comes to the convolution-free ViTs and MLP-Mixers. The first-order optimizers (e.g., <ref type="bibr">SGD and Adam (Kingma &amp; Ba, 2015)</ref>) only seek the model parameters that minimize the training error. They dismiss the higher-order information such as flatness that correlates with generalization <ref type="bibr" target="#b30">(Keskar et al., 2017;</ref><ref type="bibr" target="#b33">Kleinberg et al., 2018;</ref><ref type="bibr" target="#b29">Jastrz?bski et al., 2019;</ref><ref type="bibr" target="#b48">Smith &amp; Le, 2018;</ref><ref type="bibr" target="#b10">Chaudhari et al., 2017)</ref>.</p><p>The above study and reasoning lead us to the recently proposed sharpness-aware minimizer (SAM) <ref type="bibr" target="#b23">(Foret et al., 2021)</ref> that explicitly smooths the loss geometry during model training. SAM strives to find a solution whose entire neighborhood has low losses rather than focus on any singleton point. We show that the resultant models exhibit smoother loss landscapes, and their generalization capabilities improve tremendously across different tasks including supervised, adversarial, contrastive, and transfer learning (e.g., +5.3% and +11.0% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). The enhanced ViTs achieve better accuracy and robustness than ResNets of similar and bigger sizes when trained from scratch on ImageNet, without large-scale pre-training or strong data augmentations. Moreover, we demonstrate that SAM can even enable ViT to be effectively trained with (momentum) SGD, which usually lies far behind Adam when training Transformers <ref type="bibr" target="#b68">(Zhang et al., 2020)</ref>.</p><p>By analyzing some intrinsic model properties, we observe that SAM increases the sparsity of active neurons (especially for the first few layers), which contribute to the reduced Hessian eigenvalues. The weight norms increase, implying the commonly used weight decay may not be an effective regularization alone. A side observation is that, unlike ResNets and MLP-Mixers, ViTs have extremely sparse active neurons (see <ref type="figure" target="#fig_1">Figure 2</ref> (right)), revealing the potential for network pruning <ref type="bibr" target="#b0">(Akbari et al., 2021)</ref>. Another interesting finding is that the improved ViTs appear to have visually more interpretable attention maps. Finally, we draw similarities between SAM and strong augmentations (e.g., mixup) in that they both smooth the average loss geometry and encourage the models to behave linearly between training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>We briefly review ViTs, MLP-Mixers, and some related works in this section.  show that a pure Transformer architecture <ref type="bibr" target="#b56">(Vaswani et al., 2017)</ref> can achieve state-of-the-art accuracy on image classification by pre-training it on large datasets such as ImageNet-21k <ref type="bibr" target="#b18">(Deng et al., 2009</ref>) and JFT-300M <ref type="bibr" target="#b50">(Sun et al., 2017)</ref>. Their vision Transformer (ViT) is a stack of residual blocks, each containing a multi-head self-attention, layer normalization <ref type="bibr" target="#b2">(Ba et al., 2016)</ref>, and a MLP layer. ViT first embeds an input image x ? R H?W ?C into a sequence of features z ? R N ?D by applying a linear projection over N nonoverlapping image patches x p ? R N ?(P 2 ?C) , where D is the feature dimension, P is the patch resolution, and N = HW/P 2 is the sequence length. The self-attention layers in ViT are global and do not possess the locality and translation equivariance of convolutions. ViT is compatible with the popular architectures in NLP <ref type="bibr" target="#b19">(Devlin et al., 2018;</ref><ref type="bibr" target="#b44">Radford et al., 2018)</ref> and, similar to its NLP counterparts, requires pretraining over massive datasets <ref type="bibr" target="#b0">Akbari et al., 2021;</ref><ref type="bibr" target="#b1">Arnab et al., 2021)</ref> or  strong data augmentations <ref type="bibr" target="#b55">(Touvron et al., 2021b)</ref>. Some works specialize the ViT architectures for visual data <ref type="bibr" target="#b38">(Liu et al., 2021b;</ref><ref type="bibr" target="#b22">Fan et al., 2021;</ref><ref type="bibr" target="#b4">Bertasius et al., 2021)</ref>.</p><p>More recent works find that the self-attention in ViT is not vital for performance, resulting in several architectures exclusively based on MLPs <ref type="bibr" target="#b53">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b54">Touvron et al., 2021a;</ref><ref type="bibr" target="#b37">Liu et al., 2021a;</ref><ref type="bibr" target="#b40">Melas-Kyriazi, 2021)</ref>. Here we take MLP-Mixer <ref type="bibr" target="#b53">(Tolstikhin et al., 2021)</ref> as an example. MLP-Mixer shares the same input layer as ViT; namely, it partitions an image into a sequence of nonoverlapping patches/tokens. It then alternates between token and channel MLPs, where the former allows feature fusion from different spatial locations.</p><p>We focus on ViTs and MLP-Mixers in this paper. We denote by "S" and "B" the small and base model sizes, respectively, and by an integer the image patch resolution. For instance, ViT-B/16 is the base ViT model taking as input a sequence of 16 ? 16 patches. Appendices contain more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VITS AND MLP-MIXERS CONVERGE AT SHARP LOCAL MINIMA</head><p>The current training recipe of ViTs, MLP-Mixers, and related convolution-free architectures relies heavily on massive pre-training <ref type="bibr" target="#b1">Arnab et al., 2021;</ref><ref type="bibr" target="#b0">Akbari et al., 2021)</ref> or a bag of strong data augmentations <ref type="bibr" target="#b55">(Touvron et al., 2021b;</ref><ref type="bibr" target="#b53">Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b16">Cubuk et al., 2019;</ref><ref type="bibr" target="#b67">Zhang et al., 2018;</ref><ref type="bibr" target="#b64">Yun et al., 2019)</ref>. It highly demands data and computing, and leads to many hyperparameters to tune. Existing works report that ViTs yield inferior accuracy to the ConvNets of similar size and throughput when trained from scratch on ImageNet without the combination of those advanced data augmentations, despite using various regularization techniques (e.g., large weight decay, Dropout <ref type="bibr">(Srivastava et al., 2014), etc.)</ref>. For instance, ViT-B/16  gives rise to 74.6% top-1 accuracy on the ImageNet validation set (224 image resolution), compared with 78.5% of ResNet-152 <ref type="bibr" target="#b24">(He et al., 2016)</ref>. Mixer-B/16 <ref type="bibr" target="#b53">(Tolstikhin et al., 2021)</ref> performs even worse (66.4%). There also exists a large gap between ViTs and ResNets in robustness tests (see <ref type="table" target="#tab_2">Table 2</ref> for details).</p><p>Moreover, <ref type="bibr" target="#b15">Chen et al. (2021c)</ref> find that the gradients can spike and cause a sudden accuracy dip when training ViTs, and <ref type="bibr" target="#b55">Touvron et al. (2021b)</ref> report the training is sensitive to initialization and hyperparameters. These all point to optimization problems. In this paper, we investigate the loss ViTs and MLP-Mixers converge at extremely sharp local minima. It has been extensively studied that the convergence to a flat region whose curvature is small benefits the generalization of neural networks <ref type="bibr" target="#b30">(Keskar et al., 2017;</ref><ref type="bibr" target="#b33">Kleinberg et al., 2018;</ref><ref type="bibr" target="#b29">Jastrz?bski et al., 2019;</ref><ref type="bibr" target="#b13">Chen &amp; Hsieh, 2020;</ref><ref type="bibr" target="#b48">Smith &amp; Le, 2018;</ref><ref type="bibr" target="#b66">Zela et al., 2020;</ref><ref type="bibr" target="#b10">Chaudhari et al., 2017)</ref>. Following , we plot the loss landscapes at convergence when ResNets, ViTs, and MLP-Mixers are trained from scratch on ImageNet with the basic Inception-style preprocessing <ref type="bibr" target="#b51">(Szegedy et al., 2016)</ref>   <ref type="table" target="#tab_0">Table 1</ref>, which reveals the average flatness. Although ViT-B/16 and Mixer-B/16 achieve lower training error L train than that of ResNet-152, their loss values after random weight perturbation become much higher. We further validate the results by computing the dominate Hessian eigenvalue ? max , which is a mathematical evaluation of the worstcase landscape curvature. The ? max values of ViT and MLP-Mixer are orders of magnitude larger than that of ResNet, and MLP-Mixer suffers the largest curvature among the three species (see Section 4.4 for a detailed analysis).</p><formula xml:id="formula_0">= E ?N [L train (w + )] in</formula><p>Small training errors. This convergence at sharp regions coincides with the training dynamics shown in <ref type="figure" target="#fig_1">Figure 2</ref> (left). Although Mixer-B/16 has fewer parameters than ViT-B/16 (59M vs. 87M), it has a smaller training error (also see L train in <ref type="table" target="#tab_0">Table 1</ref>) but much worse test accuracy, implying that using the cross-token MLP to learn the interplay across image patches is more prone to overfitting than ViTs' self-attention mechanism whose behavior is restricted by a softmax. To validate this statement, we simply remove the softmax in ViT-B/16, such that the query and key matrices can freely interact with each other. Although having lower L train (0.56 vs. 0.65), the obtained ViT-B/16-Free performs much worse than the original ViT-B/16 (70.5% vs. 74.6%). Its L N train and ? max are 7.01 and 1236.2, revealing that ViT-B/16-Free converges to a sharper region than ViT-B/16 (L N train is 6.66 and ? max is 738.8) both on average and in the worst-case direction. Such a difference probably explains why it is easier for MLP-Mixers to get stuck in sharp local minima.</p><p>ViTs and MLP-Mixers have worse trainability. Furthermore, we discover that ViTs and MLP-Mixers suffer poor trainabilities, defined as the effectiveness of a network to be optimized by gradient descent <ref type="bibr" target="#b59">(Xiao et al., 2020;</ref><ref type="bibr">Burkholz &amp; Dubatovka, 2019;</ref><ref type="bibr" target="#b47">Shin &amp; Karniadakis, 2020)</ref>. <ref type="bibr" target="#b59">Xiao et al. (2020)</ref> show that the trainability of a neural network can be characterized by the condition number of the associated neural tangent kernel (NTK), ?(x, x ) = J(x)J(x ) T , where J is the Jacobian matrix. Denoting by ? 1 ? ? ? ? ? ? m the eigenvalues of NTK ? train , the smallest eigenvalue ? m converges exponentially at a rate given by the condition number ? = ? 1 /? m . If ? diverges then the network will become untrainable <ref type="bibr" target="#b59">(Xiao et al., 2020;</ref><ref type="bibr" target="#b12">Chen et al., 2021a)</ref>. As shown in <ref type="table" target="#tab_0">Table 1</ref>, ? is pretty stable for ResNets, echoing previous results that ResNets enjoy superior trainability regardless of the depth <ref type="bibr" target="#b61">(Yang &amp; Schoenholz, 2017;</ref>. However, we observe that the condition number diverges when it comes to ViT and MLP-Mixer, confirming that the training of ViTs desires extra care <ref type="bibr" target="#b15">(Chen et al., 2021c;</ref><ref type="bibr" target="#b55">Touvron et al., 2021b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A PRINCIPLED OPTIMIZER FOR CONVOLUTION-FREE ARCHITECTURES</head><p>The commonly used first-order optimizers (e.g., SGD <ref type="bibr" target="#b41">(Nesterov, 1983)</ref>, Adam (Kingma &amp; <ref type="bibr" target="#b32">Ba, 2015)</ref>) only seek to minimize the training loss L train (w). They usually dismiss the higher-order information such as curvature that correlates with the generalization <ref type="bibr" target="#b30">(Keskar et al., 2017;</ref><ref type="bibr" target="#b10">Chaudhari et al., 2017;</ref><ref type="bibr" target="#b21">Dziugaite &amp; Roy, 2017)</ref>. However, the objective L train for deep neural networks are highly non-convex, making it easy to reach near-zero training error but high generalization error L test during evaluation, let alone their robustness when the test sets have different distributions <ref type="bibr" target="#b26">(Hendrycks &amp; Dietterich, 2019;</ref><ref type="bibr" target="#b28">Hendrycks et al., 2020)</ref>. ViTs and MLPs amplify such drawbacks of first-order optimizers due to the lack of inductive bias for visual data, resulting in excessively sharp loss landscapes and poor generalization, as shown in the previous section. We hypothesize that smoothing the loss landscapes at convergence can significantly improve the generalization ability of those convolution-free architectures, leading us to the recently proposed sharpness-aware minimizer (SAM) <ref type="bibr" target="#b23">(Foret et al., 2021)</ref> that explicitly avoids sharp minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SAM: OVERVIEW</head><p>Intuitively, SAM <ref type="bibr" target="#b23">(Foret et al., 2021)</ref> seeks to find the parameter w whose entire neighbours have low training loss L train by formulating a minimax objective:</p><formula xml:id="formula_1">min w max 2?? L train (w + ),<label>(1)</label></formula><p>where ? is the size of the neighbourhood ball. Without loss of generality, here we use l 2 norm for its strong empirical results <ref type="bibr" target="#b23">(Foret et al., 2021)</ref> and omit the regularization term for simplicity. Since the exact solution of the inner maximization = arg max 2 ?? L train (w + ) is hard to obtain, they employ an efficient first-order approximation:</p><formula xml:id="formula_2">(w) = arg max 2?? L train (w) + T ? w L train (w) = ?? w L train (w)/ ? w L train (w) 2 .</formula><p>( <ref type="formula">2)</ref> Under the l 2 norm,? (w) is simply a scaled gradient of the current weight w. After computing? , SAM updates w based on the sharpness-aware gradient ? w L train (w)| w+? (w) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SHARPNESS-AWARE OPTIMIZATION IMPROVES VITS AND MLP-MIXERS</head><p>We train ViTs and MLP-Mixers with no large-scale pre-training or strong data augmentations. We directly apply SAM to the original ImageNet training pipeline of ViTs  without changing any hyperparameters. The pipeline employs the basic Inception-style preprocessing <ref type="bibr" target="#b51">(Szegedy et al., 2016)</ref>. The original training setup of MLP-Mixers <ref type="bibr" target="#b53">(Tolstikhin et al., 2021)</ref> includes a combination of strong data augmentations, and we replace it with the same Inceptionstyle preprocessing for a fair comparison. Note that we perform grid search for the learning rate, weight decay, Dropout before applying SAM. Please see Appendices for training details.</p><p>Smoother regions around the local minima. Thanks to SAM, both ViTs and MLP-Mixers converge at much smoother regions, as shown in Figures 1(d) and 1(e). Moreover, both the average and the worst-case curvature, i.e., L N train and ? max , decrease dramatically (see <ref type="table" target="#tab_0">Table 1</ref>). Higher accuracy. What comes along is tremendously improved generalization performance. On ImageNet, SAM boosts the top-1 accuracy of ViT-B/16 from 74.6% to 79.9%, and Mixer-B/16 from 66.4% to 77.4%. For comparison, the improvement on a similarly sized ResNet-152 is 0.8%. Empirically, the degree of improvement negatively correlates with the constraints of inductive biases built into the architecture. ResNets with inherent translation equivalence and locality benefit less from landscape smoothing than the attention-based ViTs. MLP-Mixers gain the most from the smoothed loss geometry. In <ref type="table" target="#tab_3">Table 3</ref>, we further train two hybrid models  to validate this observation, where the Transformer takes the feature map extracted from a ResNet-50 as the input sequence. The improvement brought by SAM decreases after we introduce the convolution to ViT, for instance, +2.7% for R50-B/16 compared to +5.3% for ViT-B/16. Moreover, SAM brings larger improvements to the models of larger capacity (e.g., +4.1% for Mixer-S/16 vs. +11.0% for Mixer-B/16) and longer patch sequence (e.g., +2.1% for ViT-S/32 vs. +5.3% for ViT-S/8). Please see <ref type="table" target="#tab_2">Table 2</ref> for more results.</p><p>SAM can be easily applied to common base optimizers. Besides Adam, we also apply SAM on top of the (momentum) SGD that usually performs much worse than Adam when training Transformers <ref type="bibr" target="#b68">(Zhang et al., 2020)</ref>. As expected, we find that under the same training budget (300 epochs), the ViT-B/16 trained with SGD only achieves 71.5% accuracy on ImageNet, whereas Adam achieves Better robustness. We also evaluate the models' robustness using ImageNet-R <ref type="bibr" target="#b28">(Hendrycks et al., 2020)</ref> and ImageNet-C <ref type="bibr" target="#b26">(Hendrycks &amp; Dietterich, 2019)</ref> and find even bigger impacts of the smoothed loss landscapes. On ImageNet-C, which corrupts images by noise, bad weather, blur, etc., we report the average accuracy against 19 corruptions across five levels. As shown in Tables 1 and 2, the accuracies of ViT-B/16 and Mixer-B/16 increase by 9.9% and 15.0% (which are 21.2% and 44.4% relative improvements), after SAM smooths their converged local regions. In comparison, SAM improves the accuracy of ResNet-152 by 2.2% (4.4% relative improvement). We can see that SAM enhances the robustness even more than the relative clean accuracy improvements (7.1%, 16.6%, and 1.0% for ViT-B/16, Mixer-B/16, and ResNet-152, respectively). The performance of an architecture is often conflated with the training strategies <ref type="bibr" target="#b3">(Bello et al., 2021)</ref>, where data augmentations play a key role <ref type="bibr" target="#b16">(Cubuk et al., 2019;</ref><ref type="bibr" target="#b67">Zhang et al., 2018;</ref><ref type="bibr" target="#b14">Chen et al., 2021b)</ref>. However, the design of augmentations requires substantial domain expertise and may not translate between images and videos, for instance. Thanks to the principled sharpness-aware optimizer, we can remove the advanced augmentations and focus on the architectures themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">VITS OUTPERFORM RESNETS WITHOUT PRE-TRAINING OR STRONG AUGMENTATIONS</head><p>When trained from scratch on ImageNet with SAM, ViTs outperform ResNets of similar and greater sizes (also comparable throughput at inference) regarding both clean accuracy (on ImageNet <ref type="bibr" target="#b18">(Deng et al., 2009</ref>), ImageNet-ReaL , and ImageNet V2 <ref type="bibr" target="#b45">(Recht et al., 2019)</ref>) and robustness (on ImageNet-R <ref type="bibr" target="#b28">(Hendrycks et al., 2020)</ref> and ImageNet-C <ref type="bibr" target="#b26">(Hendrycks &amp; Dietterich, 2019)</ref>). ViT-B/16 achieves 79.9%, 26.4%, and 56.6% top-1 accuracy on ImageNet, ImageNet-R, and ImageNet-C, while the counterpart numbers for ResNet-152 are 79.3%, 25.7%, and 52.2%, respectively (see <ref type="table" target="#tab_2">Table 2</ref>). The gaps between ViTs and ResNets are even wider for small architectures.</p><p>ViT-S/16 outperforms a similarly sized ResNet-50 by 1.4% on ImageNet, and 6.5% on ImageNet-C. SAM also significantly improves MLP-Mixers' results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">INTRINSIC CHANGES AFTER SAM</head><p>We take a deeper look into the models to understand how they intrinsically change to reduce the Hessian' eigenvalue ? max and what the changes imply in addition to the enhanced generalization.</p><p>Smoother loss landscapes for every network component. In <ref type="table" target="#tab_4">Table 4</ref>, we break down the Hessian of the whole architecture into small diagonal blocks of Hessians concerning each set of parameters, attempting to analyze what specific components cause the blowing up of ? max in the models trained without SAM. We observe that shallower layers have larger Hessian eigenvalues ? max , and the first linear embedding layer incurs the sharpest geometry. This agrees with the finding in <ref type="bibr" target="#b15">(Chen et al., 2021c</ref>) that spiking gradients happen early in the embedding layer. Additionally, the multi-head self-attention (MSA) in ViTs and the Token MLPs in MLP-Mixers, both of which mix information across spatial locations, have comparably lower ? max than the other network components. SAM consistently reduces the ? max of all network blocks.</p><p>We can gain insights into the above findings by the recursive formulation of Hessian matrices for MLPs <ref type="bibr" target="#b6">(Botev et al., 2017)</ref>. Let h k and a k be the pre-activation and post-activation values for layer k, respectively. They satisfy h k = W k a k?1 and a k = f k (h k ), where W k is the weight matrix and f k is the activation function (GELU <ref type="bibr">(Hendrycks &amp; Gimpel, 2020)</ref> in MLP-Mixers). Here we omit the bias term for simplicity. The diagonal block of Hessian matrix H k with respect to W k can be recursively calculated as:</p><formula xml:id="formula_3">H k = (a k?1 a T k?1 ) ? H k , H k = B k W T k+1 H k+1 W k+1 B k + D k ,<label>(3)</label></formula><formula xml:id="formula_4">B k = diag(f k (h k )), D k = diag(f k (h k ) ?L ?a k ),<label>(4)</label></formula><p>where ? is the Kronecker product, H k is the pre-activation Hessian for layer k, and L is the objective function. Therefore, the Hessian norm accumulates as the recursive formulation backpropagates to shallow layers, explaining why the first block has much larger ? max than the last block in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Greater weight norms. After applying SAM, we find that in most cases, the norm of the postactivation value a k?1 and the weight W k+1 become even bigger (see <ref type="table" target="#tab_4">Table 4</ref>), indicating that the commonly used weight decay may not effectively regularize ViTs and MLP-Mixers (see Appendix J for further verification when we vary the weight decay strength).</p><p>Sparser active neurons in MLP-Mixers. Given the recursive formulation Equation <ref type="formula" target="#formula_3">(3)</ref>, we identify another intrinsic measure of MLP-Mixers that contribute to the Hessian: the number of activated neurons. Indeed, B k is determined by the activated neurons whose values are greater than zero, since the first-order derivative of GELU becomes much smaller when the input is negative. As a result, the number of active GELU neurons is directly connected to the Hessian norm. <ref type="figure" target="#fig_1">Figure 2</ref> (right) shows the proportion of activated neurons for each block, counted using 10% of the ImageNet training set. We can see that SAM greatly reduces the proportion of activated neurons for the first few layers of the Mixer-B/16, pushing them to much sparser states. This result also suggests the potential redundancy of image patches.</p><p>ViTs' active neurons are highly sparse. Although Equations <ref type="formula" target="#formula_3">(3)</ref> and <ref type="formula" target="#formula_4">(4)</ref> only involve MLPs, we still observe a decrease of activated neurons in the first layer of ViTs (but not as significant as in MLP-Mixers). More interestingly, we find that the proportion of active neurons in ViT is much smaller than another two architectures -given an input image, less than 10% neurons have values greater than zero for most layers (see <ref type="figure" target="#fig_1">Figure 2 (right)</ref>). In other words, ViTs offer a huge potential for  network pruning. This sparsity may also explain why one Transformer can handle multi-modality signals (vision, text, and audio) <ref type="bibr" target="#b0">(Akbari et al., 2021)</ref>.</p><p>Visually improved attention maps in ViTs. We visualize ViT-S/16's attention map of the classification token averaged over the last multi-head attentions in <ref type="figure" target="#fig_2">Figure 3</ref> following <ref type="bibr" target="#b9">Caron et al. (2021)</ref>. Interestingly, the ViT model optimized with SAM appears to possess visually improved attention map compared with the one trained via the vanilla AdamW optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">SAM VS. STRONG AUGMENTATIONS</head><p>Previous sections show that SAM can improve the generalization (and robustness) of ViTs and MLP-Mixers. Meanwhile, another paradigm to train these models on ImageNet from scratch is to stack multiple strong augmentations <ref type="bibr" target="#b55">(Touvron et al., 2021b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b53">Tolstikhin et al., 2021)</ref>. Hence, it is interesting to study the differences and similarities between the models trained by SAM and by using strong data augmentations. For the augmentation experiments, we follow <ref type="bibr" target="#b53">Tolstikhin et al. (2021)</ref>'s pipeline that includes mixup <ref type="bibr" target="#b67">(Zhang et al., 2018)</ref> and <ref type="bibr">RandAugment (Cubuk et al., 2020)</ref>.</p><p>Generalization. <ref type="table" target="#tab_5">Table 5</ref> shows the results of strong data augmentation, SAM, and their combination on ImageNet. Each row corresponds to a training set of a different fraction of ImageNet-1k. SAM benefits ViT-B/16 and Mixer-B/16 more than the strong data augmentations, especially when the training set is small. For instance, when the training set contains only 1/10 of ImageNet training images, ViT-B/16-SAM outperforms ViT-B/16-AUG by 7.6%. Apart from the improved validation accuracy, we also observe that both SAM and strong augmentations increase the training error (see <ref type="figure" target="#fig_1">Figure 2</ref> (Middle) and <ref type="table" target="#tab_6">Table 6</ref>), indicating their regularization effects. However, they have distinct training dynamics as the loss curve for ViT-B/16-AUG is much nosier than ViT-B/16-SAM. Sharpness at convergence. Another intriguing question is as follows. Can augmentations also smooth the loss geometry similarly to SAM? To answer it, we also plot the landscape of ViT-B/16-AUG (see <ref type="figure" target="#fig_4">Figure 5</ref> in the Appendix) and compute its Hessian ? max together with the average flatness L N train in <ref type="table" target="#tab_6">Table 6</ref>. Surprisingly, strong augmentations even enlarge the ? max . However, like SAM, augmentations make ViT-B/16-AUG smoother and achieve a significantly smaller training error under random Gaussian perturbations than ViT-B/16. These results show that both SAM and augmentations make the loss landscape flat on average. The difference is that SAM enforces the smoothness by reducing the largest curvature via a minimax formulation to optimize the worst-case scenario, while augmentations ignore the worse-case curvature and instead smooth the landscape over the directions induced by the augmentations.</p><p>Interestingly, besides the similarity in smoothing the loss curvature on average, we also discover that SAM-trained models possess "linearality" resembling the property manually injected by the mixup augmentation. Following <ref type="bibr" target="#b67">Zhang et al. (2018)</ref>, we compute the prediction error in-between training data in <ref type="table" target="#tab_6">Table 6</ref>, where a prediction y is counted as a miss if it does not belong to {y i , y j } evaluated at x = 0.5x i + 0.5x j . We observe that SAM greatly reduces the missing rate (R) compared with the vanilla baseline, showing a similar effect to mixup that explicitly encourages such linearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ABLATION STUDIES</head><p>In this section, we provide a more comprehensive study about SAM's effect on various vision models and under different training setups. We refer to Appendices B to D for the adversarial, contrastive and transfer learning results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">WHEN SCALING THE TRAINING SET SIZE</head><p>Previous studies scale up training data to show massive pre-training trumps inductive biases <ref type="bibr" target="#b53">Tolstikhin et al., 2021)</ref>. Here we show SAM further enables ViTs and MLP-Mixers to handle small-scale training data well. We randomly sample 1/4 and 1/2 images from each ImageNet class to compose two smaller-scale training sets, i.e., i1k (1/4) and i1k (1/2) with 320,291 and 640,583 images, respectively. We also use ImageNet-21k to pre-train the models with SAM, followed by fine-tuning on ImageNet-1k without SAM. The ImageNet validation set remains intact. SAM can still bring improvement when pre-trained on ImageNet-21k (+0.3%, +1.4%, and 2.3% for <ref type="bibr">respectively)</ref>.</p><p>As expected, fewer training examples amplify the drawback of ViTs and MLP-Mixers' lack of the convolutional inductive bias -their accuracies decline much faster than ResNets' (see <ref type="figure" target="#fig_3">Figure 4</ref> in the Appendix and the corresponding numbers in <ref type="table" target="#tab_5">Table 5</ref>). However, SAM can drastically rescue ViTs and MLP-Mixers' performance decrease on smaller training sets. <ref type="figure" target="#fig_3">Figure 4 (right)</ref> shows that the improvement brought by SAM over vanilla SGD training is proportional to the number of training images. When trained on i1k (1/4), it boosts ViT-B/16 and Mixer-B/16 by 14.4% and 25.6%, escalating their results to 66.8% and 62.8%, respectively. It also tells that ViT-B/16-SAM matches the performance of ResNet-152-SAM even with only 1/2 ImageNet training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND LIMITATIONS</head><p>This paper presents a detailed analysis of the convolution-free ViTs and MLP-Mixers from the lens of the loss landscape geometry, intending to reduce the models' dependency on massive pre-training and/or strong data augmentations. We arrive at the sharpness-aware minimizer (SAM) after observing sharp local minima of the converged models. By explicitly regularizing the loss geometry through SAM, the models enjoy much flatter loss landscapes and improved generalization regarding accuracy and robustness. The resultant ViT models outperform ResNets of comparable size and throughput when learned with no pre-training or strong augmentations. Further investigation reveals that the smoothed loss landscapes attribute to much sparser activated neurons in the first few layers. Last but not least, we discover that SAM and strong augmentations share certain similarities to enhance the generalization. They both smooth the average loss curvature and encourage linearity.</p><p>Despite achieving better generalization, training ViTs with SAM has the following limitations which could lead to potential future work. First, SAM incurs another round of forward and backward propagations to update , which will lead to around 2x computational cost per update. Second, we notice that the effect of SAM diminishes as the training dataset becomes larger, so it is vital to develop learning algorithms that can improve/accelerate the large-scale pre-training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>We are not aware of any immediate ethical issues in our work. We hope this paper can provide new insights into the convolution-free neural architectures and their interplay with optimizers, hence benefiting future developments of advanced neural architectures that are efficient in data and computation. Possible negative societal impacts mainly hinge on the applications of convolution-free architectures, whose societal effects may translate to this work. <ref type="table" target="#tab_8">Table 8</ref> specifies the ViT <ref type="bibr" target="#b56">Vaswani et al., 2017)</ref> and MLP-Mixer <ref type="bibr" target="#b53">(Tolstikhin et al., 2021)</ref> architectures used in this paper. "S" and "B" denote the small and base model scales following <ref type="bibr" target="#b55">Touvron et al., 2021b;</ref><ref type="bibr" target="#b53">Tolstikhin et al., 2021)</ref>, followed by the size of each image patch. For instance, "B/16" means the model of base scale with non-overlapping image patches of resolution 16 ? 16. We use the input resolution 224 ? 224 throughout the paper. Following <ref type="bibr" target="#b53">Tolstikhin et al. (2021)</ref>, we sweep the batch sizes in {32, 64, . . . , 8192} on TPU-v3 and report the highest throughput for each model.    defending contrived attack <ref type="bibr" target="#b39">(Madry et al., 2018;</ref><ref type="bibr" target="#b57">Wong et al., 2020)</ref>. Moreover, similar to SAM, <ref type="bibr" target="#b46">Shafahi et al. (2019)</ref> suggest that adversarial training can flatten and smooth the loss landscape. In light of these connections, we study ViTs and MLP-Mixers under the adversarial training framework <ref type="bibr" target="#b39">Madry et al., 2018)</ref>. We use the fast adversarial training <ref type="bibr" target="#b57">(Wong et al., 2020)</ref> (FGSM with random start) with the l ? norm and maximum per-pixel change 2/255 during training. All the hyperparameters remain the same as the vanilla supervised training. When evaluating the adversarial robustness, we use the PGD attack <ref type="bibr" target="#b39">(Madry et al., 2018)</ref> with the same maximum per-pixel change 2/255. The total number of attack steps is 10, and the step size is 0.25/255. To incorporate SAM, we formulate a three-level objective:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDICES A ARCHITECTURES</head><formula xml:id="formula_5">min w max ?Ssam max ??S adv L train (w + , x + ?, y),<label>(5)</label></formula><p>where S sam and S adv denote the allowed perturbation norm balls for the model parameter w and input image x, respectively. Note that we can simultaneously obtain the gradients for computing and ? by backpropagation only once. To lower the training cost, we use fast adversarial training <ref type="bibr" target="#b57">(Wong et al., 2020)</ref> with the l ? norm for ?, and the maximum per-pixel change is set as 2/255. <ref type="table" target="#tab_7">Table 7</ref> (see Appendices) evaluates the models' clean accuracy, real-world robustness, and adversarial robustness (under 10-step PGD attack <ref type="bibr" target="#b39">(Madry et al., 2018)</ref>). It is clear that the landscape smoothing significantly improves the convolution-free architectures for both clean and adversarial accuracy. However, we observe a slight accuracy decrease on clean images for ResNets despite gain for robustness. Similar to our previous observations, ViTs surpass similar-size ResNets when adversarially trained on ImageNet with Inception-style preprocessing for both clean accuracy and adversarial robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C WHEN SAM MEETS CONTRASTIVE LEARNING</head><p>In addition to data augmentations and large-scale pre-training, another notable way of improving a neural model's generalization is (supervised) contrastive learning <ref type="bibr" target="#b25">He et al., 2020;</ref><ref type="bibr" target="#b9">Caron et al., 2021;</ref><ref type="bibr" target="#b31">Khosla et al., 2020)</ref>. We couple SAM with the supervised contrastive learning <ref type="bibr" target="#b31">(Khosla et al., 2020)</ref> for 350 epochs, followed by fine-tuning the classification head by 90 epochs for both ViT-S/16 and ViT-B/16. We train ViTs under the supervised contrastive learning framework <ref type="bibr" target="#b31">(Khosla et al., 2020)</ref>. We take the classification token output from the last layer as the encoded representation and retain the structures of the projection and classification heads <ref type="bibr" target="#b31">(Khosla et al., 2020)</ref>. We employ a batch size 2048 without memory bank <ref type="bibr" target="#b25">(He et al., 2020)</ref> and use <ref type="bibr">Au-toAugment (Cubuk et al., 2019)</ref> with strength 1.0 following <ref type="bibr" target="#b31">Khosla et al. (2020)</ref>. For the 350-epoch pretraining stage, the contrastive loss temperature is set as 0.1, and we use the LAMB optimizer <ref type="bibr" target="#b62">(You et al., 2020)</ref> with learning rate 0.001 ? batch size 256 along with a cosine decay schedule. For the second stage, we train the classification head for 90 epochs via a RMSProp optimizer <ref type="bibr" target="#b52">(Tieleman &amp; Hinton, 2012)</ref> with base learning rate 0.05 and exponential decay. The weight decays are set as 0.3 and 1e-6 for the first and second stages, respectively. We use a small SAM perturbation strength ? = 0.02.</p><p>Compared to the training procedure without SAM, we find considerable performance gain thanks to SAM's smoothing of the contrastive loss geometry, improving the ImageNet top-1 accuracy of ViT-S/16 from 77.0% to 78.1%, and ViT-B/16 from 77.4% to 80.0%. In comparison, the improvement on ResNet-152 is less significant (from 79.7% to 80.0% after using SAM).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D WHEN SAM MEETS TRANSFER LEARNING</head><p>We also study the role of smoothed loss geometry in transfer learning. We select four datasets to test ViTs and MLP-Mixers' transferabilities: CIFAR-10/100 <ref type="bibr" target="#b35">(Krizhevsky, 2009</ref>), Oxford-IIIT Pets <ref type="bibr" target="#b43">(Parkhi et al., 2012)</ref>, and Oxford Flowers-102 <ref type="bibr" target="#b42">(Nilsback &amp; Zisserman, 2008</ref>  <ref type="table" target="#tab_9">Table 9</ref>). Note that we do not employ SAM during fine-tuning. We perform a grid search over the base learning rates on small sub-splits of the training sets (10% for Flowers and Pets, 2% for CIFAR-10/100). After that, we fine-tune on the entire training sets and report the results on the respective test sets. For comparison, we also include ResNet-50-SAM and ResNet-152-SAM in the experiments. <ref type="table" target="#tab_0">Table 10</ref> summarizes the results, which confirm that the enhanced models also perform better after fine-tuning and that MLP-Mixers gain the most from the sharpness-aware optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E VISUALIZATION E.1 LOSS LANDSCAPE</head><p>We use the "filter normalization" method  to visualize the loss function curvature in <ref type="figure" target="#fig_0">Figure 1</ref> and 5. For a fair comparison, we use the cross-entropy loss when plotting the landscapes for all architectures, although the original training objective is the sigmoid loss for ViTs and MLP-Mixers. Note that their sigmoid loss geometry is even sharper. We equally sample 2,500 points on the 2D projection space and compute the losses using 10% of the ImageNet training images , i.e., the i1k (1/10) subset in the main text to save computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 ATTENTION MAP</head><p>The visualization of the ViT's attention maps <ref type="figure" target="#fig_2">(Figure 3</ref> in the main text) follows <ref type="bibr" target="#b9">(Caron et al., 2021)</ref>. We average the self-attention scores of the "classification token" from the last MSA layer to obtain a matrix A ? R H/P ?W/P , where H, W , P are the image height, width, and the patch resolution, respectively. Then we upsample A to the image shape H ? W before generating the figure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F HESSIAN EIGENVALUE</head><p>The Hessian matrix requires second-order derivative, so we compute the Hessian (and all the subdiagonal Hessian) ? max using 10% of the ImageNet training images (i.e., i1k (1/10)) via power iteration 1 , where we use 100 iterations to ensure its convergence.  Except for the experiments in Section 4.5 (SAM with strong data augmentations) and Appendix C (contrastive learning), we train all the models from scratch on ImageNet with the basic Inceptionstyle preprocessing <ref type="bibr" target="#b51">(Szegedy et al., 2016)</ref>, i.e., a random image crop and a horizontal flip with probability 50%. Please see <ref type="table" target="#tab_0">Table 12</ref> for the detailed training settings. We simply follow the original training settings of ResNet and ViT . For MLP-Mixer, we remove the strong augmentations in its original training pipeline and perform a grid search over the learning rate in {0.003, 0.001}, weight decay in {0.3, 0.1, 0.03}, Dropout rate in {0.1, 0.0}, and stochastic depth in {0.1, 0.0}. Note that training for 90 epochs is enough for ResNets to converge, and longer schedule brings almost no effect. For all the experiments, we use 128 TPU-v3 cores (2 per chip), resulting in 32 images per core. The SAM computation for? is conducted on each core independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 PERTURBATION STRENGTH IN SAM</head><p>Different architecture species favor different strengths of perturbation ?. We perform a grid search over ? and report the best results - <ref type="table" target="#tab_0">Table 11</ref> reports the corresponding strengths used in our Ima-geNet experiments. Besides, we show the results when varying ? in <ref type="table" target="#tab_0">Table 13</ref>. Similar to <ref type="bibr" target="#b23">(Foret et al., 2021)</ref>, we also find that a relative small ? ? [0.02, 0.05] works the best for ResNets. However, larger ? gives rise to the best results for ViTs and MLP-Mixers. We also observe that architectures with larger capacities and longer input sequences prefer stronger perturbation strengths. Interestingly, the choice of ? coincides with our previous observations. Since MLP-Mixers suffer the sharpest landscapes, they need the largest perturbation strength. As strong augmentations and contrastive learning already improve generalization, the suitable ? becomes significantly smaller. Note that we do not re-tune any other hyperparameters when using SAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 TRAINING ON IMAGENET SUBSETS</head><p>In Section 5.1, we train the models on ImageNet subsets, and the hyperparameters have to be adjusted accordingly. We simply change the batch size to maintain similar total iterations and keep all other settings the same, i.e., 2048 for i1k (1/2), 1024 for i1k (1/4), and 512 for i1k (1/10). We do not scale the learning rate as we find the scaling harms the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 TRAINING WITH STRONG AUGMENTATIONS</head><p>We tune the learning rate and regularization when using strong augmentations (mixup with probability 0.5, RandAugment with two layers and magnitude 15) in Section 4.5 following <ref type="bibr" target="#b53">(Tolstikhin et al., 2021)</ref>. For ViT, we use 1e-3 peak learning rate, 0.1 weight decay, 0.1 Dropout, and 0.1 stochastic depth; For MLP-Mixer, those hyperparameters are exactly the same as <ref type="bibr" target="#b53">(Tolstikhin et al., 2021)</ref>, peak learning rate as 1e-3, weight decay as 0.1, Dropout as 0.0, and stochastic depth as 0.1. Other settings are unchanged <ref type="table" target="#tab_0">(Table 12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I LONGER SCHEDULE OF VANILLA SGD</head><p>Since SAM needs another forward and backward propagation to compute? , its training overhead is ? 2? of the vanilla baseline. We also experiment with 2? schedule vanilla training (600 epochs). We observe that training longer brings no effect on both clean accuracy and robustness, indicating that the current 300 training epochs for ViTs and MLP-Mixers are enough for them to converge. In this section, we vary the strength of weight decay and see the effects of this commonly used regularization approach. As shown in <ref type="table" target="#tab_0">Table 14</ref>, weight decay helps improve the accuracy on Im-ageNet when training without SAM, the weight norm also decreases when we enlarge the decay strength as expected. However, enlarging the weight decay aggravates the problem of converging to a sharper region measured by both L N train and ? max . Another observation is that w 2 consistently increases after applying SAM for every weight decay strength in <ref type="table" target="#tab_0">Table 14</ref>, together with the improved ImageNet accuracy and smoother landscape curvature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J VARYING WEIGHT DECAY STRANGTH</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Cross-entropy loss landscapes of ResNet-152, ViT-B/16, and Mixer-B/16. ViT and MLP-Mixer converge to sharper regions than ResNet when trained on ImageNet with the basic Inceptionstyle preprocessing. SAM, a sharpness-aware optimizer, significantly smooths the landscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left and Middle: ImageNet training error and validation accuracy vs. iteration for ViTs and MLP-Mixers. Right: Percentage of active neurons for ResNet-152, ViT-B/16, and Mixer-B/16. landscapes of ViTs and MLP-Mixers to understand them from the optimization perspective, intending to reduce their dependency on the large-scale pre-training or strong data augmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Raw images (Left) and attention maps of ViT-S/16 with (Right) and without (Middle) sharpness-aware optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>ImageNet accuracy (Left) and improvement (Right) brought by SAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Cross-entropy loss landscapes of ViT-B/16, ViT-B/16-SAM, ViT-B/16-AUG, and ViT-B/16-21k. Strong augmentations and large-scale pre-training can also smooth the curvature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of parameters, NTK condition number ?, Hessian dominate eigenvalue ? max , training error at convergence L train , average flatness L N train , accuracy on ImageNet, and accuracy/robustness on ImageNet-C. ViT and MLP-Mixer suffer divergent ? and converge at sharp regions; SAM rescues that and leads to better generalization. As it is prohibitive to compute the exact NTK, we approximate the value by averaging over its subdiagonal blocks (see Appendix G for details). We average the results for 1,000 random noises when calculating L N train .</figDesc><table><row><cell></cell><cell>ResNet-152</cell><cell>ResNet-152-SAM</cell><cell>ViT-B/16</cell><cell>ViT-B/16-SAM</cell><cell>Mixer-B/16</cell><cell>Mixer-B/16-SAM</cell></row><row><cell>#Params</cell><cell cols="2">60M</cell><cell cols="2">87M</cell><cell cols="2">59M</cell></row><row><cell>NTK ?  ?</cell><cell cols="2">2801.6</cell><cell cols="2">4205.3</cell><cell cols="2">14468.0</cell></row><row><cell>Hessian ? max</cell><cell>179.8</cell><cell>42.0</cell><cell>738.8</cell><cell>20.9</cell><cell>1644.4</cell><cell>22.5</cell></row><row><cell>L train</cell><cell>0.86</cell><cell>0.90</cell><cell>0.65</cell><cell>0.82</cell><cell>0.45</cell><cell>0.97</cell></row><row><cell>L N train</cell><cell>2.39</cell><cell>2.16</cell><cell>6.66</cell><cell>0.96</cell><cell>7.78</cell><cell>1.01</cell></row><row><cell>ImageNet (%)</cell><cell>78.5</cell><cell>79.3</cell><cell>74.6</cell><cell>79.9</cell><cell>66.4</cell><cell>77.4</cell></row><row><cell>ImageNet-C (%)</cell><cell>50.0</cell><cell>52.2</cell><cell>46.6</cell><cell>56.5</cell><cell>33.8</cell><cell>48.8</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of ResNets, ViTs, and MLP-Mixers trained from scratch on ImageNet with SAM (improvement over the vanilla model is shown in the parentheses). We use the Inception-style preprocessing (with resolution 224) rather than a combination of strong data augmentations. Surprisingly, SGD + SAM can push the result to 79.1%, which is a huge +7.6% absolute improvement. Although Adam + SAM is still higher (79.9%), their gap largely shrinks.</figDesc><table><row><cell>Model</cell><cell>#params</cell><cell>Throughput (img/sec/core)</cell><cell>ImageNet</cell><cell>ReaL</cell><cell>V2</cell><cell cols="2">ImageNet-R ImageNet-C</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50-SAM</cell><cell>25M</cell><cell>2161</cell><cell>76.7 (+0.7)</cell><cell>83.1 (+0.7)</cell><cell>64.6 (+1.0)</cell><cell>23.3 (+1.1)</cell><cell>46.5 (+1.9)</cell></row><row><cell>ResNet-101-SAM</cell><cell>44M</cell><cell>1334</cell><cell>78.6 (+0.8)</cell><cell>84.8 (+0.9)</cell><cell>66.7 (+1.4)</cell><cell>25.9 (+1.5)</cell><cell>51.3 (+2.8)</cell></row><row><cell>ResNet-152-SAM</cell><cell>60M</cell><cell>935</cell><cell>79.3 (+0.8)</cell><cell>84.9 (+0.7)</cell><cell>67.3 (+1.0)</cell><cell>25.7 (+0.4)</cell><cell>52.2 (+2.2)</cell></row><row><cell>ResNet-50x2-SAM</cell><cell>98M</cell><cell>891</cell><cell>79.6 (+1.5)</cell><cell>85.3 (+1.6)</cell><cell>67.5 (+1.7)</cell><cell>26.0 (+2.9)</cell><cell>50.7 (+3.9)</cell></row><row><cell>ResNet-101x2-SAM</cell><cell>173M</cell><cell>519</cell><cell>80.9 (+2.4)</cell><cell>86.4 (+2.4)</cell><cell>69.1 (+2.8)</cell><cell>27.8 (+3.2)</cell><cell>54.0 (+4.7)</cell></row><row><cell>ResNet-152x2-SAM</cell><cell>236M</cell><cell>356</cell><cell>81.1 (+1.8)</cell><cell>86.4 (+1.9)</cell><cell>69.6 (+2.3)</cell><cell>28.1 (+2.8)</cell><cell>55.0 (+4.2)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Vision Transformer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-S/32-SAM</cell><cell>23M</cell><cell>6888</cell><cell>70.5 (+2.1)</cell><cell>77.5 (+2.3)</cell><cell>56.9 (+2.6)</cell><cell>21.4 (+2.4)</cell><cell>46.2 (+2.9)</cell></row><row><cell>ViT-S/16-SAM</cell><cell>22M</cell><cell>2043</cell><cell>78.1 (+3.7)</cell><cell>84.1 (+3.7)</cell><cell>65.6 (+3.9)</cell><cell>24.7 (+4.7)</cell><cell>53.0 (+6.5)</cell></row><row><cell>ViT-S/14-SAM</cell><cell>22M</cell><cell>1234</cell><cell>78.8 (+4.0)</cell><cell>84.8 (+4.5)</cell><cell>67.2 (+5.2)</cell><cell>24.4 (+4.7)</cell><cell>54.2 (+7.0)</cell></row><row><cell>ViT-S/8-SAM</cell><cell>22M</cell><cell>333</cell><cell>81.3 (+5.3)</cell><cell>86.7 (+5.5)</cell><cell>70.4 (+6.2)</cell><cell>25.3 (+6.1)</cell><cell>55.6 (+8.5)</cell></row><row><cell>ViT-B/32-SAM</cell><cell>88M</cell><cell>2805</cell><cell>73.6 (+4.1)</cell><cell>80.3 (+5.1)</cell><cell>60.0 (+4.7)</cell><cell>24.0 (+4.1)</cell><cell>50.7 (+6.7)</cell></row><row><cell>ViT-B/16-SAM</cell><cell>87M</cell><cell>863</cell><cell>79.9 (+5.3)</cell><cell>85.2 (+5.4)</cell><cell>67.5 (+6.2)</cell><cell>26.4 (+6.3)</cell><cell>56.5 (+9.9)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MLP-Mixer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mixer-S/32-SAM</cell><cell>19M</cell><cell>11401</cell><cell>66.7 (+2.8)</cell><cell>73.8 (+3.5)</cell><cell>52.4 (+2.9)</cell><cell>18.6 (+2.7)</cell><cell>39.3 (+4.1)</cell></row><row><cell>Mixer-S/16-SAM</cell><cell>18M</cell><cell>4005</cell><cell>72.9 (+4.1)</cell><cell>79.8 (+4.7)</cell><cell>58.9 (+4.1)</cell><cell>20.1 (+4.2)</cell><cell>42.0 (+6.4)</cell></row><row><cell>Mixer-S/8-SAM</cell><cell>20M</cell><cell>1498</cell><cell>75.9 (+5.7)</cell><cell>82.5 (+6.3)</cell><cell>62.3 (+6.2)</cell><cell>20.5 (+5.1)</cell><cell>42.4 (+7.8)</cell></row><row><cell>Mixer-B/32-SAM</cell><cell>60M</cell><cell>4209</cell><cell cols="3">72.4 (+9.9) 79.0 (+10.9) 58.0 (+10.4)</cell><cell>22.8 (+8.2)</cell><cell>46.2 (12.4)</cell></row><row><cell>Mixer-B/16-SAM</cell><cell>59M</cell><cell>1390</cell><cell cols="5">77.4 (+11.0) 83.5 (+11.4) 63.9 (+13.1) 24.7 (+10.2) 48.8 (+15.0)</cell></row><row><cell>Mixer-B/8-SAM</cell><cell>64M</cell><cell>466</cell><cell cols="3">79.0 (+10.4) 84.4 (+10.1) 65.5 (+11.6)</cell><cell>23.5 (+9.2)</cell><cell>48.9 (+16.9)</cell></row><row><cell>74.6%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy and robustness of two hybrid architectures.</figDesc><table><row><cell>Model</cell><cell>#params</cell><cell>ImageNet (%)</cell><cell>ImageNet-C (%)</cell></row><row><cell>R50-S/16 R50-S/16-SAM</cell><cell>34M</cell><cell>79.8 81.0 (+1.2)</cell><cell>53.4 57.2 (+3.8)</cell></row><row><cell>R50-B/16 R50-B/16-SAM</cell><cell>99M</cell><cell>79.7 82.4 (+2.7)</cell><cell>54.4 61.0 (+6.6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Dominant eigenvalue ? max of the sub-diagonal Hessians for different network components, and norm of the model parameter w and the post-activation a k of block k. Each ViT block consists of a MSA and a MLP, and MLP-Mixer alternates between a token MLP a channel MLP. Shallower layers have larger ? max . SAM smooths every component.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="4">? max of diagonal blocks of Hessian</cell><cell></cell><cell></cell><cell>w 2</cell><cell>a 1 2</cell><cell>a 6 2</cell><cell>a 12 2</cell></row><row><cell></cell><cell>Embedding</cell><cell>MSA/ Token MLP</cell><cell>MLP/ Channel MLP</cell><cell cols="4">Block1 Block6 Block12 Whole</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-B/16</cell><cell>300.4</cell><cell>179.8</cell><cell>281.4</cell><cell>44.4</cell><cell>32.4</cell><cell>26.9</cell><cell cols="4">738.8 269.3 104.9 104.3</cell><cell>138.1</cell></row><row><cell>ViT-B/16-SAM</cell><cell>3.8</cell><cell>8.5</cell><cell>9.6</cell><cell>1.7</cell><cell>1.7</cell><cell>1.5</cell><cell>20.9</cell><cell cols="3">353.8 117.0 120.3</cell><cell>97.2</cell></row><row><cell>Mixer-B/16</cell><cell>1042.3</cell><cell>95.8</cell><cell>417.9</cell><cell>239.3</cell><cell>41.2</cell><cell>5.1</cell><cell cols="2">1644.4 197.6</cell><cell>96.7</cell><cell>135.1</cell><cell>74.9</cell></row><row><cell>Mixer-B/16-SAM</cell><cell>18.2</cell><cell>1.4</cell><cell>9.5</cell><cell>4.0</cell><cell>1.1</cell><cell>0.3</cell><cell>22.5</cell><cell cols="3">389.9 110.9 176.0</cell><cell>216.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Data augmentations, SAM, and their combination applied to different model architectures trained on ImageNet and its subsets from scratch.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ResNet-152</cell><cell></cell><cell></cell><cell cols="2">ViT-B/16</cell><cell></cell><cell></cell><cell cols="2">Mixer-B/16</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">Vanilla SAM AUG</cell><cell>SAM + AUG</cell><cell cols="3">Vanilla SAM AUG</cell><cell>SAM + AUG</cell><cell cols="3">Vanilla SAM AUG</cell><cell>SAM + AUG</cell></row><row><cell>ImageNet</cell><cell>78.5</cell><cell>79.3</cell><cell>78.8</cell><cell>78.9</cell><cell>74.6</cell><cell>79.9</cell><cell>79.6</cell><cell>81.5</cell><cell>66.4</cell><cell>77.4</cell><cell>76.5</cell><cell>78.1</cell></row><row><cell>i1k (1/2)</cell><cell>74.2</cell><cell>75.6</cell><cell>75.1</cell><cell>75.5</cell><cell>64.9</cell><cell>75.4</cell><cell>73.1</cell><cell>75.8</cell><cell>53.9</cell><cell>71.0</cell><cell>70.4</cell><cell>73.1</cell></row><row><cell>i1k (1/4)</cell><cell>68.0</cell><cell>70.3</cell><cell>70.2</cell><cell>70.6</cell><cell>52.4</cell><cell>66.8</cell><cell>63.2</cell><cell>65.6</cell><cell>37.2</cell><cell>62.8</cell><cell>61.0</cell><cell>65.8</cell></row><row><cell>i1k (1/10)</cell><cell>54.6</cell><cell>57.1</cell><cell>59.2</cell><cell>59.5</cell><cell>32.8</cell><cell>46.1</cell><cell>38.5</cell><cell>45.7</cell><cell>21.0</cell><cell>43.5</cell><cell>43.0</cell><cell>51.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell cols="5">: Comparison between ViT-B/16-SAM</cell></row><row><cell cols="5">and ViT-B/16-AUG. R denotes the missing rate</cell></row><row><cell cols="3">under linear interpolation.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>? max</cell><cell cols="2">L train L N train</cell><cell>R(?)</cell></row><row><cell>ViT-B/16</cell><cell>738.8</cell><cell>0.65</cell><cell>6.66</cell><cell>57.9%</cell></row><row><cell>ViT-B/16-SAM</cell><cell>20.9</cell><cell>0.82</cell><cell>0.96</cell><cell>39.6%</cell></row><row><cell cols="2">ViT-B/16-AUG 1659.3</cell><cell>0.85</cell><cell>1.23</cell><cell>21.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison under the adversarial training framework on ImageNet (numbers in the parentheses denote the improvement over the standard adversarial training without SAM). With similar model size and throughput, ViTs-SAM can still outperform ResNets-SAM for clean accuracy and adversarial robustness.</figDesc><table><row><cell>Model</cell><cell>#params</cell><cell>Throughput (img/sec/core)</cell><cell>ImageNet</cell><cell>Real</cell><cell>V2</cell><cell>PGD-10</cell><cell cols="2">ImageNet-R ImageNet-C</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ResNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50-SAM</cell><cell>25M</cell><cell>2161</cell><cell>70.1 (-0.7)</cell><cell>77.9 (-0.3)</cell><cell>56.6 (-0.8)</cell><cell>54.1 (+0.9)</cell><cell>27.0 (+0.9)</cell><cell>42.7 (-0.1)</cell></row><row><cell>ResNet-101-SAM</cell><cell>44M</cell><cell>1334</cell><cell>73.6 (-0.4)</cell><cell>81.0 (+0.1)</cell><cell>60.4 (-0.6)</cell><cell>58.8 (+1.4)</cell><cell>29.5 (+0.6)</cell><cell>46.9 (+0.3)</cell></row><row><cell>ResNet-152-SAM</cell><cell>60M</cell><cell>935</cell><cell>75.1 (-0.4)</cell><cell>82.3 (+0.2)</cell><cell>62.2 (-0.4)</cell><cell>61.0 (+1.8)</cell><cell>30.8 (+1.4)</cell><cell>49.1 (+0.6)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Vision Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-S/16-SAM</cell><cell>22M</cell><cell>2043</cell><cell>73.2 (+1.2)</cell><cell>80.7 (+1.7)</cell><cell>60.2 (+1.4)</cell><cell>58.0 (+5.2)</cell><cell>28.4 (+2.4)</cell><cell>47.5 (+1.6)</cell></row><row><cell>ViT-B/32-SAM</cell><cell>88M</cell><cell>2805</cell><cell>69.9 (+3.0)</cell><cell>76.9 (+3.4)</cell><cell>55.7 (+2.5)</cell><cell>54.0 (+6.4)</cell><cell>26.0 (+3.0)</cell><cell>46.4 (+3.0)</cell></row><row><cell>ViT-B/16-SAM</cell><cell>87M</cell><cell>863</cell><cell>76.7 (+3.9)</cell><cell>82.9 (+4.1)</cell><cell>63.6 (+4.3)</cell><cell>62.0 (+7.7)</cell><cell>30.0 (+4.9)</cell><cell>51.4 (+5.0)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MLP-Mixer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mixer-S/16-SAM</cell><cell>18M</cell><cell>4005</cell><cell>67.1 (+2.2)</cell><cell>74.5 (+2.3)</cell><cell>52.8 (+2.5)</cell><cell>50.1 (+4.1)</cell><cell>22.9 (+2.6)</cell><cell>37.9 (+2.5)</cell></row><row><cell>Mixer-B/32-SAM</cell><cell>60M</cell><cell>4209</cell><cell cols="4">69.3 (+9.1) 76.4 (+10.2) 54.7 (+9.4) 54.5 (+13.9)</cell><cell>26.3 (+8.0)</cell><cell>43.7 (+8.8)</cell></row><row><cell>Mixer-B/16-SAM</cell><cell>59M</cell><cell>1390</cell><cell cols="6">73.9 (+11.1) 80.8 (+11.8) 60.2 (+11.9) 59.8 (+17.3) 29.0 (+10.5) 45.9 (+12.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Specifications of the ViT and MLP-Mixer architectures used in this paper. We train all the architectures with image resolution 224 ? 224.</figDesc><table><row><cell>Model</cell><cell>#params</cell><cell>Throughput (img/sec/core)</cell><cell>Patch Resolution</cell><cell>Sequence Length</cell><cell cols="3">Hidden Size #heads #layers</cell><cell>Token MLP Dimension</cell><cell>Channel MLP Dimension</cell></row><row><cell>ViT-S/32</cell><cell>23M</cell><cell>6888</cell><cell>32 ? 32</cell><cell>49</cell><cell>384</cell><cell>6</cell><cell>12</cell><cell>-</cell><cell>-</cell></row><row><cell>ViT-S/16</cell><cell>22M</cell><cell>2043</cell><cell>16 ? 16</cell><cell>196</cell><cell>384</cell><cell>6</cell><cell>12</cell><cell>-</cell><cell>-</cell></row><row><cell>ViT-S/14</cell><cell>22M</cell><cell>1234</cell><cell>14 ? 14</cell><cell>256</cell><cell>384</cell><cell>6</cell><cell>12</cell><cell>-</cell><cell>-</cell></row><row><cell>ViT-S/8</cell><cell>22M</cell><cell>333</cell><cell>8 ? 8</cell><cell>784</cell><cell>384</cell><cell>6</cell><cell>12</cell><cell>-</cell><cell>-</cell></row><row><cell>ViT-B/32</cell><cell>88M</cell><cell>2805</cell><cell>32 ? 32</cell><cell>49</cell><cell>768</cell><cell>12</cell><cell>12</cell><cell>-</cell><cell>-</cell></row><row><cell>ViT-B/16</cell><cell>87M</cell><cell>863</cell><cell>16 ? 16</cell><cell>196</cell><cell>768</cell><cell>12</cell><cell>12</cell><cell>-</cell><cell>-</cell></row><row><cell>Mixer-S/32</cell><cell>19M</cell><cell>11401</cell><cell>32 ? 32</cell><cell>49</cell><cell>512</cell><cell>-</cell><cell>8</cell><cell>256</cell><cell>2048</cell></row><row><cell>Mixer-S/16</cell><cell>18M</cell><cell>4005</cell><cell>16 ? 16</cell><cell>196</cell><cell>512</cell><cell>-</cell><cell>8</cell><cell>256</cell><cell>2048</cell></row><row><cell>Mixer-S/8</cell><cell>20M</cell><cell>1498</cell><cell>8 ? 8</cell><cell>784</cell><cell>512</cell><cell>-</cell><cell>8</cell><cell>256</cell><cell>2048</cell></row><row><cell>Mixer-B/32</cell><cell>60M</cell><cell>4209</cell><cell>32 ? 32</cell><cell>49</cell><cell>768</cell><cell>-</cell><cell>12</cell><cell>384</cell><cell>3072</cell></row><row><cell>Mixer-B/16</cell><cell>59M</cell><cell>1390</cell><cell>16 ? 16</cell><cell>196</cell><cell>768</cell><cell>-</cell><cell>12</cell><cell>384</cell><cell>3072</cell></row><row><cell>Mixer-B/8</cell><cell>64M</cell><cell>466</cell><cell>8 ? 8</cell><cell>784</cell><cell>768</cell><cell>-</cell><cell>12</cell><cell>384</cell><cell>3072</cell></row><row><cell cols="7">B WHEN SAM MEETS ADVERSARIAL TRAINING</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Interestingly, SAM and adversarial training are both minimax problems except that SAM's inner</cell></row><row><cell cols="10">maximization is with respect to the network weights, while the latter concerns about the input for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameters for downstream tasks. All models are fine-tuned with 224 ? 224 resolution, a batch size of 512, cosine learning rate decay, no weight decay, and grad clipping at global norm 1.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Total steps Warmup steps</cell><cell>Base LR</cell></row><row><cell>CIFAR-10</cell><cell>10K</cell><cell>500</cell><cell></cell></row><row><cell>CIFAR-100 Flowers</cell><cell>10K 500</cell><cell>500 100</cell><cell>{0.001, 0.003, 0.01, 0.03}</cell></row><row><cell>Pets</cell><cell>500</cell><cell>100</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Accuracy on downstream tasks of the models pre-trained on ImageNet. SAM improves ViTs and MLP-Mixers' transferabilities. ViTs transfer better than ResNets of similar sizes.</figDesc><table><row><cell>%</cell><cell>ResNet-50-SAM</cell><cell>ResNet-152-SAM</cell><cell>ViT-S/16</cell><cell>ViT-S/16-SAM</cell><cell>ViT-B/16</cell><cell>ViT-B/16-SAM</cell><cell>Mixer-S/16</cell><cell>Mixer-S/16-SAM</cell><cell>Mixer-B/16</cell><cell>Mixer-B/16-SAM</cell></row><row><cell>CIFAR-10</cell><cell>97.4</cell><cell>98.2</cell><cell>97.6</cell><cell>98.2</cell><cell>98.1</cell><cell>98.6</cell><cell>94.1</cell><cell>96.1</cell><cell>95.4</cell><cell>97.8</cell></row><row><cell>CIFAR-100</cell><cell>85.2</cell><cell>87.8</cell><cell>85.7</cell><cell>87.6</cell><cell>87.6</cell><cell>89.1</cell><cell>77.9</cell><cell>82.4</cell><cell>80.0</cell><cell>86.4</cell></row><row><cell>Flowers</cell><cell>90.0</cell><cell>91.1</cell><cell>86.4</cell><cell>91.5</cell><cell>88.5</cell><cell>91.8</cell><cell>83.3</cell><cell>87.9</cell><cell>82.8</cell><cell>90.0</cell></row><row><cell>Pets</cell><cell>91.6</cell><cell>93.3</cell><cell>90.4</cell><cell>92.9</cell><cell>91.9</cell><cell>93.1</cell><cell>86.1</cell><cell>88.7</cell><cell>86.1</cell><cell>92.5</cell></row><row><cell>Average</cell><cell>91.1</cell><cell>92.6</cell><cell>90.0</cell><cell>92.6</cell><cell>91.5</cell><cell>93.2</cell><cell>85.4</cell><cell>88.8</cell><cell>86.1</cell><cell>91.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>). We use image resolution 224 ? 224 during fine-tuning on downstream tasks, other settings exactly follow Dosovitskiy et al. (2021); Tolstikhin et al. (2021) (see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>The SAM perturbation strength ? for training on ImageNet. ViTs and MLP-Mixers favor larger ? than ResNets does. Larger models with longer patch sequences need stronger strengths.</figDesc><table><row><cell>Model</cell><cell>Task</cell><cell>SAM ?</cell></row><row><cell></cell><cell>ResNet</cell><cell></cell></row><row><cell>ResNet-50-SAM</cell><cell>supervised</cell><cell>0.02</cell></row><row><cell>ResNet-101-SAM</cell><cell>supervised</cell><cell>0.05</cell></row><row><cell>ResNet-152-SAM</cell><cell>supervised</cell><cell>0.02</cell></row><row><cell>ResNet-50x2-SAM</cell><cell>supervised</cell><cell>0.05</cell></row><row><cell>ResNet-101x2-SAM</cell><cell>supervised</cell><cell>0.05</cell></row><row><cell>ResNet-152x2-SAM</cell><cell>supervised</cell><cell>0.05</cell></row><row><cell>ResNet-50-SAM</cell><cell>adversarial</cell><cell>0.05</cell></row><row><cell>ResNet-101-SAM</cell><cell>adversarial</cell><cell>0.05</cell></row><row><cell>ResNet-152-SAM</cell><cell>adversarial</cell><cell>0.05</cell></row><row><cell></cell><cell>ViT</cell><cell></cell></row><row><cell>ViT-S/32-SAM</cell><cell>supervised</cell><cell>0.05</cell></row><row><cell>ViT-S/16-SAM</cell><cell>supervised</cell><cell>0.1</cell></row><row><cell>ViT-S/14-SAM</cell><cell>supervised</cell><cell>0.1</cell></row><row><cell>ViT-S/8-SAM</cell><cell>supervised</cell><cell>0.15</cell></row><row><cell>ViT-B/32-SAM</cell><cell>supervised</cell><cell>0.15</cell></row><row><cell>ViT-B/16-SAM</cell><cell>supervised</cell><cell>0.2</cell></row><row><cell>ViT-B/16-AUG-SAM</cell><cell>supervised</cell><cell>0.05</cell></row><row><cell>ViT-S/16-SAM</cell><cell>adversarial</cell><cell>0.1</cell></row><row><cell>ViT-B/32-SAM</cell><cell>adversarial</cell><cell>0.1</cell></row><row><cell>ViT-B/16-SAM</cell><cell>adversarial</cell><cell>0.1</cell></row><row><cell>ViT-S/16-SAM</cell><cell>supervised contrastive</cell><cell>0.02</cell></row><row><cell>ViT-B/16-SAM</cell><cell>supervised contrastive</cell><cell>0.02</cell></row><row><cell cols="2">MLP-Mixer</cell><cell></cell></row><row><cell>Mixer-S/32-SAM</cell><cell>supervised</cell><cell>0.1</cell></row><row><cell>Mixer-S/16-SAM</cell><cell>supervised</cell><cell>0.15</cell></row><row><cell>Mixer-S/8-SAM</cell><cell>supervised</cell><cell>0.2</cell></row><row><cell>Mixer-B/32-SAM</cell><cell>supervised</cell><cell>0.35</cell></row><row><cell>Mixer-B/16-SAM</cell><cell>supervised</cell><cell>0.6</cell></row><row><cell>Mixer-B/8-SAM</cell><cell>supervised</cell><cell>0.6</cell></row><row><cell>Mixer-B/16-AUG-SAM</cell><cell>supervised</cell><cell>0.2</cell></row><row><cell>Mixer-S/16-SAM</cell><cell>adversarial</cell><cell>0.05</cell></row><row><cell>Mixer-B/32-SAM</cell><cell>adversarial</cell><cell>0.25</cell></row><row><cell>Mixer-B/16-SAM</cell><cell>adversarial</cell><cell>0.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Hyperparameters for training from scratch on ImageNet with basic Inception-style preprocessing and 224 ? 224 image resolution.</figDesc><table><row><cell></cell><cell>ResNet</cell><cell>ViT</cell><cell>MLP-Mixer</cell></row><row><cell>Data augmentation</cell><cell></cell><cell>Inception-style</cell><cell></cell></row><row><cell>Input resolution</cell><cell></cell><cell>224 ? 224</cell><cell></cell></row><row><cell>Batch size</cell><cell></cell><cell>4,096</cell><cell></cell></row><row><cell>Epoch</cell><cell>90</cell><cell>300</cell><cell>300</cell></row><row><cell>Warmup steps</cell><cell>5K</cell><cell>10K</cell><cell>10K</cell></row><row><cell>Peak learning rate Learning rate decay</cell><cell>0.1 ? batch size 256 cosine</cell><cell>3e-3 cosine</cell><cell>3e-3 linear</cell></row><row><cell>Optimizer</cell><cell>SGD</cell><cell>AdamW</cell><cell>AdamW</cell></row><row><cell>SGD Momentum</cell><cell>0.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Adam (? 1 , ? 2 )</cell><cell>-</cell><cell cols="2">(0.9, 0.999) (0.9, 0.999)</cell></row><row><cell>Weight decay</cell><cell>1e-3</cell><cell>0.3</cell><cell>0.3</cell></row><row><cell>Dropout rate</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell></row><row><cell>Stochastic depth</cell><cell>-</cell><cell>-</cell><cell>0.1</cell></row><row><cell>Gradient clipping</cell><cell>-</cell><cell>1.0</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13</head><label>13</label><figDesc></figDesc><table><row><cell cols="8">: ImageNet top-1 accuracy (%) of ViT-B/16 and Mixer-B/16 when trained from scratch</cell></row><row><cell cols="4">with different perturbation strength ? in SAM.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAM ?</cell><cell cols="2">0.0 0.05 0.1</cell><cell cols="4">0.2 0.25 0.35 0.4</cell><cell>0.5</cell><cell>0.6 0.65</cell></row><row><cell>ViT-B/16</cell><cell cols="4">74.6 77.5 78.8 79.9 79.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Mixer-B/16 66.4 69.5</cell><cell>-</cell><cell>-</cell><cell cols="4">74.1 74.7 75.6 76.9 77.4 77.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>ImageNet accuracy and curvature analysis for ViT-B/16 when we vary the weight decay strength in Adam (AdamW).</figDesc><table><row><cell>Model</cell><cell cols="2">Weight decay ImageNet (%)</cell><cell cols="3">w 2 L train L N train</cell><cell>? max</cell></row><row><cell></cell><cell>0.2</cell><cell>74.2</cell><cell>339.8</cell><cell>0.51</cell><cell>4.22</cell><cell>507.4</cell></row><row><cell>ViT-B/16</cell><cell>0.3 0.4</cell><cell>74.6 74.7</cell><cell>269.3 236.7</cell><cell>0.65 0.77</cell><cell>6.66 7.08</cell><cell>738.8 1548.9</cell></row><row><cell></cell><cell>0.5</cell><cell>74.4</cell><cell>211.8</cell><cell>0.98</cell><cell>7.21</cell><cell>2251.7</cell></row><row><cell></cell><cell>0.2</cell><cell>79.9</cell><cell>461.4</cell><cell>0.69</cell><cell>0.72</cell><cell>13.1</cell></row><row><cell>ViT-B/16-SAM</cell><cell>0.3 0.4</cell><cell>79.9 79.4</cell><cell>353.8 301.1</cell><cell>0.82 0.85</cell><cell>0.96 0.98</cell><cell>20.9 26.1</cell></row><row><cell></cell><cell>0.5</cell><cell>78.7</cell><cell>259.6</cell><cell>0.95</cell><cell>1.33</cell><cell>45.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://en.wikipedia.org/wiki/Power_iteration</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This work is partially supported by NSF under IIS-1901527, IIS-2008173, IIS-2048280  and by Army Research Laboratory under agreement number W911NF-20-2-0158.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>We provide comprehensive experimental details and references to existing works and codebases to ensure reproducibility. The specification of all the architectures used in this paper is available in Appendix A. The instructions for plotting the landscape and the attention map are detailed in Appendix E. We also present our approach to approximating Hessian's dominant eigenvalue ? max and the NTK condition number in Appendices F and G, respectively. Finally, Appendix H describes all the necessary training configurations, data augmentations, and SAM hyperparameters to ensure the reproducibility of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G NTK CONDITION NUMBER</head><p>We approximate the neural tangent kernel on the i1k (1/10) subset by averaging over block diagonal entries (with block size 48 ? 48) in the full NTK. Notice that the computation is based on the architecture at initialization without training. As the activation plays an important role when computing NTK -we find that smoother activation functions enjoy smaller condition numbers, we replace the GELU in ViT and MLP-Mixer with ReLU for a fair comparison with ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H TRAINING DETAILS</head><p>We use image resolution 224 ? 224 during fine-tuning on downstream tasks, other settings exactly follow <ref type="bibr" target="#b53">Tolstikhin et al., 2021</ref>) (see <ref type="table">Table 9</ref>). Note that we do not employ SAM during fine-tuning. We perform a grid search over the base learning rates on small sub-splits of the training sets (10% for Flowers and Pets, 2% for CIFAR-10/100). After that, we fine-tune on the entire training sets and report the results on the respective test sets.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11178</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<title level="m">Jonathon Shlens, and Barret Zoph. Revisiting resnets: Improved training and scaling strategies</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Xiaohua Zhai, and A?ron van den Oord. Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Practical Gauss-Newton optimisation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hippolyt</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v70/botev17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Initialization of relus for dynamical isometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebekka</forename><surname>Burkholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Dubatovka</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>In</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>D&amp;apos;alch?-Buc</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf" />
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Entropy-sgd: Biasing gradient descent into wide valleys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/chen20j.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>Hal Daum? III and Aarti Singh</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural architecture search on imagenet in four GPU hours: A theoretically inspired perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Cnon5ezMHtu" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stabilizing differentiable architecture search via perturbationbased regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/chen20f.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>Hal Daum? III and Aarti Singh</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust and accurate object detection via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="16622" to="16631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW50498.2020.00359</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3008" to="3017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
		<ptr target="http://auai.org/uai2017/proceedings/papers/173.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence</title>
		<editor>Gal Elidan, Kristian Kersting, and Alexander T. Ihler</editor>
		<meeting>the Thirty-Third Conference on Uncertainty in Artificial Intelligence<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2017-08-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=6Tm1mposlrM" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJz6tiCqYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Gaussian error linear units (gelus</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the relation between the sharpest directions of DNN loss and the SGD step length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amost</forename><surname>Storkey</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkgEaj05t7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping Tak Peter</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1oyRlYgg" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An alternative view: When does SGD escape local minima?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobby</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/kleinberg18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>978-3-030-58558-7</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="491" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJzIBfZAb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02723</idno>
		<title level="m">Do you even need attention? a stack of feed-forward layers does surprisingly well on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate o(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USSR Academy of Sciences</title>
		<meeting>the USSR Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICVGIP.2008.47</idno>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2012.6248092</idno>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial training for free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Amin Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Trainability of relu networks and data-dependent initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeonjong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Em</forename><surname>Karniadakis</surname></persName>
		</author>
		<idno>2689-3967</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning for Modeling and Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="74" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A bayesian perspective on generalization and stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJij4yg0Z" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.97</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fast is better than free: Revisiting adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJx040EFvH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adversarial weight perturbation helps robust generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1ef91c212e30e14bf125e9374262401f-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2958" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Disentangling trainability and generalization in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/xiao20b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>Hal Daum? III and Aarti Singh</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mean field residual networks: On the edge of chaos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/81c650caac28cdefce4de5ddc18befa0-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Syx4wnEtvH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<idno type="DOI">10.1109/ICCV.2019.00612</idno>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1gDNyrKDS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Why are adaptive methods good for attention models? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sai Praneeth Karimireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyeon</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sra</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/b05b57f6add810d3b7490866d74c0053-Abstract.html" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
