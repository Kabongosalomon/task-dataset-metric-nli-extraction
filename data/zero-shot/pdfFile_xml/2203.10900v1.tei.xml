<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Tan</surname></persName>
							<email>qingyu.tan@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
							<email>ruidan.he@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
							<email>l.bing@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
						</author>
						<title level="a" type="main">Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of document-level relation extraction 2 (DocRE) is highly important for information extraction and NLP research. The DocRE task aims to extract relations among multiple entities within a document. The DocRE task is more challenging than its sentence-level counterpart in the following aspects: (1) The complexity of DocRE increases quadratically with the number of entities. If a document contains n entities, classification decisions must be made on n(n ? 1) entity pairs and most of them do not contain any relation. (2) Aside from the imbalance of positive and negative examples, the distribution of relation types for the positive entity pairs is also highly imbalanced. Considering the DocRED <ref type="bibr" target="#b24">(Yao et al., 2019)</ref> dataset as an example, there are 96 relation types in total, where the top 10 relations take up 59.4% of all the relation labels. This imbalance significantly increases the difficulty of the document-level RE task.</p><p>Most existing approaches of DocRE leverage dependency information to construct a documentlevel graph <ref type="bibr" target="#b26">(Zeng et al., 2021;</ref><ref type="bibr" target="#b27">Zeng et al., 2020)</ref>, and then use graph neural networks for reasoning. Another popular strand of this field uses transformer-only <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref> architecture <ref type="bibr" target="#b31">(Zhou et al., 2021;</ref><ref type="bibr" target="#b28">Zhang et al., 2021)</ref>. Such models are able to achieve state-of-theart performance without explicit graph reasoning, showing that pre-trained language models (PrLMs) are able to implicitly capture long-distance relationships. However, there are three limitations of the existing DocRE methods. Firstly, existing methods mainly focus on the syntactic features from PrLMs while neglecting the interactions between entity pairs. <ref type="bibr" target="#b28">Zhang et al. (2021)</ref> and <ref type="bibr" target="#b6">Li et al. (2021)</ref> have used CNN structure to encode the interaction between entity pairs, but CNN structure cannot capture all the elements within the two-hop reasoning paths. Secondly, there is no prior work that explicitly tackles the class-imbalance problem for DocRE. Existing works <ref type="bibr" target="#b31">(Zhou et al., 2021;</ref><ref type="bibr" target="#b28">Zhang et al., 2021;</ref><ref type="bibr" target="#b27">Zeng et al., 2020)</ref> only focus on threshold learning for balancing the positive and negative examples, but the class-imbalance problem within positive examples is not addressed. Lastly, there are very few works discussing the method of adapting distantly supervised data for the DocRE task.  has shown that distantly supervised data is able to improve the performance of document-level relation extraction. However, it only uses the distantly supervised data to pre-train the RE model in a naive manner.</p><p>To overcome the limitations of existing works, we propose a semi-supervised learning framework for document-level relation extraction. Firstly, to improve the reasoning for two-hop relations, we propose to use an axial attention module as feature extractor. This module enables us to attend to elements that are within two-hop logical paths and capture the interdependency among the relation triplets. Secondly, we propose Adaptive Focal Loss to address the imbalanced label distribution problem. The proposed loss function encourages the long-tail classes to contribute more to the overall loss. Lastly, we use knowledge distillation to overcome the differences between the annotated data and the distantly supervised data. Specifically, we first train a teacher model with a small amount of human annotated data. The teacher model will then be used to generate predictions on a large amount of distantly supervised data. The generated predictions are used as soft labels for pre-training our student model. Finally, the pre-trained student model is further fine-tuned on the human annotated data.</p><p>We conducted experiments on two datasets -the DocRED <ref type="bibr" target="#b24">(Yao et al., 2019)</ref> dataset and the Ha-cRED <ref type="bibr" target="#b1">(Cheng et al., 2021)</ref> dataset. Experimental results show that our model consistently outperforms competitive baselines. Moreover, our model significantly outperforms the existing state-of-theart SSAN-Adapt  on the DocRED leaderboard by 1.36 in F1 score and 1.46 in Ign_F1 score. 3 Besides, we provide a thorough ablation study and error analysis to identify the bottleneck of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>In this section, we describe the task formulation of document-level relation classification. Given a document D that contains a set of entities {e i } n i=1 , the document-level relation extraction task is to predict the relation types between entity pairs (e s , e o ) s,o?{1...n},s =o , where the subscripts of e s and e o refer to subject and object. The set of relations is defined as R ? {NR}, where NR stands for no relation. An entity may occur multiple times in a document, thus for each entity e i , there can be multiple mentions {m i j } Ne i j=1 . If no relation exists between the entities in the pair (e s , e o ), it will be labeled as NR. During test time, the relation labels for all entity pairs (e s , e o ) s,o?{1...n},s =o will be predicted. Essentially, this is a multi-label classification problem, as there can be multiple relations between e s and e o .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Architecture</head><p>As shown in <ref type="figure">Figure 1</ref>, our semi-supervised learning framework mainly consists of three parts: (1) representation learning; (2) adaptive focal loss; and (3) knowledge distillation for distant supervision pretraining. For representation learning, we first extract the contextual representation for each entitypair by a pre-trained language model. The entity pair representations will be further enhanced by the axial attention module, which will encode the inter-dependent information between entity pairs. We then use a feedforward neural network (FFN) classifier to obtain the logits and compute their losses. We use our proposed adaptive focal loss to better learn from long-tail classes. Finally, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. Specifically, we train a teacher model with the annotated data and use its output as soft labels. We then pre-train a student model based on the soft labels and the distant labels. The pretrained student model will be fine-tuned again with the annotated data. We will describe the details for each part in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Representation Learning Entity Representation</head><p>We use a pretrained language model as the encoder. For a document D of length l, we have D = [x t ] l t=1 , where x t is the word at location t. Following prior works for relation classification, we use special token markers to represent entities. The entity mentions will be marked by a special token "*" at the start and end position. We then use a pre-trained language model (PrLM) to obtain the contextualized embeddings H of this document.</p><formula xml:id="formula_0">H = P rLM ([x 1 , ..., x l ]) = [h 1 , ..., h l ]) (1)</formula><p>where H ? R l?d and d is the hidden dimension of the PrLM. If the document length exceeds the maximum position of the PrLM, the document will be encoded as multiple overlapping chunks, and the contextualized embeddings of the overlapping chunks will be averaged. We take the embedding of the special token "*" at the start of the mention as its embedding, which is denoted as h m j . Then, for each entity e i with mentions {m i j } Ne i j=1 , where N e i is the number of mentions for entity e i , its global representation is obtained by logsumexp pooling:</p><formula xml:id="formula_1">h e i = log Ne i j=1 exp(h m j )<label>(2)</label></formula><p>where h e i ? R d is the aggregated feature of e i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context-enhanced Entity Representation</head><p>As prior works  have shown that contextual information is crucial for the relation classification task, our model also adapts contextual pooling method from <ref type="bibr" target="#b31">Zhou et al. (2021)</ref>. For each entity e i , we first aggregate the attention output for its mentions by mean pooling</p><formula xml:id="formula_2">A e i = Ne i j=1 (a m j ),</formula><p>where a m j ? R H?l is the the self-attention weight at the position of mention m j , H is the number of attention heads, and l is the document length. Then the context query is calculated as:</p><formula xml:id="formula_3">q (s,o) = H i=1 (A i es ? A i eo ) (3) c (s,o) = H q (s,o)<label>(4)</label></formula><p>where A es ? R H?l is the aggregated attention output for entity e s , likewise for e o . q (s,o) ? R l is the mean-pooled attention weight for entity pair (e s , e o ) and H ? R l?d is the contextual embedding of the whole document. Then the context vector c (s,o) ? R d is fused with the entity representations.</p><formula xml:id="formula_4">z s = tanh(W s h es + W c c (s,o) )<label>(5)</label></formula><p>where z s ? R d is the context-enhanced representation of subject s for entity pair (e s , e o ). We obtain the object representation z o in the same manner.</p><p>Entity Pair Representation Following Zhou et al. <ref type="formula" target="#formula_1">(2021)</ref>, we use a grouped bilinear function for feature combination. The entity embedding z s will first will be split into k equal-sized groups, such that z s = [z 1 s , z 2 s , ..., z k s ]. We perform the same splitting for z o . The value g (s,o) i at each dimension of our entity pair representation is obtained by:</p><formula xml:id="formula_5">g (s,o) i = k j=1 (z j s W j g i z j o ) + b i g (s,o) = [g (s,o) 1 , g (s,o) 2 , ..., g (s,o) d ]<label>(6)</label></formula><p>where W j g i ? R d/k?d/k , for i = 1, ..., d, j = 1, ..., k, is the weight matrix for dimension i. b i is a scalar bias of dimension i. g (s,o) ? R d is our final entity pair representation.</p><p>For a given document D with n entities, we need to classify n(n ? 1) number of entity pair permutations. To help us encode all the entity pairs and their positions, we used an R n?n?d matrix G to represent all the entity pairs of document D, and the diagonal of the n ? n index is neglected during training and inference.</p><p>Axial Attention-Enhanced Entity Pair Representation Instead of using only head and tail embedding for relation classification, we propose to use two-hop attention to encode the axial neighboring information of each entity pair (e s , e o ) representation. Although there are prior works that use Convolution Neural Networks (CNNs) to encode the neighbor information for relation classification <ref type="bibr" target="#b28">(Zhang et al., 2021)</ref>, we believe that attending to the axial elements is more effective and intuitive. Given an n ? n entity table, for entity pair (e s , e o ), attending to its axial elements corresponds to attending to elements that are either (e s , e i ) or (e i , e o ). That is, if a two-hop relation (e s , e o ) can be dissected into a path (e s , e i ) and (e i , e o ), then the most informative neighbors for classifying (e s , e o ) are the one-hop candidates that share e s or e o with this entity pair. The axial attention is simply computed by self-attention along the height axis and the width axis, and each computation along the axes is followed by a residual connection. For the cell (e s , e o ), we have:</p><formula xml:id="formula_6">r (s,o) w = r (s,o) h + p?1..n sof tmaxp(q T (s,o) k (s,p) )v (s,p) r (s,o) h = g (s,o) + p?1...n sof tmaxp(q T (s,o) k (p,o) )v (p,o)<label>(7)</label></formula><p>where we denote query q</p><formula xml:id="formula_7">(i,j) = W Q g (i,j) , key k (i,j) = W K g (i,j) , and value v (i,j) = W V g (i,j)</formula><p>, which are all linear projections of the entity pair representation g at position</p><formula xml:id="formula_8">(i, j). W Q ? R d?d , W K ? R d?d , and W V ? R d?d are all learnable weight matrices. The output of the axial attention module is r (s,o) w ? R d .</formula><p>The sof tmax p function denotes a softmax function that applies to all possible p = (i, j) positions. The formulation of this mechanism resembles <ref type="bibr" target="#b21">Wang et al. (2020)</ref>. However, our motivation is different, as <ref type="bibr" target="#b21">Wang et al. (2020)</ref> aim to use this mechanism to reduce the computational complexity of semantic segmentation, whereas our motivation is to attend to the one-hop neighbors for the two-hop relation triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Adaptive Focal Loss</head><p>Finally, we have a linear layer for predicting relations:</p><formula xml:id="formula_9">l (s,o) = W l r (s,o) w + b l<label>(8)</label></formula><p>where l (s,o) ? R c denotes the output logits for all relations, W l ? R d?c is the weight matrix that maps the relation embedding to the logit of each class and c is the number of classes.</p><p>Our relation extraction problem is essentially a multi-label classification problem. Traditionally, binary cross-entropy (BCE) loss is used to tackle this problem. However, this method relies on a global probability threshold for inference. Recently Adaptive Thresholding Loss (ATL, <ref type="bibr" target="#b31">Zhou et al., 2021)</ref> has been proposed for multi-label classification. Instead of using a global probability threshold for all examples, ATL introduced a special class T H as the adaptive threshold value for each example. For each entity pair (e s , e o ), the classes whose logits are larger than the T H class logit will be predicted as positive classes, and the rest will be predicted as negative classes.</p><p>We propose Adaptive Focal Loss (AFL) as an enhancement to ATL for long-tail classes. Our loss consists of two parts, the first part is for positive classes and the second part is for negative classes. During training, the label space is divided into two subsets: positive class subset P T and negative class subset N T . The positive class subset P T contains the relations that exist in entity pair (e s , e o ), and if there is no relation between (e s , e o ), P T is empty (P T = ?). The negative subset N T , on the other hand, contains the relation classes that do not belong to the positive classes, N T = R \ P T . The probability of each positive class is computed as:</p><formula xml:id="formula_10">P (r i |e s , e o ) = exp(l (s,o) r i ) exp(l (s,o) r i ) + exp(l (s,o) T H )<label>(9)</label></formula><p>where the logit of r i is ranked with the logit of threshold class T H individually. This is different from the original ATL, where all positive logits are ranked together with a softmax function. For simplicity, P (r i |e s , e o ) is denoted as P (r i ) in this section, because we are only discussing (e s , e o ). For the negative classes, we use their logits to compute the probability of the T H class:</p><formula xml:id="formula_11">P (rT H |es, eo) = exp(l (s,o) r T H ) r j ?N T ?{T H} exp(l (s,o) r j )<label>(10)</label></formula><p>Similarly, P (r T H |e s , e o ) is referred to as P (r T H ) in the remainder of this section. Since the distribution of the positive labels is highly imbalanced, we leverage the idea of focal loss <ref type="bibr">(Lin et al., 2017)</ref> for balancing the logits of the positive classes. We have our loss function as:</p><formula xml:id="formula_12">LRE = r i ?P T (1?P (ri)) ? log(P (ri))+log(P (rT H )) (11)</formula><p>where ? is a hyper-parameter. Our loss is designed to focus more on the low-confidence classes. If P (r i ) is low, the loss contribution from the relevant class will be higher, which enables a better optimization for long-tail classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Knowledge Distillation for Distant Supervison</head><p>In this section, we describe how we utilize the distantly supervised data in a more effective manner. The distantly supervised data included in the Do-cRed dataset <ref type="bibr" target="#b24">(Yao et al., 2019)</ref> was obtained by performing entity linking on the Wikidata Knowledge Base <ref type="bibr" target="#b19">(Vrande?i? and Kr?tzsch, 2014)</ref> and the Wikipedia data dump. It is shown that pre-training from the distantly supervised data is beneficial for document-level relation extraction . However, prior work only adapts the distantly supervised data in a naive manner. The key challenge for the distant supervision adaptation is to overcome the differences between probability distributions of the distantly supervised data and the human annotated data. We compare two strategies for adapting the distantly supervised data.</p><p>Naive Adaptation Adopting from , this method first pretrains the model with the distantly supervised data with the relation extraction loss L RE (Eqn. 11), and then the model is fine-tuned on the human-annotated data with the same objective. We denote this method as Naive Adaptation (NA).</p><p>Knowledge Distillation To further utilize the annotated data, we use a relation classification model trained on the human-annotated data (#Train in Table 1) as the teacher model. The teacher model is used to generate soft labels on the distantly supervised data. Specifically, the distantly supervised data is fed into the teacher model and the predicted logits will be the soft labels used for training the student model. The student model has the same configuration as the teacher model, but is trained with two signals simultaneously. The first signal is the supervision from the hard labels of the distantly supervised data and the second is from the predicted soft labels. We denote the loss computed on the hard labels as L RE and the knowledge distillation loss computed on the soft labels as L KD . We use mean squared error (MSE) as the knowledge distillation loss function:</p><formula xml:id="formula_13">L KD = M SE(l (s,o) S , l (s,o) T )<label>(12)</label></formula><p>where l (s,o) S denotes the predicted logits of the student model and l (s,o) T is the prediction of the teacher model. The student model is further fine-tuned with human-annotated data (#Train in <ref type="table">Table 1</ref>) after it has been pre-trained on the distantly supervised data. The overall loss of pre-training with distantly supervised data is computed as: </p><formula xml:id="formula_14">L = L KD + L RE<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Statistics</head><p>We evaluated our model on two document-level relation extraction datasets -the DocRED <ref type="bibr" target="#b24">(Yao et al., 2019)</ref> benchmark and the HacRED dataset <ref type="bibr" target="#b1">(Cheng et al., 2021)</ref>. DocRED is a crowd-sourced largescale document-level relation extraction dataset. It contains 3,053/1,000/1,000 instances for training, validation, and test, respectively. HacRED is a Chinese relation extraction dataset that focuses on the hard cases of relation extraction. It contains 27 hard relations and is split into 6,231/1,500/1,500 instances for training, validation, and test. However, the test set of HacRED is not released yet. In this paper, we only provide the results on its dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We implemented our model with the PyTorch version of the Huggingface Transformers <ref type="bibr" target="#b22">(Wolf et al., 2020)</ref>. For experiments on DocRED, we experimented with Roberta-large  and Bert-base <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> as our document encoder respectively. For experiments on HacRED, we use XLM-R base <ref type="bibr" target="#b3">(Conneau et al., 2020)</ref> as the document encoder. AdamW <ref type="bibr" target="#b9">(Loshchilov and Hutter, 2019</ref>) is used as the optimizer. At the knowledge distillation stage, we trained the model with the learning rate set to 1e-5 for 2 epochs. Warmup is applied on the initial 6% steps. The dropout rates between transformer layers are set to 0.1 and the maximum gradient norm is clipped at 1.0. During the fine-tuning stage, the learning rate is set to 1e-6 and we train the model for 10 epochs. We performed grid search for ? ? [0, 0.5, 1.0, 1.5, 2.0] and set it to 0.5. Our model is trained on a single  NVIDIA V100 GPU with 32 GB memory. The main evaluation metrics are Ign_F1 and F1 score following <ref type="bibr" target="#b24">Yao et al. (2019)</ref>, where Ign_F1 refers to the F1 score that ignores the triples that appear in the annotated training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Compared Methods</head><p>We denote Bert-base and Bert-large encoders as Bb and B-l. The Roberta-large model is denoted as Rb-l. We compare our model with the state-of-theart systems on the DocRED leaderboard as well as strong baselines by our own implementation. They are the following models: <ref type="bibr" target="#b20">Wang et al. (2019)</ref> has proposed to fine-tune BERT for document-level RE with a two-step process <ref type="figure">(Two-stage-B-b</ref>). The Bert model needs to classify whether the two entities have relation and then classify their relation if the first step is positive. The Coref-Rb-l <ref type="bibr" target="#b25">(Ye et al., 2020)</ref> uses a co-reference module to aggregate the mention representations of the same entity. The SSAN  model utilizes cooccurrence information between entity mentions, leverages distantly supervised data for pretraining, and achieves the state of the art on the DocRED leaderboard. Since their best model SSAN-Adapt is equivalent to naive adaptation in our work, we denote it as SSAN-NA-Rb-l in our experiments. The GAIN <ref type="bibr" target="#b27">(Zeng et al., 2020)</ref> model adds a graph neural network on top of a pre-trained language model, constructs a document-level graph for each example, and uses the graphical structure to extract relations. SIRE <ref type="bibr" target="#b26">(Zeng et al., 2021)</ref> uses two encoders for different types of relation -a sentencelevel encoder to extract intra-sentence relations and a document encoder to extract inter-sentence relations. ATLOP <ref type="bibr" target="#b31">(Zhou et al., 2021)</ref> is purely based on the transformer architecture and a novel adaptive thresholding loss to deal with the multi-label problem for DocRE. Besides, it also fuses the contextual information with the aggregated attention weights for each entity. The DocuNet <ref type="bibr" target="#b28">(Zhang et al., 2021)</ref> model treats the relation extraction task in a similar way as semantic segmentation in computer vision. We also conducted an experiment that pretrained the ATLOP-Rb-l model with distantly supervised data, as this model is the best model by our reproduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Main Results</head><p>Our main results for the DocRED dataset are shown in <ref type="table" target="#tab_3">Table 2</ref>. Knowledge distillation is able to significantly improve the performance of our model.  The experiment results for the HacRED dataset are shown in <ref type="table" target="#tab_5">Table 3</ref>. The main difference of our method with the ATLOP baseline is the Adaptive Focal Loss and the Axial Attention Module. Our proposed method is able to exceed the ATLOP baseline by 1.12 F1. Besides the performance of the models, it is worth noting that for each method, the absolute performance of HacRED is significantly higher than its performance on DocRED. This is counter-intuitive as HacRED focuses on the hard relations whereas DocRED is more general. This can be caused by the following: 1) The human annotated training instances of the HacRED dataset are significantly more than DocRED, leading to better generalization performance. 2) Even though HacRED claims it focuses on the hard cases for relation extraction, it only has 27 classes, and the relation type distribution within the HacRED dataset is more balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>We first separate our label space into two subsets. The first subset consists of the 10 most frequent labels, accounting for 59.4% of the positive relations in the training data. The second subset is denoted as the long-tail labels, which includes the rest of the 86 relations (the total label space is 97 and there is one T H class). Since our Adaptive Focal loss function is mainly designed for improving the performance on the less frequent classes, we show the ablation study by frequent and long-tail classes in <ref type="table" target="#tab_7">Table 4</ref>. When we change the AFL loss to con-  ventional Adaptive Thresholding Loss <ref type="bibr" target="#b31">(Zhou et al., 2021)</ref>, the overall performance with KD drops by 0.89 F1, and the F1 score for the frequent labels only drops by 0.65. Meanwhile, the long-tail labels' F1 drops by 1.78, which is significantly higher than the drop in overall performance and frequent performance. This indicates that our Adaptive Focal Loss is able to balance the weight of the frequent classes and infrequent classes. The axial attention module is also more beneficial for the long-tail classes than the frequent classes, which shows that our model's performance on the frequent classes is saturated.  We also provide an ablation study on the multihop relations in <ref type="table" target="#tab_9">Table 5</ref>. We use the same evaluation method for multi-hop relations as <ref type="bibr" target="#b27">Zeng et al. (2020)</ref>. This evaluation method ignores all the onehop relation triples. Our axial attention module effectively improves Infer-F1 by 1.67, while its improvement for overall performance is only 0.63.</p><formula xml:id="formula_15">P R Infer-F1 GAIN-B-b</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Comparison of Adaptation Methods</head><p>In this section, we directly compare the knowledge adaptation methods on the development set of DocRED <ref type="table" target="#tab_11">(Table 6</ref>). We mainly compare three methods for adaptation: 1) Naive Adaptation (NA), 2) KD KL knowledge distillation with the KL divergence loss and 3) KD M SE with mean squared error loss. The adaptation performance on the development set is positively correlated with the per-   We first construct the union of our model's predictions and the ground truth triples (without NR label). Then, we categorize the union into four categories: (1) Correct (C), where prediction triples are in the ground truth. (2) Wrong (W), where the predicted head entity and tail entity are in the ground truth but the predicted relation is wrong.</p><p>(3) Missed (MS), where the model predicts no relation for a pair of head entity and tail entity with some relation in the ground truth. (4) More (MR), where the model predicts an extraneous relation for a pair of head entity and tail entity not related in the ground truth. From <ref type="table" target="#tab_12">Table 7</ref>, we observe that the error percentage of the W category is very small. This indicates that for a pair of head entity and tail entity with some relation in the ground truth, and when our model predicts that there is a relation between these two entities, it is able to predict the correct relation rather accurately. However, we observe that most of our errors are under the MR and MS categories, and their counts are about the same. To better understand the performance bottleneck of the document-level RE task, we evaluate our model on a simplified subtask <ref type="table" target="#tab_14">(Table 8</ref>). This subtask is binary classification, i.e., to determine whether two entities are related or not, and it is denoted as Binary Labels. In this subtask, we only care about predicting correctly that there is some relation between a head entity and a tail entity, but not what the exact relation is among the 97 relation classes. The performance on this simplified task is 68.64 F1 score, which is only marginally higher than the original F1 score of 67.12. This may be due to incomplete annotation of the two document-level relation extraction datasets, and we will illustrate this hypothesis in <ref type="figure" target="#fig_0">Figure 2</ref>.   In <ref type="figure" target="#fig_0">Figure 2</ref>, we show an example document from the dev set of DocRED and its predictions. We observe that many triples in the MR category are factually correct. That is, some of the pairs of entities are truly related but are labeled as NR throughout the dataset. For instance, from the ground truth, we can see that Labour Party and Hol are all entities from country Norway. Similarly, Nordland and Vestv?g?y are all in Norway, but their relations with Norway are not present in the ground truth triples. Therefore, when our model predicts these triples, its performance would be unfairly penal-ized during evaluation. This observation indicates that there are some incomplete annotations in the DocRED dataset. However, this is not the focus of this paper and we would like to leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Early works on relation extraction mainly focused on sentence-level RE <ref type="bibr" target="#b30">(Zhang et al., 2017;</ref><ref type="bibr" target="#b0">Baldini Soares et al., 2019;</ref>. However, prior works have shown that a large number of relations can only be extracted from multiple sentences <ref type="bibr" target="#b18">(Verga et al., 2018;</ref><ref type="bibr" target="#b24">Yao et al., 2019;</ref><ref type="bibr" target="#b1">Cheng et al., 2021)</ref>. Various methods have been proposed to tackle document-level relation extraction (DocRE). Graph neural networks (GNNs; <ref type="bibr" target="#b15">Scarselli et al., 2008)</ref> have been widely used for the DocRE task.  used words as nodes and dependency information as edges to construct document-level graphs. This graph will be used to extract features for each entity pair. Later works extended this idea by applying different GNN architectures <ref type="bibr" target="#b12">(Peng et al., 2017;</ref><ref type="bibr" target="#b18">Verga et al., 2018;</ref><ref type="bibr" target="#b2">Christopoulou et al., 2019;</ref><ref type="bibr" target="#b10">Nan et al., 2020;</ref><ref type="bibr" target="#b29">Zhang et al., 2018;</ref><ref type="bibr" target="#b27">Zeng et al., 2020)</ref>. In particular, <ref type="bibr" target="#b10">Nan et al. (2020)</ref> proposed the latent stucture refinement (LSR) model, which used structured attention to induce the document-level graph. <ref type="bibr" target="#b27">Zeng et al. (2020)</ref> constructed the document-level graph by entity-mention nodes and sentence edges. Besides the graph-based methods, transformer-only architectures have also proven to be highly effective for the DocRE task <ref type="bibr">(Tang et al., 2020;</ref><ref type="bibr" target="#b31">Zhou et al., 2021)</ref>. Specifically, <ref type="bibr" target="#b31">Zhou et al. (2021)</ref> proposed adaptive thresholding loss to tackle the multi-label classification problem in DocRE.</p><p>On the other hand, learning from distant supervision is another important problem for relation extraction. <ref type="bibr" target="#b13">Qin et al. (2018)</ref> used generative adversarial training for selecting informative examples and <ref type="bibr" target="#b5">Feng et al. (2018)</ref> used reinforcement learning to achieve the same goal. However, there are no existing works that jointly learn from annotated data and distant data. To this end, this paper is the first to overcome the differences between the human annotated and distantly supervised data. Moreover, this paper also tackles the under-explored class imbalance problem and the two-hop logical reasoning problem with novel solutions to the shortcomings of existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we have proposed a novel framework for document-level relation extraction, based on knowledge distillation, axial attention, and adaptive focal loss. Our proposed method is able to significantly outperform the previous state of the art on the DocRED leaderboard. Besides, we also conducted a thorough ablation study and error analysis to identify the bottleneck of the document-level relation extraction task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Example output of our model on the DocRED dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We denote this method as KD in our main experimental results section. Besides the MSE loss, we also compare different adaptation methods, such as KL-Divergence, in section 3.6.</figDesc><table><row><cell>3 Experiments</cell><cell></cell><cell></cell></row><row><cell>Statistics</cell><cell cols="2">DocRED HacRED</cell></row><row><cell># distant docs</cell><cell>101,873</cell><cell>-</cell></row><row><cell># training docs</cell><cell>3,053</cell><cell>6,231</cell></row><row><cell># dev docs</cell><cell>1,000</cell><cell>1,500</cell></row><row><cell># test docs</cell><cell>1,000</cell><cell>1,500</cell></row><row><cell># relations</cell><cell>97</cell><cell>27</cell></row><row><cell>Avg # entities per doc</cell><cell>19.5</cell><cell>10.8</cell></row><row><cell>Avg # mentions per entity</cell><cell>1.4</cell><cell>1.2</cell></row><row><cell>Avg # relations per doc</cell><cell>12.5</cell><cell>7.4</cell></row><row><cell cols="3">Table 1: Dataset statistics of the DocRED and HacRED</cell></row><row><cell>datasets.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results for the DocRED dataset. The reported metrics are F1 score and Ign_F1. We report the average of five random runs for the development set and the best checkpoint is used for the leaderboard submission for the test results. Results with</figDesc><table /><note>* are obtained by our reproduction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on HacRED dev set. Results with</figDesc><table /><note>* are implemented by us. All experiments used XLM-R-base as the encoder.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Experiment results for frequent and long-tail</cell></row><row><cell>type relations. Frequent types refer to the most popular</cell></row><row><cell>10 relation types, and long-tail relations refer to the rest</cell></row><row><cell>of the 86 relations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Ablation study for the Infer-F1 relation triples on the development set of DocRED.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Development set performance of different</cell></row><row><cell cols="3">knowledge adaptation methods for DocRED.</cell></row><row><cell cols="3">formance of downstream fine-tuning. In the distant</cell></row><row><cell cols="3">adaptation setting, our best method KD M SE is able</cell></row><row><cell cols="3">to outperform NA by 3.07 F1 and KD KL by 0.77</cell></row><row><cell cols="3">F1. Similar performance differences are observed</cell></row><row><cell cols="3">in the continue-trained setting.</cell></row><row><cell cols="3">4 Error Analysis</cell></row><row><cell cols="3">Even though our final model significantly outper-</cell></row><row><cell cols="3">forms the previous state of the art on the Do-</cell></row><row><cell cols="3">cRED leaderboard, the absolute performance of</cell></row><row><cell cols="3">our model still does not match human performance.</cell></row><row><cell cols="3">In this section, we provide a detailed error analysis</cell></row><row><cell cols="3">of our model on the development set of DocRED.</cell></row><row><cell></cell><cell></cell><cell>Ground Truth</cell></row><row><cell>Predictions</cell><cell>r ? R NR</cell><cell>r ? R C: 8,273 (51.4%) W: 242 (1.5%) MS: 3,761 (23.4%) 380,703 NR MR: 3,814 (23.7%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Statistics of our error distribution. The final evaluation score is evaluated on r ? R triples, hence the correct predictions of NR are ignored when calculating the final scores.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Performance breakdown on the DocRED dev set. Eivind Bolle ( 13 October 1923 -10 June 2012 ) was a Norwegian politician for the Labour Party. He was born in Hol. He was elected to the Norwegian Parliament from Nordland in 1973. ... On the local level he was a member of Hol municipality council from 1959 to 1963 , and later in Hol 's successor municipality Vestv?g?y. He served as mayor from 1971 to 1973 , during which term he was also a member of Nordland county council ..." More: (Nordland, country, Norwegian), (Vestv?g?y, country, Norwegian),... Correct: (Labour Party, country, Norwegian), (Hol, country, Norwegian),...</figDesc><table /><note>"</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Refer to https://competitions.codalab. org/competitions/20717, where our model is named KD-Roberta.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>We would like to thank the anonymous reviewers for their insightful feedback and comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">HacRED: A largescale relation extraction dataset toward hard cases in practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhefeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxing</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Connecting the dots: Document-level neural relation extraction with edge-oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mrn: A locally and globally mention-based reasoning network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. 2017. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning from context or names? An empirical study on neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DSGAN: Generative adversarial training for distant supervision relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fang Fang, Shi Wang, and Pengfei Yin. 2020. HIN: hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fine-tune BERT for DocRED with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transformers: State-of-theart natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP: System Demonstrations</title>
		<meeting>EMNLP: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DocRED: a large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coreferential reasoning learning for language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SIRE: Separate intra-and inter-sentential reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Document-level relation extraction as semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/551</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
