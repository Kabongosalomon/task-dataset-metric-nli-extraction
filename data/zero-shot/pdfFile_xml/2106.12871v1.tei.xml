<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DCOM: A DEEP COLUMN MAPPER FOR SEMANTIC DATA TYPE DETECTION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-25">June 25, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhadip</forename><surname>Maji</surname></persName>
							<email>maji.subhadip@optum.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Choudhary</surname></persName>
							<email>sudeep.choudhary@optum.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Data Scientist Optum Global Solutions Bangalore</orgName>
								<address>
									<postCode>560103</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Swapna Sourav Rout Senior Data Scientist Optum Global Solutions Bangalore</orgName>
								<address>
									<postCode>560103</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Data Scientist Optum Global Solutions Bangalore</orgName>
								<address>
									<postCode>560103</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DCOM: A DEEP COLUMN MAPPER FOR SEMANTIC DATA TYPE DETECTION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-25">June 25, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Semantic Data Type Detection ? Machine Learning ? Natural Language Processing ? Semantic Column Tagging ? Sensitive Data Detection ? Column Search</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detection of semantic data types is a very crucial task in data science for automated data cleaning, schema matching, data discovery, semantic data type normalization and sensitive data identification. Existing methods include regular expression-based or dictionary lookup-based methods that are not robust to dirty as well unseen data and are limited to a very less number of semantic data types to predict. Existing Machine Learning methods extract large number of engineered features from data and build logistic regression, random forest or feedforward neural network for this purpose. In this paper, we introduce DCoM, a collection of multi-input NLP-based deep neural networks to detect semantic data types where instead of extracting large number of features from the data, we feed the raw values of columns (or instances) to the model as texts. We train DCoM on 686,765 data columns extracted from VizNet corpus with 78 different semantic data types. DCoM outperforms other contemporary results with a quite significant margin on the same dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Robust data preprocessing pipeline enables organizations to take data driven business decisions. Most unstructured and semi-structured data after initial pre-processing is available in tabular format for further processing. These tabular datasets have to go through a process of data curation and quality check before consumption. Companies apply standard data quality checks/rules on business-critical columns to access and maintain the quality of data. Additionally, some of these datasets may contain sensitive information e.g. Protected Health Information (PHI), Personally Identifiable Information(PII), etc and need to be masked/de-identified i.e. identifying all columns which are Social Security Numbers, First Names, telephone numbers, etc. The first step in this data curation and quality check process is column level semantic tagging/mapping. There have been many attempts to automate this process <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>. Traditionally, Semantic tagging is usually done using handcrafted rule-based systems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref>. In some cases, column descriptions are also available. However, as the data grows exponentially, the cost associated with maintaining rule-based system also increases. The semantic data type tagging is still largely manual where data stewards manually scan through databases and map columns of interest. Most ML based approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref> use corresponding metadata, atomic datatypes, column description along with handcrafted features as input data for training the model.</p><p>We herein, propose DCoM-Deep Column Mapper; a generic collection of Deep learning based Semantic mappers to map the columns to semantic data types. These model take the raw values of the columns (or instances) as inputs considering those as texts and build NLP-based deep learning architectures to predict the semantic type of the columns (or instances). We also extracted 19 engineered features and used those as auxiliary features to DCoM models. We refrained ourselves extracting large number of features, because we wanted to make the DCoM models learn those arXiv:2106.12871v1 <ref type="bibr">[cs.</ref>LG] 24 Jun 2021 features along with some interesting high level features on their own, for better semantic data type detection. As we are exploring NLP in this problem, we make the leverage of using advanced NLP-based deep learning layers or models such as Bi-LSTM, BERT, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Trifacta <ref type="bibr" target="#b15">[16]</ref>, Microsoft Power BI <ref type="bibr" target="#b8">[9]</ref> and Google Data Studio <ref type="bibr" target="#b3">[4]</ref> use some regular expression based patterns for dictionary looks ups of column headers and values to detect a limited number of semantic data types. Venetis et al. <ref type="bibr" target="#b16">[17]</ref> build a database of value-type mappings, then assign semantic types using a maximum likelihood estimator based on column values. Syed et al. <ref type="bibr" target="#b14">[15]</ref> use column values and headers to build a Wikitology query to map columns to semantic classes. Ramnandan et al. <ref type="bibr" target="#b12">[13]</ref> use heuristics to first separate numerical and textual types, then describe those types using the Kolmogorov-Smirnov (K-S) test and Term Frequency-Inverse Document Frequency (TF-IDF), respectively. Pham et al. <ref type="bibr" target="#b10">[11]</ref> use slightly more features, including the Mann-Whitney test for numerical data and Jaccard similarity for textual data, to train logistic regression and random forest models. Goel et al. <ref type="bibr" target="#b2">[3]</ref> use conditional random fields to predict the semantic type of each value within a column, then combine these predictions into a prediction for the whole column. Limaye et al. <ref type="bibr" target="#b7">[8]</ref> use probabilistic graphical models to annotate values with entities, columns with types, and column pairs with relationships. Puranik <ref type="bibr" target="#b11">[12]</ref> proposes a "specialist approach" combining the predictions of regular expressions, dictionaries, and machine learning models. More recently, Yan and He <ref type="bibr" target="#b18">[19]</ref> introduced a system that, given a search keyword and set of positive examples, synthesizes type detection logic from open source GitHub repositories. Hulsebos et al. <ref type="bibr" target="#b5">[6]</ref> built a multi-input deep neural network model for detecting 78 semantic data types, extracting 686, 765 data columns from VizNet <ref type="bibr" target="#b4">[5]</ref> corpus. They extracted a total of 1,588 features for each column to train the model, thus resulting a support-weighted F1 score of 0.89, exceeding that of machine learning baselines, dictionary and regular expression benchmarks, and the consensus of crowdsourced annotations. Zhang et al. <ref type="bibr" target="#b19">[20]</ref> introduced SATO, a hybrid machine learning model to automatically detect the semantic types of columns in tables, exploiting the signals from the context as well as the column values. It combines a deep learning model trained on a large-scale table corpus with topic modeling and structured prediction to achieve supportweighted and macro average F1 scores of 0.925 and 0.735, respectively, exceeding the state-of-the-art performance by a significant margin.</p><p>While in this paper we do not experiment with the context information of the column values, our work is mostly aligned to Hulsebos et al. <ref type="bibr" target="#b5">[6]</ref>. Keeping that in mind, we are planning to research and implement context of the column values in tables in our DCoM models for our future works. The paper is organized as follows: Section 3 describes the data used for DCoM models. In Section 4 we present the details of data preparation and architecture for DCoM models. Section 5 contains the training, evaluation and inference procedures of DCoM models while Section 6 discussed about the results of extensive experiments. In Section 7, we talk about some known limitations from the data as well as application point of view and finally in Section 8, we present the concluding remarks and some future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>We have used the dataset prepared by Hulsebos et al. <ref type="bibr" target="#b5">[6]</ref> and compared our model performance with them considering their results to be baselines. This dataset contains 686,765 instances with 78 unique classes (or semantic data types). It is divided into 60/20/20 training/validation/testing splits. The instance count for classes varies from 584 (affiliate class) to 9088 (type class). The count distribution for classes is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. To provide a more clear picture of the data a sample of the dataset is shown in <ref type="table">Table 1</ref>. state Deletes the property, Lets you edit the value of the property, Script execution will be stopped description <ref type="table">Table 1</ref>: A sample of the dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Method</head><p>Prior approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8]</ref> to semantic type detection trained models, such as logistic regression, decision trees, feedforward neural network <ref type="bibr" target="#b5">[6]</ref>, extracting various features from the data. We, on the other hand, treated the data as natural language (text) and used the data itself as the input to the model. We used very small number of hand-engineered features unlike Hulsebos et al. <ref type="bibr" target="#b5">[6]</ref>. Therefore, our DCoM models have two inputs, the values of the instances as text or natural language and hand-engineered features. We present two types of DCoM models based on the way we feed the text input to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DCoM with Single Sequence Input</head><p>In this subsection, we discuss how the values of each instance can be fed to the model as a single sequence input to the DCoM. Considering the scenario, we can not simply pass the inputs separating values of each input with any separator token. This gives the model wrong information about the sequence of the data resulting faulty training and poor performance on the unseen data. For example, if we consider the last example from <ref type="table">Table  1</ref> and create the input, Deletes the property &lt;SEP&gt; Lets you edit the value of the property &lt;SEP&gt; Script execution will be stopped for the model, the model gets to learn that the value Lets you edit the value of the property has relative position between Deletes the property and Script execution will be stopped, which is quite wrong, because the values in an instance does not have any relative position between them.</p><p>To mitigate the problem we introduce permutations from mathematics. With permutation we order r items from the set of n i items. Here n i items are all the values of an instance i, and r ? [1, n]. Doing so, the above instance can be broken down to multiple subsets. A sample of the subset is shown in the <ref type="table">Table 2</ref>. If we feed all the new instances of the subset to the model, it does not learn any relative positional information of Lets you edit the value of the property with respect to other values in that instance unlike earlier. Therefore, for the value Deletes the property, the model only learns the relative positions of the tokens e.g. Deletes, the, property, etc in a value, but not the relative position of the values in an instance. This helps the model getting the actual information present in the data for accurate prediction on the output. This permutation method also helps in augmenting new instances which helps in training the data hungry deep learning models with enriched data. It is not feasible to generate all the possible permutations before the training because of the huge numbers of subsets. Instead, during training, we sample r between 1 and n for each of the instances and generate one permutation for each instance. More than one instances can be generated, but training the model for multiple epochs will result same for both cases.</p><p>New Instance class Deletes the property description Lets you edit the value of the property description Script execution will be stopped description Deletes the property &lt;SEP&gt; Lets you edit the value of the property description Lets you edit the value of the property &lt;SEP&gt; Script execution will be stopped description Deletes the property &lt;SEP&gt; Lets you edit the value of the property &lt;SEP&gt; Script execution will be stopped description Deletes the property &lt;SEP&gt; Script execution will be stopped &lt;SEP&gt; Lets you edit the value of the property description Lets you edit the value of the property &lt;SEP&gt; Deletes the property &lt;SEP&gt; Script execution will be stopped description <ref type="table">Table 2</ref>: Permutations on the values of a single instance from class description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DCoM with Multiple Sequences Inputs</head><p>Input preparation with this method is straight-forward compared to the earlier method. In this method, we also use permutations to generate new instances, but the value of r is fixed during training and it is used as a hyper-parameter, where r ? [1, inf). Once we decide the value of r, r number of inputs are used as text inputs to the model. Aggregation of embedding vectors of the inputs are performed once they are generated using the shared embedding weights. If r &gt; n, where n is the number of values of an instance, then this scenario can be handled in two ways. The first way is to pad r ? n inputs and aggregate the embedding vectors only for the non-padded inputs. Another way is, while generating new instances use permutation with replacement to always sample r values out of n values. Therefore, padding is not required for the latter method. We tried both the approaches and did not observe any significant difference in the result.</p><p>We used 19 out of 27 global statistical features <ref type="bibr" target="#b5">[6]</ref> as our engineered features for the DCoM models. These features are normalized before feeding to the model. The complete list of these features are shown in <ref type="table">Table 6</ref>. Once text and engineered inputs are prepared, we attach LSTM/Transformer/BERT layers to the text input. The output of the same is aggregated with the engineered inputs. We use some feed-forward layer after that. Finally we use one softmax layer with 78 units to get the probability of each of the classes as output. This is our generalized architecture design of DCoM models. Extensive hyper-parameter tuning is performed to finalize the number of layers, number of units in a layer and many other hyper-parameters in the model. This topic is discussed in detail in section 5. To name the DCoM models, type of the text input and name of the deep learning layers are used as suffixes with the name DCoM, e.g. DCoM model with single instance input and LSTM layers is named as DCoM-Single-LSTM. The architecture design of DCoM models for both single sequence and multiple sequences inputs are shown in the <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training, Evaluation and Inference</head><p>We trained our model on train dataset, validated it on validation dataset and finally reported our results on the test dataset. As class imbalance is present, like Hulsebos et al. <ref type="bibr" target="#b5">[6]</ref>, we also evaluated our model performance using the average F1-score, weighted by the number of columns per class in the test set (i.e., the support).</p><p>The DCoM models are trained in Tensorflow 2 <ref type="bibr" target="#b0">[1]</ref> and the hyper-parameters are tuned using keras-tuner <ref type="bibr" target="#b9">[10]</ref>. For inputs of DCoM models we tried permutations with replacement as well as without replacement, but we did not observe any significant difference in outputs. The value of r for which we observed best performance for DCoM-Multi models is 45. For tokenizing text inputs, we experimented with character based, word based and BERT Wordpiece <ref type="bibr" target="#b17">[18]</ref> tokenizers, and we found out BERT Wordpiece tokenizer stood out to be working better with respect to the other tokenizers because of the obvious reasons stated in the paper <ref type="bibr" target="#b17">[18]</ref>. We compared our result between with and without pre-trained embedding weights. It is observed that, though in the final output there is no considerable difference, training with pre-trained embedding weights take approximately 40% less amount of time to converge. We experimented with the small, base and large variations of several BERT architectures and finalized DistilBERT-base <ref type="bibr" target="#b13">[14]</ref> and Electra-small <ref type="bibr" target="#b1">[2]</ref> based on their performances. We used Bi-directional LSTM layers in all the DCoM-LSTM variants. For Dropout layers we used 0.3 as our dropout rate. We experimented mean, sum, concatenation and weighted sum functions for aggregation, but there was not any significant difference in outputs based on these variations. We used Adam <ref type="bibr" target="#b6">[7]</ref> optimizer with initial learning rate 10 ?4 . Along with this, we implement learning rate reducer with factor 0.5, which reduces the learning rate by 50% if the model performance does not improve on validation dataset after consecutive 5 epochs. Assigning class weights does not have much effect in the test results. As this is a multi-class (78 classes) classification problem, the DCoM models are trained with categorical-crossentropy loss and validated with accuracy and average F1 score metric.</p><p>we tried two approaches for inference on the test dataset. In the first approach we performed single time inference on each of the instances of the test dataset. While doing so, we set r to be the total number of values (n i ) for each instance for DCoM-Single models to perform inference. For DCoM-Multi, as r values if prefixed, some values will be truncated for inputs with total values n &gt; r. The other approach is to generate k (where, k &gt; 1) instances with permutation, sampling r values k times between 1 and n i , where n i is the total number of values for instance i. Therefore, we get k class predictions for each of the instances and finally with majority voting we pick the prediction class for each instance. We used k value to be 10 in our case and it is observed that for both DCoM-Single and DCoM-Multi, we observed 0.2 ? 0.5% improvement in the test average F1 score, but this improvement comes with the price of increased inference time by approximately k times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We compared DCoM against Sherlock and other types of models shown by Hulsebos et al. <ref type="bibr" target="#b5">[6]</ref> assuming those to be our baseline models. <ref type="table">Table 3</ref> presents the comparison of results on test dataset for each of the models. Columns Engineered Features and k are specific to DCoM models. Engineered Features says whether the 19 engineered features are used with the text inputs while training the model. k column tells the number of times inference is performed on a single test instance. It is discussed in detail in section 5. Runtime column is the average time in seconds needed to extract features and generate a prediction for a single sample, and Size column reports the space required by the trained models in MB. From the table it is seen that several DCoM models outperform Sherlock <ref type="bibr" target="#b5">[6]</ref> in both F1 score and inference runtime with significant margins.</p><p>Hulsebos et al. <ref type="bibr" target="#b5">[6]</ref> extracted various features e.g. global statistics, character-level distributions, word embeddings, paragraph vectors from the data and used those features to fit a feedforward neural network model. On the other hand, we treated the data as texts and feed those to more advanced NLP-based models. This allows the DCoM models to extract and learn more useful features that Hulsebos et al. <ref type="bibr" target="#b5">[6]</ref> were unable to extract using hand-engineering. Along with this, hand-engineering takes considerable amount of time to calculate the features which increases the inference time of Sherlock <ref type="bibr" target="#b5">[6]</ref> by 3 to 20 times with respect to DCoM models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Performance for Individual Types</head><p>Following Hulseboset al. <ref type="bibr" target="#b5">[6]</ref>, we also prepared --0.320 33.74 ? 0.86 - <ref type="table">Table 3</ref>: Support-weighted F1 score, runtime at prediction, and size of DCoM and other benchmarks help DCoM models to extract distinctive features with respect to the other classes. To understand types for which DCoM-Single-DistilBERT performs poorly, we include incorrectly predicted examples for the lowest precision type (Rank) and the lowest recall type (Ranking) in <ref type="table">Table 5</ref>. From the table it is observed that purely numerical values or values appearing in multiple classes, cause a challenge for the DCoM models to correctly classify the semantic type of the data. This issue is discussed in detail in section 7.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Known Limitations</head><p>The major limitation of the process is with the pre-processed data prepared by Hulsebos et al. <ref type="bibr" target="#b5">[6]</ref> used to train DCoM models. Same values of instances are present in multiple classes, thus resulting strong class overlapping among some classes. It makes the models confusing during training to learn distinctive features for proper classification. Therefore, for some classes in the test dataset, the models perform very poorly. <ref type="table">Table 7</ref> presents a sample of examples of class overlapping in the data prepared by Hulsebos et al. <ref type="bibr" target="#b5">[6]</ref>. It is to be noted that the proportion of this class overlap is considerably large in numbers with respect to the total number of instances. Therefore, it affects the training as well as the model performance on the test dataset by a significant margin. Besides the class overlap, faulty or wrong values are present in some classes.</p><p>From the application point of view of semantic data type detection models, identifying and preparing a good dataset for training is very challenging as well as time consuming. Non standardized column names are a major challenge</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Values</head><p>Class <ref type="table">F, M  gender  M  sex  F  sex  M  gender  M, F  sex  1</ref> <ref type="bibr">, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14 ranking 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14 position 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13</ref>  <ref type="table">Table 7</ref>: A sample example of class overlapping of the data used to build up models large organizations face in doing semantic detection. For example, social security numbers can be called as ssn, ssn_id, soc_sec_bnr etc. Non standardized data columns also pose major challenge in information ambiguity. For example, Gender column can have values Male, Female, unknown, or 0,1,2. Some organization can have mixed attributes which poses a major challenge in data security. For example, use of PII data like Social security Number in Customer ID columns. Some other challenges like corrupt/missing metadata also exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>DCoM presents a novel permutation based method by which the the instances can be fed to the deep learning models directly as natural language. This takes the leverage of using more advanced NLP-based layers/models unlike feedforward neural network in Sherlock <ref type="bibr" target="#b5">[6]</ref>. The permutation based method also helps in generating large number of new instances from the existing ones, helping DCoM models to boost its performance effectively, thus outperforming Sherlock <ref type="bibr" target="#b5">[6]</ref> and other recent works by quite a significant margin in both F1 score as well as inference time. We also present an ensembling approach during inference time which improves the test average F1 score by 0.2 ? 0.5%..</p><p>As mentioned earlier, the next steps of it to introduce context of the column values to DCoM models while predicting the semantic types in relational tables.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Count of instances for each of the 78 classes in the dataset<ref type="bibr" target="#b5">[6]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture diagram of (a) DCoM-Single and (b) DCoM-Multi models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4</head><label>4</label><figDesc>which displays the top and bottom five types, as measured by the F1 score achieved by the best performing DCoM model, DCoM-Single-DistilBERT for single inference (k = 1) for that type. . High performing classes such as grades, industry, ISBN, etc. contain a finite set of valid values which</figDesc><table><row><cell>Method</cell><cell>Engineered Features</cell><cell cols="4">k F1 Score Runtime (s) Size (MB)</cell></row><row><cell>DCoM-Single-LSTM</cell><cell>Yes</cell><cell>1</cell><cell>0.895</cell><cell>0.019 ? 0.01</cell><cell>112.1</cell></row><row><cell>DCoM-Single-LSTM</cell><cell>Yes</cell><cell>10</cell><cell>0.898</cell><cell>0.152 ? 0.01</cell><cell>112.1</cell></row><row><cell>DCoM-Single-LSTM</cell><cell>No</cell><cell>1</cell><cell>0.871</cell><cell>0.018 ? 0.01</cell><cell>97.8</cell></row><row><cell>DCoM-Single-LSTM</cell><cell>No</cell><cell>10</cell><cell>0.877</cell><cell>0.141 ? 0.01</cell><cell>97.8</cell></row><row><cell>DCoM-Multi-LSTM</cell><cell>Yes</cell><cell>1</cell><cell>0.878</cell><cell>0.046 ? 0.01</cell><cell>4.7</cell></row><row><cell>DCoM-Multi-LSTM</cell><cell>Yes</cell><cell>10</cell><cell>0.881</cell><cell>0.416 ? 0.01</cell><cell>4.7</cell></row><row><cell>DCoM-Multi-LSTM</cell><cell>No</cell><cell>1</cell><cell>0.869</cell><cell>0.044 ? 0.01</cell><cell>4.6</cell></row><row><cell>DCoM-Multi-LSTM</cell><cell>No</cell><cell>10</cell><cell>0.871</cell><cell>0.401 ? 0.01</cell><cell>4.6</cell></row><row><cell>DCoM-Single-DistilBERT</cell><cell>Yes</cell><cell>1</cell><cell>0.922</cell><cell>0.162 ? 0.01</cell><cell>268.2</cell></row><row><cell>DCoM-Single-DistilBERT</cell><cell>Yes</cell><cell>10</cell><cell>0.925</cell><cell>1.552 ? 0.01</cell><cell>268.2</cell></row><row><cell>DCoM-Single-DistilBERT</cell><cell>No</cell><cell>1</cell><cell>0.901</cell><cell>0.158 ? 0.01</cell><cell>202.3</cell></row><row><cell>DCoM-Single-DistilBERT</cell><cell>No</cell><cell>10</cell><cell>0.904</cell><cell>1.492 ? 0.01</cell><cell>202.3</cell></row><row><cell>DCoM-Single-Electra</cell><cell>Yes</cell><cell>1</cell><cell>0.907</cell><cell>0.093 ? 0.01</cell><cell>53.1</cell></row><row><cell>DCoM-Single-Electra</cell><cell>Yes</cell><cell>10</cell><cell>0.909</cell><cell>0.894 ? 0.01</cell><cell>53.1</cell></row><row><cell>DCoM-Single-Electra</cell><cell>No</cell><cell>1</cell><cell>0.890</cell><cell>0.092 ? 0.01</cell><cell>45.7</cell></row><row><cell>DCoM-Single-Electra</cell><cell>No</cell><cell>10</cell><cell>0.892</cell><cell>0.887 ? 0.01</cell><cell>45.7</cell></row><row><cell>Sherlock[6]</cell><cell>-</cell><cell>-</cell><cell>0.890</cell><cell>0.42 ? 0.01</cell><cell>6.2</cell></row><row><cell>Decision tree[6]</cell><cell>-</cell><cell>-</cell><cell>0.760</cell><cell>0.26 ? 0.01</cell><cell>59.1</cell></row><row><cell>Random Forest[6]</cell><cell>-</cell><cell>-</cell><cell>0.840</cell><cell>0.26 ? 0.01</cell><cell>760.4</cell></row><row><cell>Dictionary[6]</cell><cell>-</cell><cell>-</cell><cell>0.160</cell><cell>0.01 ? 0.03</cell><cell>0.5</cell></row><row><cell>Regular expression[6]</cell><cell>-</cell><cell>-</cell><cell>0.040</cell><cell>0.01 ? 0.03</cell><cell>0.01</cell></row><row><cell>Consensus[6]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Top five and bottom five types by F1 score on the test dataset for DCoM-Single-DistilBERT model variant.6.2 Feature ImportanceTo calculate the feature importance of 19 engineered features used in trained DCoM-Single-DistilBERT model, we extracted the learned feature weight matrix, W from the dense layer used after the engineered features input, where W ? R 19?D . Here D is the number of units used in the dense layer. W contains the weights/contribution of each of the 19 features on each of the D units, thus forming a 19 ? D array, which contains both positive and negative values based on the direction of contribution. We take absolute value of all the elements of W , as we are only interested on the amount of contribution of each of the engineered features, not the direction. After that, we take</figDesc><table><row><cell>Examples</cell><cell cols="2">True Type Predicted Type</cell></row><row><cell>1, 5, 4, 3, 2</cell><cell>Day</cell><cell>Rank</cell></row><row><cell cols="2">1, 2, 3, 4, 5, 6, 7, 8, 9, 10 Region</cell><cell>Rank</cell></row><row><cell>1, 2, 3</cell><cell>Position</cell><cell>Rank</cell></row><row><cell></cell><cell>(a) Low Precision</cell><cell></cell></row><row><cell>Examples</cell><cell cols="2">True Type Predicted Type</cell></row><row><cell>41, 2, 36</cell><cell>Ranking</cell><cell>Rank</cell></row><row><cell>0, 2, 4</cell><cell>Ranking</cell><cell>Plays</cell></row><row><cell cols="2">1, 2, 3, 4, 5, 6, 7, 8, 9, 10 Ranking</cell><cell>Rank</cell></row><row><cell></cell><cell>(a) Low Recall</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :Table 6 :</head><label>56</label><figDesc>Examples of low precision and low recall types on the test dataset for DCoM-Single-DistilBERT model variant. Feature importance of 19 engineered features used in DCoM-Single-DistilBERT model. mean across the 19 features that results a 19-dimensional array. We normalized the array by dividing the maximum value of the array to each of the elements. Table 6 enlists the importance of the engineered features in decreasing order. From the table it is seen that Std of # of Numeric Characters in Cells, Std of # of Alphabetic Characters in Cells and Entropy are the top 3 important features, where Mean # Alphabetic Characters in Cells, Median Length of Values and Mode Length of Values are the least 3 important features for the DCoM-Single-DistilBERT model.</figDesc><table><row><cell cols="2">Rank Aux Features</cell><cell>Score</cell></row><row><cell>1</cell><cell>Std of # of Numeric Characters in Cells</cell><cell>1.00</cell></row><row><cell>2</cell><cell>Std of # of Alphabetic Characters in Cells</cell><cell>0.63</cell></row><row><cell>3</cell><cell>Entropy</cell><cell>0.63</cell></row><row><cell>4</cell><cell>Std of # of Special Characters in Cells</cell><cell>0.62</cell></row><row><cell>5</cell><cell>Std of # of Words in Cells</cell><cell>0.53</cell></row><row><cell>6</cell><cell>Mean # Words in Cells</cell><cell>0.50</cell></row><row><cell>7</cell><cell>Mean # of Numeric Characters in Cells</cell><cell>0.48</cell></row><row><cell>8</cell><cell>Minimum Value Length</cell><cell>0.45</cell></row><row><cell>9</cell><cell>Kurtosis of the Length of Values</cell><cell>0.41</cell></row><row><cell>10</cell><cell>Mean # Special Characters in Cells</cell><cell>0.39</cell></row><row><cell>11</cell><cell>Number of Values</cell><cell>0.34</cell></row><row><cell>12</cell><cell cols="2">Fraction of Cells with Alphabetical Characters 0.33</cell></row><row><cell>13</cell><cell>Fraction of Cells with Numeric Characters</cell><cell>0.31</cell></row><row><cell>14</cell><cell>Sum of the Length of Values</cell><cell>0.31</cell></row><row><cell>15</cell><cell>Maximum Value Length</cell><cell>0.30</cell></row><row><cell>16</cell><cell>Skewness of the Length of Values</cell><cell>0.28</cell></row><row><cell>17</cell><cell>Mean # Alphabetic Characters in Cells</cell><cell>0.28</cell></row><row><cell>18</cell><cell>Median Length of Values</cell><cell>0.27</cell></row><row><cell>19</cell><cell>Mode Length of Values</cell><cell>0.20</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Optum Global Solutions, India for sponsoring this work. We are thankful to the author of paper Sherlock <ref type="bibr" target="#b5">[6]</ref>, Madelon Hulsebos and others for helping us arranging the dataset. We would also like to thank Vineet Shukla and Ravi Kumar Gottumukkala for their valuable inputs which help us enhancing the quality of this paper significantly.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<title level="m">Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<idno>arXiv: 2003.10555</idno>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting Structure within Data for Accurate Labeling Using Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Lerman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. &amp;quot;k</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In: Proceedings of the 14th International Conference on Artificial Intelligence</title>
		<imprint/>
	</monogr>
	<note>ICAI. 2012</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Google Data Studio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://datastudio.google.com" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VizNet: Towards a large-scale visualization learning and benchmarking repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Human Factors in Computing Systems (CHI)</title>
		<meeting>the 2019 Conference on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sherlock: A Deep Learning Approach to Semantic Data Type Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelon</forename><surname>Hulsebos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>arXiv: 1412.6980</idno>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
	<note>In: arXiv e-prints. cs.LG</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Annotating and Searching Web Tables Using Entities, Types and Relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girija</forename><surname>Limaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="DOI">10.14778/1920841.1921005</idno>
		<ptr target="https://doi.org/10.14778/1920841.1921005" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow. 3.1-2</title>
		<meeting>VLDB Endow. 3.1-2</meeting>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="page" from="1338" to="1347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Interactive Data Visualization BI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bi</surname></persName>
		</author>
		<ptr target="https://powerbi.microsoft.com" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malley</surname></persName>
		</author>
		<ptr target="https://github.com/keras-team/keras-tuner.2019" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semantic Labeling: A Domain-Independent Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Pham</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46523-4_27</idno>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="446" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A specialist approach for the classification of column data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Puranik</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Assigning Semantic Labels to Data Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ramnandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web. Latest Advances and New Domains. Ed. by Fabien Gandon et</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="978" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<idno>arXiv: 1910.01108</idno>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploiting a Web of Semantic Data for Interpreting Tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zareen</forename><surname>Syed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Data Wrangling Tools &amp; Software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trifacta</surname></persName>
		</author>
		<ptr target="https://trifacta.com" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recovering Semantics of Tables on the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Venetis</surname></persName>
		</author>
		<idno type="DOI">10.14778/2002938.2002939</idno>
		<idno>DOI: 10 .14778 / 2002938 . 2002939</idno>
		<ptr target="https://doi.org/10.14778/" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2150" to="8097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<idno>arXiv: 1609.08144</idno>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Synthesizing Type-Detection Logic for Rich Semantic Data Types Using Open-Source Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeye</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1145/3183713.3196888</idno>
		<ptr target="https://doi.org/10.1145/3183713.3196888" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Management of Data. SIGMOD &apos;18</title>
		<meeting>the 2018 International Conference on Management of Data. SIGMOD &apos;18<address><addrLine>Houston, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sato: Contextual Semantic Type Detection in Tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06311</idno>
		<idno>arXiv: 1911.06311</idno>
		<imprint>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
	<note>In: arXiv e-prints. cs.DB</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
